{"created":"2023-12-11 18:59:58","title":"CAD: Photorealistic 3D Generation via Adversarial Distillation","abstract":"The increased demand for 3D data in AR/VR, robotics and gaming applications, gave rise to powerful generative pipelines capable of synthesizing high-quality 3D objects. Most of these models rely on the Score Distillation Sampling (SDS) algorithm to optimize a 3D representation such that the rendered image maintains a high likelihood as evaluated by a pre-trained diffusion model. However, finding a correct mode in the high-dimensional distribution produced by the diffusion model is challenging and often leads to issues such as over-saturation, over-smoothing, and Janus-like artifacts. In this paper, we propose a novel learning paradigm for 3D synthesis that utilizes pre-trained diffusion models. Instead of focusing on mode-seeking, our method directly models the distribution discrepancy between multi-view renderings and diffusion priors in an adversarial manner, which unlocks the generation of high-fidelity and photorealistic 3D content, conditioned on a single image and prompt. Moreover, by harnessing the latent space of GANs and expressive diffusion model priors, our method facilitates a wide variety of 3D applications including single-view reconstruction, high diversity generation and continuous 3D interpolation in the open domain. The experiments demonstrate the superiority of our pipeline compared to previous works in terms of generation quality and diversity.","sentences":["The increased demand for 3D data in AR/VR, robotics and gaming applications, gave rise to powerful generative pipelines capable of synthesizing high-quality 3D objects.","Most of these models rely on the Score Distillation Sampling (SDS) algorithm to optimize a 3D representation such that the rendered image maintains a high likelihood as evaluated by a pre-trained diffusion model.","However, finding a correct mode in the high-dimensional distribution produced by the diffusion model is challenging and often leads to issues such as over-saturation, over-smoothing, and Janus-like artifacts.","In this paper, we propose a novel learning paradigm for 3D synthesis that utilizes pre-trained diffusion models.","Instead of focusing on mode-seeking, our method directly models the distribution discrepancy between multi-view renderings and diffusion priors in an adversarial manner, which unlocks the generation of high-fidelity and photorealistic 3D content, conditioned on a single image and prompt.","Moreover, by harnessing the latent space of GANs and expressive diffusion model priors, our method facilitates a wide variety of 3D applications including single-view reconstruction, high diversity generation and continuous 3D interpolation in the open domain.","The experiments demonstrate the superiority of our pipeline compared to previous works in terms of generation quality and diversity."],"url":"http://arxiv.org/abs/2312.06663v1"}
{"created":"2023-12-11 18:59:57","title":"Photorealistic Video Generation with Diffusion Models","abstract":"We present W.A.L.T, a transformer-based approach for photorealistic video generation via diffusion modeling. Our approach has two key design decisions. First, we use a causal encoder to jointly compress images and videos within a unified latent space, enabling training and generation across modalities. Second, for memory and training efficiency, we use a window attention architecture tailored for joint spatial and spatiotemporal generative modeling. Taken together these design decisions enable us to achieve state-of-the-art performance on established video (UCF-101 and Kinetics-600) and image (ImageNet) generation benchmarks without using classifier free guidance. Finally, we also train a cascade of three models for the task of text-to-video generation consisting of a base latent video diffusion model, and two video super-resolution diffusion models to generate videos of $512 \\times 896$ resolution at $8$ frames per second.","sentences":["We present W.A.L.T, a transformer-based approach for photorealistic video generation via diffusion modeling.","Our approach has two key design decisions.","First, we use a causal encoder to jointly compress images and videos within a unified latent space, enabling training and generation across modalities.","Second, for memory and training efficiency, we use a window attention architecture tailored for joint spatial and spatiotemporal generative modeling.","Taken together these design decisions enable us to achieve state-of-the-art performance on established video (UCF-101 and Kinetics-600) and image (ImageNet) generation benchmarks without using classifier free guidance.","Finally, we also train a cascade of three models for the task of text-to-video generation consisting of a base latent video diffusion model, and two video super-resolution diffusion models to generate videos of $512 \\times 896$ resolution at $8$ frames per second."],"url":"http://arxiv.org/abs/2312.06662v1"}
{"created":"2023-12-11 18:59:55","title":"UpFusion: Novel View Diffusion from Unposed Sparse View Observations","abstract":"We propose UpFusion, a system that can perform novel view synthesis and infer 3D representations for an object given a sparse set of reference images without corresponding pose information. Current sparse-view 3D inference methods typically rely on camera poses to geometrically aggregate information from input views, but are not robust in-the-wild when such information is unavailable/inaccurate. In contrast, UpFusion sidesteps this requirement by learning to implicitly leverage the available images as context in a conditional generative model for synthesizing novel views. We incorporate two complementary forms of conditioning into diffusion models for leveraging the input views: a) via inferring query-view aligned features using a scene-level transformer, b) via intermediate attentional layers that can directly observe the input image tokens. We show that this mechanism allows generating high-fidelity novel views while improving the synthesis quality given additional (unposed) images. We evaluate our approach on the Co3Dv2 and Google Scanned Objects datasets and demonstrate the benefits of our method over pose-reliant sparse-view methods as well as single-view methods that cannot leverage additional views. Finally, we also show that our learned model can generalize beyond the training categories and even allow reconstruction from self-captured images of generic objects in-the-wild.","sentences":["We propose UpFusion, a system that can perform novel view synthesis and infer 3D representations for an object given a sparse set of reference images without corresponding pose information.","Current sparse-view 3D inference methods typically rely on camera poses to geometrically aggregate information from input views, but are not robust in-the-wild when such information is unavailable/inaccurate.","In contrast, UpFusion sidesteps this requirement by learning to implicitly leverage the available images as context in a conditional generative model for synthesizing novel views.","We incorporate two complementary forms of conditioning into diffusion models for leveraging the input views: a) via inferring query-view aligned features using a scene-level transformer, b) via intermediate attentional layers that can directly observe the input image tokens.","We show that this mechanism allows generating high-fidelity novel views while improving the synthesis quality given additional (unposed) images.","We evaluate our approach on the Co3Dv2 and Google Scanned Objects datasets and demonstrate the benefits of our method over pose-reliant sparse-view methods as well as single-view methods that cannot leverage additional views.","Finally, we also show that our learned model can generalize beyond the training categories and even allow reconstruction from self-captured images of generic objects in-the-wild."],"url":"http://arxiv.org/abs/2312.06661v1"}
{"created":"2023-12-11 18:59:52","title":"EdgeSAM: Prompt-In-the-Loop Distillation for On-Device Deployment of SAM","abstract":"This paper presents EdgeSAM, an accelerated variant of the Segment Anything Model (SAM), optimized for efficient execution on edge devices with minimal compromise in performance. Our approach involves distilling the original ViT-based SAM image encoder into a purely CNN-based architecture, better suited for edge devices. We carefully benchmark various distillation strategies and demonstrate that task-agnostic encoder distillation fails to capture the full knowledge embodied in SAM. To overcome this bottleneck, we include both the prompt encoder and mask decoder in the distillation process, with box and point prompts in the loop, so that the distilled model can accurately capture the intricate dynamics between user input and mask generation. To mitigate dataset bias issues stemming from point prompt distillation, we incorporate a lightweight module within the encoder. EdgeSAM achieves a 40-fold speed increase compared to the original SAM, and it also outperforms MobileSAM, being 14 times as fast when deployed on edge devices while enhancing the mIoUs on COCO and LVIS by 2.3 and 3.2 respectively. It is also the first SAM variant that can run at over 30 FPS on an iPhone 14. Code and models are available at https://github.com/chongzhou96/EdgeSAM.","sentences":["This paper presents EdgeSAM, an accelerated variant of the Segment Anything Model (SAM), optimized for efficient execution on edge devices with minimal compromise in performance.","Our approach involves distilling the original ViT-based SAM image encoder into a purely CNN-based architecture, better suited for edge devices.","We carefully benchmark various distillation strategies and demonstrate that task-agnostic encoder distillation fails to capture the full knowledge embodied in SAM.","To overcome this bottleneck, we include both the prompt encoder and mask decoder in the distillation process, with box and point prompts in the loop, so that the distilled model can accurately capture the intricate dynamics between user input and mask generation.","To mitigate dataset bias issues stemming from point prompt distillation, we incorporate a lightweight module within the encoder.","EdgeSAM achieves a 40-fold speed increase compared to the original SAM, and it also outperforms MobileSAM, being 14 times as fast when deployed on edge devices while enhancing the mIoUs on COCO and LVIS by 2.3 and 3.2 respectively.","It is also the first SAM variant that can run at over 30 FPS on an iPhone 14.","Code and models are available at https://github.com/chongzhou96/EdgeSAM."],"url":"http://arxiv.org/abs/2312.06660v1"}
{"created":"2023-12-11 18:59:35","title":"Mean estimation in the add-remove model of differential privacy","abstract":"Differential privacy is often studied under two different models of neighboring datasets: the add-remove model and the swap model. While the swap model is used extensively in the academic literature, many practical libraries use the more conservative add-remove model. However, analysis under the add-remove model can be cumbersome, and obtaining results with tight constants requires some additional work. Here, we study the problem of one-dimensional mean estimation under the add-remove model of differential privacy. We propose a new algorithm and show that it is min-max optimal, that it has the correct constant in the leading term of the mean squared error, and that this constant is the same as the optimal algorithm in the swap model. Our results show that, for mean estimation, the add-remove and swap model give nearly identical error even though the add-remove model cannot treat the size of the dataset as public information. In addition, we demonstrate empirically that our proposed algorithm yields a factor of two improvement in mean squared error over algorithms often used in practice.","sentences":["Differential privacy is often studied under two different models of neighboring datasets: the add-remove model and the swap model.","While the swap model is used extensively in the academic literature, many practical libraries use the more conservative add-remove model.","However, analysis under the add-remove model can be cumbersome, and obtaining results with tight constants requires some additional work.","Here, we study the problem of one-dimensional mean estimation under the add-remove model of differential privacy.","We propose a new algorithm and show that it is min-max optimal, that it has the correct constant in the leading term of the mean squared error, and that this constant is the same as the optimal algorithm in the swap model.","Our results show that, for mean estimation, the add-remove and swap model give nearly identical error even though the add-remove model cannot treat the size of the dataset as public information.","In addition, we demonstrate empirically that our proposed algorithm yields a factor of two improvement in mean squared error over algorithms often used in practice."],"url":"http://arxiv.org/abs/2312.06658v1"}
{"created":"2023-12-11 18:59:31","title":"Learning Naturally Aggregated Appearance for Efficient 3D Editing","abstract":"Neural radiance fields, which represent a 3D scene as a color field and a density field, have demonstrated great progress in novel view synthesis yet are unfavorable for editing due to the implicitness. In view of such a deficiency, we propose to replace the color field with an explicit 2D appearance aggregation, also called canonical image, with which users can easily customize their 3D editing via 2D image processing. To avoid the distortion effect and facilitate convenient editing, we complement the canonical image with a projection field that maps 3D points onto 2D pixels for texture lookup. This field is carefully initialized with a pseudo canonical camera model and optimized with offset regularity to ensure naturalness of the aggregated appearance. Extensive experimental results on three datasets suggest that our representation, dubbed AGAP, well supports various ways of 3D editing (e.g., stylization, interactive drawing, and content extraction) with no need of re-optimization for each case, demonstrating its generalizability and efficiency. Project page is available at https://felixcheng97.github.io/AGAP/.","sentences":["Neural radiance fields, which represent a 3D scene as a color field and a density field, have demonstrated great progress in novel view synthesis yet are unfavorable for editing due to the implicitness.","In view of such a deficiency, we propose to replace the color field with an explicit 2D appearance aggregation, also called canonical image, with which users can easily customize their 3D editing via 2D image processing.","To avoid the distortion effect and facilitate convenient editing, we complement the canonical image with a projection field that maps 3D points onto 2D pixels for texture lookup.","This field is carefully initialized with a pseudo canonical camera model and optimized with offset regularity to ensure naturalness of the aggregated appearance.","Extensive experimental results on three datasets suggest that our representation, dubbed AGAP, well supports various ways of 3D editing (e.g., stylization, interactive drawing, and content extraction) with no need of re-optimization for each case, demonstrating its generalizability and efficiency.","Project page is available at https://felixcheng97.github.io/AGAP/."],"url":"http://arxiv.org/abs/2312.06657v1"}
{"created":"2023-12-11 18:59:18","title":"Sherpa3D: Boosting High-Fidelity Text-to-3D Generation via Coarse 3D Prior","abstract":"Recently, 3D content creation from text prompts has demonstrated remarkable progress by utilizing 2D and 3D diffusion models. While 3D diffusion models ensure great multi-view consistency, their ability to generate high-quality and diverse 3D assets is hindered by the limited 3D data. In contrast, 2D diffusion models find a distillation approach that achieves excellent generalization and rich details without any 3D data. However, 2D lifting methods suffer from inherent view-agnostic ambiguity thereby leading to serious multi-face Janus issues, where text prompts fail to provide sufficient guidance to learn coherent 3D results. Instead of retraining a costly viewpoint-aware model, we study how to fully exploit easily accessible coarse 3D knowledge to enhance the prompts and guide 2D lifting optimization for refinement. In this paper, we propose Sherpa3D, a new text-to-3D framework that achieves high-fidelity, generalizability, and geometric consistency simultaneously. Specifically, we design a pair of guiding strategies derived from the coarse 3D prior generated by the 3D diffusion model: a structural guidance for geometric fidelity and a semantic guidance for 3D coherence. Employing the two types of guidance, the 2D diffusion model enriches the 3D content with diversified and high-quality results. Extensive experiments show the superiority of our Sherpa3D over the state-of-the-art text-to-3D methods in terms of quality and 3D consistency.","sentences":["Recently, 3D content creation from text prompts has demonstrated remarkable progress by utilizing 2D and 3D diffusion models.","While 3D diffusion models ensure great multi-view consistency, their ability to generate high-quality and diverse 3D assets is hindered by the limited 3D data.","In contrast, 2D diffusion models find a distillation approach that achieves excellent generalization and rich details without any 3D data.","However, 2D lifting methods suffer from inherent view-agnostic ambiguity thereby leading to serious multi-face Janus issues, where text prompts fail to provide sufficient guidance to learn coherent 3D results.","Instead of retraining a costly viewpoint-aware model, we study how to fully exploit easily accessible coarse 3D knowledge to enhance the prompts and guide 2D lifting optimization for refinement.","In this paper, we propose Sherpa3D, a new text-to-3D framework that achieves high-fidelity, generalizability, and geometric consistency simultaneously.","Specifically, we design a pair of guiding strategies derived from the coarse 3D prior generated by the 3D diffusion model: a structural guidance for geometric fidelity and a semantic guidance for 3D coherence.","Employing the two types of guidance, the 2D diffusion model enriches the 3D content with diversified and high-quality results.","Extensive experiments show the superiority of our Sherpa3D over the state-of-the-art text-to-3D methods in terms of quality and 3D consistency."],"url":"http://arxiv.org/abs/2312.06655v1"}
{"created":"2023-12-11 18:59:13","title":"LightSim: Neural Lighting Simulation for Urban Scenes","abstract":"Different outdoor illumination conditions drastically alter the appearance of urban scenes, and they can harm the performance of image-based robot perception systems if not seen during training. Camera simulation provides a cost-effective solution to create a large dataset of images captured under different lighting conditions. Towards this goal, we propose LightSim, a neural lighting camera simulation system that enables diverse, realistic, and controllable data generation. LightSim automatically builds lighting-aware digital twins at scale from collected raw sensor data and decomposes the scene into dynamic actors and static background with accurate geometry, appearance, and estimated scene lighting. These digital twins enable actor insertion, modification, removal, and rendering from a new viewpoint, all in a lighting-aware manner. LightSim then combines physically-based and learnable deferred rendering to perform realistic relighting of modified scenes, such as altering the sun location and modifying the shadows or changing the sun brightness, producing spatially- and temporally-consistent camera videos. Our experiments show that LightSim generates more realistic relighting results than prior work. Importantly, training perception models on data generated by LightSim can significantly improve their performance.","sentences":["Different outdoor illumination conditions drastically alter the appearance of urban scenes, and they can harm the performance of image-based robot perception systems if not seen during training.","Camera simulation provides a cost-effective solution to create a large dataset of images captured under different lighting conditions.","Towards this goal, we propose LightSim, a neural lighting camera simulation system that enables diverse, realistic, and controllable data generation.","LightSim automatically builds lighting-aware digital twins at scale from collected raw sensor data and decomposes the scene into dynamic actors and static background with accurate geometry, appearance, and estimated scene lighting.","These digital twins enable actor insertion, modification, removal, and rendering from a new viewpoint, all in a lighting-aware manner.","LightSim then combines physically-based and learnable deferred rendering to perform realistic relighting of modified scenes, such as altering the sun location and modifying the shadows or changing the sun brightness, producing spatially- and temporally-consistent camera videos.","Our experiments show that LightSim generates more realistic relighting results than prior work.","Importantly, training perception models on data generated by LightSim can significantly improve their performance."],"url":"http://arxiv.org/abs/2312.06654v1"}
{"created":"2023-12-11 18:59:12","title":"Adaptive Human Trajectory Prediction via Latent Corridors","abstract":"Human trajectory prediction is typically posed as a zero-shot generalization problem: a predictor is learnt on a dataset of human motion in training scenes, and then deployed on unseen test scenes. While this paradigm has yielded tremendous progress, it fundamentally assumes that trends in human behavior within the deployment scene are constant over time. As such, current prediction models are unable to adapt to scene-specific transient human behaviors, such as crowds temporarily gathering to see buskers, pedestrians hurrying through the rain and avoiding puddles, or a protest breaking out. We formalize the problem of scene-specific adaptive trajectory prediction and propose a new adaptation approach inspired by prompt tuning called latent corridors. By augmenting the input of any pre-trained human trajectory predictor with learnable image prompts, the predictor can improve in the deployment scene by inferring trends from extremely small amounts of new data (e.g., 2 humans observed for 30 seconds). With less than 0.1% additional model parameters, we see up to 23.9% ADE improvement in MOTSynth simulated data and 16.4% ADE in MOT and Wildtrack real pedestrian data. Qualitatively, we observe that latent corridors imbue predictors with an awareness of scene geometry and scene-specific human behaviors that non-adaptive predictors struggle to capture. The project website can be found at https://neerja.me/atp_latent_corridors/.","sentences":["Human trajectory prediction is typically posed as a zero-shot generalization problem: a predictor is learnt on a dataset of human motion in training scenes, and then deployed on unseen test scenes.","While this paradigm has yielded tremendous progress, it fundamentally assumes that trends in human behavior within the deployment scene are constant over time.","As such, current prediction models are unable to adapt to scene-specific transient human behaviors, such as crowds temporarily gathering to see buskers, pedestrians hurrying through the rain and avoiding puddles, or a protest breaking out.","We formalize the problem of scene-specific adaptive trajectory prediction and propose a new adaptation approach inspired by prompt tuning called latent corridors.","By augmenting the input of any pre-trained human trajectory predictor with learnable image prompts, the predictor can improve in the deployment scene by inferring trends from extremely small amounts of new data (e.g., 2 humans observed for 30 seconds).","With less than 0.1% additional model parameters, we see up to 23.9% ADE improvement in MOTSynth simulated data and 16.4% ADE in MOT and Wildtrack real pedestrian data.","Qualitatively, we observe that latent corridors imbue predictors with an awareness of scene geometry and scene-specific human behaviors that non-adaptive predictors struggle to capture.","The project website can be found at https://neerja.me/atp_latent_corridors/."],"url":"http://arxiv.org/abs/2312.06653v1"}
{"created":"2023-12-11 18:59:09","title":"Building Domain-Specific LLMs Faithful To The Islamic Worldview: Mirage or Technical Possibility?","abstract":"Large Language Models (LLMs) have demonstrated remarkable performance across numerous natural language understanding use cases. However, this impressive performance comes with inherent limitations, such as the tendency to perpetuate stereotypical biases or fabricate non-existent facts. In the context of Islam and its representation, accurate and factual representation of its beliefs and teachings rooted in the Quran and Sunnah is key. This work focuses on the challenge of building domain-specific LLMs faithful to the Islamic worldview and proposes ways to build and evaluate such systems. Firstly, we define this open-ended goal as a technical problem and propose various solutions. Subsequently, we critically examine known challenges inherent to each approach and highlight evaluation methodologies that can be used to assess such systems. This work highlights the need for high-quality datasets, evaluations, and interdisciplinary work blending machine learning with Islamic scholarship.","sentences":["Large Language Models (LLMs) have demonstrated remarkable performance across numerous natural language understanding use cases.","However, this impressive performance comes with inherent limitations, such as the tendency to perpetuate stereotypical biases or fabricate non-existent facts.","In the context of Islam and its representation, accurate and factual representation of its beliefs and teachings rooted in the Quran and Sunnah is key.","This work focuses on the challenge of building domain-specific LLMs faithful to the Islamic worldview and proposes ways to build and evaluate such systems.","Firstly, we define this open-ended goal as a technical problem and propose various solutions.","Subsequently, we critically examine known challenges inherent to each approach and highlight evaluation methodologies that can be used to assess such systems.","This work highlights the need for high-quality datasets, evaluations, and interdisciplinary work blending machine learning with Islamic scholarship."],"url":"http://arxiv.org/abs/2312.06652v1"}
{"created":"2023-12-11 18:58:38","title":"Nuvo: Neural UV Mapping for Unruly 3D Representations","abstract":"Existing UV mapping algorithms are designed to operate on well-behaved meshes, instead of the geometry representations produced by state-of-the-art 3D reconstruction and generation techniques. As such, applying these methods to the volume densities recovered by neural radiance fields and related techniques (or meshes triangulated from such fields) results in texture atlases that are too fragmented to be useful for tasks such as view synthesis or appearance editing. We present a UV mapping method designed to operate on geometry produced by 3D reconstruction and generation techniques. Instead of computing a mapping defined on a mesh's vertices, our method Nuvo uses a neural field to represent a continuous UV mapping, and optimizes it to be a valid and well-behaved mapping for just the set of visible points, i.e. only points that affect the scene's appearance. We show that our model is robust to the challenges posed by ill-behaved geometry, and that it produces editable UV mappings that can represent detailed appearance.","sentences":["Existing UV mapping algorithms are designed to operate on well-behaved meshes, instead of the geometry representations produced by state-of-the-art 3D reconstruction and generation techniques.","As such, applying these methods to the volume densities recovered by neural radiance fields and related techniques (or meshes triangulated from such fields) results in texture atlases that are too fragmented to be useful for tasks such as view synthesis or appearance editing.","We present a UV mapping method designed to operate on geometry produced by 3D reconstruction and generation techniques.","Instead of computing a mapping defined on a mesh's vertices, our method Nuvo uses a neural field to represent a continuous UV mapping, and optimizes it to be a valid and well-behaved mapping for just the set of visible points, i.e. only points that affect the scene's appearance.","We show that our model is robust to the challenges posed by ill-behaved geometry, and that it produces editable UV mappings that can represent detailed appearance."],"url":"http://arxiv.org/abs/2312.05283v1"}
{"created":"2023-12-11 18:57:35","title":"4M: Massively Multimodal Masked Modeling","abstract":"Current machine learning models for vision are often highly specialized and limited to a single modality and task. In contrast, recent large language models exhibit a wide range of capabilities, hinting at a possibility for similarly versatile models in computer vision. In this paper, we take a step in this direction and propose a multimodal training scheme called 4M. It consists of training a single unified Transformer encoder-decoder using a masked modeling objective across a wide range of input/output modalities - including text, images, geometric, and semantic modalities, as well as neural network feature maps. 4M achieves scalability by unifying the representation space of all modalities through mapping them into discrete tokens and performing multimodal masked modeling on a small randomized subset of tokens.   4M leads to models that exhibit several key capabilities: (1) they can perform a diverse set of vision tasks out of the box, (2) they excel when fine-tuned for unseen downstream tasks or new input modalities, and (3) they can function as a generative model that can be conditioned on arbitrary modalities, enabling a wide variety of expressive multimodal editing capabilities with remarkable flexibility.   Through experimental analyses, we demonstrate the potential of 4M for training versatile and scalable foundation models for vision tasks, setting the stage for further exploration in multimodal learning for vision and other domains.","sentences":["Current machine learning models for vision are often highly specialized and limited to a single modality and task.","In contrast, recent large language models exhibit a wide range of capabilities, hinting at a possibility for similarly versatile models in computer vision.","In this paper, we take a step in this direction and propose a multimodal training scheme called 4M. It consists of training a single unified Transformer encoder-decoder using a masked modeling objective across a wide range of input/output modalities - including text, images, geometric, and semantic modalities, as well as neural network feature maps.","4M achieves scalability by unifying the representation space of all modalities through mapping them into discrete tokens and performing multimodal masked modeling on a small randomized subset of tokens.   ","4M leads to models that exhibit several key capabilities: (1) they can perform a diverse set of vision tasks out of the box, (2) they excel when fine-tuned for unseen downstream tasks or new input modalities, and (3) they can function as a generative model that can be conditioned on arbitrary modalities, enabling a wide variety of expressive multimodal editing capabilities with remarkable flexibility.   ","Through experimental analyses, we demonstrate the potential of 4M for training versatile and scalable foundation models for vision tasks, setting the stage for further exploration in multimodal learning for vision and other domains."],"url":"http://arxiv.org/abs/2312.06647v1"}
{"created":"2023-12-11 18:57:35","title":"Dense X Retrieval: What Retrieval Granularity Should We Use?","abstract":"Dense retrieval has become a prominent method to obtain relevant context or world knowledge in open-domain NLP tasks. When we use a learned dense retriever on a retrieval corpus at inference time, an often-overlooked design choice is the retrieval unit in which the corpus is indexed, e.g. document, passage, or sentence. We discover that the retrieval unit choice significantly impacts the performance of both retrieval and downstream tasks. Distinct from the typical approach of using passages or sentences, we introduce a novel retrieval unit, proposition, for dense retrieval. Propositions are defined as atomic expressions within text, each encapsulating a distinct factoid and presented in a concise, self-contained natural language format. We conduct an empirical comparison of different retrieval granularity. Our results reveal that proposition-based retrieval significantly outperforms traditional passage or sentence-based methods in dense retrieval. Moreover, retrieval by proposition also enhances the performance of downstream QA tasks, since the retrieved texts are more condensed with question-relevant information, reducing the need for lengthy input tokens and minimizing the inclusion of extraneous, irrelevant information.","sentences":["Dense retrieval has become a prominent method to obtain relevant context or world knowledge in open-domain NLP tasks.","When we use a learned dense retriever on a retrieval corpus at inference time, an often-overlooked design choice is the retrieval unit in which the corpus is indexed, e.g. document, passage, or sentence.","We discover that the retrieval unit choice significantly impacts the performance of both retrieval and downstream tasks.","Distinct from the typical approach of using passages or sentences, we introduce a novel retrieval unit, proposition, for dense retrieval.","Propositions are defined as atomic expressions within text, each encapsulating a distinct factoid and presented in a concise, self-contained natural language format.","We conduct an empirical comparison of different retrieval granularity.","Our results reveal that proposition-based retrieval significantly outperforms traditional passage or sentence-based methods in dense retrieval.","Moreover, retrieval by proposition also enhances the performance of downstream QA tasks, since the retrieved texts are more condensed with question-relevant information, reducing the need for lengthy input tokens and minimizing the inclusion of extraneous, irrelevant information."],"url":"http://arxiv.org/abs/2312.06648v1"}
{"created":"2023-12-11 18:57:20","title":"Computational Copyright: Towards A Royalty Model for AI Music Generation Platforms","abstract":"The advancement of generative AI has given rise to pressing copyright challenges, particularly in music industry. This paper focuses on the economic aspects of these challenges, emphasizing that the economic impact constitutes a central issue in the copyright arena. The complexity of the black-box generative AI technologies not only suggests but necessitates algorithmic solutions. However, such solutions have been largely missing, leading to regulatory challenges in this landscape. We aim to bridge the gap in current approaches by proposing potential royalty models for revenue sharing on AI music generation platforms. Our methodology involves a detailed analysis of existing royalty models in platforms like Spotify and YouTube, and adapting these to the unique context of AI-generated music. A significant challenge we address is the attribution of AI-generated music to influential copyrighted content in the training data. To this end, we present algorithmic solutions employing data attribution techniques. Our experimental results verify the effectiveness of these solutions. This research represents a pioneering effort in integrating technical advancements with economic and legal considerations in the field of generative AI, offering a computational copyright solution for the challenges posed by the opaque nature of AI technologies.","sentences":["The advancement of generative AI has given rise to pressing copyright challenges, particularly in music industry.","This paper focuses on the economic aspects of these challenges, emphasizing that the economic impact constitutes a central issue in the copyright arena.","The complexity of the black-box generative AI technologies not only suggests but necessitates algorithmic solutions.","However, such solutions have been largely missing, leading to regulatory challenges in this landscape.","We aim to bridge the gap in current approaches by proposing potential royalty models for revenue sharing on AI music generation platforms.","Our methodology involves a detailed analysis of existing royalty models in platforms like Spotify and YouTube, and adapting these to the unique context of AI-generated music.","A significant challenge we address is the attribution of AI-generated music to influential copyrighted content in the training data.","To this end, we present algorithmic solutions employing data attribution techniques.","Our experimental results verify the effectiveness of these solutions.","This research represents a pioneering effort in integrating technical advancements with economic and legal considerations in the field of generative AI, offering a computational copyright solution for the challenges posed by the opaque nature of AI technologies."],"url":"http://arxiv.org/abs/2312.06646v1"}
{"created":"2023-12-11 18:57:05","title":"Beyond Classification: Definition and Density-based Estimation of Calibration in Object Detection","abstract":"Despite their impressive predictive performance in various computer vision tasks, deep neural networks (DNNs) tend to make overly confident predictions, which hinders their widespread use in safety-critical applications. While there have been recent attempts to calibrate DNNs, most of these efforts have primarily been focused on classification tasks, thus neglecting DNN-based object detectors. Although several recent works addressed calibration for object detection and proposed differentiable penalties, none of them are consistent estimators of established concepts in calibration. In this work, we tackle the challenge of defining and estimating calibration error specifically for this task. In particular, we adapt the definition of classification calibration error to handle the nuances associated with object detection, and predictions in structured output spaces more generally. Furthermore, we propose a consistent and differentiable estimator of the detection calibration error, utilizing kernel density estimation. Our experiments demonstrate the effectiveness of our estimator against competing train-time and post-hoc calibration methods, while maintaining similar detection performance.","sentences":["Despite their impressive predictive performance in various computer vision tasks, deep neural networks (DNNs) tend to make overly confident predictions, which hinders their widespread use in safety-critical applications.","While there have been recent attempts to calibrate DNNs, most of these efforts have primarily been focused on classification tasks, thus neglecting DNN-based object detectors.","Although several recent works addressed calibration for object detection and proposed differentiable penalties, none of them are consistent estimators of established concepts in calibration.","In this work, we tackle the challenge of defining and estimating calibration error specifically for this task.","In particular, we adapt the definition of classification calibration error to handle the nuances associated with object detection, and predictions in structured output spaces more generally.","Furthermore, we propose a consistent and differentiable estimator of the detection calibration error, utilizing kernel density estimation.","Our experiments demonstrate the effectiveness of our estimator against competing train-time and post-hoc calibration methods, while maintaining similar detection performance."],"url":"http://arxiv.org/abs/2312.06645v1"}
{"created":"2023-12-11 18:56:37","title":"AnyHome: Open-Vocabulary Generation of Structured and Textured 3D Homes","abstract":"We introduce AnyHome, a framework that translates open-vocabulary descriptions, ranging from simple labels to elaborate paragraphs, into well-structured and textured 3D indoor scenes at a house-scale. Inspired by cognition theories, AnyHome employs an amodal structured representation to capture 3D spatial cues from textual narratives and then uses egocentric inpainting to enrich these scenes. To this end, we begin by using specially designed template prompts for Large Language Models (LLMs), which enable precise control over the textual input. We then utilize intermediate representations to maintain the spatial structure's consistency, ensuring that the 3D scenes align closely with the textual description. Then, we apply a Score Distillation Sampling process to refine the placement of objects. Lastly, an egocentric inpainting process is incorporated to enhance the realism and appearance of the scenes. AnyHome stands out due to its hierarchical structured representation combined with the versatility of open-vocabulary text interpretation. This allows for extensive customization of indoor scenes at various levels of granularity. We demonstrate that AnyHome can reliably generate a range of diverse indoor scenes, characterized by their detailed spatial structures and textures, all corresponding to the free-form textual inputs.","sentences":["We introduce AnyHome, a framework that translates open-vocabulary descriptions, ranging from simple labels to elaborate paragraphs, into well-structured and textured 3D indoor scenes at a house-scale.","Inspired by cognition theories, AnyHome employs an amodal structured representation to capture 3D spatial cues from textual narratives and then uses egocentric inpainting to enrich these scenes.","To this end, we begin by using specially designed template prompts for Large Language Models (LLMs), which enable precise control over the textual input.","We then utilize intermediate representations to maintain the spatial structure's consistency, ensuring that the 3D scenes align closely with the textual description.","Then, we apply a Score Distillation Sampling process to refine the placement of objects.","Lastly, an egocentric inpainting process is incorporated to enhance the realism and appearance of the scenes.","AnyHome stands out due to its hierarchical structured representation combined with the versatility of open-vocabulary text interpretation.","This allows for extensive customization of indoor scenes at various levels of granularity.","We demonstrate that AnyHome can reliably generate a range of diverse indoor scenes, characterized by their detailed spatial structures and textures, all corresponding to the free-form textual inputs."],"url":"http://arxiv.org/abs/2312.06644v1"}
{"created":"2023-12-11 18:56:03","title":"Gaze Detection and Analysis for Initiating Joint Activity in Industrial Human-Robot Collaboration","abstract":"Collaborative robots (cobots) are widely used in industrial applications, yet extensive research is still needed to enhance human-robot collaborations and operator experience. A potential approach to improve the collaboration experience involves adapting cobot behavior based on natural cues from the operator. Inspired by the literature on human-human interactions, we conducted a wizard-of-oz study to examine whether a gaze towards the cobot can serve as a trigger for initiating joint activities in collaborative sessions. In this study, 37 participants engaged in an assembly task while their gaze behavior was analyzed. We employ a gaze-based attention recognition model to identify when the participants look at the cobot. Our results indicate that in most cases (84.88\\%), the joint activity is preceded by a gaze towards the cobot. Furthermore, during the entire assembly cycle, the participants tend to look at the cobot around the time of the joint activity. To the best of our knowledge, this is the first study to analyze the natural gaze behavior of participants working on a joint activity with a robot during a collaborative assembly task.","sentences":["Collaborative robots (cobots) are widely used in industrial applications, yet extensive research is still needed to enhance human-robot collaborations and operator experience.","A potential approach to improve the collaboration experience involves adapting cobot behavior based on natural cues from the operator.","Inspired by the literature on human-human interactions, we conducted a wizard-of-oz study to examine whether a gaze towards the cobot can serve as a trigger for initiating joint activities in collaborative sessions.","In this study, 37 participants engaged in an assembly task while their gaze behavior was analyzed.","We employ a gaze-based attention recognition model to identify when the participants look at the cobot.","Our results indicate that in most cases (84.88\\%), the joint activity is preceded by a gaze towards the cobot.","Furthermore, during the entire assembly cycle, the participants tend to look at the cobot around the time of the joint activity.","To the best of our knowledge, this is the first study to analyze the natural gaze behavior of participants working on a joint activity with a robot during a collaborative assembly task."],"url":"http://arxiv.org/abs/2312.06643v1"}
{"created":"2023-12-11 18:55:29","title":"CorresNeRF: Image Correspondence Priors for Neural Radiance Fields","abstract":"Neural Radiance Fields (NeRFs) have achieved impressive results in novel view synthesis and surface reconstruction tasks. However, their performance suffers under challenging scenarios with sparse input views. We present CorresNeRF, a novel method that leverages image correspondence priors computed by off-the-shelf methods to supervise NeRF training. We design adaptive processes for augmentation and filtering to generate dense and high-quality correspondences. The correspondences are then used to regularize NeRF training via the correspondence pixel reprojection and depth loss terms. We evaluate our methods on novel view synthesis and surface reconstruction tasks with density-based and SDF-based NeRF models on different datasets. Our method outperforms previous methods in both photometric and geometric metrics. We show that this simple yet effective technique of using correspondence priors can be applied as a plug-and-play module across different NeRF variants. The project page is at https://yxlao.github.io/corres-nerf.","sentences":["Neural Radiance Fields (NeRFs) have achieved impressive results in novel view synthesis and surface reconstruction tasks.","However, their performance suffers under challenging scenarios with sparse input views.","We present CorresNeRF, a novel method that leverages image correspondence priors computed by off-the-shelf methods to supervise NeRF training.","We design adaptive processes for augmentation and filtering to generate dense and high-quality correspondences.","The correspondences are then used to regularize NeRF training via the correspondence pixel reprojection and depth loss terms.","We evaluate our methods on novel view synthesis and surface reconstruction tasks with density-based and SDF-based NeRF models on different datasets.","Our method outperforms previous methods in both photometric and geometric metrics.","We show that this simple yet effective technique of using correspondence priors can be applied as a plug-and-play module across different NeRF variants.","The project page is at https://yxlao.github.io/corres-nerf."],"url":"http://arxiv.org/abs/2312.06642v1"}
{"created":"2023-12-11 18:55:03","title":"Online Decision Making with History-Average Dependent Costs (Extended)","abstract":"In many online sequential decision-making scenarios, a learner's choices affect not just their current costs but also the future ones. In this work, we look at one particular case of such a situation where the costs depend on the time average of past decisions over a history horizon. We first recast this problem with history dependent costs as a problem of decision making under stage-wise constraints. To tackle this, we then propose the novel Follow-The-Adaptively-Regularized-Leader (FTARL) algorithm. Our innovative algorithm incorporates adaptive regularizers that depend explicitly on past decisions, allowing us to enforce stage-wise constraints while simultaneously enabling us to establish tight regret bounds. We also discuss the implications of the length of history horizon on design of no-regret algorithms for our problem and present impossibility results when it is the full learning horizon.","sentences":["In many online sequential decision-making scenarios, a learner's choices affect not just their current costs but also the future ones.","In this work, we look at one particular case of such a situation where the costs depend on the time average of past decisions over a history horizon.","We first recast this problem with history dependent costs as a problem of decision making under stage-wise constraints.","To tackle this, we then propose the novel Follow-The-Adaptively-Regularized-Leader (FTARL) algorithm.","Our innovative algorithm incorporates adaptive regularizers that depend explicitly on past decisions, allowing us to enforce stage-wise constraints while simultaneously enabling us to establish tight regret bounds.","We also discuss the implications of the length of history horizon on design of no-regret algorithms for our problem and present impossibility results when it is the full learning horizon."],"url":"http://arxiv.org/abs/2312.06641v1"}
{"created":"2023-12-11 18:54:52","title":"Upscale-A-Video: Temporal-Consistent Diffusion Model for Real-World Video Super-Resolution","abstract":"Text-based diffusion models have exhibited remarkable success in generation and editing, showing great promise for enhancing visual content with their generative prior. However, applying these models to video super-resolution remains challenging due to the high demands for output fidelity and temporal consistency, which is complicated by the inherent randomness in diffusion models. Our study introduces Upscale-A-Video, a text-guided latent diffusion framework for video upscaling. This framework ensures temporal coherence through two key mechanisms: locally, it integrates temporal layers into U-Net and VAE-Decoder, maintaining consistency within short sequences; globally, without training, a flow-guided recurrent latent propagation module is introduced to enhance overall video stability by propagating and fusing latent across the entire sequences. Thanks to the diffusion paradigm, our model also offers greater flexibility by allowing text prompts to guide texture creation and adjustable noise levels to balance restoration and generation, enabling a trade-off between fidelity and quality. Extensive experiments show that Upscale-A-Video surpasses existing methods in both synthetic and real-world benchmarks, as well as in AI-generated videos, showcasing impressive visual realism and temporal consistency.","sentences":["Text-based diffusion models have exhibited remarkable success in generation and editing, showing great promise for enhancing visual content with their generative prior.","However, applying these models to video super-resolution remains challenging due to the high demands for output fidelity and temporal consistency, which is complicated by the inherent randomness in diffusion models.","Our study introduces Upscale-A-Video, a text-guided latent diffusion framework for video upscaling.","This framework ensures temporal coherence through two key mechanisms: locally, it integrates temporal layers into U-Net and VAE-Decoder, maintaining consistency within short sequences; globally, without training, a flow-guided recurrent latent propagation module is introduced to enhance overall video stability by propagating and fusing latent across the entire sequences.","Thanks to the diffusion paradigm, our model also offers greater flexibility by allowing text prompts to guide texture creation and adjustable noise levels to balance restoration and generation, enabling a trade-off between fidelity and quality.","Extensive experiments show that Upscale-A-Video surpasses existing methods in both synthetic and real-world benchmarks, as well as in AI-generated videos, showcasing impressive visual realism and temporal consistency."],"url":"http://arxiv.org/abs/2312.06640v1"}
{"created":"2023-12-11 18:54:42","title":"Harmonic Mobile Manipulation","abstract":"Recent advancements in robotics have enabled robots to navigate complex scenes or manipulate diverse objects independently. However, robots are still impotent in many household tasks requiring coordinated behaviors such as opening doors. The factorization of navigation and manipulation, while effective for some tasks, fails in scenarios requiring coordinated actions. To address this challenge, we introduce, HarmonicMM, an end-to-end learning method that optimizes both navigation and manipulation, showing notable improvement over existing techniques in everyday tasks. This approach is validated in simulated and real-world environments and adapts to novel unseen settings without additional tuning. Our contributions include a new benchmark for mobile manipulation and the successful deployment in a real unseen apartment, demonstrating the potential for practical indoor robot deployment in daily life. More results are on our project site: https://rchalyang.github.io/HarmonicMM/","sentences":["Recent advancements in robotics have enabled robots to navigate complex scenes or manipulate diverse objects independently.","However, robots are still impotent in many household tasks requiring coordinated behaviors such as opening doors.","The factorization of navigation and manipulation, while effective for some tasks, fails in scenarios requiring coordinated actions.","To address this challenge, we introduce, HarmonicMM, an end-to-end learning method that optimizes both navigation and manipulation, showing notable improvement over existing techniques in everyday tasks.","This approach is validated in simulated and real-world environments and adapts to novel unseen settings without additional tuning.","Our contributions include a new benchmark for mobile manipulation and the successful deployment in a real unseen apartment, demonstrating the potential for practical indoor robot deployment in daily life.","More results are on our project site: https://rchalyang.github.io/HarmonicMM/"],"url":"http://arxiv.org/abs/2312.06639v1"}
{"created":"2023-12-11 18:54:26","title":"SurvBeNIM: The Beran-Based Neural Importance Model for Explaining the Survival Models","abstract":"A new method called the Survival Beran-based Neural Importance Model (SurvBeNIM) is proposed. It aims to explain predictions of machine learning survival models, which are in the form of survival or cumulative hazard functions. The main idea behind SurvBeNIM is to extend the Beran estimator by incorporating the importance functions into its kernels and by implementing these importance functions as a set of neural networks which are jointly trained in an end-to-end manner. Two strategies of using and training the whole neural network implementing SurvBeNIM are proposed. The first one explains a single instance, and the neural network is trained for each explained instance. According to the second strategy, the neural network only learns once on all instances from the dataset and on all generated instances. Then the neural network is used to explain any instance in a dataset domain. Various numerical experiments compare the method with different existing explanation methods. A code implementing the proposed method is publicly available.","sentences":["A new method called the Survival Beran-based Neural Importance Model (SurvBeNIM) is proposed.","It aims to explain predictions of machine learning survival models, which are in the form of survival or cumulative hazard functions.","The main idea behind SurvBeNIM is to extend the Beran estimator by incorporating the importance functions into its kernels and by implementing these importance functions as a set of neural networks which are jointly trained in an end-to-end manner.","Two strategies of using and training the whole neural network implementing SurvBeNIM are proposed.","The first one explains a single instance, and the neural network is trained for each explained instance.","According to the second strategy, the neural network only learns once on all instances from the dataset and on all generated instances.","Then the neural network is used to explain any instance in a dataset domain.","Various numerical experiments compare the method with different existing explanation methods.","A code implementing the proposed method is publicly available."],"url":"http://arxiv.org/abs/2312.06638v1"}
{"created":"2023-12-11 18:51:59","title":"Gated Linear Attention Transformers with Hardware-Efficient Training","abstract":"Transformers with linear attention allow for efficient parallel training but can simultaneously be formulated as an RNN with 2D (matrix-valued) hidden states, thus enjoying linear (with respect to output length) inference complexity. Recent works such as RetNet (Sun et al., 2023) and TransNormerLLM (Qin et al., 2023a) observe that adding a global decay term to the additive RNN update rule greatly improves performance, sometimes outperforming standard Transformers with softmax attention when trained at scale. In this work we show that adding a data-dependent gating mechanism further improves performance. We derive a parallel form of this gated linear attention layer that enables efficient training. However, a straightforward, numerically stable implementation of this parallel form requires generalized matrix multiplications in log-space for numerical stability, and thus cannot take advantage of tensor cores on modern GPUs which are optimized for standard matrix multiplications. We develop a hardware-efficient version of the parallel form that can still make use of tensor cores through block-parallel computations over sequence chunks. Experiments on moderate-scale language modeling (340M-parameter models trained on 15B tokens, 1.3B-parameter models trained on 100B tokens) show that gated linear attention (GLA) Transformers perform competitively against a strong LLaMA-architecture Transformer baseline (Touvron et al., 2023) as well as Mamba (Gu & Dao, 2023), a recently introduced state-space model with a data-dependent state transition mechanism. For training speed, our Triton-based implementation performs comparably to CUDA-optimized FlashAttention-2 (Dao, 2023) under the regular 2048 training length setting, while outperforming FlashAttention-2 when training on longer sequences beyond 4096.","sentences":["Transformers with linear attention allow for efficient parallel training but can simultaneously be formulated as an RNN with 2D (matrix-valued) hidden states, thus enjoying linear (with respect to output length) inference complexity.","Recent works such as RetNet (Sun et al., 2023) and TransNormerLLM (Qin et al., 2023a) observe that adding a global decay term to the additive RNN update rule greatly improves performance, sometimes outperforming standard Transformers with softmax attention when trained at scale.","In this work we show that adding a data-dependent gating mechanism further improves performance.","We derive a parallel form of this gated linear attention layer that enables efficient training.","However, a straightforward, numerically stable implementation of this parallel form requires generalized matrix multiplications in log-space for numerical stability, and thus cannot take advantage of tensor cores on modern GPUs which are optimized for standard matrix multiplications.","We develop a hardware-efficient version of the parallel form that can still make use of tensor cores through block-parallel computations over sequence chunks.","Experiments on moderate-scale language modeling (340M-parameter models trained on 15B tokens, 1.3B-parameter models trained on 100B tokens) show that gated linear attention (GLA) Transformers perform competitively against a strong LLaMA-architecture Transformer baseline (Touvron et al., 2023) as well as Mamba (Gu & Dao, 2023), a recently introduced state-space model with a data-dependent state transition mechanism.","For training speed, our Triton-based implementation performs comparably to CUDA-optimized FlashAttention-2 (Dao, 2023) under the regular 2048 training length setting, while outperforming FlashAttention-2","when training on longer sequences beyond 4096."],"url":"http://arxiv.org/abs/2312.06635v1"}
{"created":"2023-12-11 18:51:13","title":"Examining the Effect of Implementation Factors on Deep Learning Reproducibility","abstract":"Reproducing published deep learning papers to validate their conclusions can be difficult due to sources of irreproducibility. We investigate the impact that implementation factors have on the results and how they affect reproducibility of deep learning studies. Three deep learning experiments were ran five times each on 13 different hardware environments and four different software environments. The analysis of the 780 combined results showed that there was a greater than 6% accuracy range on the same deterministic examples introduced from hardware or software environment variations alone. To account for these implementation factors, researchers should run their experiments multiple times in different hardware and software environments to verify their conclusions are not affected.","sentences":["Reproducing published deep learning papers to validate their conclusions can be difficult due to sources of irreproducibility.","We investigate the impact that implementation factors have on the results and how they affect reproducibility of deep learning studies.","Three deep learning experiments were ran five times each on 13 different hardware environments and four different software environments.","The analysis of the 780 combined results showed that there was a greater than 6% accuracy range on the same deterministic examples introduced from hardware or software environment variations alone.","To account for these implementation factors, researchers should run their experiments multiple times in different hardware and software environments to verify their conclusions are not affected."],"url":"http://arxiv.org/abs/2312.06633v1"}
{"created":"2023-12-11 18:50:57","title":"Control Risk for Potential Misuse of Artificial Intelligence in Science","abstract":"The expanding application of Artificial Intelligence (AI) in scientific fields presents unprecedented opportunities for discovery and innovation. However, this growth is not without risks. AI models in science, if misused, can amplify risks like creation of harmful substances, or circumvention of established regulations. In this study, we aim to raise awareness of the dangers of AI misuse in science, and call for responsible AI development and use in this domain. We first itemize the risks posed by AI in scientific contexts, then demonstrate the risks by highlighting real-world examples of misuse in chemical science. These instances underscore the need for effective risk management strategies. In response, we propose a system called SciGuard to control misuse risks for AI models in science. We also propose a red-teaming benchmark SciMT-Safety to assess the safety of different systems. Our proposed SciGuard shows the least harmful impact in the assessment without compromising performance in benign tests. Finally, we highlight the need for a multidisciplinary and collaborative effort to ensure the safe and ethical use of AI models in science. We hope that our study can spark productive discussions on using AI ethically in science among researchers, practitioners, policymakers, and the public, to maximize benefits and minimize the risks of misuse.","sentences":["The expanding application of Artificial Intelligence (AI) in scientific fields presents unprecedented opportunities for discovery and innovation.","However, this growth is not without risks.","AI models in science, if misused, can amplify risks like creation of harmful substances, or circumvention of established regulations.","In this study, we aim to raise awareness of the dangers of AI misuse in science, and call for responsible AI development and use in this domain.","We first itemize the risks posed by AI in scientific contexts, then demonstrate the risks by highlighting real-world examples of misuse in chemical science.","These instances underscore the need for effective risk management strategies.","In response, we propose a system called SciGuard to control misuse risks for AI models in science.","We also propose a red-teaming benchmark SciMT-Safety to assess the safety of different systems.","Our proposed SciGuard shows the least harmful impact in the assessment without compromising performance in benign tests.","Finally, we highlight the need for a multidisciplinary and collaborative effort to ensure the safe and ethical use of AI models in science.","We hope that our study can spark productive discussions on using AI ethically in science among researchers, practitioners, policymakers, and the public, to maximize benefits and minimize the risks of misuse."],"url":"http://arxiv.org/abs/2312.06632v1"}
{"created":"2023-12-11 18:50:09","title":"TMT-VIS: Taxonomy-aware Multi-dataset Joint Training for Video Instance Segmentation","abstract":"Training on large-scale datasets can boost the performance of video instance segmentation while the annotated datasets for VIS are hard to scale up due to the high labor cost. What we possess are numerous isolated filed-specific datasets, thus, it is appealing to jointly train models across the aggregation of datasets to enhance data volume and diversity. However, due to the heterogeneity in category space, as mask precision increases with the data volume, simply utilizing multiple datasets will dilute the attention of models on different taxonomies. Thus, increasing the data scale and enriching taxonomy space while improving classification precision is important. In this work, we analyze that providing extra taxonomy information can help models concentrate on specific taxonomy, and propose our model named Taxonomy-aware Multi-dataset Joint Training for Video Instance Segmentation (TMT-VIS) to address this vital challenge. Specifically, we design a two-stage taxonomy aggregation module that first compiles taxonomy information from input videos and then aggregates these taxonomy priors into instance queries before the transformer decoder. We conduct extensive experimental evaluations on four popular and challenging benchmarks, including YouTube-VIS 2019, YouTube-VIS 2021, OVIS, and UVO. Our model shows significant improvement over the baseline solutions, and sets new state-of-the-art records on all benchmarks. These appealing and encouraging results demonstrate the effectiveness and generality of our approach. The code is available at https://github.com/rkzheng99/TMT-VIS(https://github.com/rkzheng99/TMT-VIS)","sentences":["Training on large-scale datasets can boost the performance of video instance segmentation while the annotated datasets for VIS are hard to scale up due to the high labor cost.","What we possess are numerous isolated filed-specific datasets, thus, it is appealing to jointly train models across the aggregation of datasets to enhance data volume and diversity.","However, due to the heterogeneity in category space, as mask precision increases with the data volume, simply utilizing multiple datasets will dilute the attention of models on different taxonomies.","Thus, increasing the data scale and enriching taxonomy space while improving classification precision is important.","In this work, we analyze that providing extra taxonomy information can help models concentrate on specific taxonomy, and propose our model named Taxonomy-aware Multi-dataset Joint Training for Video Instance Segmentation (TMT-VIS) to address this vital challenge.","Specifically, we design a two-stage taxonomy aggregation module that first compiles taxonomy information from input videos and then aggregates these taxonomy priors into instance queries before the transformer decoder.","We conduct extensive experimental evaluations on four popular and challenging benchmarks, including YouTube-VIS 2019, YouTube-VIS 2021, OVIS, and UVO.","Our model shows significant improvement over the baseline solutions, and sets new state-of-the-art records on all benchmarks.","These appealing and encouraging results demonstrate the effectiveness and generality of our approach.","The code is available at https://github.com/rkzheng99/TMT-VIS(https://github.com/rkzheng99/TMT-VIS)"],"url":"http://arxiv.org/abs/2312.06630v1"}
{"created":"2023-12-11 18:42:18","title":"AttenScribble: Attentive Similarity Learning for Scribble-Supervised Medical Image Segmentation","abstract":"The success of deep networks in medical image segmentation relies heavily on massive labeled training data. However, acquiring dense annotations is a time-consuming process. Weakly-supervised methods normally employ less expensive forms of supervision, among which scribbles started to gain popularity lately thanks to its flexibility. However, due to lack of shape and boundary information, it is extremely challenging to train a deep network on scribbles that generalizes on unlabeled pixels. In this paper, we present a straightforward yet effective scribble supervised learning framework. Inspired by recent advances of transformer based segmentation, we create a pluggable spatial self-attention module which could be attached on top of any internal feature layers of arbitrary fully convolutional network (FCN) backbone. The module infuses global interaction while keeping the efficiency of convolutions. Descended from this module, we construct a similarity metric based on normalized and symmetrized attention. This attentive similarity leads to a novel regularization loss that imposes consistency between segmentation prediction and visual affinity. This attentive similarity loss optimizes the alignment of FCN encoders, attention mapping and model prediction. Ultimately, the proposed FCN+Attention architecture can be trained end-to-end guided by a combination of three learning objectives: partial segmentation loss, a customized masked conditional random fields and the proposed attentive similarity loss. Extensive experiments on public datasets (ACDC and CHAOS) showed that our framework not just out-performs existing state-of-the-art, but also delivers close performance to fully-supervised benchmark. Code will be available upon publication.","sentences":["The success of deep networks in medical image segmentation relies heavily on massive labeled training data.","However, acquiring dense annotations is a time-consuming process.","Weakly-supervised methods normally employ less expensive forms of supervision, among which scribbles started to gain popularity lately thanks to its flexibility.","However, due to lack of shape and boundary information, it is extremely challenging to train a deep network on scribbles that generalizes on unlabeled pixels.","In this paper, we present a straightforward yet effective scribble supervised learning framework.","Inspired by recent advances of transformer based segmentation, we create a pluggable spatial self-attention module which could be attached on top of any internal feature layers of arbitrary fully convolutional network (FCN) backbone.","The module infuses global interaction while keeping the efficiency of convolutions.","Descended from this module, we construct a similarity metric based on normalized and symmetrized attention.","This attentive similarity leads to a novel regularization loss that imposes consistency between segmentation prediction and visual affinity.","This attentive similarity loss optimizes the alignment of FCN encoders, attention mapping and model prediction.","Ultimately, the proposed FCN+Attention architecture can be trained end-to-end guided by a combination of three learning objectives: partial segmentation loss, a customized masked conditional random fields and the proposed attentive similarity loss.","Extensive experiments on public datasets (ACDC and CHAOS) showed that our framework not just out-performs existing state-of-the-art, but also delivers close performance to fully-supervised benchmark.","Code will be available upon publication."],"url":"http://arxiv.org/abs/2312.06614v1"}
{"created":"2023-12-11 18:41:55","title":"Neural Text to Articulate Talk: Deep Text to Audiovisual Speech Synthesis achieving both Auditory and Photo-realism","abstract":"Recent advances in deep learning for sequential data have given rise to fast and powerful models that produce realistic videos of talking humans. The state of the art in talking face generation focuses mainly on lip-syncing, being conditioned on audio clips. However, having the ability to synthesize talking humans from text transcriptions rather than audio is particularly beneficial for many applications and is expected to receive more and more attention, following the recent breakthroughs in large language models. For that, most methods implement a cascaded 2-stage architecture of a text-to-speech module followed by an audio-driven talking face generator, but this ignores the highly complex interplay between audio and visual streams that occurs during speaking. In this paper, we propose the first, to the best of our knowledge, text-driven audiovisual speech synthesizer that uses Transformers and does not follow a cascaded approach. Our method, which we call NEUral Text to ARticulate Talk (NEUTART), is a talking face generator that uses a joint audiovisual feature space, as well as speech-informed 3D facial reconstructions and a lip-reading loss for visual supervision. The proposed model produces photorealistic talking face videos with human-like articulation and well-synced audiovisual streams. Our experiments on audiovisual datasets as well as in-the-wild videos reveal state-of-the-art generation quality both in terms of objective metrics and human evaluation.","sentences":["Recent advances in deep learning for sequential data have given rise to fast and powerful models that produce realistic videos of talking humans.","The state of the art in talking face generation focuses mainly on lip-syncing, being conditioned on audio clips.","However, having the ability to synthesize talking humans from text transcriptions rather than audio is particularly beneficial for many applications and is expected to receive more and more attention, following the recent breakthroughs in large language models.","For that, most methods implement a cascaded 2-stage architecture of a text-to-speech module followed by an audio-driven talking face generator, but this ignores the highly complex interplay between audio and visual streams that occurs during speaking.","In this paper, we propose the first, to the best of our knowledge, text-driven audiovisual speech synthesizer that uses Transformers and does not follow a cascaded approach.","Our method, which we call NEUral Text to ARticulate Talk (NEUTART), is a talking face generator that uses a joint audiovisual feature space, as well as speech-informed 3D facial reconstructions and a lip-reading loss for visual supervision.","The proposed model produces photorealistic talking face videos with human-like articulation and well-synced audiovisual streams.","Our experiments on audiovisual datasets as well as in-the-wild videos reveal state-of-the-art generation quality both in terms of objective metrics and human evaluation."],"url":"http://arxiv.org/abs/2312.06613v1"}
{"created":"2023-12-11 18:39:34","title":"Finer characterization of bounded languages described by GF(2)-grammars","abstract":"GF(2)-grammars are a somewhat recently introduced grammar family that have some unusual algebraic properties and are closely connected to unambiguous grammars. In \"Bounded languages described by GF(2)-grammars\", Makarov proved a necessary condition for subsets of $a_1^* a_2^* \\cdots a_k^*$ to be described by some GF(2)-grammar. By extending these methods further, we prove an even stronger upper bound for these languages. Moreover, we establish a lower bound that closely matches the proven upper bound. Also, we prove the exact characterization for the special case of linear GF(2)-grammars. Finally, by using the previous result, we show that the class of languages described by linear GF(2)-grammars is not closed under GF(2)-concatenation","sentences":["GF(2)-grammars are a somewhat recently introduced grammar family that have some unusual algebraic properties and are closely connected to unambiguous grammars.","In \"Bounded languages described by GF(2)-grammars\", Makarov proved a necessary condition for subsets of $a_1^*","a_2^* \\cdots a_k^*$ to be described by some GF(2)-grammar.","By extending these methods further, we prove an even stronger upper bound for these languages.","Moreover, we establish a lower bound that closely matches the proven upper bound.","Also, we prove the exact characterization for the special case of linear GF(2)-grammars.","Finally, by using the previous result, we show that the class of languages described by linear GF(2)-grammars is not closed under GF(2)-concatenation"],"url":"http://arxiv.org/abs/2312.06609v1"}
{"created":"2023-12-11 18:38:28","title":"DiAD: A Diffusion-based Framework for Multi-class Anomaly Detection","abstract":"Reconstruction-based approaches have achieved remarkable outcomes in anomaly detection. The exceptional image reconstruction capabilities of recently popular diffusion models have sparked research efforts to utilize them for enhanced reconstruction of anomalous images. Nonetheless, these methods might face challenges related to the preservation of image categories and pixel-wise structural integrity in the more practical multi-class setting. To solve the above problems, we propose a Difusion-based Anomaly Detection (DiAD) framework for multi-class anomaly detection, which consists of a pixel-space autoencoder, a latent-space Semantic-Guided (SG) network with a connection to the stable diffusion's denoising network, and a feature-space pre-trained feature extractor. Firstly, The SG network is proposed for reconstructing anomalous regions while preserving the original image's semantic information. Secondly, we introduce Spatial-aware Feature Fusion (SFF) block to maximize reconstruction accuracy when dealing with extensively reconstructed areas. Thirdly, the input and reconstructed images are processed by a pre-trained feature extractor to generate anomaly maps based on features extracted at different scales. Experiments on MVTec-AD and VisA datasets demonstrate the effectiveness of our approach which surpasses the state-of-the-art methods, e.g., achieving 96.8/52.6 and 97.2/99.0 (AUROC/AP) for localization and detection respectively on multi-class MVTec-AD dataset. Code will be available at https://lewandofskee.github.io/projects/diad.","sentences":["Reconstruction-based approaches have achieved remarkable outcomes in anomaly detection.","The exceptional image reconstruction capabilities of recently popular diffusion models have sparked research efforts to utilize them for enhanced reconstruction of anomalous images.","Nonetheless, these methods might face challenges related to the preservation of image categories and pixel-wise structural integrity in the more practical multi-class setting.","To solve the above problems, we propose a Difusion-based Anomaly Detection (DiAD) framework for multi-class anomaly detection, which consists of a pixel-space autoencoder, a latent-space Semantic-Guided (SG) network with a connection to the stable diffusion's denoising network, and a feature-space pre-trained feature extractor.","Firstly, The SG network is proposed for reconstructing anomalous regions while preserving the original image's semantic information.","Secondly, we introduce Spatial-aware Feature Fusion (SFF) block to maximize reconstruction accuracy when dealing with extensively reconstructed areas.","Thirdly, the input and reconstructed images are processed by a pre-trained feature extractor to generate anomaly maps based on features extracted at different scales.","Experiments on MVTec-AD and VisA datasets demonstrate the effectiveness of our approach which surpasses the state-of-the-art methods, e.g., achieving 96.8/52.6 and 97.2/99.0 (AUROC/AP) for localization and detection respectively on multi-class MVTec-AD dataset.","Code will be available at https://lewandofskee.github.io/projects/diad."],"url":"http://arxiv.org/abs/2312.06607v1"}
{"created":"2023-12-11 18:31:13","title":"Early Action Recognition with Action Prototypes","abstract":"Early action recognition is an important and challenging problem that enables the recognition of an action from a partially observed video stream where the activity is potentially unfinished or even not started. In this work, we propose a novel model that learns a prototypical representation of the full action for each class and uses it to regularize the architecture and the visual representations of the partial observations. Our model is very simple in design and also efficient. We decompose the video into short clips, where a visual encoder extracts features from each clip independently. Later, a decoder aggregates together in an online fashion features from all the clips for the final class prediction. During training, for each partial observation, the model is jointly trained to both predict the label as well as the action prototypical representation which acts as a regularizer. We evaluate our method on multiple challenging real-world datasets and outperform the current state-of-the-art by a significant margin. For example, on early recognition observing only the first 10% of each video, our method improves the SOTA by +2.23 Top-1 accuracy on Something-Something-v2, +3.55 on UCF-101, +3.68 on SSsub21, and +5.03 on EPIC-Kitchens-55, where prior work used either multi-modal inputs (e.g. optical-flow) or batched inference. Finally, we also present exhaustive ablation studies to motivate the design choices we made, as well as gather insights regarding what our model is learning semantically.","sentences":["Early action recognition is an important and challenging problem that enables the recognition of an action from a partially observed video stream where the activity is potentially unfinished or even not started.","In this work, we propose a novel model that learns a prototypical representation of the full action for each class and uses it to regularize the architecture and the visual representations of the partial observations.","Our model is very simple in design and also efficient.","We decompose the video into short clips, where a visual encoder extracts features from each clip independently.","Later, a decoder aggregates together in an online fashion features from all the clips for the final class prediction.","During training, for each partial observation, the model is jointly trained to both predict the label as well as the action prototypical representation which acts as a regularizer.","We evaluate our method on multiple challenging real-world datasets and outperform the current state-of-the-art by a significant margin.","For example, on early recognition observing only the first 10% of each video, our method improves the SOTA by +2.23 Top-1 accuracy on Something-Something-v2, +3.55 on UCF-101, +3.68 on SSsub21, and +5.03 on EPIC-Kitchens-55, where prior work used either multi-modal inputs (e.g. optical-flow) or batched inference.","Finally, we also present exhaustive ablation studies to motivate the design choices we made, as well as gather insights regarding what our model is learning semantically."],"url":"http://arxiv.org/abs/2312.06598v1"}
{"created":"2023-12-11 18:28:55","title":"Mitigating Perspective Distortion-induced Shape Ambiguity in Image Crops","abstract":"Objects undergo varying amounts of perspective distortion as they move across a camera's field of view. Models for predicting 3D from a single image often work with crops around the object of interest and ignore the location of the object in the camera's field of view. We note that ignoring this location information further exaggerates the inherent ambiguity in making 3D inferences from 2D images and can prevent models from even fitting to the training data. To mitigate this ambiguity, we propose Intrinsics-Aware Positional Encoding (KPE), which incorporates information about the location of crops in the image and camera intrinsics. Experiments on three popular 3D-from-a-single-image benchmarks: depth prediction on NYU, 3D object detection on KITTI & nuScenes, and predicting 3D shapes of articulated objects on ARCTIC, show the benefits of KPE.","sentences":["Objects undergo varying amounts of perspective distortion as they move across a camera's field of view.","Models for predicting 3D from a single image often work with crops around the object of interest and ignore the location of the object in the camera's field of view.","We note that ignoring this location information further exaggerates the inherent ambiguity in making 3D inferences from 2D images and can prevent models from even fitting to the training data.","To mitigate this ambiguity, we propose Intrinsics-Aware Positional Encoding (KPE), which incorporates information about the location of crops in the image and camera intrinsics.","Experiments on three popular 3D-from-a-single-image benchmarks: depth prediction on NYU, 3D object detection on KITTI & nuScenes, and predicting 3D shapes of articulated objects on ARCTIC, show the benefits of KPE."],"url":"http://arxiv.org/abs/2312.06594v1"}
{"created":"2023-12-11 18:27:42","title":"Flexible visual prompts for in-context learning in computer vision","abstract":"In this work, we address in-context learning (ICL) for the task of image segmentation, introducing a novel approach that adapts a modern Video Object Segmentation (VOS) technique for visual in-context learning. This adaptation is inspired by the VOS method's ability to efficiently and flexibly learn objects from a few examples. Through evaluations across a range of support set sizes and on diverse segmentation datasets, our method consistently surpasses existing techniques. Notably, it excels with data containing classes not encountered during training. Additionally, we propose a technique for support set selection, which involves choosing the most relevant images to include in this set. By employing support set selection, the performance increases for all tested methods without the need for additional training or prompt tuning. The code can be found at https://github.com/v7labs/XMem_ICL/.","sentences":["In this work, we address in-context learning (ICL) for the task of image segmentation, introducing a novel approach that adapts a modern Video Object Segmentation (VOS) technique for visual in-context learning.","This adaptation is inspired by the VOS method's ability to efficiently and flexibly learn objects from a few examples.","Through evaluations across a range of support set sizes and on diverse segmentation datasets, our method consistently surpasses existing techniques.","Notably, it excels with data containing classes not encountered during training.","Additionally, we propose a technique for support set selection, which involves choosing the most relevant images to include in this set.","By employing support set selection, the performance increases for all tested methods without the need for additional training or prompt tuning.","The code can be found at https://github.com/v7labs/XMem_ICL/."],"url":"http://arxiv.org/abs/2312.06592v1"}
{"created":"2023-12-11 18:17:43","title":"Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models","abstract":"Fine-tuning language models~(LMs) on human-generated data remains a prevalent practice. However, the performance of such models is often limited by the quantity and diversity of high-quality human data. In this paper, we explore whether we can go beyond human data on tasks where we have access to scalar feedback, for example, on math problems where one can verify correctness. To do so, we investigate a simple self-training method based on expectation-maximization, which we call ReST$^{EM}$, where we (1) generate samples from the model and filter them using binary feedback, (2) fine-tune the model on these samples, and (3) repeat this process a few times. Testing on advanced MATH reasoning and APPS coding benchmarks using PaLM-2 models, we find that ReST$^{EM}$ scales favorably with model size and significantly surpasses fine-tuning only on human data. Overall, our findings suggest self-training with feedback can substantially reduce dependence on human-generated data.","sentences":["Fine-tuning language models~(LMs) on human-generated data remains a prevalent practice.","However, the performance of such models is often limited by the quantity and diversity of high-quality human data.","In this paper, we explore whether we can go beyond human data on tasks where we have access to scalar feedback, for example, on math problems where one can verify correctness.","To do so, we investigate a simple self-training method based on expectation-maximization, which we call ReST$^{EM}$, where we (1) generate samples from the model and filter them using binary feedback, (2) fine-tune the model on these samples, and (3) repeat this process a few times.","Testing on advanced MATH reasoning and APPS coding benchmarks using PaLM-2 models, we find that ReST$^{EM}$ scales favorably with model size and significantly surpasses fine-tuning only on human data.","Overall, our findings suggest self-training with feedback can substantially reduce dependence on human-generated data."],"url":"http://arxiv.org/abs/2312.06585v1"}
{"created":"2023-12-11 18:15:47","title":"3D Hand Pose Estimation in Egocentric Images in the Wild","abstract":"We present WildHands, a method for 3D hand pose estimation in egocentric images in the wild. This is challenging due to (a) lack of 3D hand pose annotations for images in the wild, and (b) a form of perspective distortion-induced shape ambiguity that arises in the analysis of crops around hands. For the former, we use auxiliary supervision on in-the-wild data in the form of segmentation masks & grasp labels in addition to 3D supervision available in lab datasets. For the latter, we provide spatial cues about the location of the hand crop in the camera's field of view. Our approach achieves the best 3D hand pose on the ARCTIC leaderboard and outperforms FrankMocap, a popular and robust approach for estimating hand pose in the wild, by 45.3% when evaluated on 2D hand pose on our EPIC-HandKps dataset.","sentences":["We present WildHands, a method for 3D hand pose estimation in egocentric images in the wild.","This is challenging due to (a) lack of 3D hand pose annotations for images in the wild, and (b) a form of perspective distortion-induced shape ambiguity that arises in the analysis of crops around hands.","For the former, we use auxiliary supervision on in-the-wild data in the form of segmentation masks & grasp labels in addition to 3D supervision available in lab datasets.","For the latter, we provide spatial cues about the location of the hand crop in the camera's field of view.","Our approach achieves the best 3D hand pose on the ARCTIC leaderboard and outperforms FrankMocap, a popular and robust approach for estimating hand pose in the wild, by 45.3% when evaluated on 2D hand pose on our EPIC-HandKps dataset."],"url":"http://arxiv.org/abs/2312.06583v1"}
{"created":"2023-12-11 18:12:18","title":"Grokking Group Multiplication with Cosets","abstract":"We use the group Fourier transform over the symmetric group $S_n$ to reverse engineer a 1-layer feedforward network that has \"grokked\" the multiplication of $S_5$ and $S_6$. Each model discovers the true subgroup structure of the full group and converges on circuits that decompose the group multiplication into the multiplication of the group's conjugate subgroups. We demonstrate the value of using the symmetries of the data and models to understand their mechanisms and hold up the ``coset circuit'' that the model uses as a fascinating example of the way neural networks implement computations. We also draw attention to current challenges in conducting mechanistic interpretability research by comparing our work to Chughtai et al. [6] which alleges to find a different algorithm for this same problem.","sentences":["We use the group Fourier transform over the symmetric group $S_n$ to reverse engineer a 1-layer feedforward network that has \"grokked\" the multiplication of $S_5$ and $S_6$. Each model discovers the true subgroup structure of the full group and converges on circuits that decompose the group multiplication into the multiplication of the group's conjugate subgroups.","We demonstrate the value of using the symmetries of the data and models to understand their mechanisms and hold up the ``coset circuit'' that the model uses as a fascinating example of the way neural networks implement computations.","We also draw attention to current challenges in conducting mechanistic interpretability research by comparing our work to Chughtai et al.","[6] which alleges to find a different algorithm for this same problem."],"url":"http://arxiv.org/abs/2312.06581v1"}
{"created":"2023-12-11 18:10:49","title":"VGF: Value-Guided Fuzzing -- Fuzzing Hardware as Hardware","abstract":"As the complexity of logic designs increase, new avenues for testing digital hardware becomes necessary. Fuzz Testing (fuzzing) has recently received attention as a potential candidate for input vector generation on hardware designs. Using this technique, a fuzzer is used to generate an input to a logic design. Using a simulation engine, the logic design is given the generated stimulus and some metric of feedback is given to the fuzzer to aid in the input mutation. However, much like software fuzzing, hardware fuzzing uses code coverage as a metric to find new possible fuzzing paths. Unfortunately, as we show in this work, this coverage metric falls short of generic on some hardware designs where designers have taken a more direct approach at expressing a particular microarchitecture, or implementation, of the desired hardware.   With this work, we introduce a new coverage metric which employs not code coverage, but state coverage internal to a design. By observing changes in signals within the logic circuit under testing, we are able to explore the state space of the design and provide feedback to a fuzzer engine for input generation. Our approach, Value-Guided Fuzzing (VGF), provides a generic metric of coverage which can be applied to any design regardless of its implementation. In this paper, we introduce our state-based VGF metric as well as a sample implementation which can be used with any VPI, DPI, VHPI, or FLI compliant simulator, making it completely HDL agnostic. We demonstrate the generality of VGF and show how our sample implementation is capable of finding bugs considerably faster than previous approaches.","sentences":["As the complexity of logic designs increase, new avenues for testing digital hardware becomes necessary.","Fuzz Testing (fuzzing) has recently received attention as a potential candidate for input vector generation on hardware designs.","Using this technique, a fuzzer is used to generate an input to a logic design.","Using a simulation engine, the logic design is given the generated stimulus and some metric of feedback is given to the fuzzer to aid in the input mutation.","However, much like software fuzzing, hardware fuzzing uses code coverage as a metric to find new possible fuzzing paths.","Unfortunately, as we show in this work, this coverage metric falls short of generic on some hardware designs where designers have taken a more direct approach at expressing a particular microarchitecture, or implementation, of the desired hardware.   ","With this work, we introduce a new coverage metric which employs not code coverage, but state coverage internal to a design.","By observing changes in signals within the logic circuit under testing, we are able to explore the state space of the design and provide feedback to a fuzzer engine for input generation.","Our approach, Value-Guided Fuzzing (VGF), provides a generic metric of coverage which can be applied to any design regardless of its implementation.","In this paper, we introduce our state-based VGF metric as well as a sample implementation which can be used with any VPI, DPI, VHPI, or FLI compliant simulator, making it completely HDL agnostic.","We demonstrate the generality of VGF and show how our sample implementation is capable of finding bugs considerably faster than previous approaches."],"url":"http://arxiv.org/abs/2312.06580v1"}
{"created":"2023-12-11 18:09:55","title":"Multi-class Support Vector Machine with Maximizing Minimum Margin","abstract":"Support Vector Machine (SVM) stands out as a prominent machine learning technique widely applied in practical pattern recognition tasks. It achieves binary classification by maximizing the \"margin\", which represents the minimum distance between instances and the decision boundary. Although many efforts have been dedicated to expanding SVM for multi-class case through strategies such as one versus one and one versus the rest, satisfactory solutions remain to be developed. In this paper, we propose a novel method for multi-class SVM that incorporates pairwise class loss considerations and maximizes the minimum margin. Adhering to this concept, we embrace a new formulation that imparts heightened flexibility to multi-class SVM. Furthermore, the correlations between the proposed method and multiple forms of multi-class SVM are analyzed. The proposed regularizer, akin to the concept of \"margin\", can serve as a seamless enhancement over the softmax in deep learning, providing guidance for network parameter learning. Empirical evaluations demonstrate the effectiveness and superiority of our proposed method over existing multi-classification methods.Code is available at https://github.com/zz-haooo/M3SVM.","sentences":["Support Vector Machine (SVM) stands out as a prominent machine learning technique widely applied in practical pattern recognition tasks.","It achieves binary classification by maximizing the \"margin\", which represents the minimum distance between instances and the decision boundary.","Although many efforts have been dedicated to expanding SVM for multi-class case through strategies such as one versus one and one versus the rest, satisfactory solutions remain to be developed.","In this paper, we propose a novel method for multi-class SVM that incorporates pairwise class loss considerations and maximizes the minimum margin.","Adhering to this concept, we embrace a new formulation that imparts heightened flexibility to multi-class SVM.","Furthermore, the correlations between the proposed method and multiple forms of multi-class SVM are analyzed.","The proposed regularizer, akin to the concept of \"margin\", can serve as a seamless enhancement over the softmax in deep learning, providing guidance for network parameter learning.","Empirical evaluations demonstrate the effectiveness and superiority of our proposed method over existing multi-classification methods.","Code is available at https://github.com/zz-haooo/M3SVM."],"url":"http://arxiv.org/abs/2312.06578v1"}
{"created":"2023-12-11 18:00:27","title":"HyPE-GT: where Graph Transformers meet Hyperbolic Positional Encodings","abstract":"Graph Transformers (GTs) facilitate the comprehension of graph-structured data by calculating the self-attention of node pairs without considering node position information. To address this limitation, we introduce an innovative and efficient framework that introduces Positional Encodings (PEs) into the Transformer, generating a set of learnable positional encodings in the hyperbolic space, a non-Euclidean domain. This approach empowers us to explore diverse options for optimal selection of PEs for specific downstream tasks, leveraging hyperbolic neural networks or hyperbolic graph convolutional networks. Additionally, we repurpose these positional encodings to mitigate the impact of over-smoothing in deep Graph Neural Networks (GNNs). Comprehensive experiments on molecular benchmark datasets, co-author, and co-purchase networks substantiate the effectiveness of hyperbolic positional encodings in enhancing the performance of deep GNNs.","sentences":["Graph Transformers (GTs) facilitate the comprehension of graph-structured data by calculating the self-attention of node pairs without considering node position information.","To address this limitation, we introduce an innovative and efficient framework that introduces Positional Encodings (PEs) into the Transformer, generating a set of learnable positional encodings in the hyperbolic space, a non-Euclidean domain.","This approach empowers us to explore diverse options for optimal selection of PEs for specific downstream tasks, leveraging hyperbolic neural networks or hyperbolic graph convolutional networks.","Additionally, we repurpose these positional encodings to mitigate the impact of over-smoothing in deep Graph Neural Networks (GNNs).","Comprehensive experiments on molecular benchmark datasets, co-author, and co-purchase networks substantiate the effectiveness of hyperbolic positional encodings in enhancing the performance of deep GNNs."],"url":"http://arxiv.org/abs/2312.06576v1"}
{"created":"2023-12-11 17:59:46","title":"EasyVolcap: Accelerating Neural Volumetric Video Research","abstract":"Volumetric video is a technology that digitally records dynamic events such as artistic performances, sporting events, and remote conversations. When acquired, such volumography can be viewed from any viewpoint and timestamp on flat screens, 3D displays, or VR headsets, enabling immersive viewing experiences and more flexible content creation in a variety of applications such as sports broadcasting, video conferencing, gaming, and movie productions. With the recent advances and fast-growing interest in neural scene representations for volumetric video, there is an urgent need for a unified open-source library to streamline the process of volumetric video capturing, reconstruction, and rendering for both researchers and non-professional users to develop various algorithms and applications of this emerging technology. In this paper, we present EasyVolcap, a Python & Pytorch library for accelerating neural volumetric video research with the goal of unifying the process of multi-view data processing, 4D scene reconstruction, and efficient dynamic volumetric video rendering. Our source code is available at https://github.com/zju3dv/EasyVolcap.","sentences":["Volumetric video is a technology that digitally records dynamic events such as artistic performances, sporting events, and remote conversations.","When acquired, such volumography can be viewed from any viewpoint and timestamp on flat screens, 3D displays, or VR headsets, enabling immersive viewing experiences and more flexible content creation in a variety of applications such as sports broadcasting, video conferencing, gaming, and movie productions.","With the recent advances and fast-growing interest in neural scene representations for volumetric video, there is an urgent need for a unified open-source library to streamline the process of volumetric video capturing, reconstruction, and rendering for both researchers and non-professional users to develop various algorithms and applications of this emerging technology.","In this paper, we present EasyVolcap, a Python & Pytorch library for accelerating neural volumetric video research with the goal of unifying the process of multi-view data processing, 4D scene reconstruction, and efficient dynamic volumetric video rendering.","Our source code is available at https://github.com/zju3dv/EasyVolcap."],"url":"http://arxiv.org/abs/2312.06575v1"}
{"created":"2023-12-11 17:58:13","title":"Dissecting the EIP-2930 Optional Access Lists","abstract":"Ethereum introduced Transaction Access Lists (TALs) in 2020 to optimize gas costs during transaction execution. In this work, we present a comprehensive analysis of TALs in Ethereum, focusing on adoption, quality, and gas savings. Analyzing a full month of mainnet data with 31,954,474 transactions, we found that only 1.46% of transactions included a TAL, even though 42.6% of transactions would have benefited from it. On average, access lists can save around 0.29% of gas costs, equivalent to approximately 3,450 ETH (roughly US$ 5 Mio) per year. However, 19.6% of TALs included by transactions contained imperfections, causing almost 11.8% of transactions to pay more gas with TAL than without. We find that these inaccuracies are caused by the unknown state at the time of the TAL computation as well as imperfect TAL computations provided by all major Ethereum clients. We thus compare the gas savings when calculating the TAL at the beginning of the block vs. calculating it on the correct state, to find that the unknown state is a major source of TAL inaccuracies. Finally, we implement an ideal TAL computation for the Erigon client to highlight the cost of these flawed implementations.","sentences":["Ethereum introduced Transaction Access Lists (TALs) in 2020 to optimize gas costs during transaction execution.","In this work, we present a comprehensive analysis of TALs in Ethereum, focusing on adoption, quality, and gas savings.","Analyzing a full month of mainnet data with 31,954,474 transactions, we found that only 1.46% of transactions included a TAL, even though 42.6% of transactions would have benefited from it.","On average, access lists can save around 0.29% of gas costs, equivalent to approximately 3,450 ETH (roughly US$ 5 Mio) per year.","However, 19.6% of TALs included by transactions contained imperfections, causing almost 11.8% of transactions to pay more gas with TAL than without.","We find that these inaccuracies are caused by the unknown state at the time of the TAL computation as well as imperfect TAL computations provided by all major Ethereum clients.","We thus compare the gas savings when calculating the TAL at the beginning of the block vs. calculating it on the correct state, to find that the unknown state is a major source of TAL inaccuracies.","Finally, we implement an ideal TAL computation for the Erigon client to highlight the cost of these flawed implementations."],"url":"http://arxiv.org/abs/2312.06574v1"}
{"created":"2023-12-11 17:58:06","title":"ControlNet-XS: Designing an Efficient and Effective Architecture for Controlling Text-to-Image Diffusion Models","abstract":"The field of image synthesis has made tremendous strides forward in the last years. Besides defining the desired output image with text-prompts, an intuitive approach is to additionally use spatial guidance in form of an image, such as a depth map. For this, a recent and highly popular approach is to use a controlling network, such as ControlNet, in combination with a pre-trained image generation model, such as Stable Diffusion. When evaluating the design of existing controlling networks, we observe that they all suffer from the same problem of a delay in information flowing between the generation and controlling process. This, in turn, means that the controlling network must have generative capabilities. In this work we propose a new controlling architecture, called ControlNet-XS, which does not suffer from this problem, and hence can focus on the given task of learning to control. In contrast to ControlNet, our model needs only a fraction of parameters, and hence is about twice as fast during inference and training time. Furthermore, the generated images are of higher quality and the control is of higher fidelity. All code and pre-trained models will be made publicly available.","sentences":["The field of image synthesis has made tremendous strides forward in the last years.","Besides defining the desired output image with text-prompts, an intuitive approach is to additionally use spatial guidance in form of an image, such as a depth map.","For this, a recent and highly popular approach is to use a controlling network, such as ControlNet, in combination with a pre-trained image generation model, such as Stable Diffusion.","When evaluating the design of existing controlling networks, we observe that they all suffer from the same problem of a delay in information flowing between the generation and controlling process.","This, in turn, means that the controlling network must have generative capabilities.","In this work we propose a new controlling architecture, called ControlNet-XS, which does not suffer from this problem, and hence can focus on the given task of learning to control.","In contrast to ControlNet, our model needs only a fraction of parameters, and hence is about twice as fast during inference and training time.","Furthermore, the generated images are of higher quality and the control is of higher fidelity.","All code and pre-trained models will be made publicly available."],"url":"http://arxiv.org/abs/2312.06573v1"}
{"created":"2023-12-11 17:57:11","title":"From Text to Motion: Grounding GPT-4 in a Humanoid Robot \"Alter3\"","abstract":"We report the development of Alter3, a humanoid robot capable of generating spontaneous motion using a Large Language Model (LLM), specifically GPT-4. This achievement was realized by integrating GPT-4 into our proprietary android, Alter3, thereby effectively grounding the LLM with Alter's bodily movement. Typically, low-level robot control is hardware-dependent and falls outside the scope of LLM corpora, presenting challenges for direct LLM-based robot control. However, in the case of humanoid robots like Alter3, direct control is feasible by mapping the linguistic expressions of human actions onto the robot's body through program code. Remarkably, this approach enables Alter3 to adopt various poses, such as a 'selfie' stance or 'pretending to be a ghost,' and generate sequences of actions over time without explicit programming for each body part. This demonstrates the robot's zero-shot learning capabilities. Additionally, verbal feedback can adjust poses, obviating the need for fine-tuning. A video of Alter3's generated motions is available at https://tnoinkwms.github.io/ALTER-LLM/","sentences":["We report the development of Alter3, a humanoid robot capable of generating spontaneous motion using a Large Language Model (LLM), specifically GPT-4.","This achievement was realized by integrating GPT-4 into our proprietary android, Alter3, thereby effectively grounding the LLM with Alter's bodily movement.","Typically, low-level robot control is hardware-dependent and falls outside the scope of LLM corpora, presenting challenges for direct LLM-based robot control.","However, in the case of humanoid robots like Alter3, direct control is feasible by mapping the linguistic expressions of human actions onto the robot's body through program code.","Remarkably, this approach enables Alter3 to adopt various poses, such as a 'selfie' stance or 'pretending to be a ghost,' and generate sequences of actions over time without explicit programming for each body part.","This demonstrates the robot's zero-shot learning capabilities.","Additionally, verbal feedback can adjust poses, obviating the need for fine-tuning.","A video of Alter3's generated motions is available at https://tnoinkwms.github.io/ALTER-LLM/"],"url":"http://arxiv.org/abs/2312.06571v1"}
{"created":"2023-12-11 17:57:08","title":"Preserving the Artifacts of the Early Digital Era: A Study of What, Why and How?","abstract":"In this article, we report the pilot results of a survey study (N=1036) related to social attitudes towards the early digital heritage. On the basis of the answers, we consider what constitutes early digital artifacts (EDA) and outline how knowledge about them can be useful. We explore attitudes toward the historical and cultural importance of various EDAs and chart the surveyed requirements for their successful and sustainable preservation for current and future generations.","sentences":["In this article, we report the pilot results of a survey study (N=1036) related to social attitudes towards the early digital heritage.","On the basis of the answers, we consider what constitutes early digital artifacts (EDA) and outline how knowledge about them can be useful.","We explore attitudes toward the historical and cultural importance of various EDAs and chart the surveyed requirements for their successful and sustainable preservation for current and future generations."],"url":"http://arxiv.org/abs/2312.06570v1"}
{"created":"2023-12-11 17:56:22","title":"Ambient IoT: A missing link in 3GPP IoT Devices Landscape","abstract":"Ambient internet of things (IoT) is the network of devices which harvest energy from ambient sources for powering their communication. After decades of research on operation of these devices, Third Generation Partnership Project (3GPP) has started discussing energy harvesting technology in cellular networks to support massive deployment of IoT devices at low operational cost. This article provides a timely update on 3GPP studies on ambient energy harvesting devices including device types, use cases, key requirements, and related design challenges. Supported by link budget analysis for backscattering energy harvesting devices, which are a key component of this study, we provide insight on system design and show how this technology will require a new system design approach as compared to New Radio (NR) system design in 5G.","sentences":["Ambient internet of things (IoT) is the network of devices which harvest energy from ambient sources for powering their communication.","After decades of research on operation of these devices, Third Generation Partnership Project (3GPP) has started discussing energy harvesting technology in cellular networks to support massive deployment of IoT devices at low operational cost.","This article provides a timely update on 3GPP studies on ambient energy harvesting devices including device types, use cases, key requirements, and related design challenges.","Supported by link budget analysis for backscattering energy harvesting devices, which are a key component of this study, we provide insight on system design and show how this technology will require a new system design approach as compared to New Radio (NR) system design in 5G."],"url":"http://arxiv.org/abs/2312.06569v1"}
{"created":"2023-12-11 17:52:46","title":"Sparse but Strong: Crafting Adversarially Robust Graph Lottery Tickets","abstract":"Graph Lottery Tickets (GLTs), comprising a sparse adjacency matrix and a sparse graph neural network (GNN), can significantly reduce the inference latency and compute footprint compared to their dense counterparts. Despite these benefits, their performance against adversarial structure perturbations remains to be fully explored. In this work, we first investigate the resilience of GLTs against different structure perturbation attacks and observe that they are highly vulnerable and show a large drop in classification accuracy. Based on this observation, we then present an adversarially robust graph sparsification (ARGS) framework that prunes the adjacency matrix and the GNN weights by optimizing a novel loss function capturing the graph homophily property and information associated with both the true labels of the train nodes and the pseudo labels of the test nodes. By iteratively applying ARGS to prune both the perturbed graph adjacency matrix and the GNN model weights, we can find adversarially robust graph lottery tickets that are highly sparse yet achieve competitive performance under different untargeted training-time structure attacks. Evaluations conducted on various benchmarks, considering different poisoning structure attacks, namely, PGD, MetaAttack, Meta-PGD, and PR-BCD demonstrate that the GLTs generated by ARGS can significantly improve the robustness, even when subjected to high levels of sparsity.","sentences":["Graph Lottery Tickets (GLTs), comprising a sparse adjacency matrix and a sparse graph neural network (GNN), can significantly reduce the inference latency and compute footprint compared to their dense counterparts.","Despite these benefits, their performance against adversarial structure perturbations remains to be fully explored.","In this work, we first investigate the resilience of GLTs against different structure perturbation attacks and observe that they are highly vulnerable and show a large drop in classification accuracy.","Based on this observation, we then present an adversarially robust graph sparsification (ARGS) framework that prunes the adjacency matrix and the GNN weights by optimizing a novel loss function capturing the graph homophily property and information associated with both the true labels of the train nodes and the pseudo labels of the test nodes.","By iteratively applying ARGS to prune both the perturbed graph adjacency matrix and the GNN model weights, we can find adversarially robust graph lottery tickets that are highly sparse yet achieve competitive performance under different untargeted training-time structure attacks.","Evaluations conducted on various benchmarks, considering different poisoning structure attacks, namely, PGD, MetaAttack, Meta-PGD, and PR-BCD demonstrate that the GLTs generated by ARGS can significantly improve the robustness, even when subjected to high levels of sparsity."],"url":"http://arxiv.org/abs/2312.06568v1"}
{"created":"2023-12-11 17:51:56","title":"One Size Does not Fit All: Personalised Affordance Design for Social Robots","abstract":"Personalisation is essential to achieve more acceptable and effective results in human-robot interaction. Placing users in the central role, many studies have focused on enhancing the abilities of social robots to perceive and understand users. However, little is known about improving user perceptions and interpretation of a social robot in spoken interactions. The work described in the paper aims to find out what affects the personalisation of affordance of a social robot, namely its appearance, voice and language behaviours. The experimental data presented here is based on an ongoing project. It demonstrates the many and varied ways in which people change their preferences for the affordance of a social robot under different circumstances. It also examines the relationship between such preferences and expectations of characteristics of a social robot, like competence and warmth. It also shows that individuals have different perceptions of the language behaviours of the same robot. These results demonstrate that one-sized personalisation does not fit all. Personalisation should be considered a comprehensive approach, including appropriate affordance design, to suit the user expectations of social roles.","sentences":["Personalisation is essential to achieve more acceptable and effective results in human-robot interaction.","Placing users in the central role, many studies have focused on enhancing the abilities of social robots to perceive and understand users.","However, little is known about improving user perceptions and interpretation of a social robot in spoken interactions.","The work described in the paper aims to find out what affects the personalisation of affordance of a social robot, namely its appearance, voice and language behaviours.","The experimental data presented here is based on an ongoing project.","It demonstrates the many and varied ways in which people change their preferences for the affordance of a social robot under different circumstances.","It also examines the relationship between such preferences and expectations of characteristics of a social robot, like competence and warmth.","It also shows that individuals have different perceptions of the language behaviours of the same robot.","These results demonstrate that one-sized personalisation does not fit all.","Personalisation should be considered a comprehensive approach, including appropriate affordance design, to suit the user expectations of social roles."],"url":"http://arxiv.org/abs/2312.06566v1"}
{"created":"2023-12-11 17:49:25","title":"Promoting Counterfactual Robustness through Diversity","abstract":"Counterfactual explanations shed light on the decisions of black-box models by explaining how an input can be altered to obtain a favourable decision from the model (e.g., when a loan application has been rejected). However, as noted recently, counterfactual explainers may lack robustness in the sense that a minor change in the input can cause a major change in the explanation. This can cause confusion on the user side and open the door for adversarial attacks. In this paper, we study some sources of non-robustness. While there are fundamental reasons for why an explainer that returns a single counterfactual cannot be robust in all instances, we show that some interesting robustness guarantees can be given by reporting multiple rather than a single counterfactual. Unfortunately, the number of counterfactuals that need to be reported for the theoretical guarantees to hold can be prohibitively large. We therefore propose an approximation algorithm that uses a diversity criterion to select a feasible number of most relevant explanations and study its robustness empirically. Our experiments indicate that our method improves the state-of-the-art in generating robust explanations, while maintaining other desirable properties and providing competitive computational performance.","sentences":["Counterfactual explanations shed light on the decisions of black-box models by explaining how an input can be altered to obtain a favourable decision from the model (e.g., when a loan application has been rejected).","However, as noted recently, counterfactual explainers may lack robustness in the sense that a minor change in the input can cause a major change in the explanation.","This can cause confusion on the user side and open the door for adversarial attacks.","In this paper, we study some sources of non-robustness.","While there are fundamental reasons for why an explainer that returns a single counterfactual cannot be robust in all instances, we show that some interesting robustness guarantees can be given by reporting multiple rather than a single counterfactual.","Unfortunately, the number of counterfactuals that need to be reported for the theoretical guarantees to hold can be prohibitively large.","We therefore propose an approximation algorithm that uses a diversity criterion to select a feasible number of most relevant explanations and study its robustness empirically.","Our experiments indicate that our method improves the state-of-the-art in generating robust explanations, while maintaining other desirable properties and providing competitive computational performance."],"url":"http://arxiv.org/abs/2312.06564v1"}
{"created":"2023-12-11 17:46:44","title":"On Meta-Prompting","abstract":"Certain statistical models are capable of interpreting input strings as instructions, or prompts, and carry out tasks based on them. Many approaches to prompting and pre-training these models involve the automated generation of these prompts. We call these approaches meta-prompting, or prompting to obtain prompts. We propose a theoretical framework based on category theory to generalize and describe them. This framework is flexible enough to account for LLM stochasticity; and allows us to obtain formal results around task agnosticity and equivalence of various meta-prompting approaches. We experiment with meta-prompting in two active areas of model research: creativity and ideation. We find that user preference favors (p < 0.01) the prompts generated under meta-prompting, as well as their corresponding outputs, over a series of hardcoded baseline prompts that include the original task prompt. Using our framework, we argue that meta-prompting is more effective than basic prompting at generating desirable outputs.","sentences":["Certain statistical models are capable of interpreting input strings as instructions, or prompts, and carry out tasks based on them.","Many approaches to prompting and pre-training these models involve the automated generation of these prompts.","We call these approaches meta-prompting, or prompting to obtain prompts.","We propose a theoretical framework based on category theory to generalize and describe them.","This framework is flexible enough to account for LLM stochasticity; and allows us to obtain formal results around task agnosticity and equivalence of various meta-prompting approaches.","We experiment with meta-prompting in two active areas of model research: creativity and ideation.","We find that user preference favors (p < 0.01) the prompts generated under meta-prompting, as well as their corresponding outputs, over a series of hardcoded baseline prompts that include the original task prompt.","Using our framework, we argue that meta-prompting is more effective than basic prompting at generating desirable outputs."],"url":"http://arxiv.org/abs/2312.06562v1"}
{"created":"2023-12-11 17:46:25","title":"Inferring Hybrid Neural Fluid Fields from Videos","abstract":"We study recovering fluid density and velocity from sparse multiview videos. Existing neural dynamic reconstruction methods predominantly rely on optical flows; therefore, they cannot accurately estimate the density and uncover the underlying velocity due to the inherent visual ambiguities of fluid velocity, as fluids are often shapeless and lack stable visual features. The challenge is further pronounced by the turbulent nature of fluid flows, which calls for properly designed fluid velocity representations. To address these challenges, we propose hybrid neural fluid fields (HyFluid), a neural approach to jointly infer fluid density and velocity fields. Specifically, to deal with visual ambiguities of fluid velocity, we introduce a set of physics-based losses that enforce inferring a physically plausible velocity field, which is divergence-free and drives the transport of density. To deal with the turbulent nature of fluid velocity, we design a hybrid neural velocity representation that includes a base neural velocity field that captures most irrotational energy and a vortex particle-based velocity that models residual turbulent velocity. We show that our method enables recovering vortical flow details. Our approach opens up possibilities for various learning and reconstruction applications centered around 3D incompressible flow, including fluid re-simulation and editing, future prediction, and neural dynamic scene composition. Project website: https://kovenyu.com/HyFluid/","sentences":["We study recovering fluid density and velocity from sparse multiview videos.","Existing neural dynamic reconstruction methods predominantly rely on optical flows; therefore, they cannot accurately estimate the density and uncover the underlying velocity due to the inherent visual ambiguities of fluid velocity, as fluids are often shapeless and lack stable visual features.","The challenge is further pronounced by the turbulent nature of fluid flows, which calls for properly designed fluid velocity representations.","To address these challenges, we propose hybrid neural fluid fields (HyFluid), a neural approach to jointly infer fluid density and velocity fields.","Specifically, to deal with visual ambiguities of fluid velocity, we introduce a set of physics-based losses that enforce inferring a physically plausible velocity field, which is divergence-free and drives the transport of density.","To deal with the turbulent nature of fluid velocity, we design a hybrid neural velocity representation that includes a base neural velocity field that captures most irrotational energy and a vortex particle-based velocity that models residual turbulent velocity.","We show that our method enables recovering vortical flow details.","Our approach opens up possibilities for various learning and reconstruction applications centered around 3D incompressible flow, including fluid re-simulation and editing, future prediction, and neural dynamic scene composition.","Project website: https://kovenyu.com/HyFluid/"],"url":"http://arxiv.org/abs/2312.06561v1"}
{"created":"2023-12-11 17:45:10","title":"Automatic Regularization for Linear MMSE Filters","abstract":"In this work, we consider the problem of regularization in minimum mean-squared error (MMSE) linear filters. Exploiting the relationship with statistical machine learning methods, the regularization parameter is found from the observed signals in a simple and automatic manner. The proposed approach is illustrated through system identification examples, where the automatic regularization yields near-optimal results.","sentences":["In this work, we consider the problem of regularization in minimum mean-squared error (MMSE) linear filters.","Exploiting the relationship with statistical machine learning methods, the regularization parameter is found from the observed signals in a simple and automatic manner.","The proposed approach is illustrated through system identification examples, where the automatic regularization yields near-optimal results."],"url":"http://arxiv.org/abs/2312.06560v1"}
{"created":"2023-12-11 17:43:58","title":"Deep Photonic Reservoir Computer for Speech Recognition","abstract":"Speech recognition is a critical task in the field of artificial intelligence and has witnessed remarkable advancements thanks to large and complex neural networks, whose training process typically requires massive amounts of labeled data and computationally intensive operations. An alternative paradigm, reservoir computing, is energy efficient and is well adapted to implementation in physical substrates, but exhibits limitations in performance when compared to more resource-intensive machine learning algorithms. In this work we address this challenge by investigating different architectures of interconnected reservoirs, all falling under the umbrella of deep reservoir computing. We propose a photonic-based deep reservoir computer and evaluate its effectiveness on different speech recognition tasks. We show specific design choices that aim to simplify the practical implementation of a reservoir computer while simultaneously achieving high-speed processing of high-dimensional audio signals. Overall, with the present work we hope to help the advancement of low-power and high-performance neuromorphic hardware.","sentences":["Speech recognition is a critical task in the field of artificial intelligence and has witnessed remarkable advancements thanks to large and complex neural networks, whose training process typically requires massive amounts of labeled data and computationally intensive operations.","An alternative paradigm, reservoir computing, is energy efficient and is well adapted to implementation in physical substrates, but exhibits limitations in performance when compared to more resource-intensive machine learning algorithms.","In this work we address this challenge by investigating different architectures of interconnected reservoirs, all falling under the umbrella of deep reservoir computing.","We propose a photonic-based deep reservoir computer and evaluate its effectiveness on different speech recognition tasks.","We show specific design choices that aim to simplify the practical implementation of a reservoir computer while simultaneously achieving high-speed processing of high-dimensional audio signals.","Overall, with the present work we hope to help the advancement of low-power and high-performance neuromorphic hardware."],"url":"http://arxiv.org/abs/2312.06558v1"}
{"created":"2023-12-11 17:43:57","title":"Robust Graph Neural Network based on Graph Denoising","abstract":"Graph Neural Networks (GNNs) have emerged as a notorious alternative to address learning problems dealing with non-Euclidean datasets. However, although most works assume that the graph is perfectly known, the observed topology is prone to errors stemming from observational noise, graph-learning limitations, or adversarial attacks. If ignored, these perturbations may drastically hinder the performance of GNNs. To address this limitation, this work proposes a robust implementation of GNNs that explicitly accounts for the presence of perturbations in the observed topology. For any task involving GNNs, our core idea is to i) solve an optimization problem not only over the learnable parameters of the GNN but also over the true graph, and ii) augment the fitting cost with a term accounting for discrepancies on the graph. Specifically, we consider a convolutional GNN based on graph filters and follow an alternating optimization approach to handle the (non-differentiable and constrained) optimization problem by combining gradient descent and projected proximal updates. The resulting algorithm is not limited to a particular type of graph and is amenable to incorporating prior information about the perturbations. Finally, we assess the performance of the proposed method through several numerical experiments.","sentences":["Graph Neural Networks (GNNs) have emerged as a notorious alternative to address learning problems dealing with non-Euclidean datasets.","However, although most works assume that the graph is perfectly known, the observed topology is prone to errors stemming from observational noise, graph-learning limitations, or adversarial attacks.","If ignored, these perturbations may drastically hinder the performance of GNNs.","To address this limitation, this work proposes a robust implementation of GNNs that explicitly accounts for the presence of perturbations in the observed topology.","For any task involving GNNs, our core idea is to i) solve an optimization problem not only over the learnable parameters of the GNN but also over the true graph, and ii) augment the fitting cost with a term accounting for discrepancies on the graph.","Specifically, we consider a convolutional GNN based on graph filters and follow an alternating optimization approach to handle the (non-differentiable and constrained) optimization problem by combining gradient descent and projected proximal updates.","The resulting algorithm is not limited to a particular type of graph and is amenable to incorporating prior information about the perturbations.","Finally, we assess the performance of the proposed method through several numerical experiments."],"url":"http://arxiv.org/abs/2312.06557v1"}
{"created":"2023-12-11 17:41:17","title":"HOI-Diff: Text-Driven Synthesis of 3D Human-Object Interactions using Diffusion Models","abstract":"We address the problem of generating realistic 3D human-object interactions (HOIs) driven by textual prompts. Instead of a single model, our key insight is to take a modular design and decompose the complex task into simpler sub-tasks. We first develop a dual-branch diffusion model (HOI-DM) to generate both human and object motions conditioning on the input text, and encourage coherent motions by a cross-attention communication module between the human and object motion generation branches. We also develop an affordance prediction diffusion model (APDM) to predict the contacting area between the human and object during the interactions driven by the textual prompt. The APDM is independent of the results by the HOI-DM and thus can correct potential errors by the latter. Moreover, it stochastically generates the contacting points to diversify the generated motions. Finally, we incorporate the estimated contacting points into the classifier-guidance to achieve accurate and close contact between humans and objects. To train and evaluate our approach, we annotate BEHAVE dataset with text descriptions. Experimental results demonstrate that our approach is able to produce realistic HOIs with various interactions and different types of objects.","sentences":["We address the problem of generating realistic 3D human-object interactions (HOIs) driven by textual prompts.","Instead of a single model, our key insight is to take a modular design and decompose the complex task into simpler sub-tasks.","We first develop a dual-branch diffusion model (HOI-DM) to generate both human and object motions conditioning on the input text, and encourage coherent motions by a cross-attention communication module between the human and object motion generation branches.","We also develop an affordance prediction diffusion model (APDM) to predict the contacting area between the human and object during the interactions driven by the textual prompt.","The APDM is independent of the results by the HOI-DM and thus can correct potential errors by the latter.","Moreover, it stochastically generates the contacting points to diversify the generated motions.","Finally, we incorporate the estimated contacting points into the classifier-guidance to achieve accurate and close contact between humans and objects.","To train and evaluate our approach, we annotate BEHAVE dataset with text descriptions.","Experimental results demonstrate that our approach is able to produce realistic HOIs with various interactions and different types of objects."],"url":"http://arxiv.org/abs/2312.06553v1"}
{"created":"2023-12-11 17:40:23","title":"Open Data-Driven Automation of Residential Distribution Grid Modeling with Minimal Data Requirements","abstract":"In the present paper, we introduce a new method for the automated generation of residential distribution grid models based on novel building load estimation methods and a two-stage optimization for the generation of the 20 kV and 400 V grid topologies. Using the introduced load estimation methods, various open or proprietary data sources can be utilized to estimate the load of residential buildings. These data sources include available building footprints from OpenStreetMap, 3D building data from OSM Buildings, and the number of electricity meters per address provided by the respective distribution system operator (DSO).   For the evaluation of the introduced methods, we compare the resulting grid models by utilizing different available data sources for a specific suburban residential area and the real grid topology provided by the DSO. This evaluation yields two key findings: First, the automated 20 kV network generation methodology works well when compared to the real network. Second, the utilization of public 3D building data for load estimation significantly increases the resulting model accuracy compared to 2D data and enables results similar to models based on DSO-supplied meter data. This substantially reduces the dependence on such normally proprietary data.","sentences":["In the present paper, we introduce a new method for the automated generation of residential distribution grid models based on novel building load estimation methods and a two-stage optimization for the generation of the 20 kV and 400 V grid topologies.","Using the introduced load estimation methods, various open or proprietary data sources can be utilized to estimate the load of residential buildings.","These data sources include available building footprints from OpenStreetMap, 3D building data from OSM Buildings, and the number of electricity meters per address provided by the respective distribution system operator (DSO).   ","For the evaluation of the introduced methods, we compare the resulting grid models by utilizing different available data sources for a specific suburban residential area and the real grid topology provided by the DSO.","This evaluation yields two key findings:","First, the automated 20 kV network generation methodology works well when compared to the real network.","Second, the utilization of public 3D building data for load estimation significantly increases the resulting model accuracy compared to 2D data and enables results similar to models based on DSO-supplied meter data.","This substantially reduces the dependence on such normally proprietary data."],"url":"http://arxiv.org/abs/2312.06552v1"}
{"created":"2023-12-11 17:39:40","title":"Successive Bayesian Reconstructor for Channel Estimation in Flexible Antenna Systems","abstract":"Flexible antenna systems (FASs) can reconfigure their locations freely within a spatially continuous space. To keep favorable antenna positions, the channel state information (CSI) acquisition for FASs is essential. While some techniques have been proposed, most existing FAS channel estimators require several channel assumptions, such as slow variation and angular-domain sparsity. When these assumptions are not reasonable, the model mismatch may lead to unpredictable performance loss. In this paper, we propose the successive Bayesian reconstructor (S-BAR) as a general solution to estimate FAS channels. Unlike model-based estimators, the proposed S-BAR is prior-aided, which builds the experiential kernel for CSI acquisition. Inspired by Bayesian regression, the key idea of S-BAR is to model the FAS channels as a stochastic process, whose uncertainty can be successively eliminated by kernel-based sampling and regression. In this way, the predictive mean of the regressed stochastic process can be viewed as the maximum a posterior (MAP) estimator of FAS channels. Simulation results verify that, in both model-mismatched and model-matched cases, the proposed S-BAR can achieve higher estimation accuracy than the existing schemes.","sentences":["Flexible antenna systems (FASs) can reconfigure their locations freely within a spatially continuous space.","To keep favorable antenna positions, the channel state information (CSI) acquisition for FASs is essential.","While some techniques have been proposed, most existing FAS channel estimators require several channel assumptions, such as slow variation and angular-domain sparsity.","When these assumptions are not reasonable, the model mismatch may lead to unpredictable performance loss.","In this paper, we propose the successive Bayesian reconstructor (S-BAR) as a general solution to estimate FAS channels.","Unlike model-based estimators, the proposed S-BAR is prior-aided, which builds the experiential kernel for CSI acquisition.","Inspired by Bayesian regression, the key idea of S-BAR is to model the FAS channels as a stochastic process, whose uncertainty can be successively eliminated by kernel-based sampling and regression.","In this way, the predictive mean of the regressed stochastic process can be viewed as the maximum a posterior (MAP) estimator of FAS channels.","Simulation results verify that, in both model-mismatched and model-matched cases, the proposed S-BAR can achieve higher estimation accuracy than the existing schemes."],"url":"http://arxiv.org/abs/2312.06551v1"}
{"created":"2023-12-11 17:39:00","title":"LLM360: Towards Fully Transparent Open-Source LLMs","abstract":"The recent surge in open-source Large Language Models (LLMs), such as LLaMA, Falcon, and Mistral, provides diverse options for AI practitioners and researchers. However, most LLMs have only released partial artifacts, such as the final model weights or inference code, and technical reports increasingly limit their scope to high-level design choices and surface statistics. These choices hinder progress in the field by degrading transparency into the training of LLMs and forcing teams to rediscover many details in the training process. We present LLM360, an initiative to fully open-source LLMs, which advocates for all training code and data, model checkpoints, and intermediate results to be made available to the community. The goal of LLM360 is to support open and collaborative AI research by making the end-to-end LLM training process transparent and reproducible by everyone. As a first step of LLM360, we release two 7B parameter LLMs pre-trained from scratch, Amber and CrystalCoder, including their training code, data, intermediate checkpoints, and analyses (at https://www.llm360.ai). We are committed to continually pushing the boundaries of LLMs through this open-source effort. More large-scale and stronger models are underway and will be released in the future.","sentences":["The recent surge in open-source Large Language Models (LLMs), such as LLaMA, Falcon, and Mistral, provides diverse options for AI practitioners and researchers.","However, most LLMs have only released partial artifacts, such as the final model weights or inference code, and technical reports increasingly limit their scope to high-level design choices and surface statistics.","These choices hinder progress in the field by degrading transparency into the training of LLMs and forcing teams to rediscover many details in the training process.","We present LLM360, an initiative to fully open-source LLMs, which advocates for all training code and data, model checkpoints, and intermediate results to be made available to the community.","The goal of LLM360 is to support open and collaborative AI research by making the end-to-end LLM training process transparent and reproducible by everyone.","As a first step of LLM360, we release two 7B parameter LLMs pre-trained from scratch, Amber and CrystalCoder, including their training code, data, intermediate checkpoints, and analyses (at https://www.llm360.ai).","We are committed to continually pushing the boundaries of LLMs through this open-source effort.","More large-scale and stronger models are underway and will be released in the future."],"url":"http://arxiv.org/abs/2312.06550v1"}
{"created":"2023-12-11 17:38:08","title":"Exploring Crowd Dynamics: Simulating Structured Behaviors through Crowd Simulation Models","abstract":"This paper proposes the simulation of structured behaviors in a crowd of virtual agents by extending the BioCrowds simulation model.   Three behaviors were simulated and evaluated, a queue as a generic case and two specific behaviors observed at rock concerts. The extended model incorporates new parameters and modifications to replicate these behaviors accurately. Experiments were conducted to analyze the impact of parameters on simulation results, and computational performance was considered.   The results demonstrate the model's effectiveness in simulating structured behaviors and its potential for replicating complex social phenomena in diverse scenarios.","sentences":["This paper proposes the simulation of structured behaviors in a crowd of virtual agents by extending the BioCrowds simulation model.   ","Three behaviors were simulated and evaluated, a queue as a generic case and two specific behaviors observed at rock concerts.","The extended model incorporates new parameters and modifications to replicate these behaviors accurately.","Experiments were conducted to analyze the impact of parameters on simulation results, and computational performance was considered.   ","The results demonstrate the model's effectiveness in simulating structured behaviors and its potential for replicating complex social phenomena in diverse scenarios."],"url":"http://arxiv.org/abs/2312.06549v1"}
{"created":"2023-12-11 17:31:46","title":"Unsupervised KPIs-Based Clustering of Jobs in HPC Data Centers","abstract":"Performance analysis is an essential task in High-Performance Computing (HPC) systems and it is applied for different purposes such as anomaly detection, optimal resource allocation, and budget planning. HPC monitoring tasks generate a huge number of Key Performance Indicators (KPIs) to supervise the status of the jobs running in these systems. KPIs give data about CPU usage, memory usage, network (interface) traffic, or other sensors that monitor the hardware. Analyzing this data, it is possible to obtain insightful information about running jobs, such as their characteristics, performance, and failures. The main contribution in this paper is to identify which metric/s (KPIs) is/are the most appropriate to identify/classify different types of jobs according to their behavior in the HPC system. With this aim, we have applied different clustering techniques (partition and hierarchical clustering algorithms) using a real dataset from the Galician Computation Center (CESGA). We have concluded that (i) those metrics (KPIs) related to the Network (interface) traffic monitoring provide the best cohesion and separation to cluster HPC jobs, and (ii) hierarchical clustering algorithms are the most suitable for this task. Our approach was validated using a different real dataset from the same HPC center.","sentences":["Performance analysis is an essential task in High-Performance Computing (HPC) systems and it is applied for different purposes such as anomaly detection, optimal resource allocation, and budget planning.","HPC monitoring tasks generate a huge number of Key Performance Indicators (KPIs) to supervise the status of the jobs running in these systems.","KPIs give data about CPU usage, memory usage, network (interface) traffic, or other sensors that monitor the hardware.","Analyzing this data, it is possible to obtain insightful information about running jobs, such as their characteristics, performance, and failures.","The main contribution in this paper is to identify which metric/s (KPIs) is/are the most appropriate to identify/classify different types of jobs according to their behavior in the HPC system.","With this aim, we have applied different clustering techniques (partition and hierarchical clustering algorithms) using a real dataset from the Galician Computation Center (CESGA).","We have concluded that (i) those metrics (KPIs) related to the Network (interface) traffic monitoring provide the best cohesion and separation to cluster HPC jobs, and (ii) hierarchical clustering algorithms are the most suitable for this task.","Our approach was validated using a different real dataset from the same HPC center."],"url":"http://arxiv.org/abs/2312.06546v1"}
{"created":"2023-12-11 17:27:19","title":"Complexity Evaluation of Parallel Execution of the RAPiD Deep-Learning Algorithm on Intel CPU","abstract":"Knowing how many and where are people in various indoor spaces is critical for reducing HVAC energy waste, space management, spatial analytics and in emergency scenarios. While a range of technologies have been proposed to detect and track people in large indoor spaces, ceiling-mounted fisheye cameras have recently emerged as strong contenders. Currently, RAPiD is the SOTA algorithm for people detection in images captured by fisheye cameras. However, in large spaces several overhead fisheye cameras are needed to assure high accuracy of counting and thus multiple instances of RAPiD must be executed simultaneously. This report evaluates inference time when multiple instances of RAPiD run in parallel on an Ubuntu NUC PC with Intel I7 8559U CPU. We consider three mechanisms of CPU-resource allocation to handle multiple instances of RAPiD: 1) managed by Ubuntu, 2) managed by user via operating-system calls to assign logical cores, and 3) managed by user via PyTorch-library calls to limit the number of threads used by PyTorch. Each scenario was evaluated on 300 images. The experimental results show, that when one or two instances of RAPiD are executed in parallel all three approaches result in similar inference times of 1.8sec and 3.2sec, respectively. However, when three or more instances of RAPiD run in parallel, limiting the number of threads used by PyTorch results in the shortest inference times. On average, RAPiD completes inference of 2 images simultaneously in about 3sec, 4 images in 6sec and 8 images in less than 14sec. This is important for real-time system design. In HVAC-application scenarios, with a typical reaction time of 10-15min, a latency of 14sec is negligible so a single 8559U CPU can support 8 camera streams thus reducing the system cost. However, in emergency scenarios, when time is of essence, a single CPU may be needed for each camera to reduce the latency to 1.8sec.","sentences":["Knowing how many and where are people in various indoor spaces is critical for reducing HVAC energy waste, space management, spatial analytics and in emergency scenarios.","While a range of technologies have been proposed to detect and track people in large indoor spaces, ceiling-mounted fisheye cameras have recently emerged as strong contenders.","Currently, RAPiD is the SOTA algorithm for people detection in images captured by fisheye cameras.","However, in large spaces several overhead fisheye cameras are needed to assure high accuracy of counting and thus multiple instances of RAPiD must be executed simultaneously.","This report evaluates inference time when multiple instances of RAPiD run in parallel on an Ubuntu NUC PC with Intel I7 8559U CPU.","We consider three mechanisms of CPU-resource allocation to handle multiple instances of RAPiD: 1) managed by Ubuntu, 2) managed by user via operating-system calls to assign logical cores, and 3) managed by user via PyTorch-library calls to limit the number of threads used by PyTorch.","Each scenario was evaluated on 300 images.","The experimental results show, that when one or two instances of RAPiD are executed in parallel all three approaches result in similar inference times of 1.8sec and 3.2sec, respectively.","However, when three or more instances of RAPiD run in parallel, limiting the number of threads used by PyTorch results in the shortest inference times.","On average, RAPiD completes inference of 2 images simultaneously in about 3sec, 4 images in 6sec and 8 images in less than 14sec.","This is important for real-time system design.","In HVAC-application scenarios, with a typical reaction time of 10-15min, a latency of 14sec is negligible so a single 8559U CPU can support 8 camera streams thus reducing the system cost.","However, in emergency scenarios, when time is of essence, a single CPU may be needed for each camera to reduce the latency to 1.8sec."],"url":"http://arxiv.org/abs/2312.06544v1"}
{"created":"2023-12-11 17:18:23","title":"Ray-Tracing With a Coherent Ray-Space Hierarchy","abstract":"We present an algorithm for creating an n-level ray-space hierarchy (RSH) of coherent rays that runs on the GPU. Our algorithm uses rasterization to process the primary rays, then uses those results as the inputs for a RSH, that processes the secondary rays. The RSH algorithm generates bundles of rays; hashes them, according to their attributes; and sorts them. Thus we generate a ray list with adjacent coherent rays. To improve the rendering performance of the RSH vs a more classical approach. In addition the scenes geometry is partitioned into a set of bounding spheres, intersected with the RSH, to further decrease the amount of false ray bundle-primitive intersection tests. We show that our technique notably reduces the amount of ray-primitive intersection tests, required to render an image. In particular, it performs up to 50% better in this metric than other algorithms in this class.","sentences":["We present an algorithm for creating an n-level ray-space hierarchy (RSH) of coherent rays that runs on the GPU.","Our algorithm uses rasterization to process the primary rays, then uses those results as the inputs for a RSH, that processes the secondary rays.","The RSH algorithm generates bundles of rays; hashes them, according to their attributes; and sorts them.","Thus we generate a ray list with adjacent coherent rays.","To improve the rendering performance of the RSH vs a more classical approach.","In addition the scenes geometry is partitioned into a set of bounding spheres, intersected with the RSH, to further decrease the amount of false ray bundle-primitive intersection tests.","We show that our technique notably reduces the amount of ray-primitive intersection tests, required to render an image.","In particular, it performs up to 50% better in this metric than other algorithms in this class."],"url":"http://arxiv.org/abs/2312.06538v1"}
{"created":"2023-12-11 17:13:54","title":"KPIs-Based Clustering and Visualization of HPC jobs: a Feature Reduction Approach","abstract":"High-Performance Computing (HPC) systems need to be constantly monitored to ensure their stability. The monitoring systems collect a tremendous amount of data about different parameters or Key Performance Indicators (KPIs), such as resource usage, IO waiting time, etc. A proper analysis of this data, usually stored as time series, can provide insight in choosing the right management strategies as well as the early detection of issues. In this paper, we introduce a methodology to cluster HPC jobs according to their KPI indicators. Our approach reduces the inherent high dimensionality of the collected data by applying two techniques to the time series: literature-based and variance-based feature extraction. We also define a procedure to visualize the obtained clusters by combining the two previous approaches and the Principal Component Analysis (PCA). Finally, we have validated our contributions on a real data set to conclude that those KPIs related to CPU usage provide the best cohesion and separation for clustering analysis and the good results of our visualization methodology.","sentences":["High-Performance Computing (HPC) systems need to be constantly monitored to ensure their stability.","The monitoring systems collect a tremendous amount of data about different parameters or Key Performance Indicators (KPIs), such as resource usage, IO waiting time, etc.","A proper analysis of this data, usually stored as time series, can provide insight in choosing the right management strategies as well as the early detection of issues.","In this paper, we introduce a methodology to cluster HPC jobs according to their KPI indicators.","Our approach reduces the inherent high dimensionality of the collected data by applying two techniques to the time series: literature-based and variance-based feature extraction.","We also define a procedure to visualize the obtained clusters by combining the two previous approaches and the Principal Component Analysis (PCA).","Finally, we have validated our contributions on a real data set to conclude that those KPIs related to CPU usage provide the best cohesion and separation for clustering analysis and the good results of our visualization methodology."],"url":"http://arxiv.org/abs/2312.06534v1"}
{"created":"2023-12-11 17:08:05","title":"Study of Non-Verbal Behavior in Conversational Agents","abstract":"This paper studies the non-verbal behavior of a conversational agent named Arthur. We propose the development of body movements for this agent, which interacts solely through voice commands, chat, and videos with facial animations. This research aims to analyze users' perceptions regarding the gestures performed by Arthur. This study was conducted with participants who agreed to interact directly or through video with the conversational agent. The main goal is to analyze whether including nonverbal movements alters users' perception so that they feel more comfortable watching the video or interacting in real-time.","sentences":["This paper studies the non-verbal behavior of a conversational agent named Arthur.","We propose the development of body movements for this agent, which interacts solely through voice commands, chat, and videos with facial animations.","This research aims to analyze users' perceptions regarding the gestures performed by Arthur.","This study was conducted with participants who agreed to interact directly or through video with the conversational agent.","The main goal is to analyze whether including nonverbal movements alters users' perception so that they feel more comfortable watching the video or interacting in real-time."],"url":"http://arxiv.org/abs/2312.06530v1"}
{"created":"2023-12-11 17:05:25","title":"Transformers Implement Functional Gradient Descent to Learn Non-Linear Functions In Context","abstract":"Many neural network architectures have been shown to be Turing Complete, and can thus implement arbitrary algorithms. However, Transformers are unique in that they can implement gradient-based learning algorithms \\emph{under simple parameter configurations}. A line of recent work shows that linear Transformers naturally learn to implement gradient descent (GD) when trained on a linear regression in-context learning task. But the linearity assumption (either in the Transformer architecture or in the learning task) is far from realistic settings where non-linear activations crucially enable Transformers to learn complicated non-linear functions. In this paper, we provide theoretical and empirical evidence that non-linear Transformers can, and \\emph{in fact do}, learn to implement learning algorithms to learn non-linear functions in context. Our results apply to a broad class of combinations of non-linear architectures, and non-linear in-context learning tasks. Interestingly, we show that the optimal choice of non-linear activation depends in a natural way on the non-linearity of the learning task.","sentences":["Many neural network architectures have been shown to be Turing Complete, and can thus implement arbitrary algorithms.","However, Transformers are unique in that they can implement gradient-based learning algorithms \\emph{under simple parameter configurations}.","A line of recent work shows that linear Transformers naturally learn to implement gradient descent (GD) when trained on a linear regression in-context learning task.","But the linearity assumption (either in the Transformer architecture or in the learning task) is far from realistic settings where non-linear activations crucially enable Transformers to learn complicated non-linear functions.","In this paper, we provide theoretical and empirical evidence that non-linear Transformers can, and \\emph{in fact do}, learn to implement learning algorithms to learn non-linear functions in context.","Our results apply to a broad class of combinations of non-linear architectures, and non-linear in-context learning tasks.","Interestingly, we show that the optimal choice of non-linear activation depends in a natural way on the non-linearity of the learning task."],"url":"http://arxiv.org/abs/2312.06528v1"}
{"created":"2023-12-11 17:04:30","title":"Can Reinforcement Learning support policy makers? A preliminary study with Integrated Assessment Models","abstract":"Governments around the world aspire to ground decision-making on evidence. Many of the foundations of policy making - e.g. sensing patterns that relate to societal needs, developing evidence-based programs, forecasting potential outcomes of policy changes, and monitoring effectiveness of policy programs - have the potential to benefit from the use of large-scale datasets or simulations together with intelligent algorithms. These could, if designed and deployed in a way that is well grounded on scientific evidence, enable a more comprehensive, faster, and rigorous approach to policy making. Integrated Assessment Models (IAM) is a broad umbrella covering scientific models that attempt to link main features of society and economy with the biosphere into one modelling framework. At present, these systems are probed by policy makers and advisory groups in a hypothesis-driven manner. In this paper, we empirically demonstrate that modern Reinforcement Learning can be used to probe IAMs and explore the space of solutions in a more principled manner. While the implication of our results are modest since the environment is simplistic, we believe that this is a stepping stone towards more ambitious use cases, which could allow for effective exploration of policies and understanding of their consequences and limitations.","sentences":["Governments around the world aspire to ground decision-making on evidence.","Many of the foundations of policy making - e.g. sensing patterns that relate to societal needs, developing evidence-based programs, forecasting potential outcomes of policy changes, and monitoring effectiveness of policy programs - have the potential to benefit from the use of large-scale datasets or simulations together with intelligent algorithms.","These could, if designed and deployed in a way that is well grounded on scientific evidence, enable a more comprehensive, faster, and rigorous approach to policy making.","Integrated Assessment Models (IAM) is a broad umbrella covering scientific models that attempt to link main features of society and economy with the biosphere into one modelling framework.","At present, these systems are probed by policy makers and advisory groups in a hypothesis-driven manner.","In this paper, we empirically demonstrate that modern Reinforcement Learning can be used to probe IAMs and explore the space of solutions in a more principled manner.","While the implication of our results are modest since the environment is simplistic, we believe that this is a stepping stone towards more ambitious use cases, which could allow for effective exploration of policies and understanding of their consequences and limitations."],"url":"http://arxiv.org/abs/2312.06527v1"}
{"created":"2023-12-11 17:00:35","title":"Label Smoothing for Enhanced Text Sentiment Classification","abstract":"Label smoothing is a widely used technique in various domains, such as image classification and speech recognition, known for effectively combating model overfitting. However, there is few research on its application to text sentiment classification. To fill in the gap, this study investigates the implementation of label smoothing for sentiment classification by utilizing different levels of smoothing. The primary objective is to enhance sentiment classification accuracy by transforming discrete labels into smoothed label distributions. Through extensive experiments, we demonstrate the superior performance of label smoothing in text sentiment classification tasks across eight diverse datasets and deep learning architectures: TextCNN, BERT, and RoBERTa, under two learning schemes: training from scratch and fine-tuning.","sentences":["Label smoothing is a widely used technique in various domains, such as image classification and speech recognition, known for effectively combating model overfitting.","However, there is few research on its application to text sentiment classification.","To fill in the gap, this study investigates the implementation of label smoothing for sentiment classification by utilizing different levels of smoothing.","The primary objective is to enhance sentiment classification accuracy by transforming discrete labels into smoothed label distributions.","Through extensive experiments, we demonstrate the superior performance of label smoothing in text sentiment classification tasks across eight diverse datasets and deep learning architectures: TextCNN, BERT, and RoBERTa, under two learning schemes: training from scratch and fine-tuning."],"url":"http://arxiv.org/abs/2312.06522v1"}
{"created":"2023-12-11 16:52:20","title":"A GAN Approach for Node Embedding in Heterogeneous Graphs Using Subgraph Sampling","abstract":"Our research addresses class imbalance issues in heterogeneous graphs using graph neural networks (GNNs). We propose a novel method combining the strengths of Generative Adversarial Networks (GANs) with GNNs, creating synthetic nodes and edges that effectively balance the dataset. This approach directly targets and rectifies imbalances at the data level. The proposed framework resolves issues such as neglecting graph structures during data generation and creating synthetic structures usable with GNN-based classifiers in downstream tasks. It processes node and edge information concurrently, improving edge balance through node augmentation and subgraph sampling. Additionally, our framework integrates a threshold strategy, aiding in determining optimal edge thresholds during training without time-consuming parameter adjustments. Experiments on the Amazon and Yelp Review datasets highlight the effectiveness of the framework we proposed, especially in minority node identification, where it consistently outperforms baseline models across key performance metrics, demonstrating its potential in the field.","sentences":["Our research addresses class imbalance issues in heterogeneous graphs using graph neural networks (GNNs).","We propose a novel method combining the strengths of Generative Adversarial Networks (GANs) with GNNs, creating synthetic nodes and edges that effectively balance the dataset.","This approach directly targets and rectifies imbalances at the data level.","The proposed framework resolves issues such as neglecting graph structures during data generation and creating synthetic structures usable with GNN-based classifiers in downstream tasks.","It processes node and edge information concurrently, improving edge balance through node augmentation and subgraph sampling.","Additionally, our framework integrates a threshold strategy, aiding in determining optimal edge thresholds during training without time-consuming parameter adjustments.","Experiments on the Amazon and Yelp Review datasets highlight the effectiveness of the framework we proposed, especially in minority node identification, where it consistently outperforms baseline models across key performance metrics, demonstrating its potential in the field."],"url":"http://arxiv.org/abs/2312.06519v1"}
{"created":"2023-12-11 16:50:14","title":"Decoupling Meta-Reinforcement Learning with Gaussian Task Contexts and Skills","abstract":"Offline meta-reinforcement learning (meta-RL) methods, which adapt to unseen target tasks with prior experience, are essential in robot control tasks. Current methods typically utilize task contexts and skills as prior experience, where task contexts are related to the information within each task and skills represent a set of temporally extended actions for solving subtasks. However, these methods still suffer from limited performance when adapting to unseen target tasks, mainly because the learned prior experience lacks generalization, i.e., they are unable to extract effective prior experience from meta-training tasks by exploration and learning of continuous latent spaces. We propose a framework called decoupled meta-reinforcement learning (DCMRL), which (1) contrastively restricts the learning of task contexts through pulling in similar task contexts within the same task and pushing away different task contexts of different tasks, and (2) utilizes a Gaussian quantization variational autoencoder (GQ-VAE) for clustering the Gaussian distributions of the task contexts and skills respectively, and decoupling the exploration and learning processes of their spaces. These cluster centers which serve as representative and discrete distributions of task context and skill are stored in task context codebook and skill codebook, respectively. DCMRL can acquire generalizable prior experience and achieve effective adaptation to unseen target tasks during the meta-testing phase. Experiments in the navigation and robot manipulation continuous control tasks show that DCMRL is more effective than previous meta-RL methods with more generalizable prior experience.","sentences":["Offline meta-reinforcement learning (meta-RL) methods, which adapt to unseen target tasks with prior experience, are essential in robot control tasks.","Current methods typically utilize task contexts and skills as prior experience, where task contexts are related to the information within each task and skills represent a set of temporally extended actions for solving subtasks.","However, these methods still suffer from limited performance when adapting to unseen target tasks, mainly because the learned prior experience lacks generalization, i.e., they are unable to extract effective prior experience from meta-training tasks by exploration and learning of continuous latent spaces.","We propose a framework called decoupled meta-reinforcement learning (DCMRL), which (1) contrastively restricts the learning of task contexts through pulling in similar task contexts within the same task and pushing away different task contexts of different tasks, and (2) utilizes a Gaussian quantization variational autoencoder (GQ-VAE) for clustering the Gaussian distributions of the task contexts and skills respectively, and decoupling the exploration and learning processes of their spaces.","These cluster centers which serve as representative and discrete distributions of task context and skill are stored in task context codebook and skill codebook, respectively.","DCMRL can acquire generalizable prior experience and achieve effective adaptation to unseen target tasks during the meta-testing phase.","Experiments in the navigation and robot manipulation continuous control tasks show that DCMRL is more effective than previous meta-RL methods with more generalizable prior experience."],"url":"http://arxiv.org/abs/2312.06518v1"}
{"created":"2023-12-11 16:46:57","title":"Facilitating Digital Agriculture with Simple Databases","abstract":"As an on-ramp to databases, we offer several well-structured private database templates as open source resources for agriculturalists, particularly those with modest spreadsheet skills. These farmer-oriented Air table databases use simple data-validated forms, with the look and feel of a customized app, to yield operational data that is tidy, machine- and human-readable, editable, and exportable for analysis in other software. Such data can facilitate logistics, provide contextual metadata, and improve enterprise analysis. A recorded workshop explaining how to build a database for activity records is presented. These resources may facilitate infusion of digital agriculture principles through Extension and structured educational programming.","sentences":["As an on-ramp to databases, we offer several well-structured private database templates as open source resources for agriculturalists, particularly those with modest spreadsheet skills.","These farmer-oriented Air table databases use simple data-validated forms, with the look and feel of a customized app, to yield operational data that is tidy, machine- and human-readable, editable, and exportable for analysis in other software.","Such data can facilitate logistics, provide contextual metadata, and improve enterprise analysis.","A recorded workshop explaining how to build a database for activity records is presented.","These resources may facilitate infusion of digital agriculture principles through Extension and structured educational programming."],"url":"http://arxiv.org/abs/2312.06517v1"}
{"created":"2023-12-11 16:46:43","title":"Irregular Repetition Slotted Aloha with Multipacket Detection: A Density Evolution Analysis","abstract":"Irregular repetition slotted Aloha (IRSA) has shown significant advantages as a modern technique for uncoordinated random access with massive number of users due to its capability of achieving theoretically a throughput of $1$ packet per slot. When the receiver has also the multi-packet reception of multi-user (MUD) detection property, by applying successive interference cancellation, IRSA also obtains very low packet loss probabilities at low traffic loads, but is unable in general to achieve a normalized throughput close to the $1$. In this paper, we reconsider the case of IRSA with $k$-MUD receivers and derive the general density evolution equations for the non-asymptotic analysis of the packet loss rate, for arbitrary frame lengths and two variants of the first slot used for transmission. Next, using the potential function, we give new capacity bounds on the capacity of the system, showing the threshold arrival rate for zero decoding error probability. Our numerical results illustrate performance in terms of throughput and average delay for $k$-MUD IRSA with finite memory at the receiver, and also with bounded maximum delay.","sentences":["Irregular repetition slotted Aloha (IRSA) has shown significant advantages as a modern technique for uncoordinated random access with massive number of users due to its capability of achieving theoretically a throughput of $1$ packet per slot.","When the receiver has also the multi-packet reception of multi-user (MUD) detection property, by applying successive interference cancellation, IRSA also obtains very low packet loss probabilities at low traffic loads, but is unable in general to achieve a normalized throughput close to the $1$. In this paper, we reconsider the case of IRSA with $k$-MUD receivers and derive the general density evolution equations for the non-asymptotic analysis of the packet loss rate, for arbitrary frame lengths and two variants of the first slot used for transmission.","Next, using the potential function, we give new capacity bounds on the capacity of the system, showing the threshold arrival rate for zero decoding error probability.","Our numerical results illustrate performance in terms of throughput and average delay for $k$-MUD IRSA with finite memory at the receiver, and also with bounded maximum delay."],"url":"http://arxiv.org/abs/2312.06516v1"}
{"created":"2023-12-11 16:44:19","title":"A Golden-Free Formal Method for Trojan Detection in Non-Interfering Accelerators","abstract":"The threat of hardware Trojans (HTs) in security-critical IPs like cryptographic accelerators poses severe security risks. The HT detection methods available today mostly rely on golden models and detailed circuit specifications. Often they are specific to certain HT payload types, making pre-silicon verification difficult and leading to security gaps. We propose a novel formal verification method for HT detection in non-interfering accelerators at the Register Transfer Level (RTL), employing standard formal property checking. Our method guarantees the exhaustive detection of any sequential HT independently of its payload behavior, including physical side channels. It does not require a golden model or a functional specification of the design. The experimental results demonstrate efficient and effective detection of all sequential HTs in accelerators available on Trust-Hub, including those with complex triggers and payloads.","sentences":["The threat of hardware Trojans (HTs) in security-critical IPs like cryptographic accelerators poses severe security risks.","The HT detection methods available today mostly rely on golden models and detailed circuit specifications.","Often they are specific to certain HT payload types, making pre-silicon verification difficult and leading to security gaps.","We propose a novel formal verification method for HT detection in non-interfering accelerators at the Register Transfer Level (RTL), employing standard formal property checking.","Our method guarantees the exhaustive detection of any sequential HT independently of its payload behavior, including physical side channels.","It does not require a golden model or a functional specification of the design.","The experimental results demonstrate efficient and effective detection of all sequential HTs in accelerators available on Trust-Hub, including those with complex triggers and payloads."],"url":"http://arxiv.org/abs/2312.06515v1"}
{"created":"2023-12-11 16:39:52","title":"Where exactly does contextualization in a PLM happen?","abstract":"Pre-trained Language Models (PLMs) have shown to be consistently successful in a plethora of NLP tasks due to their ability to learn contextualized representations of words (Ethayarajh, 2019). BERT (Devlin et al., 2018), ELMo (Peters et al., 2018) and other PLMs encode word meaning via textual context, as opposed to static word embeddings, which encode all meanings of a word in a single vector representation. In this work, we present a study that aims to localize where exactly in a PLM word contextualization happens. In order to find the location of this word meaning transformation, we investigate representations of polysemous words in the basic BERT uncased 12 layer architecture (Devlin et al., 2018), a masked language model trained on an additional sentence adjacency objective, using qualitative and quantitative measures.","sentences":["Pre-trained Language Models (PLMs) have shown to be consistently successful in a plethora of NLP tasks due to their ability to learn contextualized representations of words (Ethayarajh, 2019).","BERT (Devlin et al., 2018), ELMo (Peters et al., 2018) and other PLMs encode word meaning via textual context, as opposed to static word embeddings, which encode all meanings of a word in a single vector representation.","In this work, we present a study that aims to localize where exactly in a PLM word contextualization happens.","In order to find the location of this word meaning transformation, we investigate representations of polysemous words in the basic BERT uncased 12 layer architecture (Devlin et al., 2018), a masked language model trained on an additional sentence adjacency objective, using qualitative and quantitative measures."],"url":"http://arxiv.org/abs/2312.06514v1"}
{"created":"2023-12-11 16:39:11","title":"Stoch BiRo: Design and Control of a low cost bipedal robot","abstract":"This paper introduces the Stoch BiRo, a cost-effective bipedal robot designed with a modular mechanical structure having point feet to navigate uneven and unfamiliar terrains. The robot employs proprioceptive actuation in abduction, hips, and knees, leveraging a Raspberry Pi4 for control. Overcoming computational limitations, a Learning-based Linear Policy controller manages balance and locomotion with only 3 degrees of freedom (DoF) per leg, distinct from the typical 5DoF in bipedal systems. Integrated within a modular control architecture, these controllers enable autonomous handling of unforeseen terrain disturbances without external sensors or prior environment knowledge. The robot's policies are trained and simulated using MuJoCo, transferring learned behaviors to the Stoch BiRo hardware for initial walking validations. This work highlights the Stoch BiRo's adaptability and cost-effectiveness in mechanical design, control strategies, and autonomous navigation, promising diverse applications in real-world robotics scenarios.","sentences":["This paper introduces the Stoch BiRo, a cost-effective bipedal robot designed with a modular mechanical structure having point feet to navigate uneven and unfamiliar terrains.","The robot employs proprioceptive actuation in abduction, hips, and knees, leveraging a Raspberry Pi4 for control.","Overcoming computational limitations, a Learning-based Linear Policy controller manages balance and locomotion with only 3 degrees of freedom (DoF) per leg, distinct from the typical 5DoF in bipedal systems.","Integrated within a modular control architecture, these controllers enable autonomous handling of unforeseen terrain disturbances without external sensors or prior environment knowledge.","The robot's policies are trained and simulated using MuJoCo, transferring learned behaviors to the Stoch BiRo hardware for initial walking validations.","This work highlights the Stoch BiRo's adaptability and cost-effectiveness in mechanical design, control strategies, and autonomous navigation, promising diverse applications in real-world robotics scenarios."],"url":"http://arxiv.org/abs/2312.06512v1"}
{"created":"2023-12-11 16:38:13","title":"Trusting a Smart Contract Means Trusting Its Owners: Understanding Centralization Risk","abstract":"Smart contract access control mechanisms can introduce centralization into supposedly decentralized ecosystems. In our view, such centralization is an overlooked risk of smart contracts that underlies well-known smart contract security incidents. Critically, mitigating the known vulnerability of missing permission verification by implementing authorization patterns can in turn introduce centralization. To delineate the issue, we define centralization risk and describe smart contract source code patterns for Ethereum and Algorand that can introduce it to smart contracts. We explain under which circumstances the centralization can be exploited. Finally, we discuss implications of centralization risk for different smart contract stakeholders.","sentences":["Smart contract access control mechanisms can introduce centralization into supposedly decentralized ecosystems.","In our view, such centralization is an overlooked risk of smart contracts that underlies well-known smart contract security incidents.","Critically, mitigating the known vulnerability of missing permission verification by implementing authorization patterns can in turn introduce centralization.","To delineate the issue, we define centralization risk and describe smart contract source code patterns for Ethereum and Algorand that can introduce it to smart contracts.","We explain under which circumstances the centralization can be exploited.","Finally, we discuss implications of centralization risk for different smart contract stakeholders."],"url":"http://arxiv.org/abs/2312.06510v1"}
{"created":"2023-12-11 16:32:09","title":"The Directed Van Kampen Theorem in Lean","abstract":"Directed topology is an area of mathematics with applications in concurrency. It extends the concept of a topological space by adding a notion of directedness, which restricts how paths can evolve through a space and enables thereby a faithful representation of computation with their direction. In this paper, we present a Lean formalisation of directed spaces and a Van Kampen theorem for them. This theorem allows the calculation of the homotopy type of a space by combining local knowledge the homotopy type of subspaces. With this theorem, the reasoning about spaces can be reduced to subspaces and, by representing concurrent systems as directed spaces, we can reduce the deduction of properties of a composed system to that of subsystems. The formalisation in Lean can serve to support computer-assisted reasoning about the behaviour of concurrent systems.","sentences":["Directed topology is an area of mathematics with applications in concurrency.","It extends the concept of a topological space by adding a notion of directedness, which restricts how paths can evolve through a space and enables thereby a faithful representation of computation with their direction.","In this paper, we present a Lean formalisation of directed spaces and a Van Kampen theorem for them.","This theorem allows the calculation of the homotopy type of a space by combining local knowledge the homotopy type of subspaces.","With this theorem, the reasoning about spaces can be reduced to subspaces and, by representing concurrent systems as directed spaces, we can reduce the deduction of properties of a composed system to that of subsystems.","The formalisation in Lean can serve to support computer-assisted reasoning about the behaviour of concurrent systems."],"url":"http://arxiv.org/abs/2312.06506v1"}
{"created":"2023-12-11 16:31:55","title":"Grounded Question-Answering in Long Egocentric Videos","abstract":"Existing approaches to video understanding, mainly designed for short videos from a third-person perspective, are limited in their applicability in certain fields, such as robotics. In this paper, we delve into open-ended question-answering (QA) in long, egocentric videos, which allows individuals or robots to inquire about their own past visual experiences. This task presents unique challenges, including the complexity of temporally grounding queries within extensive video content, the high resource demands for precise data annotation, and the inherent difficulty of evaluating open-ended answers due to their ambiguous nature. Our proposed approach tackles these challenges by (i) integrating query grounding and answering within a unified model to reduce error propagation; (ii) employing large language models for efficient and scalable data synthesis; and (iii) introducing a close-ended QA task for evaluation, to manage answer ambiguity. Extensive experiments demonstrate the effectiveness of our method, which also achieves state-of-the-art performance on the QAEgo4D and Ego4D-NLQ benchmarks. We plan to publicly release the codes, model, and constructed datasets for future research.","sentences":["Existing approaches to video understanding, mainly designed for short videos from a third-person perspective, are limited in their applicability in certain fields, such as robotics.","In this paper, we delve into open-ended question-answering (QA) in long, egocentric videos, which allows individuals or robots to inquire about their own past visual experiences.","This task presents unique challenges, including the complexity of temporally grounding queries within extensive video content, the high resource demands for precise data annotation, and the inherent difficulty of evaluating open-ended answers due to their ambiguous nature.","Our proposed approach tackles these challenges by (i) integrating query grounding and answering within a unified model to reduce error propagation; (ii) employing large language models for efficient and scalable data synthesis; and (iii) introducing a close-ended QA task for evaluation, to manage answer ambiguity.","Extensive experiments demonstrate the effectiveness of our method, which also achieves state-of-the-art performance on the QAEgo4D and Ego4D-NLQ benchmarks.","We plan to publicly release the codes, model, and constructed datasets for future research."],"url":"http://arxiv.org/abs/2312.06505v1"}
{"created":"2023-12-11 16:30:50","title":"Infinite class of quantum codes derived from duadic constacyclic codes","abstract":"We present a family of non-CSS quantum stabilizer codes using the structure of duadic constacyclic codes over $\\mathbb{F}_4$. Within this family, quantum codes can possess varying dimensions, and their minimum distances are bounded by a square root bound. For each fixed dimension, this allows us to construct an infinite sequence of binary quantum codes with a growing minimum distance. Additionally, we demonstrate that this quantum family includes an infinite subclass of degenerate codes with the mentioned properties. We also introduce a technique for extending splittings of duadic constacyclic codes, providing new insights into the minimum distance and minimum odd-like weight of specific duadic constacyclic codes. Finally, we establish that many best-known quantum codes belong to this family and provide numerical examples of quantum codes with short lengths within this family.","sentences":["We present a family of non-CSS quantum stabilizer codes using the structure of duadic constacyclic codes over $\\mathbb{F}_4$. Within this family, quantum codes can possess varying dimensions, and their minimum distances are bounded by a square root bound.","For each fixed dimension, this allows us to construct an infinite sequence of binary quantum codes with a growing minimum distance.","Additionally, we demonstrate that this quantum family includes an infinite subclass of degenerate codes with the mentioned properties.","We also introduce a technique for extending splittings of duadic constacyclic codes, providing new insights into the minimum distance and minimum odd-like weight of specific duadic constacyclic codes.","Finally, we establish that many best-known quantum codes belong to this family and provide numerical examples of quantum codes with short lengths within this family."],"url":"http://arxiv.org/abs/2312.06504v1"}
{"created":"2023-12-11 16:27:04","title":"On enforcing dyadic-type homogeneous binary function product constraints in MatBase","abstract":"Homogeneous binary function products are often encountered in the sub-universes modeled by databases, from genealogical trees to sports, from education to healthcare, etc. Their properties must be discovered and enforced by the software applications managing such data to guarantee plausibility. The (Elementary) Mathematical Data Model provides 18 dyadic-type homogeneous binary function product constraint types. MatBase, an intelligent data and knowledge base management system prototype, allows database designers to simply declare them by only clicking corresponding checkboxes and automatically generates code for enforcing them. This paper describes the algorithms that MatBase uses for enforcing all these 18 homogeneous binary function product constraint types, which may also be used by developers not having access to MatBase.","sentences":["Homogeneous binary function products are often encountered in the sub-universes modeled by databases, from genealogical trees to sports, from education to healthcare, etc.","Their properties must be discovered and enforced by the software applications managing such data to guarantee plausibility.","The (Elementary) Mathematical Data Model provides 18 dyadic-type homogeneous binary function product constraint types.","MatBase, an intelligent data and knowledge base management system prototype, allows database designers to simply declare them by only clicking corresponding checkboxes and automatically generates code for enforcing them.","This paper describes the algorithms that MatBase uses for enforcing all these 18 homogeneous binary function product constraint types, which may also be used by developers not having access to MatBase."],"url":"http://arxiv.org/abs/2312.06502v1"}
{"created":"2023-12-11 16:22:58","title":"Integrating micro-learning content in traditional e-learning platforms","abstract":"Lifelong learning requires appropriate solutions, especially for corporate training. Workers usually have difficulty combining training and their normal work. In this context, micro-learning emerges as a suitable solution, since it is based on breaking down new concepts into small fragments or pills of content, which can be consumed in short periods of time. The purpose of this paper is twofold. First, we offer an updated overview of the research on this training paradigm, as well as the different technologies leading to potential commercial solutions. Second, we introduce a proposal to add micro-learning content to more formal distance learning environments (traditional Learning Management Systems or LMS), with the aim of taking advantage of both learning philosophies. Our approach is based on a Service-Oriented Architecture (SOA) that is deployed in the cloud. In order to ensure the full integration of the micro-learning approach in traditional LMSs, we have used two well-known standards in the distance learning field: LTI (Learning Tools Interoperability) and LIS (Learning Information Service). The combination of these two technologies allows the exchange of data with the LMS to monitor the student's activity and results. Finally, we have collected the opinion of lectures from different countries in order to know their thoughts about the potential of this new approach in higher education, obtaining positive feedback.","sentences":["Lifelong learning requires appropriate solutions, especially for corporate training.","Workers usually have difficulty combining training and their normal work.","In this context, micro-learning emerges as a suitable solution, since it is based on breaking down new concepts into small fragments or pills of content, which can be consumed in short periods of time.","The purpose of this paper is twofold.","First, we offer an updated overview of the research on this training paradigm, as well as the different technologies leading to potential commercial solutions.","Second, we introduce a proposal to add micro-learning content to more formal distance learning environments (traditional Learning Management Systems or LMS), with the aim of taking advantage of both learning philosophies.","Our approach is based on a Service-Oriented Architecture (SOA) that is deployed in the cloud.","In order to ensure the full integration of the micro-learning approach in traditional LMSs, we have used two well-known standards in the distance learning field: LTI (Learning Tools Interoperability) and LIS (Learning Information Service).","The combination of these two technologies allows the exchange of data with the LMS to monitor the student's activity and results.","Finally, we have collected the opinion of lectures from different countries in order to know their thoughts about the potential of this new approach in higher education, obtaining positive feedback."],"url":"http://arxiv.org/abs/2312.06500v1"}
{"created":"2023-12-11 16:22:37","title":"TaCo: Targeted Concept Removal in Output Embeddings for NLP via Information Theory and Explainability","abstract":"The fairness of Natural Language Processing (NLP) models has emerged as a crucial concern. Information theory indicates that to achieve fairness, a model should not be able to predict sensitive variables, such as gender, ethnicity, and age. However, information related to these variables often appears implicitly in language, posing a challenge in identifying and mitigating biases effectively. To tackle this issue, we present a novel approach that operates at the embedding level of an NLP model, independent of the specific architecture. Our method leverages insights from recent advances in XAI techniques and employs an embedding transformation to eliminate implicit information from a selected variable. By directly manipulating the embeddings in the final layer, our approach enables a seamless integration into existing models without requiring significant modifications or retraining. In evaluation, we show that the proposed post-hoc approach significantly reduces gender-related associations in NLP models while preserving the overall performance and functionality of the models. An implementation of our method is available: https://github.com/fanny-jourdan/TaCo","sentences":["The fairness of Natural Language Processing (NLP) models has emerged as a crucial concern.","Information theory indicates that to achieve fairness, a model should not be able to predict sensitive variables, such as gender, ethnicity, and age.","However, information related to these variables often appears implicitly in language, posing a challenge in identifying and mitigating biases effectively.","To tackle this issue, we present a novel approach that operates at the embedding level of an NLP model, independent of the specific architecture.","Our method leverages insights from recent advances in XAI techniques and employs an embedding transformation to eliminate implicit information from a selected variable.","By directly manipulating the embeddings in the final layer, our approach enables a seamless integration into existing models without requiring significant modifications or retraining.","In evaluation, we show that the proposed post-hoc approach significantly reduces gender-related associations in NLP models while preserving the overall performance and functionality of the models.","An implementation of our method is available: https://github.com/fanny-jourdan/TaCo"],"url":"http://arxiv.org/abs/2312.06499v1"}
{"created":"2023-12-11 16:22:17","title":"Sustainability through Optimal Design of Buildings for Natural Ventilation using Updated Comfort and Occupancy Models","abstract":"This paper explores the benefits of incorporating natural ventilation (NV) simulation into a generative process of designing residential buildings to improve energy efficiency and indoor thermal comfort. Our proposed workflow uses the Wave Function Collapse algorithm to generate a diverse set of plausible floor plans. It also includes post-COVID occupant presence models while incorporating adaptive comfort models. We conduct four sets of experiments using the workflow, and the simulated results suggest that multi-mode cooling strategies combining conventional air conditioning with NV can often significantly reduce energy use while introducing only slight reductions in thermal comfort.","sentences":["This paper explores the benefits of incorporating natural ventilation (NV) simulation into a generative process of designing residential buildings to improve energy efficiency and indoor thermal comfort.","Our proposed workflow uses the Wave Function Collapse algorithm to generate a diverse set of plausible floor plans.","It also includes post-COVID occupant presence models while incorporating adaptive comfort models.","We conduct four sets of experiments using the workflow, and the simulated results suggest that multi-mode cooling strategies combining conventional air conditioning with NV can often significantly reduce energy use while introducing only slight reductions in thermal comfort."],"url":"http://arxiv.org/abs/2312.06498v1"}
{"created":"2023-12-11 16:18:56","title":"Detecting Events in Crowds Through Changes in Geometrical Dimensions of Pedestrians","abstract":"Security is an important topic in our contemporary world, and the ability to automate the detection of any events of interest that can take place in a crowd is of great interest to a population. We hypothesize that the detection of events in videos is correlated with significant changes in pedestrian behaviors. In this paper, we examine three different scenarios of crowd behavior, containing both the cases where an event triggers a change in the behavior of the crowd and two video sequences where the crowd and its motion remain mostly unchanged. With both the videos and the tracking of the individual pedestrians (performed in a pre-processed phase), we use Geomind, a software we developed to extract significant data about the scene, in particular, the geometrical features, personalities, and emotions of each person. We then examine the output, seeking a significant change in the way each person acts as a function of the time, that could be used as a basis to identify events or to model realistic crowd actions. When applied to the games area, our method can use the detected events to find some sort of pattern to be then used in agent simulation. Results indicate that our hypothesis seems valid in the sense that the visually observed events could be automatically detected using GeoMind.","sentences":["Security is an important topic in our contemporary world, and the ability to automate the detection of any events of interest that can take place in a crowd is of great interest to a population.","We hypothesize that the detection of events in videos is correlated with significant changes in pedestrian behaviors.","In this paper, we examine three different scenarios of crowd behavior, containing both the cases where an event triggers a change in the behavior of the crowd and two video sequences where the crowd and its motion remain mostly unchanged.","With both the videos and the tracking of the individual pedestrians (performed in a pre-processed phase), we use Geomind, a software we developed to extract significant data about the scene, in particular, the geometrical features, personalities, and emotions of each person.","We then examine the output, seeking a significant change in the way each person acts as a function of the time, that could be used as a basis to identify events or to model realistic crowd actions.","When applied to the games area, our method can use the detected events to find some sort of pattern to be then used in agent simulation.","Results indicate that our hypothesis seems valid in the sense that the visually observed events could be automatically detected using GeoMind."],"url":"http://arxiv.org/abs/2312.06495v1"}
{"created":"2023-12-11 16:17:43","title":"Automated Planning Techniques for Elementary Proofs in Abstract Algebra","abstract":"This paper explores the application of automated planning to automated theorem proving, which is a branch of automated reasoning concerned with the development of algorithms and computer programs to construct mathematical proofs. In particular, we investigate the use of planning to construct elementary proofs in abstract algebra, which provides a rigorous and axiomatic framework for studying algebraic structures such as groups, rings, fields, and modules. We implement basic implications, equalities, and rules in both deterministic and non-deterministic domains to model commutative rings and deduce elementary results about them. The success of this initial implementation suggests that the well-established techniques seen in automated planning are applicable to the relatively newer field of automated theorem proving. Likewise, automated theorem proving provides a new, challenging domain for automated planning.","sentences":["This paper explores the application of automated planning to automated theorem proving, which is a branch of automated reasoning concerned with the development of algorithms and computer programs to construct mathematical proofs.","In particular, we investigate the use of planning to construct elementary proofs in abstract algebra, which provides a rigorous and axiomatic framework for studying algebraic structures such as groups, rings, fields, and modules.","We implement basic implications, equalities, and rules in both deterministic and non-deterministic domains to model commutative rings and deduce elementary results about them.","The success of this initial implementation suggests that the well-established techniques seen in automated planning are applicable to the relatively newer field of automated theorem proving.","Likewise, automated theorem proving provides a new, challenging domain for automated planning."],"url":"http://arxiv.org/abs/2312.06490v1"}
{"created":"2023-12-11 16:14:04","title":"Performance-lossless Black-box Model Watermarking","abstract":"With the development of deep learning, high-value and high-cost models have become valuable assets, and related intellectual property protection technologies have become a hot topic. However, existing model watermarking work in black-box scenarios mainly originates from training-based backdoor methods, which probably degrade original task performance. To address this, we propose a branch backdoor-based model watermarking protocol to protect model intellectual property, where a construction based on a message authentication scheme is adopted as the branch indicator. We prove the lossless performance of the protocol by reduction. Taking the language generation task as an instance, we show the effectiveness of the proposed protocol.","sentences":["With the development of deep learning, high-value and high-cost models have become valuable assets, and related intellectual property protection technologies have become a hot topic.","However, existing model watermarking work in black-box scenarios mainly originates from training-based backdoor methods, which probably degrade original task performance.","To address this, we propose a branch backdoor-based model watermarking protocol to protect model intellectual property, where a construction based on a message authentication scheme is adopted as the branch indicator.","We prove the lossless performance of the protocol by reduction.","Taking the language generation task as an instance, we show the effectiveness of the proposed protocol."],"url":"http://arxiv.org/abs/2312.06488v1"}
{"created":"2023-12-11 16:12:43","title":"STDiff: Spatio-temporal Diffusion for Continuous Stochastic Video Prediction","abstract":"Predicting future frames of a video is challenging because it is difficult to learn the uncertainty of the underlying factors influencing their contents. In this paper, we propose a novel video prediction model, which has infinite-dimensional latent variables over the spatio-temporal domain. Specifically, we first decompose the video motion and content information, then take a neural stochastic differential equation to predict the temporal motion information, and finally, an image diffusion model autoregressively generates the video frame by conditioning on the predicted motion feature and the previous frame. The better expressiveness and stronger stochasticity learning capability of our model lead to state-of-the-art video prediction performances. As well, our model is able to achieve temporal continuous prediction, i.e., predicting in an unsupervised way the future video frames with an arbitrarily high frame rate. Our code is available at \\url{https://github.com/XiYe20/STDiffProject}.","sentences":["Predicting future frames of a video is challenging because it is difficult to learn the uncertainty of the underlying factors influencing their contents.","In this paper, we propose a novel video prediction model, which has infinite-dimensional latent variables over the spatio-temporal domain.","Specifically, we first decompose the video motion and content information, then take a neural stochastic differential equation to predict the temporal motion information, and finally, an image diffusion model autoregressively generates the video frame by conditioning on the predicted motion feature and the previous frame.","The better expressiveness and stronger stochasticity learning capability of our model lead to state-of-the-art video prediction performances.","As well, our model is able to achieve temporal continuous prediction, i.e., predicting in an unsupervised way the future video frames with an arbitrarily high frame rate.","Our code is available at \\url{https://github.com/XiYe20/STDiffProject}."],"url":"http://arxiv.org/abs/2312.06486v1"}
{"created":"2023-12-11 16:06:37","title":"Measuring the perception of the personalized activities with CloudIA robot","abstract":"Socially Assistive Robots represent a valid solution for improving the quality of life and the mood of older adults. In this context, this work presents the CloudIA robot, a non-human-like robot intended to promote sociality and well-being among older adults. The design of the robot and of the provided services were carried out by a multidisciplinary team of designers and technology developers in tandem with professional caregivers. The capabilities of the robot were implemented according to the received guidelines and tested in two nursing facilities by 15 older people. Qualitative and quantitative metrics were used to investigate the engagement of the participants during the interaction with the robot, and to investigate any differences in the interaction during the proposed activities. The results highlighted the general tendency of humanizing the robotic platform and demonstrated the feasibility of introducing the CloudIA robot in support of the professional caregivers' work. From this pilot test, further ideas on improving the personalization of the robotic platform emerged.","sentences":["Socially Assistive Robots represent a valid solution for improving the quality of life and the mood of older adults.","In this context, this work presents the CloudIA robot, a non-human-like robot intended to promote sociality and well-being among older adults.","The design of the robot and of the provided services were carried out by a multidisciplinary team of designers and technology developers in tandem with professional caregivers.","The capabilities of the robot were implemented according to the received guidelines and tested in two nursing facilities by 15 older people.","Qualitative and quantitative metrics were used to investigate the engagement of the participants during the interaction with the robot, and to investigate any differences in the interaction during the proposed activities.","The results highlighted the general tendency of humanizing the robotic platform and demonstrated the feasibility of introducing the CloudIA robot in support of the professional caregivers' work.","From this pilot test, further ideas on improving the personalization of the robotic platform emerged."],"url":"http://arxiv.org/abs/2312.06481v1"}
{"created":"2023-12-11 16:03:31","title":"NetROS-5G: Enhancing Personalization through 5G Network Slicing and Edge Computing in Human-Robot Interactions","abstract":"Robots are increasingly being used in a variety of applications, from manufacturing and healthcare to education and customer service. However, the mobility, power, and price points of these robots often dictate that they do not have sufficient computing power on board to run modern algorithms for personalization in human-robot interaction at desired rates. This can limit the effectiveness of the interaction and limit the potential applications for these robots. 5G connectivity provides a solution to this problem by offering high data rates, bandwidth, and low latency that can facilitate robotics services. Additionally, the widespread availability of cloud computing has made it easy to access almost unlimited computing power at a low cost. Edge computing, which involves placing compute resources closer to the action, can offer even lower latency than cloud computing. In this paper, we explore the potential of combining 5G, edge, and cloud computing to provide improved personalization in human-robot interaction. We design, develop, and demonstrate a new framework, entitled NetROS-5G, to show how the performance gained by utilizing these technologies can overcome network latency and significantly enhance personalization in robotics. Our results show that the integration of 5G network slicing, edge computing, and cloud computing can collectively offer a cost-efficient and superior level of personalization in a modern human-robot interaction scenario.","sentences":["Robots are increasingly being used in a variety of applications, from manufacturing and healthcare to education and customer service.","However, the mobility, power, and price points of these robots often dictate that they do not have sufficient computing power on board to run modern algorithms for personalization in human-robot interaction at desired rates.","This can limit the effectiveness of the interaction and limit the potential applications for these robots.","5G connectivity provides a solution to this problem by offering high data rates, bandwidth, and low latency that can facilitate robotics services.","Additionally, the widespread availability of cloud computing has made it easy to access almost unlimited computing power at a low cost.","Edge computing, which involves placing compute resources closer to the action, can offer even lower latency than cloud computing.","In this paper, we explore the potential of combining 5G, edge, and cloud computing to provide improved personalization in human-robot interaction.","We design, develop, and demonstrate a new framework, entitled NetROS-5G, to show how the performance gained by utilizing these technologies can overcome network latency and significantly enhance personalization in robotics.","Our results show that the integration of 5G network slicing, edge computing, and cloud computing can collectively offer a cost-efficient and superior level of personalization in a modern human-robot interaction scenario."],"url":"http://arxiv.org/abs/2312.06475v1"}
{"created":"2023-12-11 16:02:57","title":"Relevant Intrinsic Feature Enhancement Network for Few-Shot Semantic Segmentation","abstract":"For few-shot semantic segmentation, the primary task is to extract class-specific intrinsic information from limited labeled data. However, the semantic ambiguity and inter-class similarity of previous methods limit the accuracy of pixel-level foreground-background classification. To alleviate these issues, we propose the Relevant Intrinsic Feature Enhancement Network (RiFeNet). To improve the semantic consistency of foreground instances, we propose an unlabeled branch as an efficient data utilization method, which teaches the model how to extract intrinsic features robust to intra-class differences. Notably, during testing, the proposed unlabeled branch is excluded without extra unlabeled data and computation. Furthermore, we extend the inter-class variability between foreground and background by proposing a novel multi-level prototype generation and interaction module. The different-grained complementarity between global and local prototypes allows for better distinction between similar categories. The qualitative and quantitative performance of RiFeNet surpasses the state-of-the-art methods on PASCAL-5i and COCO benchmarks.","sentences":["For few-shot semantic segmentation, the primary task is to extract class-specific intrinsic information from limited labeled data.","However, the semantic ambiguity and inter-class similarity of previous methods limit the accuracy of pixel-level foreground-background classification.","To alleviate these issues, we propose the Relevant Intrinsic Feature Enhancement Network (RiFeNet).","To improve the semantic consistency of foreground instances, we propose an unlabeled branch as an efficient data utilization method, which teaches the model how to extract intrinsic features robust to intra-class differences.","Notably, during testing, the proposed unlabeled branch is excluded without extra unlabeled data and computation.","Furthermore, we extend the inter-class variability between foreground and background by proposing a novel multi-level prototype generation and interaction module.","The different-grained complementarity between global and local prototypes allows for better distinction between similar categories.","The qualitative and quantitative performance of RiFeNet surpasses the state-of-the-art methods on PASCAL-5i and COCO benchmarks."],"url":"http://arxiv.org/abs/2312.06474v1"}
{"created":"2023-12-11 16:00:05","title":"A priori Belief Updates as a Method for Agent Self-Recovery","abstract":"Standard epistemic logic is concerned with describing agents' epistemic attitudes given the current set of alternatives the agents consider possible. While distributed systems can (and often are) discussed without mentioning epistemics, it has been well established that epistemic phenomena lie at the heart of what agents, or processes, can and cannot do. Dynamic epistemic logic (DEL) aims to describe how epistemic attitudes of the agents/processes change based on the new information they receive, e.g., based on their observations of events and actions in a distributed system. In a broader philosophical view, this appeals to an a posteriori kind of reasoning, where agents update the set of alternatives considered possible based on their \"experiences.\"   Until recently, there was little incentive to formalize a priori reasoning, which plays a role in designing and maintaining distributed systems, e.g., in determining which states must be considered possible by agents in order to solve the distributed task at hand, and consequently in updating these states when unforeseen situations arise during runtime. With systems becoming more and more complex and large, the task of fixing design errors \"on the fly\" is shifted to individual agents, such as in the increasingly popular self-adaptive and self-organizing (SASO) systems. Rather than updating agents' a posteriori beliefs, this requires modifying their a priori beliefs about the system's global design and parameters.   The goal of this paper is to provide a formalization of such a priori reasoning by using standard epistemic semantic tools, including Kripke models and DEL-style updates, and provide heuristics that would pave the way to streamlining this inherently non-deterministic and ad hoc process for SASO systems.","sentences":["Standard epistemic logic is concerned with describing agents' epistemic attitudes given the current set of alternatives the agents consider possible.","While distributed systems can (and often are) discussed without mentioning epistemics, it has been well established that epistemic phenomena lie at the heart of what agents, or processes, can and cannot do.","Dynamic epistemic logic (DEL) aims to describe how epistemic attitudes of the agents/processes change based on the new information they receive, e.g., based on their observations of events and actions in a distributed system.","In a broader philosophical view, this appeals to an a posteriori kind of reasoning, where agents update the set of alternatives considered possible based on their \"experiences.\"   ","Until recently, there was little incentive to formalize a priori reasoning, which plays a role in designing and maintaining distributed systems, e.g., in determining which states must be considered possible by agents in order to solve the distributed task at hand, and consequently in updating these states when unforeseen situations arise during runtime.","With systems becoming more and more complex and large, the task of fixing design errors \"on the fly\" is shifted to individual agents, such as in the increasingly popular self-adaptive and self-organizing (SASO) systems.","Rather than updating agents' a posteriori beliefs, this requires modifying their a priori beliefs about the system's global design and parameters.   ","The goal of this paper is to provide a formalization of such a priori reasoning by using standard epistemic semantic tools, including Kripke models and DEL-style updates, and provide heuristics that would pave the way to streamlining this inherently non-deterministic and ad hoc process for SASO systems."],"url":"http://arxiv.org/abs/2312.06471v1"}
{"created":"2023-12-11 15:55:20","title":"Aligning brain functions boosts the decoding of visual semantics in novel subjects","abstract":"Deep learning is leading to major advances in the realm of brain decoding from functional Magnetic Resonance Imaging (fMRI). However, the large inter-subject variability in brain characteristics has limited most studies to train models on one subject at a time. Consequently, this approach hampers the training of deep learning models, which typically requires very large datasets. Here, we propose to boost brain decoding by aligning brain responses to videos and static images across subjects. Compared to the anatomically-aligned baseline, our method improves out-of-subject decoding performance by up to 75%. Moreover, it also outperforms classical single-subject approaches when fewer than 100 minutes of data is available for the tested subject. Furthermore, we propose a new multi-subject alignment method, which obtains comparable results to that of classical single-subject approaches while improving out-of-subject generalization. Finally, we show that this method aligns neural representations in accordance with brain anatomy. Overall, this study lays the foundations for leveraging extensive neuroimaging datasets and enhancing the decoding of individuals with a limited amount of brain recordings.","sentences":["Deep learning is leading to major advances in the realm of brain decoding from functional Magnetic Resonance Imaging (fMRI).","However, the large inter-subject variability in brain characteristics has limited most studies to train models on one subject at a time.","Consequently, this approach hampers the training of deep learning models, which typically requires very large datasets.","Here, we propose to boost brain decoding by aligning brain responses to videos and static images across subjects.","Compared to the anatomically-aligned baseline, our method improves out-of-subject decoding performance by up to 75%.","Moreover, it also outperforms classical single-subject approaches when fewer than 100 minutes of data is available for the tested subject.","Furthermore, we propose a new multi-subject alignment method, which obtains comparable results to that of classical single-subject approaches while improving out-of-subject generalization.","Finally, we show that this method aligns neural representations in accordance with brain anatomy.","Overall, this study lays the foundations for leveraging extensive neuroimaging datasets and enhancing the decoding of individuals with a limited amount of brain recordings."],"url":"http://arxiv.org/abs/2312.06467v1"}
{"created":"2023-12-11 15:53:57","title":"Towards Domain-Specific Cross-Corpus Speech Emotion Recognition Approach","abstract":"Cross-corpus speech emotion recognition (SER) poses a challenge due to feature distribution mismatch, potentially degrading the performance of established SER methods. In this paper, we tackle this challenge by proposing a novel transfer subspace learning method called acoustic knowledgeguided transfer linear regression (AKTLR). Unlike existing approaches, which often overlook domain-specific knowledge related to SER and simply treat cross-corpus SER as a generic transfer learning task, our AKTLR method is built upon a well-designed acoustic knowledge-guided dual sparsity constraint mechanism. This mechanism emphasizes the potential of minimalistic acoustic parameter feature sets to alleviate classifier overadaptation, which is empirically validated acoustic knowledge in SER, enabling superior generalization in cross-corpus SER tasks compared to using large feature sets. Through this mechanism, we extend a simple transfer linear regression model to AKTLR. This extension harnesses its full capability to seek emotiondiscriminative and corpus-invariant features from established acoustic parameter feature sets used for describing speech signals across two scales: contributive acoustic parameter groups and constituent elements within each contributive group. Our proposed method is evaluated through extensive cross-corpus SER experiments on three widely-used speech emotion corpora: EmoDB, eNTERFACE, and CASIA. The results confirm the effectiveness and superior performance of our method, outperforming recent state-of-the-art transfer subspace learning and deep transfer learning-based cross-corpus SER methods. Furthermore, our work provides experimental evidence supporting the feasibility and superiority of incorporating domain-specific knowledge into the transfer learning model to address cross-corpus SER tasks.","sentences":["Cross-corpus speech emotion recognition (SER) poses a challenge due to feature distribution mismatch, potentially degrading the performance of established SER methods.","In this paper, we tackle this challenge by proposing a novel transfer subspace learning method called acoustic knowledgeguided transfer linear regression (AKTLR).","Unlike existing approaches, which often overlook domain-specific knowledge related to SER and simply treat cross-corpus SER as a generic transfer learning task, our AKTLR method is built upon a well-designed acoustic knowledge-guided dual sparsity constraint mechanism.","This mechanism emphasizes the potential of minimalistic acoustic parameter feature sets to alleviate classifier overadaptation, which is empirically validated acoustic knowledge in SER, enabling superior generalization in cross-corpus SER tasks compared to using large feature sets.","Through this mechanism, we extend a simple transfer linear regression model to AKTLR.","This extension harnesses its full capability to seek emotiondiscriminative and corpus-invariant features from established acoustic parameter feature sets used for describing speech signals across two scales: contributive acoustic parameter groups and constituent elements within each contributive group.","Our proposed method is evaluated through extensive cross-corpus SER experiments on three widely-used speech emotion corpora: EmoDB, eNTERFACE, and CASIA.","The results confirm the effectiveness and superior performance of our method, outperforming recent state-of-the-art transfer subspace learning and deep transfer learning-based cross-corpus SER methods.","Furthermore, our work provides experimental evidence supporting the feasibility and superiority of incorporating domain-specific knowledge into the transfer learning model to address cross-corpus SER tasks."],"url":"http://arxiv.org/abs/2312.06466v1"}
{"created":"2023-12-11 15:51:38","title":"Cooperation Does Matter: Exploring Multi-Order Bilateral Relations for Audio-Visual Segmentation","abstract":"Recently, an audio-visual segmentation (AVS) task has been introduced, aiming to group pixels with sounding objects within a given video. This task necessitates a first-ever audio-driven pixel-level understanding of the scene, posing significant challenges. In this paper, we propose an innovative audio-visual transformer framework, termed COMBO, an acronym for COoperation of Multi-order Bilateral relatiOns. For the first time, our framework explores three types of bilateral entanglements within AVS: pixel entanglement, modality entanglement, and temporal entanglement. Regarding pixel entanglement, we employ a Siam-Encoder Module (SEM) that leverages prior knowledge to generate more precise visual features from the foundational model. For modality entanglement, we design a Bilateral-Fusion Module (BFM), enabling COMBO to align corresponding visual and auditory signals bi-directionally. As for temporal entanglement, we introduce an innovative adaptive inter-frame consistency loss according to the inherent rules of temporal. Comprehensive experiments and ablation studies on AVSBench-object (84.7 mIoU on S4, 59.2 mIou on MS3) and AVSBench-semantic (42.1 mIoU on AVSS) datasets demonstrate that COMBO surpasses previous state-of-the-art methods. Code and more results will be publicly available at https://combo-avs.github.io/.","sentences":["Recently, an audio-visual segmentation (AVS) task has been introduced, aiming to group pixels with sounding objects within a given video.","This task necessitates a first-ever audio-driven pixel-level understanding of the scene, posing significant challenges.","In this paper, we propose an innovative audio-visual transformer framework, termed COMBO, an acronym for COoperation of Multi-order Bilateral relatiOns.","For the first time, our framework explores three types of bilateral entanglements within AVS: pixel entanglement, modality entanglement, and temporal entanglement.","Regarding pixel entanglement, we employ a Siam-Encoder Module (SEM) that leverages prior knowledge to generate more precise visual features from the foundational model.","For modality entanglement, we design a Bilateral-Fusion Module (BFM), enabling COMBO to align corresponding visual and auditory signals bi-directionally.","As for temporal entanglement, we introduce an innovative adaptive inter-frame consistency loss according to the inherent rules of temporal.","Comprehensive experiments and ablation studies on AVSBench-object (84.7 mIoU on S4, 59.2 mIou on MS3) and AVSBench-semantic (42.1 mIoU on AVSS) datasets demonstrate that COMBO surpasses previous state-of-the-art methods.","Code and more results will be publicly available at https://combo-avs.github.io/."],"url":"http://arxiv.org/abs/2312.06462v1"}
{"created":"2023-12-11 15:47:12","title":"ASF-YOLO: A Novel YOLO Model with Attentional Scale Sequence Fusion for Cell Instance Segmentation","abstract":"We propose a novel Attentional Scale Sequence Fusion based You Only Look Once (YOLO) framework (ASF-YOLO) which combines spatial and scale features for accurate and fast cell instance segmentation. Built on the YOLO segmentation framework, we employ the Scale Sequence Feature Fusion (SSFF) module to enhance the multi-scale information extraction capability of the network, and the Triple Feature Encoder (TPE) module to fuse feature maps of different scales to increase detailed information. We further introduce a Channel and Position Attention Mechanism (CPAM) to integrate both the SSFF and TPE modules, which focus on informative channels and spatial position-related small objects for improved detection and segmentation performance. Experimental validations on two cell datasets show remarkable segmentation accuracy and speed of the proposed ASF-YOLO model. It achieves a box mAP of 0.91, mask mAP of 0.887, and an inference speed of 47.3 FPS on the 2018 Data Science Bowl dataset, outperforming the state-of-the-art methods. The source code is available at https://github.com/mkang315/ASF-YOLO.","sentences":["We propose a novel Attentional Scale Sequence Fusion based You Only Look Once (YOLO) framework (ASF-YOLO) which combines spatial and scale features for accurate and fast cell instance segmentation.","Built on the YOLO segmentation framework, we employ the Scale Sequence Feature Fusion (SSFF) module to enhance the multi-scale information extraction capability of the network, and the Triple Feature Encoder (TPE) module to fuse feature maps of different scales to increase detailed information.","We further introduce a Channel and Position Attention Mechanism (CPAM) to integrate both the SSFF and TPE modules, which focus on informative channels and spatial position-related small objects for improved detection and segmentation performance.","Experimental validations on two cell datasets show remarkable segmentation accuracy and speed of the proposed ASF-YOLO model.","It achieves a box mAP of 0.91, mask mAP of 0.887, and an inference speed of 47.3 FPS on the 2018 Data Science Bowl dataset, outperforming the state-of-the-art methods.","The source code is available at https://github.com/mkang315/ASF-YOLO."],"url":"http://arxiv.org/abs/2312.06458v1"}
{"created":"2023-12-11 15:45:27","title":"Large Language Models with Retrieval-Augmented Generation for Zero-Shot Disease Phenotyping","abstract":"Identifying disease phenotypes from electronic health records (EHRs) is critical for numerous secondary uses. Manually encoding physician knowledge into rules is particularly challenging for rare diseases due to inadequate EHR coding, necessitating review of clinical notes. Large language models (LLMs) offer promise in text understanding but may not efficiently handle real-world clinical documentation. We propose a zero-shot LLM-based method enriched by retrieval-augmented generation and MapReduce, which pre-identifies disease-related text snippets to be used in parallel as queries for the LLM to establish diagnosis. We show that this method as applied to pulmonary hypertension (PH), a rare disease characterized by elevated arterial pressures in the lungs, significantly outperforms physician logic rules ($F_1$ score of 0.62 vs. 0.75). This method has the potential to enhance rare disease cohort identification, expanding the scope of robust clinical research and care gap identification.","sentences":["Identifying disease phenotypes from electronic health records (EHRs) is critical for numerous secondary uses.","Manually encoding physician knowledge into rules is particularly challenging for rare diseases due to inadequate EHR coding, necessitating review of clinical notes.","Large language models (LLMs) offer promise in text understanding but may not efficiently handle real-world clinical documentation.","We propose a zero-shot LLM-based method enriched by retrieval-augmented generation and MapReduce, which pre-identifies disease-related text snippets to be used in parallel as queries for the LLM to establish diagnosis.","We show that this method as applied to pulmonary hypertension (PH), a rare disease characterized by elevated arterial pressures in the lungs, significantly outperforms physician logic rules ($F_1$ score of 0.62 vs. 0.75).","This method has the potential to enhance rare disease cohort identification, expanding the scope of robust clinical research and care gap identification."],"url":"http://arxiv.org/abs/2312.06457v1"}
{"created":"2023-12-11 15:44:55","title":"Ownership Types for Verification of Programs with Pointer Arithmetic","abstract":"Toman et al. have proposed a type system for automatic verification of low-level programs, which combines ownership types and refinement types to enable strong updates of refinement types in the presence of pointer aliases. We extend their type system to support pointer arithmetic, and prove its soundness. Based on the proposed type system, we have implemented a prototype tool for automated verification of the lack of assertion errors of low-level programs with pointer arithmetic, and confirmed its effectiveness through experiments.","sentences":["Toman et al. have proposed a type system for automatic verification of low-level programs, which combines ownership types and refinement types to enable strong updates of refinement types in the presence of pointer aliases.","We extend their type system to support pointer arithmetic, and prove its soundness.","Based on the proposed type system, we have implemented a prototype tool for automated verification of the lack of assertion errors of low-level programs with pointer arithmetic, and confirmed its effectiveness through experiments."],"url":"http://arxiv.org/abs/2312.06455v1"}
{"created":"2023-12-11 15:39:41","title":"Semantic Image Synthesis for Abdominal CT","abstract":"As a new emerging and promising type of generative models, diffusion models have proven to outperform Generative Adversarial Networks (GANs) in multiple tasks, including image synthesis. In this work, we explore semantic image synthesis for abdominal CT using conditional diffusion models, which can be used for downstream applications such as data augmentation. We systematically evaluated the performance of three diffusion models, as well as to other state-of-the-art GAN-based approaches, and studied the different conditioning scenarios for the semantic mask. Experimental results demonstrated that diffusion models were able to synthesize abdominal CT images with better quality. Additionally, encoding the mask and the input separately is more effective than na\\\"ive concatenating.","sentences":["As a new emerging and promising type of generative models, diffusion models have proven to outperform Generative Adversarial Networks (GANs) in multiple tasks, including image synthesis.","In this work, we explore semantic image synthesis for abdominal CT using conditional diffusion models, which can be used for downstream applications such as data augmentation.","We systematically evaluated the performance of three diffusion models, as well as to other state-of-the-art GAN-based approaches, and studied the different conditioning scenarios for the semantic mask.","Experimental results demonstrated that diffusion models were able to synthesize abdominal CT images with better quality.","Additionally, encoding the mask and the input separately is more effective than na\\\"ive concatenating."],"url":"http://arxiv.org/abs/2312.06453v1"}
{"created":"2023-12-11 15:30:46","title":"Optimal Publishing Strategies on a Base Layer","abstract":"A growing number of products use layer 2 solutions to expand the capabilities of primary blockchains like Ethereum, where computation is off-loaded from the root chain, and the results are published to it in bulk. Those include optimistic and zero-knowledge rollups, information oracles, and app-specific chains. This work presents an analysis of layer 2 blockchain strategies determining the optimal times for publishing transactions on the root chain. There is a trade-off between waiting for a better layer 1 gas price and the urgency to finalize layer 2 transactions. We present a model for the problem that captures this trade-off, generalizing previous works, and we analyze the properties of optimal publishing strategies. We show that such optimal strategies hold a computable simple form for a large class of cost functions.","sentences":["A growing number of products use layer 2 solutions to expand the capabilities of primary blockchains like Ethereum, where computation is off-loaded from the root chain, and the results are published to it in bulk.","Those include optimistic and zero-knowledge rollups, information oracles, and app-specific chains.","This work presents an analysis of layer 2 blockchain strategies determining the optimal times for publishing transactions on the root chain.","There is a trade-off between waiting for a better layer 1 gas price and the urgency to finalize layer 2 transactions.","We present a model for the problem that captures this trade-off, generalizing previous works, and we analyze the properties of optimal publishing strategies.","We show that such optimal strategies hold a computable simple form for a large class of cost functions."],"url":"http://arxiv.org/abs/2312.06448v1"}
{"created":"2023-12-11 15:28:47","title":"Experimental demonstration of a robust training method for strongly defective neuromorphic hardware","abstract":"The increasing scale of neural networks needed to support more complex applications has led to an increasing requirement for area- and energy-efficient hardware. One route to meeting the budget for these applications is to circumvent the von Neumann bottleneck by performing computation in or near memory. An inevitability of transferring neural networks onto hardware is that non-idealities such as device-to-device variations or poor device yield impact performance. Methods such as hardware-aware training, where substrate non-idealities are incorporated during network training, are one way to recover performance at the cost of solution generality. In this work, we demonstrate inference on hardware neural networks consisting of 20,000 magnetic tunnel junction arrays integrated on a complementary metal-oxide-semiconductor chips that closely resembles market-ready spin transfer-torque magnetoresistive random access memory technology. Using 36 dies, each containing a crossbar array with its own non-idealities, we show that even a small number of defects in physically mapped networks significantly degrades the performance of networks trained without defects and show that, at the cost of generality, hardware-aware training accounting for specific defects on each die can recover to comparable performance with ideal networks. We then demonstrate a robust training method that extends hardware-aware training to statistics-aware training, producing network weights that perform well on most defective dies regardless of their specific defect locations. When evaluated on the 36 physical dies, statistics-aware trained solutions can achieve a mean misclassification error on the MNIST dataset that differs from the software-baseline by only 2 %. This statistics-aware training method could be generalized to networks with many layers that are mapped to hardware suited for industry-ready applications.","sentences":["The increasing scale of neural networks needed to support more complex applications has led to an increasing requirement for area- and energy-efficient hardware.","One route to meeting the budget for these applications is to circumvent the von Neumann bottleneck by performing computation in or near memory.","An inevitability of transferring neural networks onto hardware is that non-idealities such as device-to-device variations or poor device yield impact performance.","Methods such as hardware-aware training, where substrate non-idealities are incorporated during network training, are one way to recover performance at the cost of solution generality.","In this work, we demonstrate inference on hardware neural networks consisting of 20,000 magnetic tunnel junction arrays integrated on a complementary metal-oxide-semiconductor chips that closely resembles market-ready spin transfer-torque magnetoresistive random access memory technology.","Using 36 dies, each containing a crossbar array with its own non-idealities, we show that even a small number of defects in physically mapped networks significantly degrades the performance of networks trained without defects and show that, at the cost of generality, hardware-aware training accounting for specific defects on each die can recover to comparable performance with ideal networks.","We then demonstrate a robust training method that extends hardware-aware training to statistics-aware training, producing network weights that perform well on most defective dies regardless of their specific defect locations.","When evaluated on the 36 physical dies, statistics-aware trained solutions can achieve a mean misclassification error on the MNIST dataset that differs from the software-baseline by only 2 %.","This statistics-aware training method could be generalized to networks with many layers that are mapped to hardware suited for industry-ready applications."],"url":"http://arxiv.org/abs/2312.06446v1"}
