{"created":"2023-05-31","title":"AI for Low-Code for AI","abstract":"Low-code programming allows citizen developers to create programs with minimal coding effort, typically via visual (e.g. drag-and-drop) interfaces. In parallel, recent AI-powered tools such as Copilot and ChatGPT generate programs from natural language instructions. We argue that these modalities are complementary: tools like ChatGPT greatly reduce the need to memorize large APIs but still require their users to read (and modify) programs, whereas visual tools abstract away most or all programming but struggle to provide easy access to large APIs. At their intersection, we propose LowCoder, the first low-code tool for developing AI pipelines that supports both a visual programming interface (LowCoder_VP) and an AI-powered natural language interface (LowCoder_NL). We leverage this tool to provide some of the first insights into whether and how these two modalities help programmers by conducting a user study. We task 20 developers with varying levels of AI expertise with implementing four ML pipelines using LowCoder, replacing the LowCoder_NL component with a simple keyword search in half the tasks. Overall, we find that LowCoder is especially useful for (i) Discoverability: using LowCoder_NL, participants discovered new operators in 75% of the tasks, compared to just 32.5% and 27.5% using web search or scrolling through options respectively in the keyword-search condition, and (ii) Iterative Composition: 82.5% of tasks were successfully completed and many initial pipelines were further successfully improved. Qualitative analysis shows that AI helps users discover how to implement constructs when they know what to do, but still fails to support novices when they lack clarity on what they want to accomplish. Overall, our work highlights the benefits of combining the power of AI with low-code programming.","sentences":["Low-code programming allows citizen developers to create programs with minimal coding effort, typically via visual (e.g. drag-and-drop) interfaces.","In parallel, recent AI-powered tools such as Copilot and ChatGPT generate programs from natural language instructions.","We argue that these modalities are complementary: tools like ChatGPT greatly reduce the need to memorize large APIs but still require their users to read (and modify) programs, whereas visual tools abstract away most or all programming but struggle to provide easy access to large APIs.","At their intersection, we propose LowCoder, the first low-code tool for developing AI pipelines that supports both a visual programming interface (LowCoder_VP) and an AI-powered natural language interface (LowCoder_NL).","We leverage this tool to provide some of the first insights into whether and how these two modalities help programmers by conducting a user study.","We task 20 developers with varying levels of AI expertise with implementing four ML pipelines using LowCoder, replacing the LowCoder_NL component with a simple keyword search in half the tasks.","Overall, we find that LowCoder is especially useful for (i) Discoverability: using LowCoder_NL, participants discovered new operators in 75% of the tasks, compared to just 32.5% and 27.5% using web search or scrolling through options respectively in the keyword-search condition, and (ii) Iterative Composition: 82.5% of tasks were successfully completed and many initial pipelines were further successfully improved.","Qualitative analysis shows that AI helps users discover how to implement constructs when they know what to do, but still fails to support novices when they lack clarity on what they want to accomplish.","Overall, our work highlights the benefits of combining the power of AI with low-code programming."],"url":"http://arxiv.org/abs/2305.20015v1"}
{"created":"2023-05-31","title":"ChatGPT an ENFJ, Bard an ISTJ: Empirical Study on Personalities of Large Language Models","abstract":"Large Language Models (LLMs) have made remarkable advancements in the field of artificial intelligence, significantly reshaping the human-computer interaction. We not only focus on the performance of LLMs, but also explores their features from a psychological perspective, acknowledging the importance of understanding their behavioral characteristics. Our study examines the behavioral patterns displayed by LLMs by employing trait theory, a psychological framework. We first focus on evaluating the consistency of personality types exhibited by ChatGPT. Furthermore, experiments include cross-lingual effects on seven additional languages, and the investigation of four other LLMs. Moreover, the study investigates whether ChatGPT can exhibit personality changes in response to instructions or contextual cues. The findings show that ChatGPT consistently maintains its ENFJ personality regardless of instructions or contexts. By shedding light on the personalization of LLMs, we anticipate that our study will serve as a catalyst for further research in this field.","sentences":["Large Language Models (LLMs) have made remarkable advancements in the field of artificial intelligence, significantly reshaping the human-computer interaction.","We not only focus on the performance of LLMs, but also explores their features from a psychological perspective, acknowledging the importance of understanding their behavioral characteristics.","Our study examines the behavioral patterns displayed by LLMs by employing trait theory, a psychological framework.","We first focus on evaluating the consistency of personality types exhibited by ChatGPT.","Furthermore, experiments include cross-lingual effects on seven additional languages, and the investigation of four other LLMs.","Moreover, the study investigates whether ChatGPT can exhibit personality changes in response to instructions or contextual cues.","The findings show that ChatGPT consistently maintains its ENFJ personality regardless of instructions or contexts.","By shedding light on the personalization of LLMs, we anticipate that our study will serve as a catalyst for further research in this field."],"url":"http://arxiv.org/abs/2305.19926v1"}
{"created":"2023-05-31","title":"Catalysis distillation neural network for the few shot open catalyst challenge","abstract":"The integration of artificial intelligence and science has resulted in substantial progress in computational chemistry methods for the design and discovery of novel catalysts. Nonetheless, the challenges of electrocatalytic reactions and developing a large-scale language model in catalysis persist, and the recent success of ChatGPT's (Chat Generative Pre-trained Transformer) few-shot methods surpassing BERT (Bidirectional Encoder Representation from Transformers) underscores the importance of addressing limited data, expensive computations, time constraints and structure-activity relationship in research. Hence, the development of few-shot techniques for catalysis is critical and essential, regardless of present and future requirements. This paper introduces the Few-Shot Open Catalyst Challenge 2023, a competition aimed at advancing the application of machine learning technology for predicting catalytic reactions on catalytic surfaces, with a specific focus on dual-atom catalysts in hydrogen peroxide electrocatalysis. To address the challenge of limited data in catalysis, we propose a machine learning approach based on MLP-Like and a framework called Catalysis Distillation Graph Neural Network (CDGNN). Our results demonstrate that CDGNN effectively learns embeddings from catalytic structures, enabling the capture of structure-adsorption relationships. This accomplishment has resulted in the utmost advanced and efficient determination of the reaction pathway for hydrogen peroxide, surpassing the current graph neural network approach by 16.1%.. Consequently, CDGNN presents a promising approach for few-shot learning in catalysis.","sentences":["The integration of artificial intelligence and science has resulted in substantial progress in computational chemistry methods for the design and discovery of novel catalysts.","Nonetheless, the challenges of electrocatalytic reactions and developing a large-scale language model in catalysis persist, and the recent success of ChatGPT's (Chat Generative Pre-trained Transformer) few-shot methods surpassing BERT (Bidirectional Encoder Representation from Transformers) underscores the importance of addressing limited data, expensive computations, time constraints and structure-activity relationship in research.","Hence, the development of few-shot techniques for catalysis is critical and essential, regardless of present and future requirements.","This paper introduces the Few-Shot Open Catalyst Challenge 2023, a competition aimed at advancing the application of machine learning technology for predicting catalytic reactions on catalytic surfaces, with a specific focus on dual-atom catalysts in hydrogen peroxide electrocatalysis.","To address the challenge of limited data in catalysis, we propose a machine learning approach based on MLP-Like and a framework called Catalysis Distillation Graph Neural Network (CDGNN).","Our results demonstrate that CDGNN effectively learns embeddings from catalytic structures, enabling the capture of structure-adsorption relationships.","This accomplishment has resulted in the utmost advanced and efficient determination of the reaction pathway for hydrogen peroxide, surpassing the current graph neural network approach by 16.1%..","Consequently, CDGNN presents a promising approach for few-shot learning in catalysis."],"url":"http://arxiv.org/abs/2305.19545v1"}
{"created":"2023-05-31","title":"Improving CLIP Training with Language Rewrites","abstract":"Contrastive Language-Image Pre-training (CLIP) stands as one of the most effective and scalable methods for training transferable vision models using paired image and text data. CLIP models are trained using contrastive loss, which typically relies on data augmentations to prevent overfitting and shortcuts. However, in the CLIP training paradigm, data augmentations are exclusively applied to image inputs, while language inputs remain unchanged throughout the entire training process, limiting the exposure of diverse texts to the same image. In this paper, we introduce Language augmented CLIP (LaCLIP), a simple yet highly effective approach to enhance CLIP training through language rewrites. Leveraging the in-context learning capability of large language models, we rewrite the text descriptions associated with each image. These rewritten texts exhibit diversity in sentence structure and vocabulary while preserving the original key concepts and meanings. During training, LaCLIP randomly selects either the original texts or the rewritten versions as text augmentations for each image. Extensive experiments on CC3M, CC12M, RedCaps and LAION-400M datasets show that CLIP pre-training with language rewrites significantly improves the transfer performance without computation or memory overhead during training. Specifically for ImageNet zero-shot accuracy, LaCLIP outperforms CLIP by 8.2% on CC12M and 2.4% on LAION-400M. Code is available at https://github.com/LijieFan/LaCLIP.","sentences":["Contrastive Language-Image Pre-training (CLIP) stands as one of the most effective and scalable methods for training transferable vision models using paired image and text data.","CLIP models are trained using contrastive loss, which typically relies on data augmentations to prevent overfitting and shortcuts.","However, in the CLIP training paradigm, data augmentations are exclusively applied to image inputs, while language inputs remain unchanged throughout the entire training process, limiting the exposure of diverse texts to the same image.","In this paper, we introduce Language augmented CLIP (LaCLIP), a simple yet highly effective approach to enhance CLIP training through language rewrites.","Leveraging the in-context learning capability of large language models, we rewrite the text descriptions associated with each image.","These rewritten texts exhibit diversity in sentence structure and vocabulary while preserving the original key concepts and meanings.","During training, LaCLIP randomly selects either the original texts or the rewritten versions as text augmentations for each image.","Extensive experiments on CC3M, CC12M, RedCaps and LAION-400M datasets show that CLIP pre-training with language rewrites significantly improves the transfer performance without computation or memory overhead during training.","Specifically for ImageNet zero-shot accuracy, LaCLIP outperforms CLIP by 8.2% on CC12M and 2.4% on LAION-400M. Code is available at https://github.com/LijieFan/LaCLIP."],"url":"http://arxiv.org/abs/2305.20088v1"}
{"created":"2023-05-31","title":"Too Large; Data Reduction for Vision-Language Pre-Training","abstract":"This paper examines the problems of severe image-text misalignment and high redundancy in the widely-used large-scale Vision-Language Pre-Training (VLP) datasets. To address these issues, we propose an efficient and straightforward Vision-Language learning algorithm called TL;DR, which aims to compress the existing large VLP data into a small, high-quality set. Our approach consists of two major steps. First, a codebook-based encoder-decoder captioner is developed to select representative samples. Second, a new caption is generated to complement the original captions for selected samples, mitigating the text-image misalignment problem while maintaining uniqueness. As the result, TL;DR enables us to reduce the large dataset into a small set of high-quality data, which can serve as an alternative pre-training dataset. This algorithm significantly speeds up the time-consuming pretraining process. Specifically, TL;DR can compress the mainstream VLP datasets at a high ratio, e.g., reduce well-cleaned CC3M dataset from 2.82M to 0.67M ($\\sim$24\\%) and noisy YFCC15M from 15M to 2.5M ($\\sim$16.7\\%). Extensive experiments with three popular VLP models over seven downstream tasks show that VLP model trained on the compressed dataset provided by TL;DR can perform similar or even better results compared with training on the full-scale dataset. The code will be made available at \\url{https://github.com/showlab/data-centric.vlp}.","sentences":["This paper examines the problems of severe image-text misalignment and high redundancy in the widely-used large-scale Vision-Language Pre-Training (VLP) datasets.","To address these issues, we propose an efficient and straightforward Vision-Language learning algorithm called TL;DR, which aims to compress the existing large VLP data into a small, high-quality set.","Our approach consists of two major steps.","First, a codebook-based encoder-decoder captioner is developed to select representative samples.","Second, a new caption is generated to complement the original captions for selected samples, mitigating the text-image misalignment problem while maintaining uniqueness.","As the result, TL;DR enables us to reduce the large dataset into a small set of high-quality data, which can serve as an alternative pre-training dataset.","This algorithm significantly speeds up the time-consuming pretraining process.","Specifically, TL;DR can compress the mainstream VLP datasets at a high ratio, e.g., reduce well-cleaned CC3M dataset from 2.82M to 0.67M ($\\sim$24\\%) and noisy YFCC15M from 15M to 2.5M ($\\sim$16.7\\%).","Extensive experiments with three popular VLP models over seven downstream tasks show that VLP model trained on the compressed dataset provided by TL;DR can perform similar or even better results compared with training on the full-scale dataset.","The code will be made available at \\url{https://github.com/showlab/data-centric.vlp}."],"url":"http://arxiv.org/abs/2305.20087v1"}
{"created":"2023-05-31","title":"Efficient Diffusion Policies for Offline Reinforcement Learning","abstract":"Offline reinforcement learning (RL) aims to learn optimal policies from offline datasets, where the parameterization of policies is crucial but often overlooked. Recently, Diffsuion-QL significantly boosts the performance of offline RL by representing a policy with a diffusion model, whose success relies on a parametrized Markov Chain with hundreds of steps for sampling. However, Diffusion-QL suffers from two critical limitations. 1) It is computationally inefficient to forward and backward through the whole Markov chain during training. 2) It is incompatible with maximum likelihood-based RL algorithms (e.g., policy gradient methods) as the likelihood of diffusion models is intractable. Therefore, we propose efficient diffusion policy (EDP) to overcome these two challenges. EDP approximately constructs actions from corrupted ones at training to avoid running the sampling chain. We conduct extensive experiments on the D4RL benchmark. The results show that EDP can reduce the diffusion policy training time from 5 days to 5 hours on gym-locomotion tasks. Moreover, we show that EDP is compatible with various offline RL algorithms (TD3, CRR, and IQL) and achieves new state-of-the-art on D4RL by large margins over previous methods. Our code is available at https://github.com/sail-sg/edp.","sentences":["Offline reinforcement learning (RL) aims to learn optimal policies from offline datasets, where the parameterization of policies is crucial but often overlooked.","Recently, Diffsuion-QL significantly boosts the performance of offline RL by representing a policy with a diffusion model, whose success relies on a parametrized Markov Chain with hundreds of steps for sampling.","However, Diffusion-QL suffers from two critical limitations.","1) It is computationally inefficient to forward and backward through the whole Markov chain during training.","2) It is incompatible with maximum likelihood-based RL algorithms (e.g., policy gradient methods) as the likelihood of diffusion models is intractable.","Therefore, we propose efficient diffusion policy (EDP) to overcome these two challenges.","EDP approximately constructs actions from corrupted ones at training to avoid running the sampling chain.","We conduct extensive experiments on the D4RL benchmark.","The results show that EDP can reduce the diffusion policy training time from 5 days to 5 hours on gym-locomotion tasks.","Moreover, we show that EDP is compatible with various offline RL algorithms (TD3, CRR, and IQL) and achieves new state-of-the-art on D4RL by large margins over previous methods.","Our code is available at https://github.com/sail-sg/edp."],"url":"http://arxiv.org/abs/2305.20081v1"}
{"created":"2023-05-31","title":"Chatting Makes Perfect -- Chat-based Image Retrieval","abstract":"Chats emerge as an effective user-friendly approach for information retrieval, and are successfully employed in many domains, such as customer service, healthcare, and finance. However, existing image retrieval approaches typically address the case of a single query-to-image round, and the use of chats for image retrieval has been mostly overlooked. In this work, we introduce ChatIR: a chat-based image retrieval system that engages in a conversation with the user to elicit information, in addition to an initial query, in order to clarify the user's search intent. Motivated by the capabilities of today's foundation models, we leverage Large Language Models to generate follow-up questions to an initial image description. These questions form a dialog with the user in order to retrieve the desired image from a large corpus. In this study, we explore the capabilities of such a system tested on a large dataset and reveal that engaging in a dialog yields significant gains in image retrieval. We start by building an evaluation pipeline from an existing manually generated dataset and explore different modules and training strategies for ChatIR. Our comparison includes strong baselines derived from related applications trained with Reinforcement Learning. Our system is capable of retrieving the target image from a pool of 50K images with over 78% success rate after 5 dialogue rounds, compared to 75% when questions are asked by humans, and 64% for a single shot text-to-image retrieval. Extensive evaluations reveal the strong capabilities and examine the limitations of CharIR under different settings.","sentences":["Chats emerge as an effective user-friendly approach for information retrieval, and are successfully employed in many domains, such as customer service, healthcare, and finance.","However, existing image retrieval approaches typically address the case of a single query-to-image round, and the use of chats for image retrieval has been mostly overlooked.","In this work, we introduce ChatIR: a chat-based image retrieval system that engages in a conversation with the user to elicit information, in addition to an initial query, in order to clarify the user's search intent.","Motivated by the capabilities of today's foundation models, we leverage Large Language Models to generate follow-up questions to an initial image description.","These questions form a dialog with the user in order to retrieve the desired image from a large corpus.","In this study, we explore the capabilities of such a system tested on a large dataset and reveal that engaging in a dialog yields significant gains in image retrieval.","We start by building an evaluation pipeline from an existing manually generated dataset and explore different modules and training strategies for ChatIR.","Our comparison includes strong baselines derived from related applications trained with Reinforcement Learning.","Our system is capable of retrieving the target image from a pool of 50K images with over 78% success rate after 5 dialogue rounds, compared to 75% when questions are asked by humans, and 64% for a single shot text-to-image retrieval.","Extensive evaluations reveal the strong capabilities and examine the limitations of CharIR under different settings."],"url":"http://arxiv.org/abs/2305.20062v1"}
{"created":"2023-05-31","title":"Exploiting Mechanics-Based Priors for Lateral Displacement Estimation in Ultrasound Elastography","abstract":"Tracking the displacement between the pre- and post-deformed radio-frequency (RF) frames is a pivotal step of ultrasound elastography, which depicts tissue mechanical properties to identify pathologies. Due to ultrasound's poor ability to capture information pertaining to the lateral direction, the existing displacement estimation techniques fail to generate an accurate lateral displacement or strain map. The attempts made in the literature to mitigate this well-known issue suffer from one of the following limitations: 1) Sampling size is substantially increased, rendering the method computationally and memory expensive. 2) The lateral displacement estimation entirely depends on the axial one, ignoring data fidelity and creating large errors. This paper proposes exploiting the effective Poisson's ratio (EPR)-based mechanical correspondence between the axial and lateral strains along with the RF data fidelity and displacement continuity to improve the lateral displacement and strain estimation accuracies. We call our techniques MechSOUL (Mechanically-constrained Second-Order Ultrasound eLastography) and L1-MechSOUL (L1-norm-based MechSOUL), which optimize L2- and L1-norm-based penalty functions, respectively. Extensive validation experiments with simulated, phantom, and in vivo datasets demonstrate that MechSOUL and L1-MechSOUL's lateral strain and EPR estimation abilities are substantially superior to those of the recently-published elastography techniques. We have published the MATLAB codes of MechSOUL and L1-MechSOUL at http://code.sonography.ai.","sentences":["Tracking the displacement between the pre- and post-deformed radio-frequency (RF) frames is a pivotal step of ultrasound elastography, which depicts tissue mechanical properties to identify pathologies.","Due to ultrasound's poor ability to capture information pertaining to the lateral direction, the existing displacement estimation techniques fail to generate an accurate lateral displacement or strain map.","The attempts made in the literature to mitigate this well-known issue suffer from one of the following limitations: 1) Sampling size is substantially increased, rendering the method computationally and memory expensive.","2)","The lateral displacement estimation entirely depends on the axial one, ignoring data fidelity and creating large errors.","This paper proposes exploiting the effective Poisson's ratio (EPR)-based mechanical correspondence between the axial and lateral strains along with the RF data fidelity and displacement continuity to improve the lateral displacement and strain estimation accuracies.","We call our techniques MechSOUL (Mechanically-constrained Second-Order Ultrasound eLastography) and L1-MechSOUL (L1-norm-based MechSOUL), which optimize L2- and L1-norm-based penalty functions, respectively.","Extensive validation experiments with simulated, phantom, and in vivo datasets demonstrate that MechSOUL and L1-MechSOUL's lateral strain and EPR estimation abilities are substantially superior to those of the recently-published elastography techniques.","We have published the MATLAB codes of MechSOUL and L1-MechSOUL at http://code.sonography.ai."],"url":"http://arxiv.org/abs/2305.20059v1"}
{"created":"2023-05-31","title":"Cross-Domain Car Detection Model with Integrated Convolutional Block Attention Mechanism","abstract":"Car detection, particularly through camera vision, has become a major focus in the field of computer vision and has gained widespread adoption. While current car detection systems are capable of good detection, reliable detection can still be challenging due to factors such as proximity between the car, light intensity, and environmental visibility. To address these issues, we propose a cross-domain car detection model that we apply to car recognition for autonomous driving and other areas. Our model includes several novelties: 1)Building a complete cross-domain target detection framework. 2)Developing an unpaired target domain picture generation module with an integrated convolutional attention mechanism. 3)Adopting Generalized Intersection over Union (GIOU) as the loss function of the target detection framework. 4)Designing an object detection model integrated with two-headed Convolutional Block Attention Module(CBAM). 5)Utilizing an effective data enhancement method. To evaluate the model's effectiveness, we performed a reduced will resolution process on the data in the SSLAD dataset and used it as the benchmark dataset for our task. Experimental results show that the performance of the cross-domain car target detection model improves by 40% over the model without our framework, and our improvements have a significant impact on cross-domain car recognition.","sentences":["Car detection, particularly through camera vision, has become a major focus in the field of computer vision and has gained widespread adoption.","While current car detection systems are capable of good detection, reliable detection can still be challenging due to factors such as proximity between the car, light intensity, and environmental visibility.","To address these issues, we propose a cross-domain car detection model that we apply to car recognition for autonomous driving and other areas.","Our model includes several novelties: 1)Building a complete cross-domain target detection framework.","2)Developing an unpaired target domain picture generation module with an integrated convolutional attention mechanism.","3)Adopting Generalized Intersection over Union (GIOU) as the loss function of the target detection framework.","4)Designing an object detection model integrated with two-headed Convolutional Block Attention Module(CBAM).","5)Utilizing an effective data enhancement method.","To evaluate the model's effectiveness, we performed a reduced will resolution process on the data in the SSLAD dataset and used it as the benchmark dataset for our task.","Experimental results show that the performance of the cross-domain car target detection model improves by 40% over the model without our framework, and our improvements have a significant impact on cross-domain car recognition."],"url":"http://arxiv.org/abs/2305.20055v1"}
{"created":"2023-05-31","title":"Let's Verify Step by Step","abstract":"In recent years, large language models have greatly improved in their ability to perform complex multi-step reasoning. However, even state-of-the-art models still regularly produce logical mistakes. To train more reliable models, we can turn either to outcome supervision, which provides feedback for a final result, or process supervision, which provides feedback for each intermediate reasoning step. Given the importance of training reliable models, and given the high cost of human feedback, it is important to carefully compare the both methods. Recent work has already begun this comparison, but many questions still remain. We conduct our own investigation, finding that process supervision significantly outperforms outcome supervision for training models to solve problems from the challenging MATH dataset. Our process-supervised model solves 78% of problems from a representative subset of the MATH test set. Additionally, we show that active learning significantly improves the efficacy of process supervision. To support related research, we also release PRM800K, the complete dataset of 800,000 step-level human feedback labels used to train our best reward model.","sentences":["In recent years, large language models have greatly improved in their ability to perform complex multi-step reasoning.","However, even state-of-the-art models still regularly produce logical mistakes.","To train more reliable models, we can turn either to outcome supervision, which provides feedback for a final result, or process supervision, which provides feedback for each intermediate reasoning step.","Given the importance of training reliable models, and given the high cost of human feedback, it is important to carefully compare the both methods.","Recent work has already begun this comparison, but many questions still remain.","We conduct our own investigation, finding that process supervision significantly outperforms outcome supervision for training models to solve problems from the challenging MATH dataset.","Our process-supervised model solves 78% of problems from a representative subset of the MATH test set.","Additionally, we show that active learning significantly improves the efficacy of process supervision.","To support related research, we also release PRM800K, the complete dataset of 800,000 step-level human feedback labels used to train our best reward model."],"url":"http://arxiv.org/abs/2305.20050v1"}
{"created":"2023-05-31","title":"FD: On understanding the role of deep feature spaces on face generation evaluation","abstract":"Perceptual metrics, like the Fr\\'echet Inception Distance (FID), are widely used to assess the similarity between synthetically generated and ground truth (real) images. The key idea behind these metrics is to compute errors in a deep feature space that captures perceptually and semantically rich image features. Despite their popularity, the effect that different deep features and their design choices have on a perceptual metric has not been well studied. In this work, we perform a causal analysis linking differences in semantic attributes and distortions between face image distributions to Fr\\'echet distances (FD) using several popular deep feature spaces. A key component of our analysis is the creation of synthetic counterfactual faces using deep face generators. Our experiments show that the FD is heavily influenced by its feature space's training dataset and objective function. For example, FD using features extracted from ImageNet-trained models heavily emphasize hats over regions like the eyes and mouth. Moreover, FD using features from a face gender classifier emphasize hair length more than distances in an identity (recognition) feature space. Finally, we evaluate several popular face generation models across feature spaces and find that StyleGAN2 consistently ranks higher than other face generators, except with respect to identity (recognition) features. This suggests the need for considering multiple feature spaces when evaluating generative models and using feature spaces that are tuned to nuances of the domain of interest.","sentences":["Perceptual metrics, like the Fr\\'echet Inception Distance (FID), are widely used to assess the similarity between synthetically generated and ground truth (real) images.","The key idea behind these metrics is to compute errors in a deep feature space that captures perceptually and semantically rich image features.","Despite their popularity, the effect that different deep features and their design choices have on a perceptual metric has not been well studied.","In this work, we perform a causal analysis linking differences in semantic attributes and distortions between face image distributions to Fr\\'echet distances (FD) using several popular deep feature spaces.","A key component of our analysis is the creation of synthetic counterfactual faces using deep face generators.","Our experiments show that the FD is heavily influenced by its feature space's training dataset and objective function.","For example, FD using features extracted from ImageNet-trained models heavily emphasize hats over regions like the eyes and mouth.","Moreover, FD using features from a face gender classifier emphasize hair length more than distances in an identity (recognition) feature space.","Finally, we evaluate several popular face generation models across feature spaces and find that StyleGAN2 consistently ranks higher than other face generators, except with respect to identity (recognition) features.","This suggests the need for considering multiple feature spaces when evaluating generative models and using feature spaces that are tuned to nuances of the domain of interest."],"url":"http://arxiv.org/abs/2305.20048v1"}
{"created":"2023-05-31","title":"LOWA: Localize Objects in the Wild with Attributes","abstract":"We present LOWA, a novel method for localizing objects with attributes effectively in the wild. It aims to address the insufficiency of current open-vocabulary object detectors, which are limited by the lack of instance-level attribute classification and rare class names. To train LOWA, we propose a hybrid vision-language training strategy to learn object detection and recognition with class names as well as attribute information. With LOWA, users can not only detect objects with class names, but also able to localize objects by attributes. LOWA is built on top of a two-tower vision-language architecture and consists of a standard vision transformer as the image encoder and a similar transformer as the text encoder. To learn the alignment between visual and text inputs at the instance level, we train LOWA with three training steps: object-level training, attribute-aware learning, and free-text joint training of objects and attributes. This hybrid training strategy first ensures correct object detection, then incorporates instance-level attribute information, and finally balances the object class and attribute sensitivity. We evaluate our model performance of attribute classification and attribute localization on the Open-Vocabulary Attribute Detection (OVAD) benchmark and the Visual Attributes in the Wild (VAW) dataset, and experiments indicate strong zero-shot performance. Ablation studies additionally demonstrate the effectiveness of each training step of our approach.","sentences":["We present LOWA, a novel method for localizing objects with attributes effectively in the wild.","It aims to address the insufficiency of current open-vocabulary object detectors, which are limited by the lack of instance-level attribute classification and rare class names.","To train LOWA, we propose a hybrid vision-language training strategy to learn object detection and recognition with class names as well as attribute information.","With LOWA, users can not only detect objects with class names, but also able to localize objects by attributes.","LOWA is built on top of a two-tower vision-language architecture and consists of a standard vision transformer as the image encoder and a similar transformer as the text encoder.","To learn the alignment between visual and text inputs at the instance level, we train LOWA with three training steps: object-level training, attribute-aware learning, and free-text joint training of objects and attributes.","This hybrid training strategy first ensures correct object detection, then incorporates instance-level attribute information, and finally balances the object class and attribute sensitivity.","We evaluate our model performance of attribute classification and attribute localization on the Open-Vocabulary Attribute Detection (OVAD) benchmark and the Visual Attributes in the Wild (VAW) dataset, and experiments indicate strong zero-shot performance.","Ablation studies additionally demonstrate the effectiveness of each training step of our approach."],"url":"http://arxiv.org/abs/2305.20047v1"}
{"created":"2023-05-31","title":"ActiveAED: A Human in the Loop Improves Annotation Error Detection","abstract":"Manually annotated datasets are crucial for training and evaluating Natural Language Processing models. However, recent work has discovered that even widely-used benchmark datasets contain a substantial number of erroneous annotations. This problem has been addressed with Annotation Error Detection (AED) models, which can flag such errors for human re-annotation. However, even though many of these AED methods assume a final curation step in which a human annotator decides whether the annotation is erroneous, they have been developed as static models without any human-in-the-loop component. In this work, we propose ActiveAED, an AED method that can detect errors more accurately by repeatedly querying a human for error corrections in its prediction loop. We evaluate ActiveAED on eight datasets spanning five different tasks and find that it leads to improvements over the state of the art on seven of them, with gains of up to six percentage points in average precision.","sentences":["Manually annotated datasets are crucial for training and evaluating Natural Language Processing models.","However, recent work has discovered that even widely-used benchmark datasets contain a substantial number of erroneous annotations.","This problem has been addressed with Annotation Error Detection (AED) models, which can flag such errors for human re-annotation.","However, even though many of these AED methods assume a final curation step in which a human annotator decides whether the annotation is erroneous, they have been developed as static models without any human-in-the-loop component.","In this work, we propose ActiveAED, an AED method that can detect errors more accurately by repeatedly querying a human for error corrections in its prediction loop.","We evaluate ActiveAED on eight datasets spanning five different tasks and find that it leads to improvements over the state of the art on seven of them, with gains of up to six percentage points in average precision."],"url":"http://arxiv.org/abs/2305.20045v1"}
{"created":"2023-05-31","title":"Crowdsourcing subjective annotations using pairwise comparisons reduces bias and error compared to the majority-vote method","abstract":"How to better reduce measurement variability and bias introduced by subjectivity in crowdsourced labelling remains an open question. We introduce a theoretical framework for understanding how random error and measurement bias enter into crowdsourced annotations of subjective constructs. We then propose a pipeline that combines pairwise comparison labelling with Elo scoring, and demonstrate that it outperforms the ubiquitous majority-voting method in reducing both types of measurement error. To assess the performance of the labelling approaches, we constructed an agent-based model of crowdsourced labelling that lets us introduce different types of subjectivity into the tasks. We find that under most conditions with task subjectivity, the comparison approach produced higher $f_1$ scores. Further, the comparison approach is less susceptible to inflating bias, which majority voting tends to do. To facilitate applications, we show with simulated and real-world data that the number of required random comparisons for the same classification accuracy scales log-linearly $O(N \\log N)$ with the number of labelled items. We also implemented the Elo system as an open-source Python package.","sentences":["How to better reduce measurement variability and bias introduced by subjectivity in crowdsourced labelling remains an open question.","We introduce a theoretical framework for understanding how random error and measurement bias enter into crowdsourced annotations of subjective constructs.","We then propose a pipeline that combines pairwise comparison labelling with Elo scoring, and demonstrate that it outperforms the ubiquitous majority-voting method in reducing both types of measurement error.","To assess the performance of the labelling approaches, we constructed an agent-based model of crowdsourced labelling that lets us introduce different types of subjectivity into the tasks.","We find that under most conditions with task subjectivity, the comparison approach produced higher $f_1$ scores.","Further, the comparison approach is less susceptible to inflating bias, which majority voting tends to do.","To facilitate applications, we show with simulated and real-world data that the number of required random comparisons for the same classification accuracy scales log-linearly $O(N \\log N)$ with the number of labelled items.","We also implemented the Elo system as an open-source Python package."],"url":"http://arxiv.org/abs/2305.20042v1"}
{"created":"2023-05-31","title":"The Tag-Team Approach: Leveraging CLS and Language Tagging for Enhancing Multilingual ASR","abstract":"Building a multilingual Automated Speech Recognition (ASR) system in a linguistically diverse country like India can be a challenging task due to the differences in scripts and the limited availability of speech data. This problem can be solved by exploiting the fact that many of these languages are phonetically similar. These languages can be converted into a Common Label Set (CLS) by mapping similar sounds to common labels. In this paper, new approaches are explored and compared to improve the performance of CLS based multilingual ASR model. Specific language information is infused in the ASR model by giving Language ID or using CLS to Native script converter on top of the CLS Multilingual model. These methods give a significant improvement in Word Error Rate (WER) compared to the CLS baseline. These methods are further tried on out-of-distribution data to check their robustness.","sentences":["Building a multilingual Automated Speech Recognition (ASR) system in a linguistically diverse country like India can be a challenging task due to the differences in scripts and the limited availability of speech data.","This problem can be solved by exploiting the fact that many of these languages are phonetically similar.","These languages can be converted into a Common Label Set (CLS) by mapping similar sounds to common labels.","In this paper, new approaches are explored and compared to improve the performance of CLS based multilingual ASR model.","Specific language information is infused in the ASR model by giving Language ID or using CLS to Native script converter on top of the CLS Multilingual model.","These methods give a significant improvement in Word Error Rate (WER) compared to the CLS baseline.","These methods are further tried on out-of-distribution data to check their robustness."],"url":"http://arxiv.org/abs/2305.19584v1"}
{"created":"2023-05-31","title":"SVVAD: Personal Voice Activity Detection for Speaker Verification","abstract":"Voice activity detection (VAD) improves the performance of speaker verification (SV) by preserving speech segments and attenuating the effects of non-speech. However, this scheme is not ideal: (1) it fails in noisy environments or multi-speaker conversations; (2) it is trained based on inaccurate non-SV sensitive labels. To address this, we propose a speaker verification-based voice activity detection (SVVAD) framework that can adapt the speech features according to which are most informative for SV. To achieve this, we introduce a label-free training method with triplet-like losses that completely avoids the performance degradation of SV due to incorrect labeling. Extensive experiments show that SVVAD significantly outperforms the baseline in terms of equal error rate (EER) under conditions where other speakers are mixed at different ratios. Moreover, the decision boundaries reveal the importance of the different parts of speech, which are largely consistent with human judgments.","sentences":["Voice activity detection (VAD) improves the performance of speaker verification (SV) by preserving speech segments and attenuating the effects of non-speech.","However, this scheme is not ideal: (1) it fails in noisy environments or multi-speaker conversations; (2) it is trained based on inaccurate non-SV sensitive labels.","To address this, we propose a speaker verification-based voice activity detection (SVVAD) framework that can adapt the speech features according to which are most informative for SV.","To achieve this, we introduce a label-free training method with triplet-like losses that completely avoids the performance degradation of SV due to incorrect labeling.","Extensive experiments show that SVVAD significantly outperforms the baseline in terms of equal error rate (EER) under conditions where other speakers are mixed at different ratios.","Moreover, the decision boundaries reveal the importance of the different parts of speech, which are largely consistent with human judgments."],"url":"http://arxiv.org/abs/2305.19581v1"}
{"created":"2023-05-31","title":"Improving Handwritten OCR with Training Samples Generated by Glyph Conditional Denoising Diffusion Probabilistic Model","abstract":"Constructing a highly accurate handwritten OCR system requires large amounts of representative training data, which is both time-consuming and expensive to collect. To mitigate the issue, we propose a denoising diffusion probabilistic model (DDPM) to generate training samples. This model conditions on a printed glyph image and creates mappings between printed characters and handwritten images, thus enabling the generation of photo-realistic handwritten samples with diverse styles and unseen text contents. However, the text contents in synthetic images are not always consistent with the glyph conditional images, leading to unreliable labels of synthetic samples. To address this issue, we further propose a progressive data filtering strategy to add those samples with a high confidence of correctness to the training set. Experimental results on IAM benchmark task show that OCR model trained with augmented DDPM-synthesized training samples can achieve about 45% relative word error rate reduction compared with the one trained on real data only.","sentences":["Constructing a highly accurate handwritten OCR system requires large amounts of representative training data, which is both time-consuming and expensive to collect.","To mitigate the issue, we propose a denoising diffusion probabilistic model (DDPM) to generate training samples.","This model conditions on a printed glyph image and creates mappings between printed characters and handwritten images, thus enabling the generation of photo-realistic handwritten samples with diverse styles and unseen text contents.","However, the text contents in synthetic images are not always consistent with the glyph conditional images, leading to unreliable labels of synthetic samples.","To address this issue, we further propose a progressive data filtering strategy to add those samples with a high confidence of correctness to the training set.","Experimental results on IAM benchmark task show that OCR model trained with augmented DDPM-synthesized training samples can achieve about 45% relative word error rate reduction compared with the one trained on real data only."],"url":"http://arxiv.org/abs/2305.19543v1"}
{"created":"2023-05-31","title":"Shallow Depth Factoring Based on Quantum Feasibility Labeling and Variational Quantum Search","abstract":"Large integer factorization is a prominent research challenge, particularly in the context of quantum computing. The classical computation of prime factors for an integer entails exponential time complexity. Quantum computing offers the potential for significantly faster computational processes compared to classical processors. We proposed a new quantum algorithm, Shallow Depth Factoring (SDF), to factor an integer. SDF consists of three steps. First, it converts a factoring problem to an optimization problem without an objective function. Then, we use a Quantum Feasibility Labeling (QFL) to label every possible solution according to whether it is feasible or infeasible for the optimization problem. Finally, the Variational Quantum Search (VQS) is used to find all feasible solutions. The SDF algorithm utilizes shallow-depth quantum circuits for efficient factorization, with the circuit depth scaling linearly as the integer to be factorized increases. Through minimizing the number of gates in the circuit, the algorithm enhances feasibility and reduces vulnerability to errors.","sentences":["Large integer factorization is a prominent research challenge, particularly in the context of quantum computing.","The classical computation of prime factors for an integer entails exponential time complexity.","Quantum computing offers the potential for significantly faster computational processes compared to classical processors.","We proposed a new quantum algorithm, Shallow Depth Factoring (SDF), to factor an integer.","SDF consists of three steps.","First, it converts a factoring problem to an optimization problem without an objective function.","Then, we use a Quantum Feasibility Labeling (QFL) to label every possible solution according to whether it is feasible or infeasible for the optimization problem.","Finally, the Variational Quantum Search (VQS) is used to find all feasible solutions.","The SDF algorithm utilizes shallow-depth quantum circuits for efficient factorization, with the circuit depth scaling linearly as the integer to be factorized increases.","Through minimizing the number of gates in the circuit, the algorithm enhances feasibility and reduces vulnerability to errors."],"url":"http://arxiv.org/abs/2305.19542v1"}
{"created":"2023-05-31","title":"Noisy-label Learning with Sample Selection based on Noise Rate Estimate","abstract":"Noisy-labels are challenging for deep learning due to the high capacity of the deep models that can overfit noisy-label training samples. Arguably the most realistic and coincidentally challenging type of label noise is the instance-dependent noise (IDN), where the labelling errors are caused by the ambivalent information present in the images. The most successful label noise learning techniques to address IDN problems usually contain a noisy-label sample selection stage to separate clean and noisy-label samples during training. Such sample selection depends on a criterion, such as loss or gradient, and on a curriculum to define the proportion of training samples to be classified as clean at each training epoch.   Even though the estimated noise rate from the training set appears to be a natural signal to be used in the definition of this curriculum, previous approaches generally rely on arbitrary thresholds or pre-defined selection functions to the best of our knowledge. This paper addresses this research gap by proposing a new noisy-label learning graphical model that can easily accommodate state-of-the-art (SOTA) noisy-label learning methods and provide them with a reliable noise rate estimate to be used in a new sample selection curriculum. We show empirically that our model integrated with many SOTA methods can improve their results in many IDN benchmarks, including synthetic and real-world datasets.","sentences":["Noisy-labels are challenging for deep learning due to the high capacity of the deep models that can overfit noisy-label training samples.","Arguably the most realistic and coincidentally challenging type of label noise is the instance-dependent noise (IDN), where the labelling errors are caused by the ambivalent information present in the images.","The most successful label noise learning techniques to address IDN problems usually contain a noisy-label sample selection stage to separate clean and noisy-label samples during training.","Such sample selection depends on a criterion, such as loss or gradient, and on a curriculum to define the proportion of training samples to be classified as clean at each training epoch.   ","Even though the estimated noise rate from the training set appears to be a natural signal to be used in the definition of this curriculum, previous approaches generally rely on arbitrary thresholds or pre-defined selection functions to the best of our knowledge.","This paper addresses this research gap by proposing a new noisy-label learning graphical model that can easily accommodate state-of-the-art (SOTA) noisy-label learning methods and provide them with a reliable noise rate estimate to be used in a new sample selection curriculum.","We show empirically that our model integrated with many SOTA methods can improve their results in many IDN benchmarks, including synthetic and real-world datasets."],"url":"http://arxiv.org/abs/2305.19486v1"}
{"created":"2023-05-31","title":"Too Large; Data Reduction for Vision-Language Pre-Training","abstract":"This paper examines the problems of severe image-text misalignment and high redundancy in the widely-used large-scale Vision-Language Pre-Training (VLP) datasets. To address these issues, we propose an efficient and straightforward Vision-Language learning algorithm called TL;DR, which aims to compress the existing large VLP data into a small, high-quality set. Our approach consists of two major steps. First, a codebook-based encoder-decoder captioner is developed to select representative samples. Second, a new caption is generated to complement the original captions for selected samples, mitigating the text-image misalignment problem while maintaining uniqueness. As the result, TL;DR enables us to reduce the large dataset into a small set of high-quality data, which can serve as an alternative pre-training dataset. This algorithm significantly speeds up the time-consuming pretraining process. Specifically, TL;DR can compress the mainstream VLP datasets at a high ratio, e.g., reduce well-cleaned CC3M dataset from 2.82M to 0.67M ($\\sim$24\\%) and noisy YFCC15M from 15M to 2.5M ($\\sim$16.7\\%). Extensive experiments with three popular VLP models over seven downstream tasks show that VLP model trained on the compressed dataset provided by TL;DR can perform similar or even better results compared with training on the full-scale dataset. The code will be made available at \\url{https://github.com/showlab/data-centric.vlp}.","sentences":["This paper examines the problems of severe image-text misalignment and high redundancy in the widely-used large-scale Vision-Language Pre-Training (VLP) datasets.","To address these issues, we propose an efficient and straightforward Vision-Language learning algorithm called TL;DR, which aims to compress the existing large VLP data into a small, high-quality set.","Our approach consists of two major steps.","First, a codebook-based encoder-decoder captioner is developed to select representative samples.","Second, a new caption is generated to complement the original captions for selected samples, mitigating the text-image misalignment problem while maintaining uniqueness.","As the result, TL;DR enables us to reduce the large dataset into a small set of high-quality data, which can serve as an alternative pre-training dataset.","This algorithm significantly speeds up the time-consuming pretraining process.","Specifically, TL;DR can compress the mainstream VLP datasets at a high ratio, e.g., reduce well-cleaned CC3M dataset from 2.82M to 0.67M ($\\sim$24\\%) and noisy YFCC15M from 15M to 2.5M ($\\sim$16.7\\%).","Extensive experiments with three popular VLP models over seven downstream tasks show that VLP model trained on the compressed dataset provided by TL;DR can perform similar or even better results compared with training on the full-scale dataset.","The code will be made available at \\url{https://github.com/showlab/data-centric.vlp}."],"url":"http://arxiv.org/abs/2305.20087v1"}
{"created":"2023-05-31","title":"Centralised Design and Production of the Ultra-High Vacuum and Laser-Stabilisation Systems for the AION Ultra-Cold Strontium Laboratories","abstract":"This paper outlines the centralised design and production of the Ultra-High-Vacuum sidearm and Laser-Stabilisation systems for the AION Ultra-Cold Strontium Laboratories. Commissioning data on the residual gas and steady-state pressures in the sidearm chambers, on magnetic field quality, on laser stabilisation, and on the loading rate for the 3D Magneto-Optical Trap are presented. Streamlining the design and production of the sidearm and laser stabilisation systems enabled the AION Collaboration to build and equip in parallel five state-of-the-art Ultra-Cold Strontium Laboratories within 24 months by leveraging key expertise in the collaboration. This approach could serve as a model for the development and construction of other cold atom experiments, such as atomic clock experiments and neutral atom quantum computing systems, by establishing dedicated design and production units at national laboratories.","sentences":["This paper outlines the centralised design and production of the Ultra-High-Vacuum sidearm and Laser-Stabilisation systems for the AION Ultra-Cold Strontium Laboratories.","Commissioning data on the residual gas and steady-state pressures in the sidearm chambers, on magnetic field quality, on laser stabilisation, and on the loading rate for the 3D Magneto-Optical Trap are presented.","Streamlining the design and production of the sidearm and laser stabilisation systems enabled the AION Collaboration to build and equip in parallel five state-of-the-art Ultra-Cold Strontium Laboratories within 24 months by leveraging key expertise in the collaboration.","This approach could serve as a model for the development and construction of other cold atom experiments, such as atomic clock experiments and neutral atom quantum computing systems, by establishing dedicated design and production units at national laboratories."],"url":"http://arxiv.org/abs/2305.20060v1"}
{"created":"2023-05-31","title":"Modelling the Performance of High Capacity Access Networks for the Benefit of End-Users and Public Policies","abstract":"This paper deals with the challenge of modeling the performance of planned ultrabroadband access networks while maintaining technological neutrality and accuracy in measurable quality. We highlight the importance of such modeling also for addressing public funding policies compared to models mainly based on the maximum nominal speed of the access networks, taking also into account the widespread use of measurement tools like \"speed test\" that have influenced the perceived quality by end-users. We present a performance modelling approach based on the extension of well-known traffic models that accurately characterizes the performance of broadband access networks. We also show how the presented model has been validated with data from two network operators and has been applied to address the recent Italian public interventions for the development of ultrabroadband access networks in market failure areas.","sentences":["This paper deals with the challenge of modeling the performance of planned ultrabroadband access networks while maintaining technological neutrality and accuracy in measurable quality.","We highlight the importance of such modeling also for addressing public funding policies compared to models mainly based on the maximum nominal speed of the access networks, taking also into account the widespread use of measurement tools like \"speed test\" that have influenced the perceived quality by end-users.","We present a performance modelling approach based on the extension of well-known traffic models that accurately characterizes the performance of broadband access networks.","We also show how the presented model has been validated with data from two network operators and has been applied to address the recent Italian public interventions for the development of ultrabroadband access networks in market failure areas."],"url":"http://arxiv.org/abs/2305.20035v1"}
{"created":"2023-05-31","title":"A Study of Bayesian Neural Network Surrogates for Bayesian Optimization","abstract":"Bayesian optimization is a highly efficient approach to optimizing objective functions which are expensive to query. These objectives are typically represented by Gaussian process (GP) surrogate models which are easy to optimize and support exact inference. While standard GP surrogates have been well-established in Bayesian optimization, Bayesian neural networks (BNNs) have recently become practical function approximators, with many benefits over standard GPs such as the ability to naturally handle non-stationarity and learn representations for high-dimensional data. In this paper, we study BNNs as alternatives to standard GP surrogates for optimization. We consider a variety of approximate inference procedures for finite-width BNNs, including high-quality Hamiltonian Monte Carlo, low-cost stochastic MCMC, and heuristics such as deep ensembles. We also consider infinite-width BNNs and partially stochastic models such as deep kernel learning. We evaluate this collection of surrogate models on diverse problems with varying dimensionality, number of objectives, non-stationarity, and discrete and continuous inputs. We find: (i) the ranking of methods is highly problem dependent, suggesting the need for tailored inductive biases; (ii) HMC is the most successful approximate inference procedure for fully stochastic BNNs; (iii) full stochasticity may be unnecessary as deep kernel learning is relatively competitive; (iv) infinite-width BNNs are particularly promising, especially in high dimensions.","sentences":["Bayesian optimization is a highly efficient approach to optimizing objective functions which are expensive to query.","These objectives are typically represented by Gaussian process (GP) surrogate models which are easy to optimize and support exact inference.","While standard GP surrogates have been well-established in Bayesian optimization, Bayesian neural networks (BNNs) have recently become practical function approximators, with many benefits over standard GPs such as the ability to naturally handle non-stationarity and learn representations for high-dimensional data.","In this paper, we study BNNs as alternatives to standard GP surrogates for optimization.","We consider a variety of approximate inference procedures for finite-width BNNs, including high-quality Hamiltonian Monte Carlo, low-cost stochastic MCMC, and heuristics such as deep ensembles.","We also consider infinite-width BNNs and partially stochastic models such as deep kernel learning.","We evaluate this collection of surrogate models on diverse problems with varying dimensionality, number of objectives, non-stationarity, and discrete and continuous inputs.","We find: (i) the ranking of methods is highly problem dependent, suggesting the need for tailored inductive biases; (ii) HMC is the most successful approximate inference procedure for fully stochastic BNNs; (iii) full stochasticity may be unnecessary as deep kernel learning is relatively competitive; (iv) infinite-width BNNs are particularly promising, especially in high dimensions."],"url":"http://arxiv.org/abs/2305.20028v1"}
{"created":"2023-05-31","title":"Scalable Learning of Latent Language Structure With Logical Offline Cycle Consistency","abstract":"We introduce Logical Offline Cycle Consistency Optimization (LOCCO), a scalable, semi-supervised method for training a neural semantic parser. Conceptually, LOCCO can be viewed as a form of self-learning where the semantic parser being trained is used to generate annotations for unlabeled text that are then used as new supervision. To increase the quality of annotations, our method utilizes a count-based prior over valid formal meaning representations and a cycle-consistency score produced by a neural text generation model as additional signals. Both the prior and semantic parser are updated in an alternate fashion from full passes over the training data, which can be seen as approximating the marginalization of latent structures through stochastic variational inference. The use of a count-based prior, frozen text generation model, and offline annotation process yields an approach with negligible complexity and latency increases as compared to conventional self-learning. As an added bonus, the annotations produced by LOCCO can be trivially repurposed to train a neural text generation model. We demonstrate the utility of LOCCO on the well-known WebNLG benchmark where we obtain an improvement of 2 points against a self-learning parser under equivalent conditions, an improvement of 1.3 points against the previous state-of-the-art parser, and competitive text generation performance in terms of BLEU score.","sentences":["We introduce Logical Offline Cycle Consistency Optimization (LOCCO), a scalable, semi-supervised method for training a neural semantic parser.","Conceptually, LOCCO can be viewed as a form of self-learning where the semantic parser being trained is used to generate annotations for unlabeled text that are then used as new supervision.","To increase the quality of annotations, our method utilizes a count-based prior over valid formal meaning representations and a cycle-consistency score produced by a neural text generation model as additional signals.","Both the prior and semantic parser are updated in an alternate fashion from full passes over the training data, which can be seen as approximating the marginalization of latent structures through stochastic variational inference.","The use of a count-based prior, frozen text generation model, and offline annotation process yields an approach with negligible complexity and latency increases as compared to conventional self-learning.","As an added bonus, the annotations produced by LOCCO can be trivially repurposed to train a neural text generation model.","We demonstrate the utility of LOCCO on the well-known WebNLG benchmark where we obtain an improvement of 2 points against a self-learning parser under equivalent conditions, an improvement of 1.3 points against the previous state-of-the-art parser, and competitive text generation performance in terms of BLEU score."],"url":"http://arxiv.org/abs/2305.20018v1"}
{"created":"2023-05-31","title":"Detecting and Characterizing Mg II absorption in DESI Survey Validation Quasar Spectra","abstract":"In this paper we will present findings on the detection of Magnesium II (MgII, lambda = 2796 {\\AA}, 2803 {\\AA}) absorption systems observed in data from the Early Data Release (EDR) of the Dark Energy Spectroscopic Instrument (DESI). DESI is projected to obtain spectroscopy of approximately 3 million quasars (QSOs), of which over 99% are anticipated to be found at redshifts greater than z < 0.3, such that DESI would be able to observe an associated or intervening Mg II absorber illuminated by the background QSO. We have developed an autonomous supplementary spectral pipeline that detects such systems through an initial line-fitting process and then confirms line properties using a Markov Chain Monte Carlo (MCMC) sampler. Based upon both a visual inspection and the reanalysis of coadded observations, we estimate this sample of absorption systems to have a completeness of 82.56% and purity of 99.08%. As the spectra in which Mg II systems are detected are the result of coadding multiple observations, we can determine the sensitivity, and therefore completeness, of the sample by searching for known Mg II systems in coadded data with fewer observations (and therefore lower signal-to-noise). From a parent catalog containing 83,207 quasars, we detect a total of 23,921 Mg II absorption systems following a series of quality cuts. Extrapolating from this occurrence rate of 28.75% implies a catalog at the completion of the five-year DESI survey that contains over eight hundred thousand Mg II absorbers. The cataloging of these systems will enable significant further research as they carry information regarding circumgalactic medium (CGM) environments, the distribution of intervening galaxies, and the growth of metallicity across the redshift range 0.3 < z < 2.5.","sentences":["In this paper we will present findings on the detection of Magnesium II (MgII, lambda = 2796 {\\AA}, 2803 {\\AA}) absorption systems observed in data from the Early Data Release (EDR) of the Dark Energy Spectroscopic Instrument (DESI).","DESI is projected to obtain spectroscopy of approximately 3 million quasars (QSOs), of which over 99% are anticipated to be found at redshifts greater than z < 0.3, such that DESI would be able to observe an associated or intervening Mg II absorber illuminated by the background QSO.","We have developed an autonomous supplementary spectral pipeline that detects such systems through an initial line-fitting process and then confirms line properties using a Markov Chain Monte Carlo (MCMC) sampler.","Based upon both a visual inspection and the reanalysis of coadded observations, we estimate this sample of absorption systems to have a completeness of 82.56% and purity of 99.08%.","As the spectra in which Mg II systems are detected are the result of coadding multiple observations, we can determine the sensitivity, and therefore completeness, of the sample by searching for known Mg II systems in coadded data with fewer observations (and therefore lower signal-to-noise).","From a parent catalog containing 83,207 quasars, we detect a total of 23,921 Mg II absorption systems following a series of quality cuts.","Extrapolating from this occurrence rate of 28.75% implies a catalog at the completion of the five-year DESI survey that contains over eight hundred thousand Mg II absorbers.","The cataloging of these systems will enable significant further research as they carry information regarding circumgalactic medium (CGM) environments, the distribution of intervening galaxies, and the growth of metallicity across the redshift range 0.3 <","z < 2.5."],"url":"http://arxiv.org/abs/2305.20016v1"}
{"created":"2023-05-31","title":"A Novel Black Box Process Quality Optimization Approach based on Hit Rate","abstract":"Hit rate is a key performance metric in predicting process product quality in integrated industrial processes. It represents the percentage of products accepted by downstream processes within a controlled range of quality. However, optimizing hit rate is a non-convex and challenging problem. To address this issue, we propose a data-driven quasi-convex approach that combines factorial hidden Markov models, multitask elastic net, and quasi-convex optimization. Our approach converts the original non-convex problem into a set of convex feasible problems, achieving an optimal hit rate. We verify the convex optimization property and quasi-convex frontier through Monte Carlo simulations and real-world experiments in steel production. Results demonstrate that our approach outperforms classical models, improving hit rates by at least 41.11% and 31.01% on two real datasets. Furthermore, the quasi-convex frontier provides a reference explanation and visualization for the deterioration of solutions obtained by conventional models.","sentences":["Hit rate is a key performance metric in predicting process product quality in integrated industrial processes.","It represents the percentage of products accepted by downstream processes within a controlled range of quality.","However, optimizing hit rate is a non-convex and challenging problem.","To address this issue, we propose a data-driven quasi-convex approach that combines factorial hidden Markov models, multitask elastic net, and quasi-convex optimization.","Our approach converts the original non-convex problem into a set of convex feasible problems, achieving an optimal hit rate.","We verify the convex optimization property and quasi-convex frontier through Monte Carlo simulations and real-world experiments in steel production.","Results demonstrate that our approach outperforms classical models, improving hit rates by at least 41.11% and 31.01% on two real datasets.","Furthermore, the quasi-convex frontier provides a reference explanation and visualization for the deterioration of solutions obtained by conventional models."],"url":"http://arxiv.org/abs/2305.20003v1"}
{"created":"2023-05-31","title":"Treasure in Distribution: A Domain Randomization based Multi-Source Domain Generalization for 2D Medical Image Segmentation","abstract":"Although recent years have witnessed the great success of convolutional neural networks (CNNs) in medical image segmentation, the domain shift issue caused by the highly variable image quality of medical images hinders the deployment of CNNs in real-world clinical applications. Domain generalization (DG) methods aim to address this issue by training a robust model on the source domain, which has a strong generalization ability. Previously, many DG methods based on feature-space domain randomization have been proposed, which, however, suffer from the limited and unordered search space of feature styles. In this paper, we propose a multi-source DG method called Treasure in Distribution (TriD), which constructs an unprecedented search space to obtain the model with strong robustness by randomly sampling from a uniform distribution. To learn the domain-invariant representations explicitly, we further devise a style-mixing strategy in our TriD, which mixes the feature styles by randomly mixing the augmented and original statistics along the channel wise and can be extended to other DG methods. Extensive experiments on two medical segmentation tasks with different modalities demonstrate that our TriD achieves superior generalization performance on unseen target-domain data. Code is available at https://github.com/Chen-Ziyang/TriD.","sentences":["Although recent years have witnessed the great success of convolutional neural networks (CNNs) in medical image segmentation, the domain shift issue caused by the highly variable image quality of medical images hinders the deployment of CNNs in real-world clinical applications.","Domain generalization (DG) methods aim to address this issue by training a robust model on the source domain, which has a strong generalization ability.","Previously, many DG methods based on feature-space domain randomization have been proposed, which, however, suffer from the limited and unordered search space of feature styles.","In this paper, we propose a multi-source DG method called Treasure in Distribution (TriD), which constructs an unprecedented search space to obtain the model with strong robustness by randomly sampling from a uniform distribution.","To learn the domain-invariant representations explicitly, we further devise a style-mixing strategy in our TriD, which mixes the feature styles by randomly mixing the augmented and original statistics along the channel wise and can be extended to other DG methods.","Extensive experiments on two medical segmentation tasks with different modalities demonstrate that our TriD achieves superior generalization performance on unseen target-domain data.","Code is available at https://github.com/Chen-Ziyang/TriD."],"url":"http://arxiv.org/abs/2305.19949v1"}
{"created":"2023-05-31","title":"A Geometric Perspective on Diffusion Models","abstract":"Recent years have witnessed significant progress in developing efficient training and fast sampling approaches for diffusion models. A recent remarkable advancement is the use of stochastic differential equations (SDEs) to describe data perturbation and generative modeling in a unified mathematical framework. In this paper, we reveal several intriguing geometric structures of diffusion models and contribute a simple yet powerful interpretation to their sampling dynamics. Through carefully inspecting a popular variance-exploding SDE and its marginal-preserving ordinary differential equation (ODE) for sampling, we discover that the data distribution and the noise distribution are smoothly connected with an explicit, quasi-linear sampling trajectory, and another implicit denoising trajectory, which even converges faster in terms of visual quality. We also establish a theoretical relationship between the optimal ODE-based sampling and the classic mean-shift (mode-seeking) algorithm, with which we can characterize the asymptotic behavior of diffusion models and identify the score deviation. These new geometric observations enable us to improve previous sampling algorithms, re-examine latent interpolation, as well as re-explain the working principles of distillation-based fast sampling techniques.","sentences":["Recent years have witnessed significant progress in developing efficient training and fast sampling approaches for diffusion models.","A recent remarkable advancement is the use of stochastic differential equations (SDEs) to describe data perturbation and generative modeling in a unified mathematical framework.","In this paper, we reveal several intriguing geometric structures of diffusion models and contribute a simple yet powerful interpretation to their sampling dynamics.","Through carefully inspecting a popular variance-exploding SDE and its marginal-preserving ordinary differential equation (ODE) for sampling, we discover that the data distribution and the noise distribution are smoothly connected with an explicit, quasi-linear sampling trajectory, and another implicit denoising trajectory, which even converges faster in terms of visual quality.","We also establish a theoretical relationship between the optimal ODE-based sampling and the classic mean-shift (mode-seeking) algorithm, with which we can characterize the asymptotic behavior of diffusion models and identify the score deviation.","These new geometric observations enable us to improve previous sampling algorithms, re-examine latent interpolation, as well as re-explain the working principles of distillation-based fast sampling techniques."],"url":"http://arxiv.org/abs/2305.19947v1"}
{"created":"2023-05-31","title":"MetaDiffuser: Diffusion Model as Conditional Planner for Offline Meta-RL","abstract":"Recently, diffusion model shines as a promising backbone for the sequence modeling paradigm in offline reinforcement learning(RL). However, these works mostly lack the generalization ability across tasks with reward or dynamics change. To tackle this challenge, in this paper we propose a task-oriented conditioned diffusion planner for offline meta-RL(MetaDiffuser), which considers the generalization problem as conditional trajectory generation task with contextual representation. The key is to learn a context conditioned diffusion model which can generate task-oriented trajectories for planning across diverse tasks. To enhance the dynamics consistency of the generated trajectories while encouraging trajectories to achieve high returns, we further design a dual-guided module in the sampling process of the diffusion model. The proposed framework enjoys the robustness to the quality of collected warm-start data from the testing task and the flexibility to incorporate with different task representation method. The experiment results on MuJoCo benchmarks show that MetaDiffuser outperforms other strong offline meta-RL baselines, demonstrating the outstanding conditional generation ability of diffusion architecture.","sentences":["Recently, diffusion model shines as a promising backbone for the sequence modeling paradigm in offline reinforcement learning(RL).","However, these works mostly lack the generalization ability across tasks with reward or dynamics change.","To tackle this challenge, in this paper we propose a task-oriented conditioned diffusion planner for offline meta-RL(MetaDiffuser), which considers the generalization problem as conditional trajectory generation task with contextual representation.","The key is to learn a context conditioned diffusion model which can generate task-oriented trajectories for planning across diverse tasks.","To enhance the dynamics consistency of the generated trajectories while encouraging trajectories to achieve high returns, we further design a dual-guided module in the sampling process of the diffusion model.","The proposed framework enjoys the robustness to the quality of collected warm-start data from the testing task and the flexibility to incorporate with different task representation method.","The experiment results on MuJoCo benchmarks show that MetaDiffuser outperforms other strong offline meta-RL baselines, demonstrating the outstanding conditional generation ability of diffusion architecture."],"url":"http://arxiv.org/abs/2305.19923v1"}
{"created":"2023-05-31","title":"Managed Geo-Distributed Feature Store: Architecture and System Design","abstract":"Companies are using machine learning to solve real-world problems and are developing hundreds to thousands of features in the process. They are building feature engineering pipelines as part of MLOps life cycle to transform data from various data sources and materialize the same for future consumption. Without feature stores, different teams across various business groups would maintain the above process independently, which can lead to conflicting and duplicated features in the system. Data scientists find it hard to search for and reuse existing features and it is painful to maintain version control. Furthermore, feature correctness violations related to online (inferencing) - offline (training) skews and data leakage are common. Although the machine learning community has extensively discussed the need for feature stores and their purpose, this paper aims to capture the core architectural components that make up a managed feature store and to share the design learning in building such a system.","sentences":["Companies are using machine learning to solve real-world problems and are developing hundreds to thousands of features in the process.","They are building feature engineering pipelines as part of MLOps life cycle to transform data from various data sources and materialize the same for future consumption.","Without feature stores, different teams across various business groups would maintain the above process independently, which can lead to conflicting and duplicated features in the system.","Data scientists find it hard to search for and reuse existing features and it is painful to maintain version control.","Furthermore, feature correctness violations related to online (inferencing) - offline (training) skews and data leakage are common.","Although the machine learning community has extensively discussed the need for feature stores and their purpose, this paper aims to capture the core architectural components that make up a managed feature store and to share the design learning in building such a system."],"url":"http://arxiv.org/abs/2305.20077v1"}
