{"created":"2024-01-16 14:11:54","title":"Robotic Imitation of Human Actions","abstract":"Imitation can allow us to quickly gain an understanding of a new task. Through a demonstration, we can gain direct knowledge about which actions need to be performed and which goals they have. In this paper, we introduce a new approach to imitation learning that tackles the challenges of a robot imitating a human, such as the change in perspective and body schema. Our approach can use a single human demonstration to abstract information about the demonstrated task, and use that information to generalise and replicate it. We facilitate this ability by a new integration of two state-of-the-art methods: a diffusion action segmentation model to abstract temporal information from the demonstration and an open vocabulary object detector for spatial information. Furthermore, we refine the abstracted information and use symbolic reasoning to create an action plan utilising inverse kinematics, to allow the robot to imitate the demonstrated action.","sentences":["Imitation can allow us to quickly gain an understanding of a new task.","Through a demonstration, we can gain direct knowledge about which actions need to be performed and which goals they have.","In this paper, we introduce a new approach to imitation learning that tackles the challenges of a robot imitating a human, such as the change in perspective and body schema.","Our approach can use a single human demonstration to abstract information about the demonstrated task, and use that information to generalise and replicate it.","We facilitate this ability by a new integration of two state-of-the-art methods: a diffusion action segmentation model to abstract temporal information from the demonstration and an open vocabulary object detector for spatial information.","Furthermore, we refine the abstracted information and use symbolic reasoning to create an action plan utilising inverse kinematics, to allow the robot to imitate the demonstrated action."],"url":"http://arxiv.org/abs/2401.08381v1"}
{"created":"2024-01-16 14:07:48","title":"KADEL: Knowledge-Aware Denoising Learning for Commit Message Generation","abstract":"Commit messages are natural language descriptions of code changes, which are important for software evolution such as code understanding and maintenance. However, previous methods are trained on the entire dataset without considering the fact that a portion of commit messages adhere to good practice (i.e., good-practice commits), while the rest do not. On the basis of our empirical study, we discover that training on good-practice commits significantly contributes to the commit message generation. Motivated by this finding, we propose a novel knowledge-aware denoising learning method called KADEL. Considering that good-practice commits constitute only a small proportion of the dataset, we align the remaining training samples with these good-practice commits. To achieve this, we propose a model that learns the commit knowledge by training on good-practice commits. This knowledge model enables supplementing more information for training samples that do not conform to good practice. However, since the supplementary information may contain noise or prediction errors, we propose a dynamic denoising training method. This method composes a distribution-aware confidence function and a dynamic distribution list, which enhances the effectiveness of the training process. Experimental results on the whole MCMD dataset demonstrate that our method overall achieves state-of-the-art performance compared with previous methods. Our source code and data are available at https://github.com/DeepSoftwareAnalytics/KADEL","sentences":["Commit messages are natural language descriptions of code changes, which are important for software evolution such as code understanding and maintenance.","However, previous methods are trained on the entire dataset without considering the fact that a portion of commit messages adhere to good practice (i.e., good-practice commits), while the rest do not.","On the basis of our empirical study, we discover that training on good-practice commits significantly contributes to the commit message generation.","Motivated by this finding, we propose a novel knowledge-aware denoising learning method called KADEL.","Considering that good-practice commits constitute only a small proportion of the dataset, we align the remaining training samples with these good-practice commits.","To achieve this, we propose a model that learns the commit knowledge by training on good-practice commits.","This knowledge model enables supplementing more information for training samples that do not conform to good practice.","However, since the supplementary information may contain noise or prediction errors, we propose a dynamic denoising training method.","This method composes a distribution-aware confidence function and a dynamic distribution list, which enhances the effectiveness of the training process.","Experimental results on the whole MCMD dataset demonstrate that our method overall achieves state-of-the-art performance compared with previous methods.","Our source code and data are available at https://github.com/DeepSoftwareAnalytics/KADEL"],"url":"http://arxiv.org/abs/2401.08376v1"}
{"created":"2024-01-16 14:00:28","title":"Cross-lingual neural fuzzy matching for exploiting target-language monolingual corpora in computer-aided translation","abstract":"Computer-aided translation (CAT) tools based on translation memories (MT) play a prominent role in the translation workflow of professional translators. However, the reduced availability of in-domain TMs, as compared to in-domain monolingual corpora, limits its adoption for a number of translation tasks. In this paper, we introduce a novel neural approach aimed at overcoming this limitation by exploiting not only TMs, but also in-domain target-language (TL) monolingual corpora, and still enabling a similar functionality to that offered by conventional TM-based CAT tools. Our approach relies on cross-lingual sentence embeddings to retrieve translation proposals from TL monolingual corpora, and on a neural model to estimate their post-editing effort. The paper presents an automatic evaluation of these techniques on four language pairs that shows that our approach can successfully exploit monolingual texts in a TM-based CAT environment, increasing the amount of useful translation proposals, and that our neural model for estimating the post-editing effort enables the combination of translation proposals obtained from monolingual corpora and from TMs in the usual way. A human evaluation performed on a single language pair confirms the results of the automatic evaluation and seems to indicate that the translation proposals retrieved with our approach are more useful than what the automatic evaluation shows.","sentences":["Computer-aided translation (CAT) tools based on translation memories (MT) play a prominent role in the translation workflow of professional translators.","However, the reduced availability of in-domain TMs, as compared to in-domain monolingual corpora, limits its adoption for a number of translation tasks.","In this paper, we introduce a novel neural approach aimed at overcoming this limitation by exploiting not only TMs, but also in-domain target-language (TL) monolingual corpora, and still enabling a similar functionality to that offered by conventional TM-based CAT tools.","Our approach relies on cross-lingual sentence embeddings to retrieve translation proposals from TL monolingual corpora, and on a neural model to estimate their post-editing effort.","The paper presents an automatic evaluation of these techniques on four language pairs that shows that our approach can successfully exploit monolingual texts in a TM-based CAT environment, increasing the amount of useful translation proposals, and that our neural model for estimating the post-editing effort enables the combination of translation proposals obtained from monolingual corpora and from TMs in the usual way.","A human evaluation performed on a single language pair confirms the results of the automatic evaluation and seems to indicate that the translation proposals retrieved with our approach are more useful than what the automatic evaluation shows."],"url":"http://arxiv.org/abs/2401.08374v1"}
{"created":"2024-01-16 13:52:25","title":"Morphology and Syntax of the Tamil Language","abstract":"This paper provides an overview of the morphology and syntax of the Tamil language, focusing on its contemporary usage. The paper also highlights the complexity and richness of Tamil in terms of its morphological and syntactic features, which will be useful for linguists analysing the language and conducting comparative studies. In addition, the paper will be useful for those developing computational resources for the Tamil language. It is proven as a rule-based morphological analyser cum generator and a computational grammar for Tamil have already been developed based on this paper. To enhance accessibility for a broader audience, the analysis is conducted without relying on any specific grammatical formalism.","sentences":["This paper provides an overview of the morphology and syntax of the Tamil language, focusing on its contemporary usage.","The paper also highlights the complexity and richness of Tamil in terms of its morphological and syntactic features, which will be useful for linguists analysing the language and conducting comparative studies.","In addition, the paper will be useful for those developing computational resources for the Tamil language.","It is proven as a rule-based morphological analyser cum generator and a computational grammar for Tamil have already been developed based on this paper.","To enhance accessibility for a broader audience, the analysis is conducted without relying on any specific grammatical formalism."],"url":"http://arxiv.org/abs/2401.08367v1"}
{"created":"2024-01-16 13:48:35","title":"On the formalization of the notion of an algorithm","abstract":"The starting point of this paper is a collection of properties of an algorithm that have been distilled from the informal descriptions of what an algorithm is that are given in standard works from the mathematical and computer science literature. Based on that, the notion of a proto-algorithm is introduced. The thought is that algorithms are equivalence classes of proto-algorithms under some equivalence relation. Three equivalence relations are defined. Two of them give bounds between which an appropriate equivalence relation must lie. The third lies in between these two and is likely an appropriate equivalence relation. A sound method is presented to prove, using an imperative process algebra based on ACP, that this equivalence relation holds between two proto-algorithms.","sentences":["The starting point of this paper is a collection of properties of an algorithm that have been distilled from the informal descriptions of what an algorithm is that are given in standard works from the mathematical and computer science literature.","Based on that, the notion of a proto-algorithm is introduced.","The thought is that algorithms are equivalence classes of proto-algorithms under some equivalence relation.","Three equivalence relations are defined.","Two of them give bounds between which an appropriate equivalence relation must lie.","The third lies in between these two and is likely an appropriate equivalence relation.","A sound method is presented to prove, using an imperative process algebra based on ACP, that this equivalence relation holds between two proto-algorithms."],"url":"http://arxiv.org/abs/2401.08366v1"}
{"created":"2024-01-16 13:46:10","title":"Weighted Spectral Filters for Kernel Interpolation on Spheres: Estimates of Prediction Accuracy for Noisy Data","abstract":"Spherical radial-basis-based kernel interpolation abounds in image sciences including geophysical image reconstruction, climate trends description and image rendering due to its excellent spatial localization property and perfect approximation performance. However, in dealing with noisy data, kernel interpolation frequently behaves not so well due to the large condition number of the kernel matrix and instability of the interpolation process. In this paper, we introduce a weighted spectral filter approach to reduce the condition number of the kernel matrix and then stabilize kernel interpolation. The main building blocks of the proposed method are the well developed spherical positive quadrature rules and high-pass spectral filters. Using a recently developed integral operator approach for spherical data analysis, we theoretically demonstrate that the proposed weighted spectral filter approach succeeds in breaking through the bottleneck of kernel interpolation, especially in fitting noisy data. We provide optimal approximation rates of the new method to show that our approach does not compromise the predicting accuracy. Furthermore, we conduct both toy simulations and two real-world data experiments with synthetically added noise in geophysical image reconstruction and climate image processing to verify our theoretical assertions and show the feasibility of the weighted spectral filter approach.","sentences":["Spherical radial-basis-based kernel interpolation abounds in image sciences including geophysical image reconstruction, climate trends description and image rendering due to its excellent spatial localization property and perfect approximation performance.","However, in dealing with noisy data, kernel interpolation frequently behaves not so well due to the large condition number of the kernel matrix and instability of the interpolation process.","In this paper, we introduce a weighted spectral filter approach to reduce the condition number of the kernel matrix and then stabilize kernel interpolation.","The main building blocks of the proposed method are the well developed spherical positive quadrature rules and high-pass spectral filters.","Using a recently developed integral operator approach for spherical data analysis, we theoretically demonstrate that the proposed weighted spectral filter approach succeeds in breaking through the bottleneck of kernel interpolation, especially in fitting noisy data.","We provide optimal approximation rates of the new method to show that our approach does not compromise the predicting accuracy.","Furthermore, we conduct both toy simulations and two real-world data experiments with synthetically added noise in geophysical image reconstruction and climate image processing to verify our theoretical assertions and show the feasibility of the weighted spectral filter approach."],"url":"http://arxiv.org/abs/2401.08364v1"}
{"created":"2024-01-16 13:45:54","title":"Mitigating Bias in Machine Learning Models for Phishing Webpage Detection","abstract":"The widespread accessibility of the Internet has led to a surge in online fraudulent activities, underscoring the necessity of shielding users' sensitive information from cybercriminals. Phishing, a well-known cyberattack, revolves around the creation of phishing webpages and the dissemination of corresponding URLs, aiming to deceive users into sharing their sensitive information, often for identity theft or financial gain. Various techniques are available for preemptively categorizing zero-day phishing URLs by distilling unique attributes and constructing predictive models. However, these existing techniques encounter unresolved issues. This proposal delves into persistent challenges within phishing detection solutions, particularly concentrated on the preliminary phase of assembling comprehensive datasets, and proposes a potential solution in the form of a tool engineered to alleviate bias in ML models. Such a tool can generate phishing webpages for any given set of legitimate URLs, infusing randomly selected content and visual-based phishing features. Furthermore, we contend that the tool holds the potential to assess the efficacy of existing phishing detection solutions, especially those trained on confined datasets.","sentences":["The widespread accessibility of the Internet has led to a surge in online fraudulent activities, underscoring the necessity of shielding users' sensitive information from cybercriminals.","Phishing, a well-known cyberattack, revolves around the creation of phishing webpages and the dissemination of corresponding URLs, aiming to deceive users into sharing their sensitive information, often for identity theft or financial gain.","Various techniques are available for preemptively categorizing zero-day phishing URLs by distilling unique attributes and constructing predictive models.","However, these existing techniques encounter unresolved issues.","This proposal delves into persistent challenges within phishing detection solutions, particularly concentrated on the preliminary phase of assembling comprehensive datasets, and proposes a potential solution in the form of a tool engineered to alleviate bias in ML models.","Such a tool can generate phishing webpages for any given set of legitimate URLs, infusing randomly selected content and visual-based phishing features.","Furthermore, we contend that the tool holds the potential to assess the efficacy of existing phishing detection solutions, especially those trained on confined datasets."],"url":"http://arxiv.org/abs/2401.08363v1"}
{"created":"2024-01-16 13:37:45","title":"AdaSem: Adaptive Goal-Oriented Semantic Communications for End-to-End Camera Relocalization","abstract":"Recently, deep autoencoders have gained traction as a powerful method for implementing goal-oriented semantic communications systems. The idea is to train a mapping from the source domain directly to channel symbols, and vice versa. However, prior studies often focused on rate-distortion tradeoff and transmission delay, at the cost of increasing end-to-end complexity and thus latency. Moreover, the datasets used are often not reflective of real-world environments, and the results were not validated against real-world baseline systems, leading to an unfair comparison. In this paper, we study the problem of remote camera pose estimation and propose AdaSem, an adaptive semantic communications approach that optimizes the tradeoff between inference accuracy and end-to-end latency. We develop an adaptive semantic codec model, which encodes the source data into a dynamic number of symbols, based on the latent space distribution and the channel state feedback. We utilize a lightweight model for both transmitter and receiver to ensure comparable complexity to the baseline implemented in a real-world system. Extensive experiments on real-environment data show the effectiveness of our approach. When compared to a real implementation of a client-server camera relocalization service, AdaSem outperforms the baseline by reducing the end-to-end delay and estimation error by over 75% and 63%, respectively.","sentences":["Recently, deep autoencoders have gained traction as a powerful method for implementing goal-oriented semantic communications systems.","The idea is to train a mapping from the source domain directly to channel symbols, and vice versa.","However, prior studies often focused on rate-distortion tradeoff and transmission delay, at the cost of increasing end-to-end complexity and thus latency.","Moreover, the datasets used are often not reflective of real-world environments, and the results were not validated against real-world baseline systems, leading to an unfair comparison.","In this paper, we study the problem of remote camera pose estimation and propose AdaSem, an adaptive semantic communications approach that optimizes the tradeoff between inference accuracy and end-to-end latency.","We develop an adaptive semantic codec model, which encodes the source data into a dynamic number of symbols, based on the latent space distribution and the channel state feedback.","We utilize a lightweight model for both transmitter and receiver to ensure comparable complexity to the baseline implemented in a real-world system.","Extensive experiments on real-environment data show the effectiveness of our approach.","When compared to a real implementation of a client-server camera relocalization service, AdaSem outperforms the baseline by reducing the end-to-end delay and estimation error by over 75% and 63%, respectively."],"url":"http://arxiv.org/abs/2401.08360v1"}
{"created":"2024-01-16 13:36:07","title":"Hallucination Detection and Hallucination Mitigation: An Investigation","abstract":"Large language models (LLMs), including ChatGPT, Bard, and Llama, have achieved remarkable successes over the last two years in a range of different applications. In spite of these successes, there exist concerns that limit the wide application of LLMs. A key problem is the problem of hallucination. Hallucination refers to the fact that in addition to correct responses, LLMs can also generate seemingly correct but factually incorrect responses. This report aims to present a comprehensive review of the current literature on both hallucination detection and hallucination mitigation. We hope that this report can serve as a good reference for both engineers and researchers who are interested in LLMs and applying them to real world tasks.","sentences":["Large language models (LLMs), including ChatGPT, Bard, and Llama, have achieved remarkable successes over the last two years in a range of different applications.","In spite of these successes, there exist concerns that limit the wide application of LLMs.","A key problem is the problem of hallucination.","Hallucination refers to the fact that in addition to correct responses, LLMs can also generate seemingly correct but factually incorrect responses.","This report aims to present a comprehensive review of the current literature on both hallucination detection and hallucination mitigation.","We hope that this report can serve as a good reference for both engineers and researchers who are interested in LLMs and applying them to real world tasks."],"url":"http://arxiv.org/abs/2401.08358v1"}
{"created":"2024-01-16 13:35:28","title":"SAMF: Small-Area-Aware Multi-focus Image Fusion for Object Detection","abstract":"Existing multi-focus image fusion (MFIF) methods often fail to preserve the uncertain transition region and detect small focus areas within large defocused regions accurately. To address this issue, this study proposes a new small-area-aware MFIF algorithm for enhancing object detection capability. First, we enhance the pixel attributes within the small focus and boundary regions, which are subsequently combined with visual saliency detection to obtain the pre-fusion results used to discriminate the distribution of focused pixels. To accurately ensure pixel focus, we consider the source image as a combination of focused, defocused, and uncertain regions and propose a three-region segmentation strategy. Finally, we design an effective pixel selection rule to generate segmentation decision maps and obtain the final fusion results. Experiments demonstrated that the proposed method can accurately detect small and smooth focus areas while improving object detection performance, outperforming existing methods in both subjective and objective evaluations. The source code is available at https://github.com/ixilai/SAMF.","sentences":["Existing multi-focus image fusion (MFIF) methods often fail to preserve the uncertain transition region and detect small focus areas within large defocused regions accurately.","To address this issue, this study proposes a new small-area-aware MFIF algorithm for enhancing object detection capability.","First, we enhance the pixel attributes within the small focus and boundary regions, which are subsequently combined with visual saliency detection to obtain the pre-fusion results used to discriminate the distribution of focused pixels.","To accurately ensure pixel focus, we consider the source image as a combination of focused, defocused, and uncertain regions and propose a three-region segmentation strategy.","Finally, we design an effective pixel selection rule to generate segmentation decision maps and obtain the final fusion results.","Experiments demonstrated that the proposed method can accurately detect small and smooth focus areas while improving object detection performance, outperforming existing methods in both subjective and objective evaluations.","The source code is available at https://github.com/ixilai/SAMF."],"url":"http://arxiv.org/abs/2401.08357v1"}
{"created":"2024-01-16 13:30:37","title":"Personalized Federated Learning of Probabilistic Models: A PAC-Bayesian Approach","abstract":"Federated learning aims to infer a shared model from private and decentralized data stored locally by multiple clients. Personalized federated learning (PFL) goes one step further by adapting the global model to each client, enhancing the model's fit for different clients. A significant level of personalization is required for highly heterogeneous clients, but can be challenging to achieve especially when they have small datasets. To address this problem, we propose a PFL algorithm named PAC-PFL for learning probabilistic models within a PAC-Bayesian framework that utilizes differential privacy to handle data-dependent priors. Our algorithm collaboratively learns a shared hyper-posterior and regards each client's posterior inference as the personalization step. By establishing and minimizing a generalization bound on the average true risk of clients, PAC-PFL effectively combats over-fitting. PACPFL achieves accurate and well-calibrated predictions, supported by experiments on a dataset of photovoltaic panel power generation, FEMNIST dataset (Caldas et al., 2019), and Dirichlet-partitioned EMNIST dataset (Cohen et al., 2017).","sentences":["Federated learning aims to infer a shared model from private and decentralized data stored locally by multiple clients.","Personalized federated learning (PFL) goes one step further by adapting the global model to each client, enhancing the model's fit for different clients.","A significant level of personalization is required for highly heterogeneous clients, but can be challenging to achieve especially when they have small datasets.","To address this problem, we propose a PFL algorithm named PAC-PFL for learning probabilistic models within a PAC-Bayesian framework that utilizes differential privacy to handle data-dependent priors.","Our algorithm collaboratively learns a shared hyper-posterior and regards each client's posterior inference as the personalization step.","By establishing and minimizing a generalization bound on the average true risk of clients, PAC-PFL effectively combats over-fitting.","PACPFL achieves accurate and well-calibrated predictions, supported by experiments on a dataset of photovoltaic panel power generation, FEMNIST dataset (Caldas et al., 2019), and Dirichlet-partitioned EMNIST dataset (Cohen et al., 2017)."],"url":"http://arxiv.org/abs/2401.08351v1"}
{"created":"2024-01-16 13:30:09","title":"Salute the Classic: Revisiting Challenges of Machine Translation in the Age of Large Language Models","abstract":"The evolution of Neural Machine Translation (NMT) has been significantly influenced by six core challenges (Koehn and Knowles, 2017), which have acted as benchmarks for progress in this field. This study revisits these challenges, offering insights into their ongoing relevance in the context of advanced Large Language Models (LLMs): domain mismatch, amount of parallel data, rare word prediction, translation of long sentences, attention model as word alignment, and sub-optimal beam search. Our empirical findings indicate that LLMs effectively lessen the reliance on parallel data for major languages in the pretraining phase. Additionally, the LLM-based translation system significantly enhances the translation of long sentences that contain approximately 80 words and shows the capability to translate documents of up to 512 words. However, despite these significant improvements, the challenges of domain mismatch and prediction of rare words persist. While the challenges of word alignment and beam search, specifically associated with NMT, may not apply to LLMs, we identify three new challenges for LLMs in translation tasks: inference efficiency, translation of low-resource languages in the pretraining phase, and human-aligned evaluation. The datasets and models are released at https://github.com/pangjh3/LLM4MT.","sentences":["The evolution of Neural Machine Translation (NMT) has been significantly influenced by six core challenges (Koehn and Knowles, 2017), which have acted as benchmarks for progress in this field.","This study revisits these challenges, offering insights into their ongoing relevance in the context of advanced Large Language Models (LLMs): domain mismatch, amount of parallel data, rare word prediction, translation of long sentences, attention model as word alignment, and sub-optimal beam search.","Our empirical findings indicate that LLMs effectively lessen the reliance on parallel data for major languages in the pretraining phase.","Additionally, the LLM-based translation system significantly enhances the translation of long sentences that contain approximately 80 words and shows the capability to translate documents of up to 512 words.","However, despite these significant improvements, the challenges of domain mismatch and prediction of rare words persist.","While the challenges of word alignment and beam search, specifically associated with NMT, may not apply to LLMs, we identify three new challenges for LLMs in translation tasks: inference efficiency, translation of low-resource languages in the pretraining phase, and human-aligned evaluation.","The datasets and models are released at https://github.com/pangjh3/LLM4MT."],"url":"http://arxiv.org/abs/2401.08350v1"}
{"created":"2024-01-16 13:29:30","title":"We don't need no labels: Estimating post-deployment model performance under covariate shift without ground truth","abstract":"The performance of machine learning models often degrades after deployment due to data distribution shifts. In many use cases, it is impossible to calculate the post-deployment performance because labels are unavailable or significantly delayed. Proxy methods for evaluating model performance stability, like drift detection techniques, do not properly quantify data distribution shift impact. As a solution, we propose a robust and accurate performance estimation method for evaluating ML classification models on unlabeled data that accurately quantifies the impact of covariate shift on model performance. We call it multi-calibrated confidence-based performance estimation (M-CBPE). It is model and data-type agnostic and works for any performance metric. It does not require access to the monitored model - it uses the model predictions and probability estimates. M-CBPE does not need user input on the nature of the covariate shift as it fully learns from the data. We evaluate it with over 600 dataset-model pairs from US census data and compare it with multiple benchmarks using several evaluation metrics. Results show that M-CBPE is the best method to estimate the performance of classification models in any evaluation context.","sentences":["The performance of machine learning models often degrades after deployment due to data distribution shifts.","In many use cases, it is impossible to calculate the post-deployment performance because labels are unavailable or significantly delayed.","Proxy methods for evaluating model performance stability, like drift detection techniques, do not properly quantify data distribution shift impact.","As a solution, we propose a robust and accurate performance estimation method for evaluating ML classification models on unlabeled data that accurately quantifies the impact of covariate shift on model performance.","We call it multi-calibrated confidence-based performance estimation (M-CBPE).","It is model and data-type agnostic and works for any performance metric.","It does not require access to the monitored model - it uses the model predictions and probability estimates.","M-CBPE does not need user input on the nature of the covariate shift as it fully learns from the data.","We evaluate it with over 600 dataset-model pairs from US census data and compare it with multiple benchmarks using several evaluation metrics.","Results show that M-CBPE is the best method to estimate the performance of classification models in any evaluation context."],"url":"http://arxiv.org/abs/2401.08348v1"}
{"created":"2024-01-16 13:23:51","title":"Multi-view Distillation based on Multi-modal Fusion for Few-shot Action Recognition(CLIP-$\\mathrm{M^2}$DF)","abstract":"In recent years, few-shot action recognition has attracted increasing attention. It generally adopts the paradigm of meta-learning. In this field, overcoming the overlapping distribution of classes and outliers is still a challenging problem based on limited samples. We believe the combination of Multi-modal and Multi-view can improve this issue depending on information complementarity. Therefore, we propose a method of Multi-view Distillation based on Multi-modal Fusion. Firstly, a Probability Prompt Selector for the query is constructed to generate probability prompt embedding based on the comparison score between the prompt embeddings of the support and the visual embedding of the query. Secondly, we establish a Multi-view. In each view, we fuse the prompt embedding as consistent information with visual and the global or local temporal context to overcome the overlapping distribution of classes and outliers. Thirdly, we perform the distance fusion for the Multi-view and the mutual distillation of matching ability from one to another, enabling the model to be more robust to the distribution bias. Our code is available at the URL: \\url{https://github.com/cofly2014/MDMF}.","sentences":["In recent years, few-shot action recognition has attracted increasing attention.","It generally adopts the paradigm of meta-learning.","In this field, overcoming the overlapping distribution of classes and outliers is still a challenging problem based on limited samples.","We believe the combination of Multi-modal and Multi-view can improve this issue depending on information complementarity.","Therefore, we propose a method of Multi-view Distillation based on Multi-modal Fusion.","Firstly, a Probability Prompt Selector for the query is constructed to generate probability prompt embedding based on the comparison score between the prompt embeddings of the support and the visual embedding of the query.","Secondly, we establish a Multi-view.","In each view, we fuse the prompt embedding as consistent information with visual and the global or local temporal context to overcome the overlapping distribution of classes and outliers.","Thirdly, we perform the distance fusion for the Multi-view and the mutual distillation of matching ability from one to another, enabling the model to be more robust to the distribution bias.","Our code is available at the URL: \\url{https://github.com/cofly2014/MDMF}."],"url":"http://arxiv.org/abs/2401.08345v1"}
{"created":"2024-01-16 13:17:33","title":"Direct-Conflict Resolution in Intent-Driven Autonomous Networks","abstract":"As network systems evolve, there is an escalating demand for automated tools to facilitate efficient management and configuration. This paper explores conflict resolution in Intent-Based Network (IBN) management, an innovative approach that holds promise for effective network administration, especially within radio access domain. Nevertheless, when multiple intents are in operation concurrently, conflicts may emerge, presenting a significant issue that remains under-addressed in the current literature. In response to this challenge, our research expands the range of conflict resolution strategies beyond the established Nash Bargaining Solution (NBS), to incorporate the Weighted Nash Bargaining Solution (WNBS), the Kalai-Smorodinsky Bargaining Solution (KSBS), and the Shannon Entropy Bargaining Solution (SEBS). These methods are employed with the objective to identify optimal parameter values, aiming to ensure fairness in conflict resolution. Through simulations, it is demonstrated that distinct antenna tilt values are yielded as the respective solutions for each method. Ultimately, based on Jain Fairness Index, the KSBS is identified as the most equitable method under the given conditions.","sentences":["As network systems evolve, there is an escalating demand for automated tools to facilitate efficient management and configuration.","This paper explores conflict resolution in Intent-Based Network (IBN) management, an innovative approach that holds promise for effective network administration, especially within radio access domain.","Nevertheless, when multiple intents are in operation concurrently, conflicts may emerge, presenting a significant issue that remains under-addressed in the current literature.","In response to this challenge, our research expands the range of conflict resolution strategies beyond the established Nash Bargaining Solution (NBS), to incorporate the Weighted Nash Bargaining Solution (WNBS), the Kalai-Smorodinsky Bargaining Solution (KSBS), and the Shannon Entropy Bargaining Solution (SEBS).","These methods are employed with the objective to identify optimal parameter values, aiming to ensure fairness in conflict resolution.","Through simulations, it is demonstrated that distinct antenna tilt values are yielded as the respective solutions for each method.","Ultimately, based on Jain Fairness Index, the KSBS is identified as the most equitable method under the given conditions."],"url":"http://arxiv.org/abs/2401.08341v1"}
{"created":"2024-01-16 12:57:35","title":"dabih -- encrypted data storage and sharing platform","abstract":"Background: The secure management of sensitive clinical data, particularly human genomics data, has become a critical requirement in modern biomedical research. Although the necessary software and algorithms are readily available, their use by non-IT experts poses significant challenges.   Methods: We developed dabih, an open-source web application specifically designed to facilitate user-friendly encrypted data management. dabih enables web-based uploading, storing, sharing, and downloading of sensitive data in any format. Its approach to data security involves a two-stage envelope encryption process. We combine symmetric-key encryption for data and public-key encryption as key encapsulation mechanism. The private key necessary for decrypting the data remains exclusively on the owner's device. Thus, accessing data is impossible without explicit permission from the keyholder.   Results: dabih is available open-source on GitHub https://github.com/spang-lab/dabih, as ready to use containers on docker hub and includes a command line interface and a graphical bulk upload tool as pre-built binaries. Documentation is available as part of the web application.   Conclusions: dabih enables everyone to use strong cryptography for their data, while being just as simple to use as other, non-encrypted, data storage solutions. All the cryptography occurs seamlessly in the background as users interact with a secure web portal, simply by dragging and dropping files.","sentences":["Background: The secure management of sensitive clinical data, particularly human genomics data, has become a critical requirement in modern biomedical research.","Although the necessary software and algorithms are readily available, their use by non-IT experts poses significant challenges.   ","Methods: We developed dabih, an open-source web application specifically designed to facilitate user-friendly encrypted data management.","dabih enables web-based uploading, storing, sharing, and downloading of sensitive data in any format.","Its approach to data security involves a two-stage envelope encryption process.","We combine symmetric-key encryption for data and public-key encryption as key encapsulation mechanism.","The private key necessary for decrypting the data remains exclusively on the owner's device.","Thus, accessing data is impossible without explicit permission from the keyholder.   ","Results: dabih is available open-source on GitHub https://github.com/spang-lab/dabih, as ready to use containers on docker hub and includes a command line interface and a graphical bulk upload tool as pre-built binaries.","Documentation is available as part of the web application.   ","Conclusions: dabih enables everyone to use strong cryptography for their data, while being just as simple to use as other, non-encrypted, data storage solutions.","All the cryptography occurs seamlessly in the background as users interact with a secure web portal, simply by dragging and dropping files."],"url":"http://arxiv.org/abs/2401.08333v1"}
{"created":"2024-01-16 12:53:42","title":"Generative Denoise Distillation: Simple Stochastic Noises Induce Efficient Knowledge Transfer for Dense Prediction","abstract":"Knowledge distillation is the process of transferring knowledge from a more powerful large model (teacher) to a simpler counterpart (student). Numerous current approaches involve the student imitating the knowledge of the teacher directly. However, redundancy still exists in the learned representations through these prevalent methods, which tend to learn each spatial location's features indiscriminately. To derive a more compact representation (concept feature) from the teacher, inspired by human cognition, we suggest an innovative method, termed Generative Denoise Distillation (GDD), where stochastic noises are added to the concept feature of the student to embed them into the generated instance feature from a shallow network. Then, the generated instance feature is aligned with the knowledge of the instance from the teacher. We extensively experiment with object detection, instance segmentation, and semantic segmentation to demonstrate the versatility and effectiveness of our method. Notably, GDD achieves new state-of-the-art performance in the tasks mentioned above. We have achieved substantial improvements in semantic segmentation by enhancing PspNet and DeepLabV3, both of which are based on ResNet-18, resulting in mIoU scores of 74.67 and 77.69, respectively, surpassing their previous scores of 69.85 and 73.20 on the Cityscapes dataset of 20 categories. The source code of GDD is available at https://github.com/ZhgLiu/GDD.","sentences":["Knowledge distillation is the process of transferring knowledge from a more powerful large model (teacher) to a simpler counterpart (student).","Numerous current approaches involve the student imitating the knowledge of the teacher directly.","However, redundancy still exists in the learned representations through these prevalent methods, which tend to learn each spatial location's features indiscriminately.","To derive a more compact representation (concept feature) from the teacher, inspired by human cognition, we suggest an innovative method, termed Generative Denoise Distillation (GDD), where stochastic noises are added to the concept feature of the student to embed them into the generated instance feature from a shallow network.","Then, the generated instance feature is aligned with the knowledge of the instance from the teacher.","We extensively experiment with object detection, instance segmentation, and semantic segmentation to demonstrate the versatility and effectiveness of our method.","Notably, GDD achieves new state-of-the-art performance in the tasks mentioned above.","We have achieved substantial improvements in semantic segmentation by enhancing PspNet and DeepLabV3, both of which are based on ResNet-18, resulting in mIoU scores of 74.67 and 77.69, respectively, surpassing their previous scores of 69.85 and 73.20 on the Cityscapes dataset of 20 categories.","The source code of GDD is available at https://github.com/ZhgLiu/GDD."],"url":"http://arxiv.org/abs/2401.08332v1"}
{"created":"2024-01-16 12:49:10","title":"Boosting Gradient Ascent for Continuous DR-submodular Maximization","abstract":"Projected Gradient Ascent (PGA) is the most commonly used optimization scheme in machine learning and operations research areas. Nevertheless, numerous studies and examples have shown that the PGA methods may fail to achieve the tight approximation ratio for continuous DR-submodular maximization problems. To address this challenge, we present a boosting technique in this paper, which can efficiently improve the approximation guarantee of the standard PGA to \\emph{optimal} with only small modifications on the objective function. The fundamental idea of our boosting technique is to exploit non-oblivious search to derive a novel auxiliary function $F$, whose stationary points are excellent approximations to the global maximum of the original DR-submodular objective $f$. Specifically, when $f$ is monotone and $\\gamma$-weakly DR-submodular, we propose an auxiliary function $F$ whose stationary points can provide a better $(1-e^{-\\gamma})$-approximation than the $(\\gamma^2/(1+\\gamma^2))$-approximation guaranteed by the stationary points of $f$ itself. Similarly, for the non-monotone case, we devise another auxiliary function $F$ whose stationary points can achieve an optimal $\\frac{1-\\min_{\\boldsymbol{x}\\in\\mathcal{C}}\\|\\boldsymbol{x}\\|_{\\infty}}{4}$-approximation guarantee where $\\mathcal{C}$ is a convex constraint set. In contrast, the stationary points of the original non-monotone DR-submodular function can be arbitrarily bad~\\citep{chen2023continuous}. Furthermore, we demonstrate the scalability of our boosting technique on four problems. In all of these four problems, our resulting variants of boosting PGA algorithm beat the previous standard PGA in several aspects such as approximation ratio and efficiency. Finally, we corroborate our theoretical findings with numerical experiments, which demonstrate the effectiveness of our boosting PGA methods.","sentences":["Projected Gradient Ascent (PGA) is the most commonly used optimization scheme in machine learning and operations research areas.","Nevertheless, numerous studies and examples have shown that the PGA methods may fail to achieve the tight approximation ratio for continuous DR-submodular maximization problems.","To address this challenge, we present a boosting technique in this paper, which can efficiently improve the approximation guarantee of the standard PGA to \\emph{optimal} with only small modifications on the objective function.","The fundamental idea of our boosting technique is to exploit non-oblivious search to derive a novel auxiliary function $F$, whose stationary points are excellent approximations to the global maximum of the original DR-submodular objective $f$. Specifically, when $f$ is monotone and $\\gamma$-weakly DR-submodular, we propose an auxiliary function $F$ whose stationary points can provide a better $(1-e^{-\\gamma})$-approximation than the $(\\gamma^2/(1+\\gamma^2))$-approximation guaranteed by the stationary points of $f$ itself.","Similarly, for the non-monotone case, we devise another auxiliary function $F$ whose stationary points can achieve an optimal $\\frac{1-\\min_{\\boldsymbol{x}\\in\\mathcal{C}}\\|\\boldsymbol{x}\\|_{\\infty}}{4}$-approximation guarantee where $\\mathcal{C}$ is a convex constraint set.","In contrast, the stationary points of the original non-monotone DR-submodular function can be arbitrarily bad~\\citep{chen2023continuous}.","Furthermore, we demonstrate the scalability of our boosting technique on four problems.","In all of these four problems, our resulting variants of boosting PGA algorithm beat the previous standard PGA in several aspects such as approximation ratio and efficiency.","Finally, we corroborate our theoretical findings with numerical experiments, which demonstrate the effectiveness of our boosting PGA methods."],"url":"http://arxiv.org/abs/2401.08330v1"}
{"created":"2024-01-16 12:49:00","title":"Understanding User Experience in Large Language Model Interactions","abstract":"In the rapidly evolving landscape of large language models (LLMs), most research has primarily viewed them as independent individuals, focusing on assessing their capabilities through standardized benchmarks and enhancing their general intelligence. This perspective, however, tends to overlook the vital role of LLMs as user-centric services in human-AI collaboration. This gap in research becomes increasingly critical as LLMs become more integrated into people's everyday and professional interactions. This study addresses the important need to understand user satisfaction with LLMs by exploring four key aspects: comprehending user intents, scrutinizing user experiences, addressing major user concerns about current LLM services, and charting future research paths to bolster human-AI collaborations. Our study develops a taxonomy of 7 user intents in LLM interactions, grounded in analysis of real-world user interaction logs and human verification. Subsequently, we conduct a user survey to gauge their satisfaction with LLM services, encompassing usage frequency, experiences across intents, and predominant concerns. This survey, compiling 411 anonymous responses, uncovers 11 first-hand insights into the current state of user engagement with LLMs. Based on this empirical analysis, we pinpoint 6 future research directions prioritizing the user perspective in LLM developments. This user-centered approach is essential for crafting LLMs that are not just technologically advanced but also resonate with the intricate realities of human interactions and real-world applications.","sentences":["In the rapidly evolving landscape of large language models (LLMs), most research has primarily viewed them as independent individuals, focusing on assessing their capabilities through standardized benchmarks and enhancing their general intelligence.","This perspective, however, tends to overlook the vital role of LLMs as user-centric services in human-AI collaboration.","This gap in research becomes increasingly critical as LLMs become more integrated into people's everyday and professional interactions.","This study addresses the important need to understand user satisfaction with LLMs by exploring four key aspects: comprehending user intents, scrutinizing user experiences, addressing major user concerns about current LLM services, and charting future research paths to bolster human-AI collaborations.","Our study develops a taxonomy of 7 user intents in LLM interactions, grounded in analysis of real-world user interaction logs and human verification.","Subsequently, we conduct a user survey to gauge their satisfaction with LLM services, encompassing usage frequency, experiences across intents, and predominant concerns.","This survey, compiling 411 anonymous responses, uncovers 11 first-hand insights into the current state of user engagement with LLMs.","Based on this empirical analysis, we pinpoint 6 future research directions prioritizing the user perspective in LLM developments.","This user-centered approach is essential for crafting LLMs that are not just technologically advanced but also resonate with the intricate realities of human interactions and real-world applications."],"url":"http://arxiv.org/abs/2401.08329v1"}
{"created":"2024-01-16 12:48:52","title":"Un-Mixing Test-Time Normalization Statistics: Combatting Label Temporal Correlation","abstract":"In an era where test-time adaptation methods increasingly rely on the nuanced manipulation of batch normalization (BN) parameters, one critical assumption often goes overlooked: that of independently and identically distributed (i.i.d.) test batches with respect to unknown labels. This assumption culminates in biased estimates of BN statistics and jeopardizes system stability under non-i.i.d. conditions. This paper pioneers a departure from the i.i.d. paradigm by introducing a groundbreaking strategy termed \"Un-Mixing Test-Time Normalization Statistics\" (UnMix-TNS). UnMix-TNS re-calibrates the instance-wise statistics used to normalize each instance in a batch by mixing it with multiple unmixed statistics components, thus inherently simulating the i.i.d. environment. The key lies in our innovative online unmixing procedure, which persistently refines these statistics components by drawing upon the closest instances from an incoming test batch. Remarkably generic in its design, UnMix-TNS seamlessly integrates with an array of state-of-the-art test-time adaptation methods and pre-trained architectures equipped with BN layers. Empirical evaluations corroborate the robustness of UnMix-TNS under varied scenarios ranging from single to continual and mixed domain shifts. UnMix-TNS stands out when handling test data streams with temporal correlation, including those with corrupted real-world non-i.i.d. streams, sustaining its efficacy even with minimal batch sizes and individual samples. Our results set a new standard for test-time adaptation, demonstrating significant improvements in both stability and performance across multiple benchmarks.","sentences":["In an era where test-time adaptation methods increasingly rely on the nuanced manipulation of batch normalization (BN) parameters, one critical assumption often goes overlooked: that of independently and identically distributed (i.i.d.)","test batches with respect to unknown labels.","This assumption culminates in biased estimates of BN statistics and jeopardizes system stability under non-i.i.d. conditions.","This paper pioneers a departure from the i.i.d. paradigm by introducing a groundbreaking strategy termed \"Un-Mixing Test-Time Normalization Statistics\" (UnMix-TNS).","UnMix-TNS re-calibrates the instance-wise statistics used to normalize each instance in a batch by mixing it with multiple unmixed statistics components, thus inherently simulating the i.i.d. environment.","The key lies in our innovative online unmixing procedure, which persistently refines these statistics components by drawing upon the closest instances from an incoming test batch.","Remarkably generic in its design, UnMix-TNS seamlessly integrates with an array of state-of-the-art test-time adaptation methods and pre-trained architectures equipped with BN layers.","Empirical evaluations corroborate the robustness of UnMix-TNS under varied scenarios ranging from single to continual and mixed domain shifts.","UnMix-TNS stands out when handling test data streams with temporal correlation, including those with corrupted real-world non-i.i.d. streams, sustaining its efficacy even with minimal batch sizes and individual samples.","Our results set a new standard for test-time adaptation, demonstrating significant improvements in both stability and performance across multiple benchmarks."],"url":"http://arxiv.org/abs/2401.08328v1"}
{"created":"2024-01-16 12:45:15","title":"RoTBench: A Multi-Level Benchmark for Evaluating the Robustness of Large Language Models in Tool Learning","abstract":"Tool learning has generated widespread interest as a vital means of interaction between Large Language Models (LLMs) and the physical world. Current research predominantly emphasizes LLMs' capacity to utilize tools in well-structured environments while overlooking their stability when confronted with the inevitable noise of the real world. To bridge this gap, we introduce RoTBench, a multi-level benchmark for evaluating the robustness of LLMs in tool learning. Specifically, we establish five external environments, each featuring varying levels of noise (i.e., Clean, Slight, Medium, Heavy, and Union), providing an in-depth analysis of the model's resilience across three critical phases: tool selection, parameter identification, and content filling. Experiments involving six widely-used models underscore the urgent necessity for enhancing the robustness of LLMs in tool learning. For instance, the performance of GPT-4 even drops significantly from 80.00 to 58.10 when there is no substantial change in manual accuracy. More surprisingly, the noise correction capability inherent in the GPT family paradoxically impedes its adaptability in the face of mild noise. In light of these findings, we propose RoTTuning, a strategy that enriches the diversity of training environments to bolster the robustness of LLMs in tool learning. The code and data are available at https://github.com/Junjie-Ye/RoTBench.","sentences":["Tool learning has generated widespread interest as a vital means of interaction between Large Language Models (LLMs) and the physical world.","Current research predominantly emphasizes LLMs' capacity to utilize tools in well-structured environments while overlooking their stability when confronted with the inevitable noise of the real world.","To bridge this gap, we introduce RoTBench, a multi-level benchmark for evaluating the robustness of LLMs in tool learning.","Specifically, we establish five external environments, each featuring varying levels of noise (i.e., Clean, Slight, Medium, Heavy, and Union), providing an in-depth analysis of the model's resilience across three critical phases: tool selection, parameter identification, and content filling.","Experiments involving six widely-used models underscore the urgent necessity for enhancing the robustness of LLMs in tool learning.","For instance, the performance of GPT-4 even drops significantly from 80.00 to 58.10 when there is no substantial change in manual accuracy.","More surprisingly, the noise correction capability inherent in the GPT family paradoxically impedes its adaptability in the face of mild noise.","In light of these findings, we propose RoTTuning, a strategy that enriches the diversity of training environments to bolster the robustness of LLMs in tool learning.","The code and data are available at https://github.com/Junjie-Ye/RoTBench."],"url":"http://arxiv.org/abs/2401.08326v1"}
{"created":"2024-01-16 12:45:15","title":"Learn What You Need in Personalized Federated Learning","abstract":"Personalized federated learning aims to address data heterogeneity across local clients in federated learning. However, current methods blindly incorporate either full model parameters or predefined partial parameters in personalized federated learning. They fail to customize the collaboration manner according to each local client's data characteristics, causing unpleasant aggregation results. To address this essential issue, we propose $\\textit{Learn2pFed}$, a novel algorithm-unrolling-based personalized federated learning framework, enabling each client to adaptively select which part of its local model parameters should participate in collaborative training. The key novelty of the proposed $\\textit{Learn2pFed}$ is to optimize each local model parameter's degree of participant in collaboration as learnable parameters via algorithm unrolling methods. This approach brings two benefits: 1) mathmatically determining the participation degree of local model parameters in the federated collaboration, and 2) obtaining more stable and improved solutions. Extensive experiments on various tasks, including regression, forecasting, and image classification, demonstrate that $\\textit{Learn2pFed}$ significantly outperforms previous personalized federated learning methods.","sentences":["Personalized federated learning aims to address data heterogeneity across local clients in federated learning.","However, current methods blindly incorporate either full model parameters or predefined partial parameters in personalized federated learning.","They fail to customize the collaboration manner according to each local client's data characteristics, causing unpleasant aggregation results.","To address this essential issue, we propose $\\textit{Learn2pFed}$, a novel algorithm-unrolling-based personalized federated learning framework, enabling each client to adaptively select which part of its local model parameters should participate in collaborative training.","The key novelty of the proposed $\\textit{Learn2pFed}$ is to optimize each local model parameter's degree of participant in collaboration as learnable parameters via algorithm unrolling methods.","This approach brings two benefits: 1) mathmatically determining the participation degree of local model parameters in the federated collaboration, and 2) obtaining more stable and improved solutions.","Extensive experiments on various tasks, including regression, forecasting, and image classification, demonstrate that $\\textit{Learn2pFed}$ significantly outperforms previous personalized federated learning methods."],"url":"http://arxiv.org/abs/2401.08327v1"}
{"created":"2024-01-16 12:36:17","title":"OpenDPD: An Open-Source End-to-End Learning & Benchmarking Framework for Wideband Power Amplifier Modeling and Digital Pre-Distortion","abstract":"With the rise in communication capacity, deep neural networks (DNN) for digital pre-distortion (DPD) to correct non-linearity in wideband power amplifiers (PAs) have become prominent. Yet, there is a void in open-source and measurement-setup-independent platforms for fast DPD exploration and objective DPD model comparison. This paper presents an open-source framework, OpenDPD, crafted in PyTorch, with an associated dataset for PA modeling and DPD learning. We introduce a Dense Gated Recurrent Unit (DGRU)-DPD, trained via a novel end-to-end learning architecture, outperforming previous DPD models on a digital PA DPA in the new digital transmitter (DTX) architecture with unconventional transfer characteristics compared to analog PAs. Measurements show our DGRU-DPD achieves an ACPR of -44.69/-44.47 dBc and an EVM of -35.22 dB for 200 MHz OFDM signals. OpenDPD code, datasets, and documentation are publicly available at https://github.com/lab-emi/OpenDPD.","sentences":["With the rise in communication capacity, deep neural networks (DNN) for digital pre-distortion (DPD) to correct non-linearity in wideband power amplifiers (PAs) have become prominent.","Yet, there is a void in open-source and measurement-setup-independent platforms for fast DPD exploration and objective DPD model comparison.","This paper presents an open-source framework, OpenDPD, crafted in PyTorch, with an associated dataset for PA modeling and DPD learning.","We introduce a Dense Gated Recurrent Unit (DGRU)-DPD, trained via a novel end-to-end learning architecture, outperforming previous DPD models on a digital PA DPA in the new digital transmitter (DTX) architecture with unconventional transfer characteristics compared to analog PAs.","Measurements show our DGRU-DPD achieves an ACPR of -44.69/-44.47 dBc and an EVM of -35.22 dB for 200 MHz OFDM signals.","OpenDPD code, datasets, and documentation are publicly available at https://github.com/lab-emi/OpenDPD."],"url":"http://arxiv.org/abs/2401.08318v1"}
{"created":"2024-01-16 12:30:56","title":"Application of LLM Agents in Recruitment: A Novel Framework for Resume Screening","abstract":"The automation of resume screening is a crucial aspect of the recruitment process in organizations. Automated resume screening systems often encompass a range of natural language processing (NLP) tasks. The advent of Large Language Models (LLMs) has notably enhanced the efficacy of these systems, showcasing their robust generalization abilities across diverse language-related tasks. Accompanying these developments are various agents based on LLMs, which facilitate their application in practical scenarios. This paper introduces a novel LLM-based agent framework for resume screening, aimed at enhancing efficiency and time management in recruitment processes. Our framework is distinct in its ability to efficiently summarize and grade each resume from a large dataset. Moreover, it utilizes LLM agents for decision-making, determining which candidates receive job offers, or which ones to bring in for interviews. To evaluate our framework, we constructed a dataset from actual resumes and conducted simulate a resume screening process. Subsequently, the outcomes of the simulation experiment were compared and subjected to detailed analysis. The results demonstrate that our automated resume screening framework is 11 times faster than traditional manual methods. Furthermore, by fine-tuning the LLMs, we observed a significant improvement in the F1 score, reaching 87.73\\%, during the resume sentence classification phase. In the resume summarization and grading phase, our fine-tuned model surpassed the baseline performance of the GPT-3.5 model. Analysis of the decision-making efficacy of the LLM agents in the final offer stage further underscores the potential of LLM agents in transforming resume screening processes.","sentences":["The automation of resume screening is a crucial aspect of the recruitment process in organizations.","Automated resume screening systems often encompass a range of natural language processing (NLP) tasks.","The advent of Large Language Models (LLMs) has notably enhanced the efficacy of these systems, showcasing their robust generalization abilities across diverse language-related tasks.","Accompanying these developments are various agents based on LLMs, which facilitate their application in practical scenarios.","This paper introduces a novel LLM-based agent framework for resume screening, aimed at enhancing efficiency and time management in recruitment processes.","Our framework is distinct in its ability to efficiently summarize and grade each resume from a large dataset.","Moreover, it utilizes LLM agents for decision-making, determining which candidates receive job offers, or which ones to bring in for interviews.","To evaluate our framework, we constructed a dataset from actual resumes and conducted simulate a resume screening process.","Subsequently, the outcomes of the simulation experiment were compared and subjected to detailed analysis.","The results demonstrate that our automated resume screening framework is 11 times faster than traditional manual methods.","Furthermore, by fine-tuning the LLMs, we observed a significant improvement in the F1 score, reaching 87.73\\%, during the resume sentence classification phase.","In the resume summarization and grading phase, our fine-tuned model surpassed the baseline performance of the GPT-3.5 model.","Analysis of the decision-making efficacy of the LLM agents in the final offer stage further underscores the potential of LLM agents in transforming resume screening processes."],"url":"http://arxiv.org/abs/2401.08315v1"}
{"created":"2024-01-16 12:10:49","title":"Anchor function: a type of benchmark functions for studying language models","abstract":"Understanding transformer-based language models is becoming increasingly crucial, particularly as they play pivotal roles in advancing towards artificial general intelligence. However, language model research faces significant challenges, especially for academic research groups with constrained resources. These challenges include complex data structures, unknown target functions, high computational costs and memory requirements, and a lack of interpretability in the inference process, etc. Drawing a parallel to the use of simple models in scientific research, we propose the concept of an anchor function. This is a type of benchmark function designed for studying language models in learning tasks that follow an \"anchor-key\" pattern. By utilizing the concept of an anchor function, we can construct a series of functions to simulate various language tasks. The anchor function plays a role analogous to that of mice in diabetes research, particularly suitable for academic research. We demonstrate the utility of the anchor function with an example, revealing two basic operations by attention structures in language models: shifting tokens and broadcasting one token from one position to many positions. These operations are also commonly observed in large language models. The anchor function framework, therefore, opens up a series of valuable and accessible research questions for further exploration, especially for theoretical study.","sentences":["Understanding transformer-based language models is becoming increasingly crucial, particularly as they play pivotal roles in advancing towards artificial general intelligence.","However, language model research faces significant challenges, especially for academic research groups with constrained resources.","These challenges include complex data structures, unknown target functions, high computational costs and memory requirements, and a lack of interpretability in the inference process, etc.","Drawing a parallel to the use of simple models in scientific research, we propose the concept of an anchor function.","This is a type of benchmark function designed for studying language models in learning tasks that follow an \"anchor-key\" pattern.","By utilizing the concept of an anchor function, we can construct a series of functions to simulate various language tasks.","The anchor function plays a role analogous to that of mice in diabetes research, particularly suitable for academic research.","We demonstrate the utility of the anchor function with an example, revealing two basic operations by attention structures in language models: shifting tokens and broadcasting one token from one position to many positions.","These operations are also commonly observed in large language models.","The anchor function framework, therefore, opens up a series of valuable and accessible research questions for further exploration, especially for theoretical study."],"url":"http://arxiv.org/abs/2401.08309v1"}
{"created":"2024-01-16 11:50:54","title":"Evaluating online elasticity estimation of soft objects using standard robot grippers","abstract":"Standard robot grippers are not designed for elasticity estimation. In this work, a professional biaxial compression device was used as a control setup to study the accuracy with which material properties can be estimated by two standard parallel jaw grippers and a force/torque sensor mounted at the robot wrist. Using three sets of deformable objects, different parameters were varied to observe their effect on measuring material characteristics: (1) repeated compression cycles, (2) compression speed, and (3) the surface area of the gripper jaws. Gripper effort versus position curves were obtained and transformed into stress/strain curves. The modulus of elasticity was estimated at different strain points. Viscoelasticity was assessed using the energy absorbed in a compression/decompression cycle, the Kelvin-Voigt, and Hunt-Crossley models. Our results can be summarized as follows: (1) better results were obtained with slower compression speeds, while additional compression cycles or surface area did not improve estimation; (2) the robot grippers, even after calibration, were found to have a limited capability of delivering accurate estimates of absolute values of Young's modulus and viscoelasticity; (3) relative ordering of material characteristics was largely consistent across different grippers; (4) despite the nonlinear characteristics of deformable objects, fitting linear stress/strain approximations led to more stable results than local estimates of Young's modulus; (5) to assess viscoelasticity, the Hunt-Crossley model worked best. Finally, we show that a two-dimensional space representing elasticity and viscoelasticity estimates is advantageous for the discrimination of deformable objects. A single-grasp, online, classification and sorting of such objects is thus possible. An additional contribution is the dataset and data processing codes that we make publicly available.","sentences":["Standard robot grippers are not designed for elasticity estimation.","In this work, a professional biaxial compression device was used as a control setup to study the accuracy with which material properties can be estimated by two standard parallel jaw grippers and a force/torque sensor mounted at the robot wrist.","Using three sets of deformable objects, different parameters were varied to observe their effect on measuring material characteristics: (1) repeated compression cycles, (2) compression speed, and (3) the surface area of the gripper jaws.","Gripper effort versus position curves were obtained and transformed into stress/strain curves.","The modulus of elasticity was estimated at different strain points.","Viscoelasticity was assessed using the energy absorbed in a compression/decompression cycle, the Kelvin-Voigt, and Hunt-Crossley models.","Our results can be summarized as follows: (1) better results were obtained with slower compression speeds, while additional compression cycles or surface area did not improve estimation; (2) the robot grippers, even after calibration, were found to have a limited capability of delivering accurate estimates of absolute values of Young's modulus and viscoelasticity; (3) relative ordering of material characteristics was largely consistent across different grippers; (4) despite the nonlinear characteristics of deformable objects, fitting linear stress/strain approximations led to more stable results than local estimates of Young's modulus; (5) to assess viscoelasticity, the Hunt-Crossley model worked best.","Finally, we show that a two-dimensional space representing elasticity and viscoelasticity estimates is advantageous for the discrimination of deformable objects.","A single-grasp, online, classification and sorting of such objects is thus possible.","An additional contribution is the dataset and data processing codes that we make publicly available."],"url":"http://arxiv.org/abs/2401.08298v1"}
{"created":"2024-01-16 11:47:02","title":"The extension of zbMATH Open by arXiv preprints","abstract":"zbMATH Open has started a new feature -- relevant preprints posted at arXiv will also be displayed in the database. In this article we introduce this new feature and the underlying editorial policy. We also describe some of the technical issues involved and discuss the challenges this presents for future developments.","sentences":["zbMATH Open has started a new feature -- relevant preprints posted at arXiv will also be displayed in the database.","In this article we introduce this new feature and the underlying editorial policy.","We also describe some of the technical issues involved and discuss the challenges this presents for future developments."],"url":"http://arxiv.org/abs/2401.08297v1"}
{"created":"2024-01-16 11:45:03","title":"DAPT: A Dual Attention Framework for Parameter-Efficient Continual Learning of Large Language Models","abstract":"The continual learning (CL) ability is vital for deploying large language models (LLMs) in the dynamic world. Based on parameter-efficient tuning (PET), existing methods devise the learning module and the selection module to handle the challenges of catastrophic forgetting (CF) and knowledge transfer (KT) in CL. The learning module allocates separate PET blocks for each continually emerged task and the selection module function to choose the correct one for the input at testing time. However, there are limitations in their deigns of both modules and they ignore the potential of aligning the two module to address CF and KT simultaneously. To this end, we propose a novel Dual Attention Framework , to align the PET learning and selection via the Dual Attentive Learning\\&Selection module. Extensive Experiments on two CL benchmarks demonstrate the superiority of DAPT to resist CF and facilitate KT at the same time. Moreover, DAPT exhibits the superiority when we scale it to different model sizes (from 770M to 11B) and unseen tasks.","sentences":["The continual learning (CL) ability is vital for deploying large language models (LLMs) in the dynamic world.","Based on parameter-efficient tuning (PET), existing methods devise the learning module and the selection module to handle the challenges of catastrophic forgetting (CF) and knowledge transfer (KT) in CL.","The learning module allocates separate PET blocks for each continually emerged task and the selection module function to choose the correct one for the input at testing time.","However, there are limitations in their deigns of both modules and they ignore the potential of aligning the two module to address CF and KT simultaneously.","To this end, we propose a novel Dual Attention Framework , to align the PET learning and selection via the Dual Attentive Learning\\&Selection module.","Extensive Experiments on two CL benchmarks demonstrate the superiority of DAPT to resist CF and facilitate KT at the same time.","Moreover, DAPT exhibits the superiority when we scale it to different model sizes (from 770M to 11B) and unseen tasks."],"url":"http://arxiv.org/abs/2401.08295v1"}
{"created":"2024-01-16 11:39:09","title":"Inferflow: an Efficient and Highly Configurable Inference Engine for Large Language Models","abstract":"We present Inferflow, an efficient and highly configurable inference engine for large language models (LLMs). With Inferflow, users can serve most of the common transformer models by simply modifying some lines in corresponding configuration files, without writing a single line of source code. Compared with most existing inference engines, Inferflow has some key features. First, by implementing a modular framework of atomic build-blocks and technologies, Inferflow is compositionally generalizable to new models. Second, 3.5-bit quantization is introduced in Inferflow as a tradeoff between 3-bit and 4-bit quantization. Third, hybrid model partitioning for multi-GPU inference is introduced in Inferflow to better balance inference speed and throughput than the existing partition-by-layer and partition-by-tensor strategies.","sentences":["We present Inferflow, an efficient and highly configurable inference engine for large language models (LLMs).","With Inferflow, users can serve most of the common transformer models by simply modifying some lines in corresponding configuration files, without writing a single line of source code.","Compared with most existing inference engines, Inferflow has some key features.","First, by implementing a modular framework of atomic build-blocks and technologies, Inferflow is compositionally generalizable to new models.","Second, 3.5-bit quantization is introduced in Inferflow as a tradeoff between 3-bit and 4-bit quantization.","Third, hybrid model partitioning for multi-GPU inference is introduced in Inferflow to better balance inference speed and throughput than the existing partition-by-layer and partition-by-tensor strategies."],"url":"http://arxiv.org/abs/2401.08294v1"}
{"created":"2024-01-16 11:37:09","title":"ULT-model: Towards a one-legged unified locomotion template model for forward hopping with an upright trunk","abstract":"While many advancements have been made in the development of template models for describing upright-trunk locomotion, the majority of the effort has been focused on the stance phase. In this paper, we develop a new compact dynamic model as a first step toward a fully unified locomotion template model (ULT-model) of an upright-trunk forward hopping system, which will also require a unified control law in the next step. We demonstrate that all locomotion subfunctions are enabled by adding just a point foot mass and a parallel leg actuator to the well-known trunk SLIP model and that a stable limit cycle can be achieved. This brings us closer toward the ultimate goal of enabling closed-loop dynamics for anchor matching and thus achieving simple, efficient, robust and stable upright-trunk gait control, as observed in biological systems.","sentences":["While many advancements have been made in the development of template models for describing upright-trunk locomotion, the majority of the effort has been focused on the stance phase.","In this paper, we develop a new compact dynamic model as a first step toward a fully unified locomotion template model (ULT-model) of an upright-trunk forward hopping system, which will also require a unified control law in the next step.","We demonstrate that all locomotion subfunctions are enabled by adding just a point foot mass and a parallel leg actuator to the well-known trunk SLIP model and that a stable limit cycle can be achieved.","This brings us closer toward the ultimate goal of enabling closed-loop dynamics for anchor matching and thus achieving simple, efficient, robust and stable upright-trunk gait control, as observed in biological systems."],"url":"http://arxiv.org/abs/2401.08292v1"}
{"created":"2024-01-16 11:29:16","title":"RichWasm: Bringing Safe, Fine-Grained, Shared-Memory Interoperability Down to WebAssembly","abstract":"Safe, shared-memory interoperability between languages with different type systems and memory-safety guarantees is an intricate problem as crossing language boundaries may result in memory-safety violations. In this paper, we present RichWasm, a novel richly typed intermediate language designed to serve as a compilation target for typed high-level languages with different memory-safety guarantees. RichWasm is based on WebAssembly and enables safe shared-memory interoperability by incorporating a variety of type features that support fine-grained memory ownership and sharing. RichWasm is rich enough to serve as a typed compilation target for both typed garbage-collected languages and languages with an ownership-based type system and manually managed memory. We demonstrate this by providing compilers from core ML and L3, a type-safe language with strong updates, to RichWasm. RichWasm is compiled to regular Wasm, allowing for use in existing environments. We formalize RichWasm in Coq and prove type safety.","sentences":["Safe, shared-memory interoperability between languages with different type systems and memory-safety guarantees is an intricate problem as crossing language boundaries may result in memory-safety violations.","In this paper, we present RichWasm, a novel richly typed intermediate language designed to serve as a compilation target for typed high-level languages with different memory-safety guarantees.","RichWasm is based on WebAssembly and enables safe shared-memory interoperability by incorporating a variety of type features that support fine-grained memory ownership and sharing.","RichWasm is rich enough to serve as a typed compilation target for both typed garbage-collected languages and languages with an ownership-based type system and manually managed memory.","We demonstrate this by providing compilers from core ML and L3, a type-safe language with strong updates, to RichWasm.","RichWasm is compiled to regular Wasm, allowing for use in existing environments.","We formalize RichWasm in Coq and prove type safety."],"url":"http://arxiv.org/abs/2401.08287v1"}
{"created":"2024-01-16 11:15:34","title":"Nonlinear stiffness allows passive dynamic hopping for one-legged robots with an upright trunk","abstract":"Template models are frequently used to simplify the control dynamics for robot hopping or running. Passive limit cycles can emerge for such systems and be exploited for energy-efficient control. A grand challenge in locomotion is trunk stabilization when the hip is offset from the center of mass (CoM). The swing phase plays a major role in this process due to the moment of inertia of the leg; however, many template models ignore the leg mass. In this work, the authors consider a robot hopper model (RHM) with a rigid trunk and leg plus a hip that is displaced from the CoM. It has been previously shown that no passive limit cycle exists for such a model given a linear hip spring. In this work, we show that passive limit cycles can be found when a nonlinear hip spring is used instead. To the authors' knowledge, this is the first time that a passive limit cycle has been found for this type of system.","sentences":["Template models are frequently used to simplify the control dynamics for robot hopping or running.","Passive limit cycles can emerge for such systems and be exploited for energy-efficient control.","A grand challenge in locomotion is trunk stabilization when the hip is offset from the center of mass (CoM).","The swing phase plays a major role in this process due to the moment of inertia of the leg; however, many template models ignore the leg mass.","In this work, the authors consider a robot hopper model (RHM) with a rigid trunk and leg plus a hip that is displaced from the CoM. It has been previously shown that no passive limit cycle exists for such a model given a linear hip spring.","In this work, we show that passive limit cycles can be found when a nonlinear hip spring is used instead.","To the authors' knowledge, this is the first time that a passive limit cycle has been found for this type of system."],"url":"http://arxiv.org/abs/2401.08282v1"}
{"created":"2024-01-16 11:12:36","title":"The Faiss library","abstract":"Vector databases manage large collections of embedding vectors. As AI applications are growing rapidly, so are the number of embeddings that need to be stored and indexed. The Faiss library is dedicated to vector similarity search, a core functionality of vector databases. Faiss is a toolkit of indexing methods and related primitives used to search, cluster, compress and transform vectors. This paper first describes the tradeoff space of vector search, then the design principles of Faiss in terms of structure, approach to optimization and interfacing. We benchmark key features of the library and discuss a few selected applications to highlight its broad applicability.","sentences":["Vector databases manage large collections of embedding vectors.","As AI applications are growing rapidly, so are the number of embeddings that need to be stored and indexed.","The Faiss library is dedicated to vector similarity search, a core functionality of vector databases.","Faiss is a toolkit of indexing methods and related primitives used to search, cluster, compress and transform vectors.","This paper first describes the tradeoff space of vector search, then the design principles of Faiss in terms of structure, approach to optimization and interfacing.","We benchmark key features of the library and discuss a few selected applications to highlight its broad applicability."],"url":"http://arxiv.org/abs/2401.08281v1"}
{"created":"2024-01-16 10:58:07","title":"AesBench: An Expert Benchmark for Multimodal Large Language Models on Image Aesthetics Perception","abstract":"With collective endeavors, multimodal large language models (MLLMs) are undergoing a flourishing development. However, their performances on image aesthetics perception remain indeterminate, which is highly desired in real-world applications. An obvious obstacle lies in the absence of a specific benchmark to evaluate the effectiveness of MLLMs on aesthetic perception. This blind groping may impede the further development of more advanced MLLMs with aesthetic perception capacity. To address this dilemma, we propose AesBench, an expert benchmark aiming to comprehensively evaluate the aesthetic perception capacities of MLLMs through elaborate design across dual facets. (1) We construct an Expert-labeled Aesthetics Perception Database (EAPD), which features diversified image contents and high-quality annotations provided by professional aesthetic experts. (2) We propose a set of integrative criteria to measure the aesthetic perception abilities of MLLMs from four perspectives, including Perception (AesP), Empathy (AesE), Assessment (AesA) and Interpretation (AesI). Extensive experimental results underscore that the current MLLMs only possess rudimentary aesthetic perception ability, and there is still a significant gap between MLLMs and humans. We hope this work can inspire the community to engage in deeper explorations on the aesthetic potentials of MLLMs. Source data will be available at https://github.com/yipoh/AesBench.","sentences":["With collective endeavors, multimodal large language models (MLLMs) are undergoing a flourishing development.","However, their performances on image aesthetics perception remain indeterminate, which is highly desired in real-world applications.","An obvious obstacle lies in the absence of a specific benchmark to evaluate the effectiveness of MLLMs on aesthetic perception.","This blind groping may impede the further development of more advanced MLLMs with aesthetic perception capacity.","To address this dilemma, we propose AesBench, an expert benchmark aiming to comprehensively evaluate the aesthetic perception capacities of MLLMs through elaborate design across dual facets.","(1) We construct an Expert-labeled Aesthetics Perception Database (EAPD), which features diversified image contents and high-quality annotations provided by professional aesthetic experts.","(2) We propose a set of integrative criteria to measure the aesthetic perception abilities of MLLMs from four perspectives, including Perception (AesP), Empathy (AesE), Assessment (AesA) and Interpretation (AesI).","Extensive experimental results underscore that the current MLLMs only possess rudimentary aesthetic perception ability, and there is still a significant gap between MLLMs and humans.","We hope this work can inspire the community to engage in deeper explorations on the aesthetic potentials of MLLMs.","Source data will be available at https://github.com/yipoh/AesBench."],"url":"http://arxiv.org/abs/2401.08276v1"}
{"created":"2024-01-16 10:54:37","title":"Modeling Spoof Noise by De-spoofing Diffusion and its Application in Face Anti-spoofing","abstract":"Face anti-spoofing is crucial for ensuring the security and reliability of face recognition systems. Several existing face anti-spoofing methods utilize GAN-like networks to detect presentation attacks by estimating the noise pattern of a spoof image and recovering the corresponding genuine image. But GAN's limited face appearance space results in the denoised faces cannot cover the full data distribution of genuine faces, thereby undermining the generalization performance of such methods. In this work, we present a pioneering attempt to employ diffusion models to denoise a spoof image and restore the genuine image. The difference between these two images is considered as the spoof noise, which can serve as a discriminative cue for face anti-spoofing. We evaluate our proposed method on several intra-testing and inter-testing protocols, where the experimental results showcase the effectiveness of our method in achieving competitive performance in terms of both accuracy and generalization.","sentences":["Face anti-spoofing is crucial for ensuring the security and reliability of face recognition systems.","Several existing face anti-spoofing methods utilize GAN-like networks to detect presentation attacks by estimating the noise pattern of a spoof image and recovering the corresponding genuine image.","But GAN's limited face appearance space results in the denoised faces cannot cover the full data distribution of genuine faces, thereby undermining the generalization performance of such methods.","In this work, we present a pioneering attempt to employ diffusion models to denoise a spoof image and restore the genuine image.","The difference between these two images is considered as the spoof noise, which can serve as a discriminative cue for face anti-spoofing.","We evaluate our proposed method on several intra-testing and inter-testing protocols, where the experimental results showcase the effectiveness of our method in achieving competitive performance in terms of both accuracy and generalization."],"url":"http://arxiv.org/abs/2401.08275v1"}
{"created":"2024-01-16 10:53:11","title":"Large Language Models are Null-Shot Learners","abstract":"This paper presents null-shot prompting. Null-shot prompting exploits hallucination in large language models (LLMs) by instructing LLMs to utilize information from the \"Examples\" section that never exists within the provided context to perform a task. While reducing hallucination is crucial and non-negligible for daily and critical uses of LLMs, we propose that in the current landscape in which these LLMs still hallucinate, it is possible, in fact, to exploit hallucination to increase performance in performing tasks compared to standard zero-shot prompting. Experiments with six LLMs show improvements in performance across the majority of eight datasets, including reading comprehension, arithmetic reasoning, and closed-book question answering. The observed inconsistency in increased relative performance across LLMs also potentially indicates a different degree of inherent hallucination in each model. These differences show that it is possible to utilize null-shot prompting as a way to detect degrees of hallucination in LLMs using existing benchmarking datasets. We also perform ablation studies, including experimenting with a modified version of null-shot prompting that incorporates ideas from zero-shot chain-of-thought prompting, which shows different trends of results.","sentences":["This paper presents null-shot prompting.","Null-shot prompting exploits hallucination in large language models (LLMs) by instructing LLMs to utilize information from the \"Examples\" section that never exists within the provided context to perform a task.","While reducing hallucination is crucial and non-negligible for daily and critical uses of LLMs, we propose that in the current landscape in which these LLMs still hallucinate, it is possible, in fact, to exploit hallucination to increase performance in performing tasks compared to standard zero-shot prompting.","Experiments with six LLMs show improvements in performance across the majority of eight datasets, including reading comprehension, arithmetic reasoning, and closed-book question answering.","The observed inconsistency in increased relative performance across LLMs also potentially indicates a different degree of inherent hallucination in each model.","These differences show that it is possible to utilize null-shot prompting as a way to detect degrees of hallucination in LLMs using existing benchmarking datasets.","We also perform ablation studies, including experimenting with a modified version of null-shot prompting that incorporates ideas from zero-shot chain-of-thought prompting, which shows different trends of results."],"url":"http://arxiv.org/abs/2401.08273v1"}
{"created":"2024-01-16 10:51:55","title":"Siamese Content-based Search Engine for a More Transparent Skin and Breast Cancer Diagnosis through Histological Imaging","abstract":"Computer Aid Diagnosis (CAD) has developed digital pathology with Deep Learning (DL)-based tools to assist pathologists in decision-making. Content-Based Histopathological Image Retrieval (CBHIR) is a novel tool to seek highly correlated patches in terms of similarity in histopathological features. In this work, we proposed two CBHIR approaches on breast (Breast-twins) and skin cancer (Skin-twins) data sets for robust and accurate patch-level retrieval, integrating a custom-built Siamese network as a feature extractor. The proposed Siamese network is able to generalize for unseen images by focusing on the similar histopathological features of the input pairs. The proposed CBHIR approaches are evaluated on the Breast (public) and Skin (private) data sets with top K accuracy. Finding the optimum amount of K is challenging, but also, as much as K increases, the dissimilarity between the query and the returned images increases which might mislead the pathologists. To the best of the author's belief, this paper is tackling this issue for the first time on histopathological images by evaluating the top first retrieved images. The Breast-twins model achieves 70% of the F1score at the top first, which exceeds the other state-of-the-art methods at a higher amount of K such as 5 and 400. Skin-twins overpasses the recently proposed Convolutional Auto Encoder (CAE) by 67%, increasing the precision. Besides, the Skin-twins model tackles the challenges of Spitzoid Tumors of Uncertain Malignant Potential (STUMP) to assist pathologists with retrieving top K images and their corresponding labels. So, this approach can offer a more explainable CAD tool to pathologists in terms of transparency, trustworthiness, or reliability among other characteristics.","sentences":["Computer Aid Diagnosis (CAD) has developed digital pathology with Deep Learning (DL)-based tools to assist pathologists in decision-making.","Content-Based Histopathological Image Retrieval (CBHIR) is a novel tool to seek highly correlated patches in terms of similarity in histopathological features.","In this work, we proposed two CBHIR approaches on breast (Breast-twins) and skin cancer (Skin-twins) data sets for robust and accurate patch-level retrieval, integrating a custom-built Siamese network as a feature extractor.","The proposed Siamese network is able to generalize for unseen images by focusing on the similar histopathological features of the input pairs.","The proposed CBHIR approaches are evaluated on the Breast (public) and Skin (private) data sets with top K accuracy.","Finding the optimum amount of K is challenging, but also, as much as K increases, the dissimilarity between the query and the returned images increases which might mislead the pathologists.","To the best of the author's belief, this paper is tackling this issue for the first time on histopathological images by evaluating the top first retrieved images.","The Breast-twins model achieves 70% of the F1score at the top first, which exceeds the other state-of-the-art methods at a higher amount of K such as 5 and 400.","Skin-twins overpasses the recently proposed Convolutional Auto Encoder (CAE) by 67%, increasing the precision.","Besides, the Skin-twins model tackles the challenges of Spitzoid Tumors of Uncertain Malignant Potential (STUMP) to assist pathologists with retrieving top K images and their corresponding labels.","So, this approach can offer a more explainable CAD tool to pathologists in terms of transparency, trustworthiness, or reliability among other characteristics."],"url":"http://arxiv.org/abs/2401.08272v1"}
{"created":"2024-01-16 10:41:09","title":"Ranking Heterogeneous Search Result Pages using the Interactive Probability Ranking Principle","abstract":"The Probability Ranking Principle (PRP) ranks search results based on their expected utility derived solely from document contents, often overlooking the nuances of presentation and user interaction. However, with the evolution of Search Engine Result Pages (SERPs), now comprising a variety of result cards, the manner in which these results are presented is pivotal in influencing user engagement and satisfaction. This shift prompts the question: How does the PRP and its user-centric counterpart, the Interactive Probability Ranking Principle (iPRP), compare in the context of these heterogeneous SERPs? Our study draws a comparison between the PRP and the iPRP, revealing significant differences in their output. The iPRP, accounting for item-specific costs and interaction probabilities to determine the ``Expected Perceived Utility\" (EPU), yields different result orderings compared to the PRP. We evaluate the effect of the EPU on the ordering of results by observing changes in the ranking within a heterogeneous SERP compared to the traditional ``ten blue links''. We find that changing the presentation affects the ranking of items according to the (iPRP) by up to 48\\% (with respect to DCG, TBG and RBO) in ad-hoc search tasks on the TREC WaPo Collection. This work suggests that the iPRP should be employed when ranking heterogeneous SERPs to provide a user-centric ranking that adapts the ordering based on the presentation and user engagement.","sentences":["The Probability Ranking Principle (PRP) ranks search results based on their expected utility derived solely from document contents, often overlooking the nuances of presentation and user interaction.","However, with the evolution of Search Engine Result Pages (SERPs), now comprising a variety of result cards, the manner in which these results are presented is pivotal in influencing user engagement and satisfaction.","This shift prompts the question: How does the PRP and its user-centric counterpart, the Interactive Probability Ranking Principle (iPRP), compare in the context of these heterogeneous SERPs?","Our study draws a comparison between the PRP and the iPRP, revealing significant differences in their output.","The iPRP, accounting for item-specific costs and interaction probabilities to determine the ``Expected Perceived Utility\" (EPU), yields different result orderings compared to the PRP.","We evaluate the effect of the EPU on the ordering of results by observing changes in the ranking within a heterogeneous SERP compared to the traditional ``ten blue links''.","We find that changing the presentation affects the ranking of items according to the (iPRP) by up to 48\\% (with respect to DCG, TBG and RBO) in ad-hoc search tasks on the TREC WaPo Collection.","This work suggests that the iPRP should be employed when ranking heterogeneous SERPs to provide a user-centric ranking that adapts the ordering based on the presentation and user engagement."],"url":"http://arxiv.org/abs/2401.08267v1"}
{"created":"2024-01-16 10:35:59","title":"Towards a Transpiler for C/C++ to Safer Rust","abstract":"Rust is a multi-paradigm programming language developed by Mozilla that focuses on performance and safety. Rust code is arguably known best for its speed and memory safety, a property essential while developing embedded systems. Thus, it becomes one of the alternatives when developing operating systems for embedded devices. How to convert an existing C++ code base to Rust is also gaining greater attention. In this work, we focus on the process of transpiling C++ code to a Rust codebase in a robust and safe manner. The manual transpilation process is carried out to understand the different constructs of the Rust language and how they correspond to C++ constructs. Based on the learning from the manual transpilation, a transpilation table is created to aid in future transpilation efforts and to develop an automated transpiler. We also studied the existing automated transpilers and identified the problems and inefficiencies they involved. The results of the transpilation process were closely monitored and evaluated, showing improved memory safety without compromising performance and reliability of the resulting codebase. The study concludes with a comprehensive analysis of the findings, an evaluation of the implications for future research, and recommendations for the same in this area.","sentences":["Rust is a multi-paradigm programming language developed by Mozilla that focuses on performance and safety.","Rust code is arguably known best for its speed and memory safety, a property essential while developing embedded systems.","Thus, it becomes one of the alternatives when developing operating systems for embedded devices.","How to convert an existing C++ code base to Rust is also gaining greater attention.","In this work, we focus on the process of transpiling C++ code to a Rust codebase in a robust and safe manner.","The manual transpilation process is carried out to understand the different constructs of the Rust language and how they correspond to C++ constructs.","Based on the learning from the manual transpilation, a transpilation table is created to aid in future transpilation efforts and to develop an automated transpiler.","We also studied the existing automated transpilers and identified the problems and inefficiencies they involved.","The results of the transpilation process were closely monitored and evaluated, showing improved memory safety without compromising performance and reliability of the resulting codebase.","The study concludes with a comprehensive analysis of the findings, an evaluation of the implications for future research, and recommendations for the same in this area."],"url":"http://arxiv.org/abs/2401.08264v1"}
{"created":"2024-01-16 10:35:01","title":"Multi-Technique Sequential Information Consistency For Dynamic Visual Place Recognition In Changing Environments","abstract":"Visual place recognition (VPR) is an essential component of robot navigation and localization systems that allows them to identify a place using only image data. VPR is challenging due to the significant changes in a place's appearance driven by different daily illumination, seasonal weather variations and diverse viewpoints. Currently, no single VPR technique excels in every environmental condition, each exhibiting unique benefits and shortcomings, and therefore combining multiple techniques can achieve more reliable VPR performance. Present multi-method approaches either rely on online ground-truth information, which is often not available, or on brute-force technique combination, potentially lowering performance with high variance technique sets. Addressing these shortcomings, we propose a VPR system dubbed Multi-Sequential Information Consistency (MuSIC) which leverages sequential information to select the most cohesive technique on an online per-frame basis. For each technique in a set, MuSIC computes their respective sequential consistencies by analysing the frame-to-frame continuity of their top match candidates, which are then directly compared to select the optimal technique for the current query image. The use of sequential information to select between VPR methods results in an overall VPR performance increase across different benchmark datasets, while avoiding the need for extra ground-truth of the runtime environment.","sentences":["Visual place recognition (VPR) is an essential component of robot navigation and localization systems that allows them to identify a place using only image data.","VPR is challenging due to the significant changes in a place's appearance driven by different daily illumination, seasonal weather variations and diverse viewpoints.","Currently, no single VPR technique excels in every environmental condition, each exhibiting unique benefits and shortcomings, and therefore combining multiple techniques can achieve more reliable VPR performance.","Present multi-method approaches either rely on online ground-truth information, which is often not available, or on brute-force technique combination, potentially lowering performance with high variance technique sets.","Addressing these shortcomings, we propose a VPR system dubbed Multi-Sequential Information Consistency (MuSIC) which leverages sequential information to select the most cohesive technique on an online per-frame basis.","For each technique in a set, MuSIC computes their respective sequential consistencies by analysing the frame-to-frame continuity of their top match candidates, which are then directly compared to select the optimal technique for the current query image.","The use of sequential information to select between VPR methods results in an overall VPR performance increase across different benchmark datasets, while avoiding the need for extra ground-truth of the runtime environment."],"url":"http://arxiv.org/abs/2401.08263v1"}
{"created":"2024-01-16 10:32:13","title":"Probabilistically Robust Watermarking of Neural Networks","abstract":"As deep learning (DL) models are widely and effectively used in Machine Learning as a Service (MLaaS) platforms, there is a rapidly growing interest in DL watermarking techniques that can be used to confirm the ownership of a particular model. Unfortunately, these methods usually produce watermarks susceptible to model stealing attacks. In our research, we introduce a novel trigger set-based watermarking approach that demonstrates resilience against functionality stealing attacks, particularly those involving extraction and distillation. Our approach does not require additional model training and can be applied to any model architecture. The key idea of our method is to compute the trigger set, which is transferable between the source model and the set of proxy models with a high probability. In our experimental study, we show that if the probability of the set being transferable is reasonably high, it can be effectively used for ownership verification of the stolen model. We evaluate our method on multiple benchmarks and show that our approach outperforms current state-of-the-art watermarking techniques in all considered experimental setups.","sentences":["As deep learning (DL) models are widely and effectively used in Machine Learning as a Service (MLaaS) platforms, there is a rapidly growing interest in DL watermarking techniques that can be used to confirm the ownership of a particular model.","Unfortunately, these methods usually produce watermarks susceptible to model stealing attacks.","In our research, we introduce a novel trigger set-based watermarking approach that demonstrates resilience against functionality stealing attacks, particularly those involving extraction and distillation.","Our approach does not require additional model training and can be applied to any model architecture.","The key idea of our method is to compute the trigger set, which is transferable between the source model and the set of proxy models with a high probability.","In our experimental study, we show that if the probability of the set being transferable is reasonably high, it can be effectively used for ownership verification of the stolen model.","We evaluate our method on multiple benchmarks and show that our approach outperforms current state-of-the-art watermarking techniques in all considered experimental setups."],"url":"http://arxiv.org/abs/2401.08261v1"}
{"created":"2024-01-16 10:28:12","title":"Time, Simultaneity, and Causality in Wireless Networks with Sensing and Communications","abstract":"Wireless systems beyond 5G evolve towards embracing both sensing and communication, resulting in increased convergence of the digital and the physical world. The existence of fused digital-physical realms raises critical questions regarding temporal ordering, causality, and the synchronization of events. This paper addresses the temporal challenges arising from the fact that the wireless infrastructure becomes an entity with multisensory perception. With the growing reliance on real-time interactions and applications such as digital twins, extended reality, and the metaverse, the need for accurate timestamping and temporal forensics becomes crucial. The paper introduces a model that incorporates Temporal Windows of Integration (TWI) to emulate human multisensory perception and discusses the implications for setting timing constraints in real-time applications and enabling temporal forensics. The analysis explores trade-offs, probabilities, and bounds for simultaneity and causality violation in the context of wireless systems evolving towards perceptive networks. This work underscores the significance of timestamping in the evolving wireless landscape, provide insights into system-level implications, and points out new research avenues for systems that combine sensing and communications.","sentences":["Wireless systems beyond 5G evolve towards embracing both sensing and communication, resulting in increased convergence of the digital and the physical world.","The existence of fused digital-physical realms raises critical questions regarding temporal ordering, causality, and the synchronization of events.","This paper addresses the temporal challenges arising from the fact that the wireless infrastructure becomes an entity with multisensory perception.","With the growing reliance on real-time interactions and applications such as digital twins, extended reality, and the metaverse, the need for accurate timestamping and temporal forensics becomes crucial.","The paper introduces a model that incorporates Temporal Windows of Integration (TWI) to emulate human multisensory perception and discusses the implications for setting timing constraints in real-time applications and enabling temporal forensics.","The analysis explores trade-offs, probabilities, and bounds for simultaneity and causality violation in the context of wireless systems evolving towards perceptive networks.","This work underscores the significance of timestamping in the evolving wireless landscape, provide insights into system-level implications, and points out new research avenues for systems that combine sensing and communications."],"url":"http://arxiv.org/abs/2401.08258v1"}
{"created":"2024-01-16 10:18:57","title":"Multitask Learning in Minimally Invasive Surgical Vision: A Review","abstract":"Minimally invasive surgery (MIS) has revolutionized many procedures and led to reduced recovery time and risk of patient injury. However, MIS poses additional complexity and burden on surgical teams. Data-driven surgical vision algorithms are thought to be key building blocks in the development of future MIS systems with improved autonomy. Recent advancements in machine learning and computer vision have led to successful applications in analyzing videos obtained from MIS with the promise of alleviating challenges in MIS videos. Surgical scene and action understanding encompasses multiple related tasks that, when solved individually, can be memory-intensive, inefficient, and fail to capture task relationships. Multitask learning (MTL), a learning paradigm that leverages information from multiple related tasks to improve performance and aid generalization, is wellsuited for fine-grained and high-level understanding of MIS data. This review provides an overview of the current state-of-the-art MTL systems that leverage videos obtained from MIS. Beyond listing published approaches, we discuss the benefits and limitations of these MTL systems. Moreover, this manuscript presents an analysis of the literature for various application fields of MTL in MIS, including those with large models, highlighting notable trends, new directions of research, and developments.","sentences":["Minimally invasive surgery (MIS) has revolutionized many procedures and led to reduced recovery time and risk of patient injury.","However, MIS poses additional complexity and burden on surgical teams.","Data-driven surgical vision algorithms are thought to be key building blocks in the development of future MIS systems with improved autonomy.","Recent advancements in machine learning and computer vision have led to successful applications in analyzing videos obtained from MIS with the promise of alleviating challenges in MIS videos.","Surgical scene and action understanding encompasses multiple related tasks that, when solved individually, can be memory-intensive, inefficient, and fail to capture task relationships.","Multitask learning (MTL), a learning paradigm that leverages information from multiple related tasks to improve performance and aid generalization, is wellsuited for fine-grained and high-level understanding of MIS data.","This review provides an overview of the current state-of-the-art MTL systems that leverage videos obtained from MIS.","Beyond listing published approaches, we discuss the benefits and limitations of these MTL systems.","Moreover, this manuscript presents an analysis of the literature for various application fields of MTL in MIS, including those with large models, highlighting notable trends, new directions of research, and developments."],"url":"http://arxiv.org/abs/2401.08256v1"}
{"created":"2024-01-16 10:14:27","title":"A Generative Adversarial Attack for Multilingual Text Classifiers","abstract":"Current adversarial attack algorithms, where an adversary changes a text to fool a victim model, have been repeatedly shown to be effective against text classifiers. These attacks, however, generally assume that the victim model is monolingual and cannot be used to target multilingual victim models, a significant limitation given the increased use of these models. For this reason, in this work we propose an approach to fine-tune a multilingual paraphrase model with an adversarial objective so that it becomes able to generate effective adversarial examples against multilingual classifiers. The training objective incorporates a set of pre-trained models to ensure text quality and language consistency of the generated text. In addition, all the models are suitably connected to the generator by vocabulary-mapping matrices, allowing for full end-to-end differentiability of the overall training pipeline. The experimental validation over two multilingual datasets and five languages has shown the effectiveness of the proposed approach compared to existing baselines, particularly in terms of query efficiency. We also provide a detailed analysis of the generated attacks and discuss limitations and opportunities for future research.","sentences":["Current adversarial attack algorithms, where an adversary changes a text to fool a victim model, have been repeatedly shown to be effective against text classifiers.","These attacks, however, generally assume that the victim model is monolingual and cannot be used to target multilingual victim models, a significant limitation given the increased use of these models.","For this reason, in this work we propose an approach to fine-tune a multilingual paraphrase model with an adversarial objective so that it becomes able to generate effective adversarial examples against multilingual classifiers.","The training objective incorporates a set of pre-trained models to ensure text quality and language consistency of the generated text.","In addition, all the models are suitably connected to the generator by vocabulary-mapping matrices, allowing for full end-to-end differentiability of the overall training pipeline.","The experimental validation over two multilingual datasets and five languages has shown the effectiveness of the proposed approach compared to existing baselines, particularly in terms of query efficiency.","We also provide a detailed analysis of the generated attacks and discuss limitations and opportunities for future research."],"url":"http://arxiv.org/abs/2401.08255v1"}
{"created":"2024-01-16 10:04:29","title":"A techno-economic model for avoiding conflicts of interest between owners of offshore wind farms and maintenance suppliers","abstract":"Currently, wind energy is one of the most important sources of renewable energy. Offshore locations for wind turbines are increasingly exploited because of their numerous advantages. However, offshore wind farms require high investment in maintenance service. Due to its complexity and special requirements, maintenance service is usually outsourced by wind farm owners. In this paper, we propose a novel approach to determine, quantify, and reduce the possible conflicts of interest between owners and maintenance suppliers. We created a complete techno-economic model to address this problem from an impartial point of view. An iterative process was developed to obtain statistical results that can help stakeholders negotiate the terms of the contract, in which the availability of the wind farm is the reference parameter by which to determine penalisations and incentives. Moreover, a multi-objective programming problem was addressed that maximises the profits of both parties without losing the alignment of their interests. The main scientific contribution of this paper is the maintenance analysis of offshore wind farms from two perspectives: that of the owner and the maintenance supplier. This analysis evaluates the conflicts of interest of both parties. In addition, we demonstrate that proper adjustment of some parameters, such as penalisation, incentives, and resources, and adequate control of availability can help reduce this conflict of interests.","sentences":["Currently, wind energy is one of the most important sources of renewable energy.","Offshore locations for wind turbines are increasingly exploited because of their numerous advantages.","However, offshore wind farms require high investment in maintenance service.","Due to its complexity and special requirements, maintenance service is usually outsourced by wind farm owners.","In this paper, we propose a novel approach to determine, quantify, and reduce the possible conflicts of interest between owners and maintenance suppliers.","We created a complete techno-economic model to address this problem from an impartial point of view.","An iterative process was developed to obtain statistical results that can help stakeholders negotiate the terms of the contract, in which the availability of the wind farm is the reference parameter by which to determine penalisations and incentives.","Moreover, a multi-objective programming problem was addressed that maximises the profits of both parties without losing the alignment of their interests.","The main scientific contribution of this paper is the maintenance analysis of offshore wind farms from two perspectives: that of the owner and the maintenance supplier.","This analysis evaluates the conflicts of interest of both parties.","In addition, we demonstrate that proper adjustment of some parameters, such as penalisation, incentives, and resources, and adequate control of availability can help reduce this conflict of interests."],"url":"http://arxiv.org/abs/2401.08251v1"}
{"created":"2024-01-16 10:01:46","title":"Graph-based Algorithms for Linear Computation Coding","abstract":"We revisit existing linear computation coding (LCC) algorithms, and introduce a new framework that measures the computational cost of computing multidimensional linear functions, not only in terms of the number of additions, but also with respect to their suitability for parallel processing. Utilizing directed acyclic graphs, which correspond to signal flow graphs in hardware, we propose a novel LCC algorithm that controls the trade-off between the total number of operations and their parallel executability. Numerical evaluations show that the proposed algorithm, constrained to a fully parallel structure, outperforms existing schemes.","sentences":["We revisit existing linear computation coding (LCC) algorithms, and introduce a new framework that measures the computational cost of computing multidimensional linear functions, not only in terms of the number of additions, but also with respect to their suitability for parallel processing.","Utilizing directed acyclic graphs, which correspond to signal flow graphs in hardware, we propose a novel LCC algorithm that controls the trade-off between the total number of operations and their parallel executability.","Numerical evaluations show that the proposed algorithm, constrained to a fully parallel structure, outperforms existing schemes."],"url":"http://arxiv.org/abs/2401.08249v1"}
{"created":"2024-01-16 09:59:36","title":"Optimizing $k$ in $k$NN Graphs with Graph Learning Perspective","abstract":"In this paper, we propose a method, based on graph signal processing, to optimize the choice of $k$ in $k$-nearest neighbor graphs ($k$NNGs). $k$NN is one of the most popular approaches and is widely used in machine learning and signal processing. The parameter $k$ represents the number of neighbors that are connected to the target node; however, its appropriate selection is still a challenging problem. Therefore, most $k$NNGs use ad hoc selection methods for $k$. In the proposed method, we assume that a different $k$ can be chosen for each node. We formulate a discrete optimization problem to seek the best $k$ with a constraint on the sum of distances of the connected nodes. The optimal $k$ values are efficiently obtained without solving a complex optimization. Furthermore, we reveal that the proposed method is closely related to existing graph learning methods. In experiments on real datasets, we demonstrate that the $k$NNGs obtained with our method are sparse and can determine an appropriate variable number of edges per node. We validate the effectiveness of the proposed method for point cloud denoising, comparing our denoising performance with achievable graph construction methods that can be scaled to typical point cloud sizes (e.g., thousands of nodes).","sentences":["In this paper, we propose a method, based on graph signal processing, to optimize the choice of $k$ in $k$-nearest neighbor graphs ($k$NNGs).","$k$NN is one of the most popular approaches and is widely used in machine learning and signal processing.","The parameter $k$ represents the number of neighbors that are connected to the target node; however, its appropriate selection is still a challenging problem.","Therefore, most $k$NNGs use ad hoc selection methods for $k$. In the proposed method, we assume that a different $k$ can be chosen for each node.","We formulate a discrete optimization problem to seek the best $k$ with a constraint on the sum of distances of the connected nodes.","The optimal $k$ values are efficiently obtained without solving a complex optimization.","Furthermore, we reveal that the proposed method is closely related to existing graph learning methods.","In experiments on real datasets, we demonstrate that the $k$NNGs obtained with our method are sparse and can determine an appropriate variable number of edges per node.","We validate the effectiveness of the proposed method for point cloud denoising, comparing our denoising performance with achievable graph construction methods that can be scaled to typical point cloud sizes (e.g., thousands of nodes)."],"url":"http://arxiv.org/abs/2401.08245v1"}
{"created":"2024-01-16 09:53:48","title":"Polygonal Sequence-driven Triangulation Validator: An Incremental Approach to 2D Triangulation Verification","abstract":"Two-dimensional Delaunay triangulation is a fundamental aspect of computational geometry. This paper presents a novel algorithm that is specifically designed to ensure the correctness of 2D Delaunay triangulation, namely the Polygonal Sequence-driven Triangulation Validator (PSTV). Our research highlights the paramount importance of proper triangulation and the often overlooked, yet profound, impact of rounding errors in numerical computations on the precision of triangulation. The primary objective of the PSTV algorithm is to identify these computational errors and ensure the accuracy of the triangulation output. In addition to validating the correctness of triangulation, this study underscores the significance of the Delaunay property for the quality of finite element methods. Effective strategies are proposed to verify this property for a triangulation and correct it when necessary. While acknowledging the difficulty of rectifying complex triangulation errors such as overlapping triangles, these strategies provide valuable insights on identifying the locations of these errors and remedying them. The unique feature of the PSTV algorithm lies in its adoption of floating-point filters in place of interval arithmetic, striking an effective balance between computational efficiency and precision. This research sets a vital precedent for error reduction and precision enhancement in computational geometry.","sentences":["Two-dimensional Delaunay triangulation is a fundamental aspect of computational geometry.","This paper presents a novel algorithm that is specifically designed to ensure the correctness of 2D Delaunay triangulation, namely the Polygonal Sequence-driven Triangulation Validator (PSTV).","Our research highlights the paramount importance of proper triangulation and the often overlooked, yet profound, impact of rounding errors in numerical computations on the precision of triangulation.","The primary objective of the PSTV algorithm is to identify these computational errors and ensure the accuracy of the triangulation output.","In addition to validating the correctness of triangulation, this study underscores the significance of the Delaunay property for the quality of finite element methods.","Effective strategies are proposed to verify this property for a triangulation and correct it when necessary.","While acknowledging the difficulty of rectifying complex triangulation errors such as overlapping triangles, these strategies provide valuable insights on identifying the locations of these errors and remedying them.","The unique feature of the PSTV algorithm lies in its adoption of floating-point filters in place of interval arithmetic, striking an effective balance between computational efficiency and precision.","This research sets a vital precedent for error reduction and precision enhancement in computational geometry."],"url":"http://arxiv.org/abs/2401.08242v1"}
{"created":"2024-01-16 09:53:39","title":"Adapt/Exchange decisions or generic choices: Does framing influence how people integrate qualitatively different risks?","abstract":"In complex systems, decision makers often have to consider qualitatively different risks when choosing between options. Do their strategies of integrating these risks depend on the framing of problem contents? In the present study, participants were either instructed that they were choosing between two ways of solving a complex problem, or between two generic options. The former was framed as a modular plant scenario that required choices between modifying parameter settings in a current module (Adapt) and replacing the module by another one (Exchange). The risk was higher for Adapt to harm the product and for Exchange to harm the plant. These risks were presented as probabilities, and participants were either told that the consequences of both risks were equally severe (content-same group), or that harming the plant was much worse (content-different group). A third group made decisions based on the same probabilities, but received a generic task framing (no-content group). We expected framing to affect risk integration, leading the content-same group to make different choices than the no-content group. Contrary to this hypothesis, these two groups were strikingly similar in their decision outcomes and strategies, but clearly differed from the content-different group. These findings question whether ecological validity can be enhanced merely by framing a task in terms of real-world problem contents.","sentences":["In complex systems, decision makers often have to consider qualitatively different risks when choosing between options.","Do their strategies of integrating these risks depend on the framing of problem contents?","In the present study, participants were either instructed that they were choosing between two ways of solving a complex problem, or between two generic options.","The former was framed as a modular plant scenario that required choices between modifying parameter settings in a current module (Adapt) and replacing the module by another one (Exchange).","The risk was higher for Adapt to harm the product and for Exchange to harm the plant.","These risks were presented as probabilities, and participants were either told that the consequences of both risks were equally severe (content-same group), or that harming the plant was much worse (content-different group).","A third group made decisions based on the same probabilities, but received a generic task framing (no-content group).","We expected framing to affect risk integration, leading the content-same group to make different choices than the no-content group.","Contrary to this hypothesis, these two groups were strikingly similar in their decision outcomes and strategies, but clearly differed from the content-different group.","These findings question whether ecological validity can be enhanced merely by framing a task in terms of real-world problem contents."],"url":"http://arxiv.org/abs/2401.08241v1"}
{"created":"2024-01-16 09:49:19","title":"Phase-free Dynamic Movement Primitives Applied to Kinesthetic Guidance in Robotic Co-manipulation Tasks","abstract":"When there is a need to define and adapt a robotic task based on a reference motion, Dynamic Movement Primitives (DMP) is a standard and efficient method for encoding it. The nominal trajectory is typically obtained through a Programming by Demonstration (PbD) approach, where the robot is taught a specific task through kinesthetic guidance. Subsequently, the motion is reproduced by the manipulator in terms of both geometric path and timing law. The basic approach for modifying the duration of the execution involves adjusting a time constant characterizing the model. On the contrary, the goal of this paper is to achieve complete decoupling between the geometric information of the task, encoded into the DMP, and the phase law governing the execution, allowing them to be chosen independently. This enables the optimization of the task duration to satisfy constraints such as velocity or acceleration or even to define a phase law dependent on external inputs, such as the force applied by a user in a co-manipulation task. As an example, this mechanism will be exploited to define a rehabilitation activity where the cobot assists humans in performing various pre-planned exercises.","sentences":["When there is a need to define and adapt a robotic task based on a reference motion, Dynamic Movement Primitives (DMP) is a standard and efficient method for encoding it.","The nominal trajectory is typically obtained through a Programming by Demonstration (PbD) approach, where the robot is taught a specific task through kinesthetic guidance.","Subsequently, the motion is reproduced by the manipulator in terms of both geometric path and timing law.","The basic approach for modifying the duration of the execution involves adjusting a time constant characterizing the model.","On the contrary, the goal of this paper is to achieve complete decoupling between the geometric information of the task, encoded into the DMP, and the phase law governing the execution, allowing them to be chosen independently.","This enables the optimization of the task duration to satisfy constraints such as velocity or acceleration or even to define a phase law dependent on external inputs, such as the force applied by a user in a co-manipulation task.","As an example, this mechanism will be exploited to define a rehabilitation activity where the cobot assists humans in performing various pre-planned exercises."],"url":"http://arxiv.org/abs/2401.08238v1"}
{"created":"2024-01-16 09:44:09","title":"Interpreting Node Embedding Distances Through $n$-order Proximity Neighbourhoods","abstract":"In the field of node representation learning the task of interpreting latent dimensions has become a prominent, well-studied research topic. The contribution of this work focuses on appraising the interpretability of another rarely-exploited feature of node embeddings increasingly utilised in recommendation and consumption diversity studies: inter-node embedded distances. Introducing a new method to measure how understandable the distances between nodes are, our work assesses how well the proximity weights derived from a network before embedding relate to the node closeness measurements after embedding. Testing several classical node embedding models, our findings reach a conclusion familiar to practitioners albeit rarely cited in literature - the matrix factorisation model SVD is the most interpretable through 1, 2 and even higher-order proximities.","sentences":["In the field of node representation learning the task of interpreting latent dimensions has become a prominent, well-studied research topic.","The contribution of this work focuses on appraising the interpretability of another rarely-exploited feature of node embeddings increasingly utilised in recommendation and consumption diversity studies: inter-node embedded distances.","Introducing a new method to measure how understandable the distances between nodes are, our work assesses how well the proximity weights derived from a network before embedding relate to the node closeness measurements after embedding.","Testing several classical node embedding models, our findings reach a conclusion familiar to practitioners albeit rarely cited in literature - the matrix factorisation model SVD is the most interpretable through 1, 2 and even higher-order proximities."],"url":"http://arxiv.org/abs/2401.08236v1"}
{"created":"2024-01-16 09:34:17","title":"Enhancing Wind Speed and Wind Power Forecasting Using Shape-Wise Feature Engineering: A Novel Approach for Improved Accuracy and Robustness","abstract":"Accurate prediction of wind speed and power is vital for enhancing the efficiency of wind energy systems. Numerous solutions have been implemented to date, demonstrating their potential to improve forecasting. Among these, deep learning is perceived as a revolutionary approach in the field. However, despite their effectiveness, the noise present in the collected data remains a significant challenge. This noise has the potential to diminish the performance of these algorithms, leading to inaccurate predictions. In response to this, this study explores a novel feature engineering approach. This approach involves altering the data input shape in both Convolutional Neural Network-Long Short-Term Memory (CNN-LSTM) and Autoregressive models for various forecasting horizons. The results reveal substantial enhancements in model resilience against noise resulting from step increases in data. The approach could achieve an impressive 83% accuracy in predicting unseen data up to the 24th steps. Furthermore, this method consistently provides high accuracy for short, mid, and long-term forecasts, outperforming the performance of individual models. These findings pave the way for further research on noise reduction strategies at different forecasting horizons through shape-wise feature engineering.","sentences":["Accurate prediction of wind speed and power is vital for enhancing the efficiency of wind energy systems.","Numerous solutions have been implemented to date, demonstrating their potential to improve forecasting.","Among these, deep learning is perceived as a revolutionary approach in the field.","However, despite their effectiveness, the noise present in the collected data remains a significant challenge.","This noise has the potential to diminish the performance of these algorithms, leading to inaccurate predictions.","In response to this, this study explores a novel feature engineering approach.","This approach involves altering the data input shape in both Convolutional Neural Network-Long Short-Term Memory (CNN-LSTM) and Autoregressive models for various forecasting horizons.","The results reveal substantial enhancements in model resilience against noise resulting from step increases in data.","The approach could achieve an impressive 83% accuracy in predicting unseen data up to the 24th steps.","Furthermore, this method consistently provides high accuracy for short, mid, and long-term forecasts, outperforming the performance of individual models.","These findings pave the way for further research on noise reduction strategies at different forecasting horizons through shape-wise feature engineering."],"url":"http://arxiv.org/abs/2401.08233v1"}
{"created":"2024-01-16 09:33:29","title":"Multi-scale 2D Temporal Map Diffusion Models for Natural Language Video Localization","abstract":"Natural Language Video Localization (NLVL), grounding phrases from natural language descriptions to corresponding video segments, is a complex yet critical task in video understanding. Despite ongoing advancements, many existing solutions lack the capability to globally capture temporal dynamics of the video data. In this study, we present a novel approach to NLVL that aims to address this issue. Our method involves the direct generation of a global 2D temporal map via a conditional denoising diffusion process, based on the input video and language query. The main challenges are the inherent sparsity and discontinuity of a 2D temporal map in devising the diffusion decoder. To address these challenges, we introduce a multi-scale technique and develop an innovative diffusion decoder. Our approach effectively encapsulates the interaction between the query and video data across various time scales. Experiments on the Charades and DiDeMo datasets underscore the potency of our design.","sentences":["Natural Language Video Localization (NLVL), grounding phrases from natural language descriptions to corresponding video segments, is a complex yet critical task in video understanding.","Despite ongoing advancements, many existing solutions lack the capability to globally capture temporal dynamics of the video data.","In this study, we present a novel approach to NLVL that aims to address this issue.","Our method involves the direct generation of a global 2D temporal map via a conditional denoising diffusion process, based on the input video and language query.","The main challenges are the inherent sparsity and discontinuity of a 2D temporal map in devising the diffusion decoder.","To address these challenges, we introduce a multi-scale technique and develop an innovative diffusion decoder.","Our approach effectively encapsulates the interaction between the query and video data across various time scales.","Experiments on the Charades and DiDeMo datasets underscore the potency of our design."],"url":"http://arxiv.org/abs/2401.08232v1"}
{"created":"2024-01-16 09:29:18","title":"Experimental Analysis of Type II Singularities and Assembly Change Points in a 3UPS+RPU Parallel Robot","abstract":"Parallel robots (PRs) have singular configurations where the robot gains at least one degree of freedom and loses control. Theoretically, such singularity occurs when the Forward Jacobian-matrix determinant becomes zero (Type II). However, actual PRs could lose control owing to Type II singularities for determinant values near zero, but not zero, because manufacturing tolerances introduce errors that are complex to model due to their low repeatability.   Thus, using an actual 3UPS+RPU PR, this paper presents three contributions: i) a proximity detection index for Type II singularities based on the angle between two Output Twist Screws. The index can identify which kinematic chains contribute to the singularity. ii) an experimental benchmark to study Type II singularities. iii) PR configurations where the proposed index is zero and the Forward Jacobian determinant is not. In this last configuration, the findings show that the actual robot is unable to handle external actions applied to the PR.","sentences":["Parallel robots (PRs) have singular configurations where the robot gains at least one degree of freedom and loses control.","Theoretically, such singularity occurs when the Forward Jacobian-matrix determinant becomes zero (Type II).","However, actual PRs could lose control owing to Type II singularities for determinant values near zero, but not zero, because manufacturing tolerances introduce errors that are complex to model due to their low repeatability.   ","Thus, using an actual 3UPS+RPU PR, this paper presents three contributions: i) a proximity detection index for Type II singularities based on the angle between two Output Twist Screws.","The index can identify which kinematic chains contribute to the singularity.","ii) an experimental benchmark to study Type II singularities.","iii) PR configurations where the proposed index is zero and the Forward Jacobian determinant is not.","In this last configuration, the findings show that the actual robot is unable to handle external actions applied to the PR."],"url":"http://arxiv.org/abs/2401.08229v1"}
{"created":"2024-01-16 09:27:28","title":"MCRPL: A Pretrain, Prompt & Fine-tune Paradigm for Non-overlapping Many-to-one Cross-domain Recommendation","abstract":"Cross-domain Recommendation (CR) is the task that tends to improve the recommendations in the sparse target domain by leveraging the information from other rich domains. Existing methods of cross-domain recommendation mainly focus on overlapping scenarios by assuming users are totally or partially overlapped, which are taken as bridges to connect different domains. However, this assumption does not always hold since it is illegal to leak users' identity information to other domains. Conducting Non-overlapping MCR (NMCR) is challenging since 1) The absence of overlapping information prevents us from directly aligning different domains, and this situation may get worse in the MCR scenario. 2) The distribution between source and target domains makes it difficult for us to learn common information across domains. To overcome the above challenges, we focus on NMCR, and devise MCRPL as our solution. To address Challenge 1, we first learn shared domain-agnostic and domain-dependent prompts, and pre-train them in the pre-training stage. To address Challenge 2, we further update the domain-dependent prompts with other parameters kept fixed to transfer the domain knowledge to the target domain. We conduct experiments on five real-world domains, and the results show the advance of our MCRPL method compared with several recent SOTA baselines.","sentences":["Cross-domain Recommendation (CR) is the task that tends to improve the recommendations in the sparse target domain by leveraging the information from other rich domains.","Existing methods of cross-domain recommendation mainly focus on overlapping scenarios by assuming users are totally or partially overlapped, which are taken as bridges to connect different domains.","However, this assumption does not always hold since it is illegal to leak users' identity information to other domains.","Conducting Non-overlapping MCR (NMCR) is challenging since 1)","The absence of overlapping information prevents us from directly aligning different domains, and this situation may get worse in the MCR scenario.","2)","The distribution between source and target domains makes it difficult for us to learn common information across domains.","To overcome the above challenges, we focus on NMCR, and devise MCRPL as our solution.","To address Challenge 1, we first learn shared domain-agnostic and domain-dependent prompts, and pre-train them in the pre-training stage.","To address Challenge 2, we further update the domain-dependent prompts with other parameters kept fixed to transfer the domain knowledge to the target domain.","We conduct experiments on five real-world domains, and the results show the advance of our MCRPL method compared with several recent SOTA baselines."],"url":"http://arxiv.org/abs/2401.08228v1"}
{"created":"2024-01-16 09:24:51","title":"Core-periphery Detection Based on Masked Bayesian Non-negative Matrix Factorization","abstract":"Core-periphery structure is an essential mesoscale feature in complex networks. Previous researches mostly focus on discriminative approaches while in this work, we propose a generative model called masked Bayesian non-negative matrix factorization. We build the model using two pair affiliation matrices to indicate core-periphery pair associations and using a mask matrix to highlight connections to core nodes. We propose an approach to infer the model parameters, and prove the convergence of variables with our approach. Besides the abilities as traditional approaches, it is able to identify core scores with overlapping core-periphery pairs. We verify the effectiveness of our method using randomly generated networks and real-world networks. Experimental results demonstrate that the proposed method outperforms traditional approaches.","sentences":["Core-periphery structure is an essential mesoscale feature in complex networks.","Previous researches mostly focus on discriminative approaches while in this work, we propose a generative model called masked Bayesian non-negative matrix factorization.","We build the model using two pair affiliation matrices to indicate core-periphery pair associations and using a mask matrix to highlight connections to core nodes.","We propose an approach to infer the model parameters, and prove the convergence of variables with our approach.","Besides the abilities as traditional approaches, it is able to identify core scores with overlapping core-periphery pairs.","We verify the effectiveness of our method using randomly generated networks and real-world networks.","Experimental results demonstrate that the proposed method outperforms traditional approaches."],"url":"http://arxiv.org/abs/2401.08227v1"}
{"created":"2024-01-16 09:22:38","title":"Efficient and Mathematically Robust Operations for Certified Neural Networks Inference","abstract":"In recent years, machine learning (ML) and neural networks (NNs) have gained widespread use and attention across various domains, particularly in transportation for achieving autonomy, including the emergence of flying taxis for urban air mobility (UAM). However, concerns about certification have come up, compelling the development of standardized processes encompassing the entire ML and NN pipeline. This paper delves into the inference stage and the requisite hardware, highlighting the challenges associated with IEEE 754 floating-point arithmetic and proposing alternative number representations. By evaluating diverse summation and dot product algorithms, we aim to mitigate issues related to non-associativity. Additionally, our exploration of fixed-point arithmetic reveals its advantages over floating-point methods, demonstrating significant hardware efficiencies. Employing an empirical approach, we ascertain the optimal bit-width necessary to attain an acceptable level of accuracy, considering the inherent complexity of bit-width optimization.","sentences":["In recent years, machine learning (ML) and neural networks (NNs) have gained widespread use and attention across various domains, particularly in transportation for achieving autonomy, including the emergence of flying taxis for urban air mobility (UAM).","However, concerns about certification have come up, compelling the development of standardized processes encompassing the entire ML and NN pipeline.","This paper delves into the inference stage and the requisite hardware, highlighting the challenges associated with IEEE 754 floating-point arithmetic and proposing alternative number representations.","By evaluating diverse summation and dot product algorithms, we aim to mitigate issues related to non-associativity.","Additionally, our exploration of fixed-point arithmetic reveals its advantages over floating-point methods, demonstrating significant hardware efficiencies.","Employing an empirical approach, we ascertain the optimal bit-width necessary to attain an acceptable level of accuracy, considering the inherent complexity of bit-width optimization."],"url":"http://arxiv.org/abs/2401.08225v1"}
{"created":"2024-01-16 09:15:43","title":"Towards Causal Relationship in Indefinite Data: Baseline Model and New Datasets","abstract":"Integrating deep learning and causal discovery has encouraged us to spot that learning causal structures and representations in dialogue and video is full of challenges. We defined These data forms as \"Indefinite Data\", characterized by multi-structure data and multi-value representations. Unlike existing adaptable data forms, Indefinite Data still faces gaps in datasets and methods. To address the dataset gap, we release two high-quality datasets - Causalogue and Causaction, containing text dialogue samples and video action samples with causal annotations respectively. Moreover, the method gap arises from the coexistence of multi-structure data and multi-value representations, breaking the assumptions of all current methods and rendering them infeasible on Indefinite Data. To this end, we propose a probabilistic framework as a baseline, incorporating three designed highlights for this gap: 1) establishing Causation Condition of representations using the independence of noise terms under non-fixed causal structures, 2) treating causal strength as a latent variable and measuring the reconstruction loss in the correlation space, and 3) estimating the effects of latent confounders. These highpoints make the probabilistic model capable of overcoming challenges brought by the coexistence of multi-structure data and multi-value representations and pave the way for the extension of latent confounders. Comprehensive experiments have evaluated baseline results of causal structures, causal representations, and confounding disentanglement.","sentences":["Integrating deep learning and causal discovery has encouraged us to spot that learning causal structures and representations in dialogue and video is full of challenges.","We defined These data forms as \"Indefinite Data\", characterized by multi-structure data and multi-value representations.","Unlike existing adaptable data forms, Indefinite Data still faces gaps in datasets and methods.","To address the dataset gap, we release two high-quality datasets - Causalogue and Causaction, containing text dialogue samples and video action samples with causal annotations respectively.","Moreover, the method gap arises from the coexistence of multi-structure data and multi-value representations, breaking the assumptions of all current methods and rendering them infeasible on Indefinite Data.","To this end, we propose a probabilistic framework as a baseline, incorporating three designed highlights for this gap: 1) establishing Causation Condition of representations using the independence of noise terms under non-fixed causal structures, 2) treating causal strength as a latent variable and measuring the reconstruction loss in the correlation space, and 3) estimating the effects of latent confounders.","These highpoints make the probabilistic model capable of overcoming challenges brought by the coexistence of multi-structure data and multi-value representations and pave the way for the extension of latent confounders.","Comprehensive experiments have evaluated baseline results of causal structures, causal representations, and confounding disentanglement."],"url":"http://arxiv.org/abs/2401.08221v1"}
{"created":"2024-01-16 09:10:05","title":"Monoidal Extended Stone Duality","abstract":"Extensions of Stone-type dualities have a long history in algebraic logic and have also been instrumental for proving results in algebraic language theory. We show how to extend abstract categorical dualities via monoidal adjunctions, subsuming various incarnations of classical extended Stone and Priestley duality as a special case. Guided by these categorical foundations, we investigate residuation algebras, which are algebraic models of language derivatives, and show the subcategory of derivation algebras to be dually equivalent to the category of profinite ordered monoids, restricting to a duality between boolean residuation algebras and profinite monoids. We further extend this duality to capture relational morphisms of profinite ordered monoids, which dualize to natural morphisms of residuation algebras.","sentences":["Extensions of Stone-type dualities have a long history in algebraic logic and have also been instrumental for proving results in algebraic language theory.","We show how to extend abstract categorical dualities via monoidal adjunctions, subsuming various incarnations of classical extended Stone and Priestley duality as a special case.","Guided by these categorical foundations, we investigate residuation algebras, which are algebraic models of language derivatives, and show the subcategory of derivation algebras to be dually equivalent to the category of profinite ordered monoids, restricting to a duality between boolean residuation algebras and profinite monoids.","We further extend this duality to capture relational morphisms of profinite ordered monoids, which dualize to natural morphisms of residuation algebras."],"url":"http://arxiv.org/abs/2401.08219v1"}
{"created":"2024-01-16 09:04:17","title":"LLM-Guided Multi-View Hypergraph Learning for Human-Centric Explainable Recommendation","abstract":"As personalized recommendation systems become vital in the age of information overload, traditional methods relying solely on historical user interactions often fail to fully capture the multifaceted nature of human interests. To enable more human-centric modeling of user preferences, this work proposes a novel explainable recommendation framework, i.e., LLMHG, synergizing the reasoning capabilities of large language models (LLMs) and the structural advantages of hypergraph neural networks. By effectively profiling and interpreting the nuances of individual user interests, our framework pioneers enhancements to recommendation systems with increased explainability. We validate that explicitly accounting for the intricacies of human preferences allows our human-centric and explainable LLMHG approach to consistently outperform conventional models across diverse real-world datasets. The proposed plug-and-play enhancement framework delivers immediate gains in recommendation performance while offering a pathway to apply advanced LLMs for better capturing the complexity of human interests across machine learning applications.","sentences":["As personalized recommendation systems become vital in the age of information overload, traditional methods relying solely on historical user interactions often fail to fully capture the multifaceted nature of human interests.","To enable more human-centric modeling of user preferences, this work proposes a novel explainable recommendation framework, i.e., LLMHG, synergizing the reasoning capabilities of large language models (LLMs) and the structural advantages of hypergraph neural networks.","By effectively profiling and interpreting the nuances of individual user interests, our framework pioneers enhancements to recommendation systems with increased explainability.","We validate that explicitly accounting for the intricacies of human preferences allows our human-centric and explainable LLMHG approach to consistently outperform conventional models across diverse real-world datasets.","The proposed plug-and-play enhancement framework delivers immediate gains in recommendation performance while offering a pathway to apply advanced LLMs for better capturing the complexity of human interests across machine learning applications."],"url":"http://arxiv.org/abs/2401.08217v1"}
{"created":"2024-01-16 09:02:34","title":"Towards Efficient and Certified Recovery from Poisoning Attacks in Federated Learning","abstract":"Federated learning (FL) is vulnerable to poisoning attacks, where malicious clients manipulate their updates to affect the global model. Although various methods exist for detecting those clients in FL, identifying malicious clients requires sufficient model updates, and hence by the time malicious clients are detected, FL models have been already poisoned. Thus, a method is needed to recover an accurate global model after malicious clients are identified. Current recovery methods rely on (i) all historical information from participating FL clients and (ii) the initial model unaffected by the malicious clients, leading to a high demand for storage and computational resources. In this paper, we show that highly effective recovery can still be achieved based on (i) selective historical information rather than all historical information and (ii) a historical model that has not been significantly affected by malicious clients rather than the initial model. In this scenario, while maintaining comparable recovery performance, we can accelerate the recovery speed and decrease memory consumption. Following this concept, we introduce Crab, an efficient and certified recovery method, which relies on selective information storage and adaptive model rollback. Theoretically, we demonstrate that the difference between the global model recovered by Crab and the one recovered by train-from-scratch can be bounded under certain assumptions. Our empirical evaluation, conducted across three datasets over multiple machine learning models, and a variety of untargeted and targeted poisoning attacks reveals that Crab is both accurate and efficient, and consistently outperforms previous approaches in terms of both recovery speed and memory consumption.","sentences":["Federated learning (FL) is vulnerable to poisoning attacks, where malicious clients manipulate their updates to affect the global model.","Although various methods exist for detecting those clients in FL, identifying malicious clients requires sufficient model updates, and hence by the time malicious clients are detected, FL models have been already poisoned.","Thus, a method is needed to recover an accurate global model after malicious clients are identified.","Current recovery methods rely on (i) all historical information from participating FL clients and (ii) the initial model unaffected by the malicious clients, leading to a high demand for storage and computational resources.","In this paper, we show that highly effective recovery can still be achieved based on (i) selective historical information rather than all historical information and (ii) a historical model that has not been significantly affected by malicious clients rather than the initial model.","In this scenario, while maintaining comparable recovery performance, we can accelerate the recovery speed and decrease memory consumption.","Following this concept, we introduce Crab, an efficient and certified recovery method, which relies on selective information storage and adaptive model rollback.","Theoretically, we demonstrate that the difference between the global model recovered by Crab and the one recovered by train-from-scratch can be bounded under certain assumptions.","Our empirical evaluation, conducted across three datasets over multiple machine learning models, and a variety of untargeted and targeted poisoning attacks reveals that Crab is both accurate and efficient, and consistently outperforms previous approaches in terms of both recovery speed and memory consumption."],"url":"http://arxiv.org/abs/2401.08216v1"}
{"created":"2024-01-16 08:56:52","title":"Human vs. LMMs: Exploring the Discrepancy in Emoji Interpretation and Usage in Digital Communication","abstract":"Leveraging Large Multimodal Models (LMMs) to simulate human behaviors when processing multimodal information, especially in the context of social media, has garnered immense interest due to its broad potential and far-reaching implications. Emojis, as one of the most unique aspects of digital communication, are pivotal in enriching and often clarifying the emotional and tonal dimensions. Yet, there is a notable gap in understanding how these advanced models, such as GPT-4V, interpret and employ emojis in the nuanced context of online interaction. This study intends to bridge this gap by examining the behavior of GPT-4V in replicating human-like use of emojis. The findings reveal a discernible discrepancy between human and GPT-4V behaviors, likely due to the subjective nature of human interpretation and the limitations of GPT-4V's English-centric training, suggesting cultural biases and inadequate representation of non-English cultures.","sentences":["Leveraging Large Multimodal Models (LMMs) to simulate human behaviors when processing multimodal information, especially in the context of social media, has garnered immense interest due to its broad potential and far-reaching implications.","Emojis, as one of the most unique aspects of digital communication, are pivotal in enriching and often clarifying the emotional and tonal dimensions.","Yet, there is a notable gap in understanding how these advanced models, such as GPT-4V, interpret and employ emojis in the nuanced context of online interaction.","This study intends to bridge this gap by examining the behavior of GPT-4V in replicating human-like use of emojis.","The findings reveal a discernible discrepancy between human and GPT-4V behaviors, likely due to the subjective nature of human interpretation and the limitations of GPT-4V's English-centric training, suggesting cultural biases and inadequate representation of non-English cultures."],"url":"http://arxiv.org/abs/2401.08212v1"}
{"created":"2024-01-16 08:54:21","title":"ModelNet-O: A Large-Scale Synthetic Dataset for Occlusion-Aware Point Cloud Classification","abstract":"Recently, 3D point cloud classification has made significant progress with the help of many datasets. However, these datasets do not reflect the incomplete nature of real-world point clouds caused by occlusion, which limits the practical application of current methods. To bridge this gap, we propose ModelNet-O, a large-scale synthetic dataset of 123,041 samples that emulate real-world point clouds with self-occlusion caused by scanning from monocular cameras. ModelNet-O is 10 times larger than existing datasets and offers more challenging cases to evaluate the robustness of existing methods. Our observation on ModelNet-O reveals that well-designed sparse structures can preserve structural information of point clouds under occlusion, motivating us to propose a robust point cloud processing method that leverages a critical point sampling (CPS) strategy in a multi-level manner. We term our method PointMLS. Through extensive experiments, we demonstrate that our PointMLS achieves state-of-the-art results on ModelNet-O and competitive results on regular datasets, and it is robust and effective. More experiments also demonstrate the robustness and effectiveness of PointMLS.","sentences":["Recently, 3D point cloud classification has made significant progress with the help of many datasets.","However, these datasets do not reflect the incomplete nature of real-world point clouds caused by occlusion, which limits the practical application of current methods.","To bridge this gap, we propose ModelNet-O, a large-scale synthetic dataset of 123,041 samples that emulate real-world point clouds with self-occlusion caused by scanning from monocular cameras.","ModelNet-O is 10 times larger than existing datasets and offers more challenging cases to evaluate the robustness of existing methods.","Our observation on ModelNet-O reveals that well-designed sparse structures can preserve structural information of point clouds under occlusion, motivating us to propose a robust point cloud processing method that leverages a critical point sampling (CPS) strategy in a multi-level manner.","We term our method PointMLS.","Through extensive experiments, we demonstrate that our PointMLS achieves state-of-the-art results on ModelNet-O and competitive results on regular datasets, and it is robust and effective.","More experiments also demonstrate the robustness and effectiveness of PointMLS."],"url":"http://arxiv.org/abs/2401.08210v1"}
{"created":"2024-01-16 08:50:44","title":"Transcending the Limit of Local Window: Advanced Super-Resolution Transformer with Adaptive Token Dictionary","abstract":"Single Image Super-Resolution is a classic computer vision problem that involves estimating high-resolution (HR) images from low-resolution (LR) ones. Although deep neural networks (DNNs), especially Transformers for super-resolution, have seen significant advancements in recent years, challenges still remain, particularly in limited receptive field caused by window-based self-attention. To address these issues, we introduce a group of auxiliary Adapeive Token Dictionary to SR Transformer and establish an ATD-SR method. The introduced token dictionary could learn prior information from training data and adapt the learned prior to specific testing image through an adaptive refinement step. The refinement strategy could not only provide global information to all input tokens but also group image tokens into categories. Based on category partitions, we further propose a category-based self-attention mechanism designed to leverage distant but similar tokens for enhancing input features. The experimental results show that our method achieves the best performance on various single image super-resolution benchmarks.","sentences":["Single Image Super-Resolution is a classic computer vision problem that involves estimating high-resolution (HR) images from low-resolution (LR) ones.","Although deep neural networks (DNNs), especially Transformers for super-resolution, have seen significant advancements in recent years, challenges still remain, particularly in limited receptive field caused by window-based self-attention.","To address these issues, we introduce a group of auxiliary Adapeive Token Dictionary to SR Transformer and establish an ATD-SR method.","The introduced token dictionary could learn prior information from training data and adapt the learned prior to specific testing image through an adaptive refinement step.","The refinement strategy could not only provide global information to all input tokens but also group image tokens into categories.","Based on category partitions, we further propose a category-based self-attention mechanism designed to leverage distant but similar tokens for enhancing input features.","The experimental results show that our method achieves the best performance on various single image super-resolution benchmarks."],"url":"http://arxiv.org/abs/2401.08209v1"}
{"created":"2024-01-16 08:44:29","title":"Generative Multi-Modal Knowledge Retrieval with Large Language Models","abstract":"Knowledge retrieval with multi-modal queries plays a crucial role in supporting knowledge-intensive multi-modal applications. However, existing methods face challenges in terms of their effectiveness and training efficiency, especially when it comes to training and integrating multiple retrievers to handle multi-modal queries. In this paper, we propose an innovative end-to-end generative framework for multi-modal knowledge retrieval. Our framework takes advantage of the fact that large language models (LLMs) can effectively serve as virtual knowledge bases, even when trained with limited data. We retrieve knowledge via a two-step process: 1) generating knowledge clues related to the queries, and 2) obtaining the relevant document by searching databases using the knowledge clue. In particular, we first introduce an object-aware prefix-tuning technique to guide multi-grained visual learning. Then, we align multi-grained visual features into the textual feature space of the LLM, employing the LLM to capture cross-modal interactions. Subsequently, we construct instruction data with a unified format for model training. Finally, we propose the knowledge-guided generation strategy to impose prior constraints in the decoding steps, thereby promoting the generation of distinctive knowledge clues. Through experiments conducted on three benchmarks, we demonstrate significant improvements ranging from 3.0% to 14.6% across all evaluation metrics when compared to strong baselines.","sentences":["Knowledge retrieval with multi-modal queries plays a crucial role in supporting knowledge-intensive multi-modal applications.","However, existing methods face challenges in terms of their effectiveness and training efficiency, especially when it comes to training and integrating multiple retrievers to handle multi-modal queries.","In this paper, we propose an innovative end-to-end generative framework for multi-modal knowledge retrieval.","Our framework takes advantage of the fact that large language models (LLMs) can effectively serve as virtual knowledge bases, even when trained with limited data.","We retrieve knowledge via a two-step process: 1) generating knowledge clues related to the queries, and 2) obtaining the relevant document by searching databases using the knowledge clue.","In particular, we first introduce an object-aware prefix-tuning technique to guide multi-grained visual learning.","Then, we align multi-grained visual features into the textual feature space of the LLM, employing the LLM to capture cross-modal interactions.","Subsequently, we construct instruction data with a unified format for model training.","Finally, we propose the knowledge-guided generation strategy to impose prior constraints in the decoding steps, thereby promoting the generation of distinctive knowledge clues.","Through experiments conducted on three benchmarks, we demonstrate significant improvements ranging from 3.0% to 14.6% across all evaluation metrics when compared to strong baselines."],"url":"http://arxiv.org/abs/2401.08206v1"}
{"created":"2024-01-16 08:30:41","title":"IsamasRed: A Public Dataset Tracking Reddit Discussions on Israel-Hamas Conflict","abstract":"The conflict between Israel and Palestinians significantly escalated after the October 7, 2023 Hamas attack, capturing global attention. To understand the public discourse on this conflict, we present a meticulously compiled dataset--IsamasRed--comprising nearly 400,000 conversations and over 8 million comments from Reddit, spanning from August 2023 to November 2023. We introduce an innovative keyword extraction framework leveraging a large language model to effectively identify pertinent keywords, ensuring a comprehensive data collection. Our initial analysis on the dataset, examining topics, controversy, emotional and moral language trends over time, highlights the emotionally charged and complex nature of the discourse. This dataset aims to enrich the understanding of online discussions, shedding light on the complex interplay between ideology, sentiment, and community engagement in digital spaces.","sentences":["The conflict between Israel and Palestinians significantly escalated after the October 7, 2023 Hamas attack, capturing global attention.","To understand the public discourse on this conflict, we present a meticulously compiled dataset--IsamasRed--comprising nearly 400,000 conversations and over 8 million comments from Reddit, spanning from August 2023 to November 2023.","We introduce an innovative keyword extraction framework leveraging a large language model to effectively identify pertinent keywords, ensuring a comprehensive data collection.","Our initial analysis on the dataset, examining topics, controversy, emotional and moral language trends over time, highlights the emotionally charged and complex nature of the discourse.","This dataset aims to enrich the understanding of online discussions, shedding light on the complex interplay between ideology, sentiment, and community engagement in digital spaces."],"url":"http://arxiv.org/abs/2401.08202v1"}
{"created":"2024-01-16 08:25:29","title":"Matrix Completion with Hypergraphs:Sharp Thresholds and Efficient Algorithms","abstract":"This paper considers the problem of completing a rating matrix based on sub-sampled matrix entries as well as observed social graphs and hypergraphs. We show that there exists a \\emph{sharp threshold} on the sample probability for the task of exactly completing the rating matrix -- the task is achievable when the sample probability is above the threshold, and is impossible otherwise -- demonstrating a phase transition phenomenon. The threshold can be expressed as a function of the ``quality'' of hypergraphs, enabling us to \\emph{quantify} the amount of reduction in sample probability due to the exploitation of hypergraphs. This also highlights the usefulness of hypergraphs in the matrix completion problem. En route to discovering the sharp threshold, we develop a computationally efficient matrix completion algorithm that effectively exploits the observed graphs and hypergraphs. Theoretical analyses show that our algorithm succeeds with high probability as long as the sample probability exceeds the aforementioned threshold, and this theoretical result is further validated by synthetic experiments. Moreover, our experiments on a real social network dataset (with both graphs and hypergraphs) show that our algorithm outperforms other state-of-the-art matrix completion algorithms.","sentences":["This paper considers the problem of completing a rating matrix based on sub-sampled matrix entries as well as observed social graphs and hypergraphs.","We show that there exists a \\emph{sharp threshold} on the sample probability for the task of exactly completing the rating matrix -- the task is achievable when the sample probability is above the threshold, and is impossible otherwise -- demonstrating a phase transition phenomenon.","The threshold can be expressed as a function of the ``quality'' of hypergraphs, enabling us to \\emph{quantify} the amount of reduction in sample probability due to the exploitation of hypergraphs.","This also highlights the usefulness of hypergraphs in the matrix completion problem.","En route to discovering the sharp threshold, we develop a computationally efficient matrix completion algorithm that effectively exploits the observed graphs and hypergraphs.","Theoretical analyses show that our algorithm succeeds with high probability as long as the sample probability exceeds the aforementioned threshold, and this theoretical result is further validated by synthetic experiments.","Moreover, our experiments on a real social network dataset (with both graphs and hypergraphs) show that our algorithm outperforms other state-of-the-art matrix completion algorithms."],"url":"http://arxiv.org/abs/2401.08197v1"}
{"created":"2024-01-16 08:22:28","title":"On Cryptographic Mechanisms for the Selective Disclosure of Verifiable Credentials","abstract":"Verifiable credentials are a digital analogue of physical credentials. Their authenticity and integrity are protected by means of cryptographic techniques, and they can be presented to verifiers to reveal attributes or even predicates about the attributes included in the credential. One way to preserve privacy during presentation consists in selectively disclosing the attributes in a credential. In this paper we present the most widespread cryptographic mechanisms used to enable selective disclosure of attributes identifying two categories: the ones based on hiding commitments - e.g., mdl ISO/IEC 18013-5 - and the ones based on non-interactive zero-knowledge proofs - e.g., BBS signatures. We also include a description of the cryptographic primitives used to design such cryptographic mechanisms. We describe the design of the cryptographic mechanisms and compare them by performing an analysis on their standard maturity in terms of standardization, cryptographic agility and quantum safety, then we compare the features that they support with main focus on the unlinkability of presentations, the ability to create predicate proofs and support for threshold credential issuance. Finally we perform an experimental evaluation based on the Rust open source implementations that we have considered most relevant. In particular we evaluate the size of credentials and presentations built using different cryptographic mechanisms and the time needed to generate and verify them. We also highlight some trade-offs that must be considered in the instantiation of the cryptographic mechanisms.","sentences":["Verifiable credentials are a digital analogue of physical credentials.","Their authenticity and integrity are protected by means of cryptographic techniques, and they can be presented to verifiers to reveal attributes or even predicates about the attributes included in the credential.","One way to preserve privacy during presentation consists in selectively disclosing the attributes in a credential.","In this paper we present the most widespread cryptographic mechanisms used to enable selective disclosure of attributes identifying two categories: the ones based on hiding commitments - e.g., mdl ISO/IEC 18013-5 - and the ones based on non-interactive zero-knowledge proofs - e.g., BBS signatures.","We also include a description of the cryptographic primitives used to design such cryptographic mechanisms.","We describe the design of the cryptographic mechanisms and compare them by performing an analysis on their standard maturity in terms of standardization, cryptographic agility and quantum safety, then we compare the features that they support with main focus on the unlinkability of presentations, the ability to create predicate proofs and support for threshold credential issuance.","Finally we perform an experimental evaluation based on the Rust open source implementations that we have considered most relevant.","In particular we evaluate the size of credentials and presentations built using different cryptographic mechanisms and the time needed to generate and verify them.","We also highlight some trade-offs that must be considered in the instantiation of the cryptographic mechanisms."],"url":"http://arxiv.org/abs/2401.08196v1"}
{"created":"2024-01-16 08:17:58","title":"Three classes of propagation rules for GRS and EGRS codes and their applications to EAQECCs","abstract":"In this paper, we study the Hermitian hulls of (extended) generalized Reed-Solomon (GRS and EGRS) codes over finite fields. For a given class of (extended) GRS codes, by increasing the length, increasing the dimensions and increasing both the length and the dimensions, we obtain three new classes of (extended) GRS codes with Hermitian hulls of arbitrary dimensions. Furthermore, we obtain several new classes of $q^2$-ary maximum distance separable (MDS) codes with Hermitian hulls of arbitrary dimensions. And the dimension of these MDS codes can be taken from $1$ to $\\frac{n}{2}$. By propagation rules, the parameters of the obtained code can be more flexible. As an application, a lot of new (MDS) entanglement-assisted quantum error correction codes (EAQECCs) can be constructed from previous known (extended) GRS codes. We derive three new propagation rules on (MDS) EAQECCs constructed from (extended) GRS codes. Finally, we present several new classes of (MDS) EAQECCs with flexible parameters. Notably, the distance parameters of our codes can range from $2$ to $\\frac{n+2}{2}$.","sentences":["In this paper, we study the Hermitian hulls of (extended) generalized Reed-Solomon (GRS and EGRS) codes over finite fields.","For a given class of (extended) GRS codes, by increasing the length, increasing the dimensions and increasing both the length and the dimensions, we obtain three new classes of (extended) GRS codes with Hermitian hulls of arbitrary dimensions.","Furthermore, we obtain several new classes of $q^2$-ary maximum distance separable (MDS) codes with Hermitian hulls of arbitrary dimensions.","And the dimension of these MDS codes can be taken from $1$ to $\\frac{n}{2}$. By propagation rules, the parameters of the obtained code can be more flexible.","As an application, a lot of new (MDS) entanglement-assisted quantum error correction codes (EAQECCs) can be constructed from previous known (extended) GRS codes.","We derive three new propagation rules on (MDS) EAQECCs constructed from (extended) GRS codes.","Finally, we present several new classes of (MDS) EAQECCs with flexible parameters.","Notably, the distance parameters of our codes can range from $2$ to $\\frac{n+2}{2}$."],"url":"http://arxiv.org/abs/2401.08195v1"}
{"created":"2024-01-16 08:16:10","title":"End-to-End Optimized Image Compression with the Frequency-Oriented Transform","abstract":"Image compression constitutes a significant challenge amidst the era of information explosion. Recent studies employing deep learning methods have demonstrated the superior performance of learning-based image compression methods over traditional codecs. However, an inherent challenge associated with these methods lies in their lack of interpretability. Following an analysis of the varying degrees of compression degradation across different frequency bands, we propose the end-to-end optimized image compression model facilitated by the frequency-oriented transform. The proposed end-to-end image compression model consists of four components: spatial sampling, frequency-oriented transform, entropy estimation, and frequency-aware fusion. The frequency-oriented transform separates the original image signal into distinct frequency bands, aligning with the human-interpretable concept. Leveraging the non-overlapping hypothesis, the model enables scalable coding through the selective transmission of arbitrary frequency components. Extensive experiments are conducted to demonstrate that our model outperforms all traditional codecs including next-generation standard H.266/VVC on MS-SSIM metric. Moreover, visual analysis tasks (i.e., object detection and semantic segmentation) are conducted to verify the proposed compression method could preserve semantic fidelity besides signal-level precision.","sentences":["Image compression constitutes a significant challenge amidst the era of information explosion.","Recent studies employing deep learning methods have demonstrated the superior performance of learning-based image compression methods over traditional codecs.","However, an inherent challenge associated with these methods lies in their lack of interpretability.","Following an analysis of the varying degrees of compression degradation across different frequency bands, we propose the end-to-end optimized image compression model facilitated by the frequency-oriented transform.","The proposed end-to-end image compression model consists of four components: spatial sampling, frequency-oriented transform, entropy estimation, and frequency-aware fusion.","The frequency-oriented transform separates the original image signal into distinct frequency bands, aligning with the human-interpretable concept.","Leveraging the non-overlapping hypothesis, the model enables scalable coding through the selective transmission of arbitrary frequency components.","Extensive experiments are conducted to demonstrate that our model outperforms all traditional codecs including next-generation standard H.266/VVC on MS-SSIM metric.","Moreover, visual analysis tasks (i.e., object detection and semantic segmentation) are conducted to verify the proposed compression method could preserve semantic fidelity besides signal-level precision."],"url":"http://arxiv.org/abs/2401.08194v1"}
{"created":"2024-01-16 08:11:29","title":"Mechatronic Design, Experimental Setup and Control Architecture Design of a Novel 4 DoF Parallel Manipulator","abstract":"Although parallel manipulators (PMs) started with the introduction of architectures with 6 Degrees of Freedom (DoF), a vast number of applications require less than 6 DoF. Consequently, scholars have proposed architectures with 3 DoF and 4 DoF, but relatively few 4 DoF PMs have become prototypes, especially of the two rotation (2R) and two translation (2T) motion types. In this paper, we explain the mechatronics design, prototype and control architecture design of a 4 DoF PM with 2R2T motions. We chose to design a 4 DoF manipulator based on the motion needed to complete the tasks of lower limb rehabilitation.   To the author's best knowledge, PMs between 3 and 6 DoF for rehabilitation of lower limbs have not been proposed to date. The developed architecture enhances the three minimum DoF required by adding a 4 DoF which allows combinations of normal or tangential efforts in the joints, or torque acting on the knee. We put forward the inverse and forward displacement equations, describe the prototype, perform the experimental setup, and develop the hardware and control architecture. The tracking accuracy experiments from the proposed controller show that the manipulator can accomplish the required application.","sentences":["Although parallel manipulators (PMs) started with the introduction of architectures with 6 Degrees of Freedom (DoF), a vast number of applications require less than 6 DoF. Consequently, scholars have proposed architectures with 3 DoF and 4 DoF, but relatively few 4 DoF PMs have become prototypes, especially of the two rotation (2R) and two translation (2T) motion types.","In this paper, we explain the mechatronics design, prototype and control architecture design of a 4 DoF PM with 2R2T motions.","We chose to design a 4 DoF manipulator based on the motion needed to complete the tasks of lower limb rehabilitation.   ","To the author's best knowledge, PMs between 3 and 6 DoF for rehabilitation of lower limbs have not been proposed to date.","The developed architecture enhances the three minimum DoF required by adding a 4 DoF which allows combinations of normal or tangential efforts in the joints, or torque acting on the knee.","We put forward the inverse and forward displacement equations, describe the prototype, perform the experimental setup, and develop the hardware and control architecture.","The tracking accuracy experiments from the proposed controller show that the manipulator can accomplish the required application."],"url":"http://arxiv.org/abs/2401.08192v1"}
{"created":"2024-01-16 08:10:20","title":"Reconfiguration of a parallel kinematic manipulator with 2T2R motions for avoiding singularities through minimizing actuator forces","abstract":"This paper aims to develop an approach for the reconfiguration of a parallel kinematic manipulator (PKM) with four degrees of freedom (DoF) designed to tackle tasks of diagnosis and rehabilitation in an injured knee. The original layout of the 4-DoF manipulator presents Type-II singular configurations within its workspace. Thus, we proposed to reconfigure the manipulator to avoid such singularities (owing to the Forward Jacobian of the PKM) during typical rehabilitation trajectories. We achieve the reconfiguration of the PKM through a minimization problem where the design variables correspond to the anchoring points of the robot limbs on fixed and mobile platforms. The objective function relies on the minimization of the forces exerted by the actuators for a specific trajectory. The minimization problem considers constraint equations to avoid Type-II singularities, which guarantee the feasibility of the active generalized coordinates for a particular path. To evaluate the proposed conceptual strategy, we build a prototype where reconfiguration occurs by moving the position of the anchoring points to holes bored in the fixed and mobile platforms. Simulations and experiments of several study cases enable testing the strategy performance. The results show that the reconfiguration strategy allows obtaining trajectories having minimum actuation forces without Type-II singularities.","sentences":["This paper aims to develop an approach for the reconfiguration of a parallel kinematic manipulator (PKM) with four degrees of freedom (DoF) designed to tackle tasks of diagnosis and rehabilitation in an injured knee.","The original layout of the 4-DoF manipulator presents Type-II singular configurations within its workspace.","Thus, we proposed to reconfigure the manipulator to avoid such singularities (owing to the Forward Jacobian of the PKM) during typical rehabilitation trajectories.","We achieve the reconfiguration of the PKM through a minimization problem where the design variables correspond to the anchoring points of the robot limbs on fixed and mobile platforms.","The objective function relies on the minimization of the forces exerted by the actuators for a specific trajectory.","The minimization problem considers constraint equations to avoid Type-II singularities, which guarantee the feasibility of the active generalized coordinates for a particular path.","To evaluate the proposed conceptual strategy, we build a prototype where reconfiguration occurs by moving the position of the anchoring points to holes bored in the fixed and mobile platforms.","Simulations and experiments of several study cases enable testing the strategy performance.","The results show that the reconfiguration strategy allows obtaining trajectories having minimum actuation forces without Type-II singularities."],"url":"http://arxiv.org/abs/2401.08191v1"}
{"created":"2024-01-16 08:08:01","title":"MARIO: MAth Reasoning with code Interpreter Output -- A Reproducible Pipeline","abstract":"Large language models (LLMs) have seen considerable advancements in natural language understanding tasks, yet there remains a gap to bridge before attaining true artificial general intelligence, especially concerning shortcomings in mathematical reasoning capabilities. We postulate that the inherent nature of LLM training, which focuses on predicting probabilities of next token, presents challenges in effectively modeling mathematical reasoning that demands exact calculations, both from data-driven and theoretical standpoints. In this paper, we address this challenge by enriching the data landscape and introducing a novel math dataset, enhanced with a capability to utilize a Python code interpreter. This dataset is derived from GSM8K and MATH and has been further refined through a combination of GPT-4 annotations, human review, and self-training processes, where the errors in the original GSM8K training set have been fixed. Additionally, we propose a tentative, easily replicable protocol for the fine-tuning of math-specific LLMs, which has led to a significant improvement in the performance of a 7B-parameter LLM on the GSM8K and MATH datasets. We are committed to advancing the field of mathematical reasoning in LLMs and, to that end, we have made the model checkpoints and will make the dataset publicly available. We hope this will facilitate further research and development within the community.","sentences":["Large language models (LLMs) have seen considerable advancements in natural language understanding tasks, yet there remains a gap to bridge before attaining true artificial general intelligence, especially concerning shortcomings in mathematical reasoning capabilities.","We postulate that the inherent nature of LLM training, which focuses on predicting probabilities of next token, presents challenges in effectively modeling mathematical reasoning that demands exact calculations, both from data-driven and theoretical standpoints.","In this paper, we address this challenge by enriching the data landscape and introducing a novel math dataset, enhanced with a capability to utilize a Python code interpreter.","This dataset is derived from GSM8K and MATH and has been further refined through a combination of GPT-4 annotations, human review, and self-training processes, where the errors in the original GSM8K training set have been fixed.","Additionally, we propose a tentative, easily replicable protocol for the fine-tuning of math-specific LLMs, which has led to a significant improvement in the performance of a 7B-parameter LLM on the GSM8K and MATH datasets.","We are committed to advancing the field of mathematical reasoning in LLMs and, to that end, we have made the model checkpoints and will make the dataset publicly available.","We hope this will facilitate further research and development within the community."],"url":"http://arxiv.org/abs/2401.08190v1"}
{"created":"2024-01-16 08:04:50","title":"PRewrite: Prompt Rewriting with Reinforcement Learning","abstract":"Prompt engineering is critical for the development of LLM-based applications. However, it is usually done manually in a \"trial and error\" fashion. This manual procedure can be time consuming, ineffective, and the generated prompts are, in a lot of cases, sub-optimal. Even for the prompts which seemingly work well, there is always a lingering question: can the prompts be made better with further modifications?   To address these questions, in this paper, we investigate prompt engineering automation. We consider a specific use case scenario in which developers/users have drafted initial prompts, but lack the time/expertise to optimize them. We propose PRewrite, an automated tool to rewrite these drafts and to generate highly effective new prompts. PRewrite is based on the Reinforcement Learning (RL) framework which allows for end-to-end optimization and our design allows the RL search to happen in a large action space. The automated tool leverages manually crafted prompts as starting points which makes the rewriting procedure more guided and efficient. The generated prompts are human readable, and self-explanatory, unlike some of those in previous works. We conducted extensive experiments on diverse datasets and found that the prompts generated with this new method not only outperform professionally crafted prompts, but also prompts generated with other previously proposed methods.","sentences":["Prompt engineering is critical for the development of LLM-based applications.","However, it is usually done manually in a \"trial and error\" fashion.","This manual procedure can be time consuming, ineffective, and the generated prompts are, in a lot of cases, sub-optimal.","Even for the prompts which seemingly work well, there is always a lingering question: can the prompts be made better with further modifications?   ","To address these questions, in this paper, we investigate prompt engineering automation.","We consider a specific use case scenario in which developers/users have drafted initial prompts, but lack the time/expertise to optimize them.","We propose PRewrite, an automated tool to rewrite these drafts and to generate highly effective new prompts.","PRewrite is based on the Reinforcement Learning (RL) framework which allows for end-to-end optimization and our design allows the RL search to happen in a large action space.","The automated tool leverages manually crafted prompts as starting points which makes the rewriting procedure more guided and efficient.","The generated prompts are human readable, and self-explanatory, unlike some of those in previous works.","We conducted extensive experiments on diverse datasets and found that the prompts generated with this new method not only outperform professionally crafted prompts, but also prompts generated with other previously proposed methods."],"url":"http://arxiv.org/abs/2401.08189v1"}
{"created":"2024-01-16 08:01:09","title":"DPAFNet:Dual Path Attention Fusion Network for Single Image Deraining","abstract":"Rainy weather will have a significant impact on the regular operation of the imaging system. Based on this premise, image rain removal has always been a popular branch of low-level visual tasks, especially methods using deep neural networks. However, most neural networks are but-branched, such as only using convolutional neural networks or Transformers, which is unfavourable for the multidimensional fusion of image features. In order to solve this problem, this paper proposes a dual-branch attention fusion network. Firstly, a two-branch network structure is proposed. Secondly, an attention fusion module is proposed to selectively fuse the features extracted by the two branches rather than simply adding them. Finally, complete ablation experiments and sufficient comparison experiments prove the rationality and effectiveness of the proposed method.","sentences":["Rainy weather will have a significant impact on the regular operation of the imaging system.","Based on this premise, image rain removal has always been a popular branch of low-level visual tasks, especially methods using deep neural networks.","However, most neural networks are but-branched, such as only using convolutional neural networks or Transformers, which is unfavourable for the multidimensional fusion of image features.","In order to solve this problem, this paper proposes a dual-branch attention fusion network.","Firstly, a two-branch network structure is proposed.","Secondly, an attention fusion module is proposed to selectively fuse the features extracted by the two branches rather than simply adding them.","Finally, complete ablation experiments and sufficient comparison experiments prove the rationality and effectiveness of the proposed method."],"url":"http://arxiv.org/abs/2401.08185v1"}
{"created":"2024-01-16 07:54:59","title":"LiveScaler: Live control of the harmony of an electronic music track","abstract":"In Electronic Dance Music (EDM), many artists use DJing techniques in order to perform their own productions live. As a consequence, they do not have access during the performance to the internal structure of their tracks, and specifically to their equivalent of a partition: MIDI files. On the other hand, if an artist attempts to remix or interpret their own production live, the number of tracks that they can simultaneously control is limited without suitable software. This article introduces LiveScaler, a software that allows live control of the harmony and pitch of electronic music. A set of pitch transformations, termed affine transformations, is presented. These transformations are applied to all MIDI streams of a prepared track. A MaxMSP implementation, in conjunction with Ableton Live, is proposed. Special attention is given to control issues, mapping, and practical live experimentation in the context of EDM.","sentences":["In Electronic Dance Music (EDM), many artists use DJing techniques in order to perform their own productions live.","As a consequence, they do not have access during the performance to the internal structure of their tracks, and specifically to their equivalent of a partition:","MIDI files.","On the other hand, if an artist attempts to remix or interpret their own production live, the number of tracks that they can simultaneously control is limited without suitable software.","This article introduces LiveScaler, a software that allows live control of the harmony and pitch of electronic music.","A set of pitch transformations, termed affine transformations, is presented.","These transformations are applied to all MIDI streams of a prepared track.","A MaxMSP implementation, in conjunction with Ableton Live, is proposed.","Special attention is given to control issues, mapping, and practical live experimentation in the context of EDM."],"url":"http://arxiv.org/abs/2401.08181v1"}
{"created":"2024-01-16 07:51:15","title":"DeMM: A Decoupled Matrix Multiplication Engine Supporting Relaxed Structured Sparsity","abstract":"Deep Learning (DL) has achieved unprecedented success in various application domains. Meanwhile, model pruning has emerged as a viable solution to reduce the footprint of DL models in mobile applications, without compromising their accuracy. To enable the matrix engines built for dense DL models to also handle their pruned counterparts, pruned DL models follow a fine-grained structured sparsity pattern of 1:4, or 2:4, whereby in each group of four contiguous values, at least one, or two, respectively, must be non-zero. Structured sparsity has recently also moved to coarser (relaxed) cases of N:128, or N:256, for small values of N, targeting a wider range of sparsity (10%-90%) for the DL models. In this work, we design an accelerator that operates, by construction, on wide blocks with relaxed structured sparsity. In contrast to the conventional systolic array archetype, the new engine decouples the memory part of the systolic array from the multiply-add units. The memory block comprises 1 write and N read ports, with the number of read ports being equal to the number of non-zero elements per row. The multiply-add units connect directly to each read port and complete the multiplication in a row-wise product-first order. More importantly, simple reconfiguration facilitates more dense patterns. The experimental evaluation demonstrates substantial latency improvements over current state-of-the-art systolic array engines built for fine-grained and relaxed structured sparsity.","sentences":["Deep Learning (DL) has achieved unprecedented success in various application domains.","Meanwhile, model pruning has emerged as a viable solution to reduce the footprint of DL models in mobile applications, without compromising their accuracy.","To enable the matrix engines built for dense DL models to also handle their pruned counterparts, pruned DL models follow a fine-grained structured sparsity pattern of 1:4, or 2:4, whereby in each group of four contiguous values, at least one, or two, respectively, must be non-zero.","Structured sparsity has recently also moved to coarser (relaxed) cases of N:128, or N:256, for small values of N, targeting a wider range of sparsity (10%-90%) for the DL models.","In this work, we design an accelerator that operates, by construction, on wide blocks with relaxed structured sparsity.","In contrast to the conventional systolic array archetype, the new engine decouples the memory part of the systolic array from the multiply-add units.","The memory block comprises 1 write and N read ports, with the number of read ports being equal to the number of non-zero elements per row.","The multiply-add units connect directly to each read port and complete the multiplication in a row-wise product-first order.","More importantly, simple reconfiguration facilitates more dense patterns.","The experimental evaluation demonstrates substantial latency improvements over current state-of-the-art systolic array engines built for fine-grained and relaxed structured sparsity."],"url":"http://arxiv.org/abs/2401.08179v1"}
