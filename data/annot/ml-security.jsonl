{"text":"In this work, we present a detector for marine debris built on a deep segmentation model that outputs a probability for marine debris at the pixel level.","cats":{"new-dataset":0,"ml-security":0}}
{"text":"To testify the effectiveness and superiority of the proposed approach, we conduct extensive experiments on benchmark datasets.","cats":{"new-dataset":0,"ml-security":0}}
{"text":"This paper proposes a data-efficient detection method for deep neural networks against backdoor attacks under a black-box scenario.","cats":{"new-dataset":0,"ml-security":1}}
{"text":"The proposed approach is motivated by the intuition that features corresponding to triggers have a higher influence in determining the backdoored network output than any other benign features.","cats":{"new-dataset":0,"ml-security":1}}
{"text":"To calculate the five-metric values for a given input, we first generate several synthetic samples by injecting the input's partial contents into clean validation samples.","cats":{"new-dataset":0,"ml-security":0}}
{"text":"Then, the five metrics are computed by using the output labels of the corresponding synthetic samples.","cats":{"new-dataset":0,"ml-security":0}}
{"text":"One contribution of this work is the use of a tiny clean validation dataset.","cats":{"new-dataset":0,"ml-security":0}}
{"text":"Having the computed five metrics, five novelty detectors are trained from the validation dataset.","cats":{"new-dataset":0,"ml-security":0}}
{"text":"A meta novelty detector fuses the output of the five trained novelty detectors to generate a meta confidence score.","cats":{"new-dataset":0,"ml-security":0}}
{"text":"During online testing, our method determines if online samples are poisoned or not via assessing their meta confidence scores output by the meta novelty detector.","cats":{"new-dataset":0,"ml-security":0}}
{"text":"We show the efficacy of our methodology through a broad range of backdoor attacks, including ablation studies and comparison to existing approaches.","cats":{"new-dataset":0,"ml-security":0}}
{"text":"Our methodology is promising since the proposed five metrics quantify the inherent differences between clean and poisoned samples.","cats":{"new-dataset":0,"ml-security":0}}
{"text":"Additionally, our detection method can be incrementally improved by appending more metrics that may be proposed to address future advanced attacks.","cats":{"new-dataset":0,"ml-security":1}}
{"text":"Adversarial attacks have the potential to mislead deep neural network classifiers by introducing slight perturbations.","cats":{"new-dataset":0,"ml-security":1}}
{"text":"Motivated by the performance of each RNNs, a meta-model is developed to improve the overall recognition performance by combining the predictions of the individual RNNs.","cats":{"dev-research":0,"ml-security":0}}
{"text":"Our results show that (1) security defects are not prevalently discussed in code review, (2) more than half of the reviewers provided explicit fixing strategies/solutions to help developers fix security defects, (3) developers tend to follow reviewers' suggestions and action the changes, (4) Not worth fixing the defect now and Disagreement between the developer and the reviewer are the main causes for not resolving security defects.","cats":{"dev-research":1,"ml-security":0}}
{"text":"Fourth, as programmable networks and machine learning (ML) techniques are increasingly becoming adopted by the community, their current applications in network security are discussed.","cats":{"ml-security":0}}
{"text":"Especially when both the ML model and the input data's confidentiality must be protected.","cats":{"ml-security":1}}
{"text":"We can expect such systems to be vulnerable to some adversarial-ML attacks.","cats":{"ml-security":1}}
{"text":"Traditional adversarial training is a popular methodology for robustifying ML models against attacks.","cats":{"ml-security":0}}
{"text":"The remarkable success of the use of machine learning-based solutions for network security problems has been impeded by the developed ML models' inability to maintain efficacy when used in different network environments exhibiting different network behaviors.","cats":{"ml-security":1}}
{"text":"Machine learning (ML) models can leak information about users, and differential privacy (DP) provides a rigorous way to bound that leakage under a given budget.","cats":{"ml-security":1}}
{"text":"However, compared to attacks on other ML-based systems, attackers face a level of indirection as they cannot interact directly with the learned model.","cats":{"ml-security":1}}
{"text":"However, due to increasing demand for users' privacy and security, we often need to remove users' data information from Machine Learning (ML) models to satisfy specific privacy and security requirements.","cats":{"ml-security":0}}
{"text":"We analyze the root causes of potentially-increased attack surface in learned systems and develop a framework for identifying vulnerabilities that stem from the use of ML.","cats":{"ml-security":1}}
{"text":"Since machine-learning is being deployed in safety-critical and security-sensitive domains, such attacks may have catastrophic security and safety consequences.","cats":{"ml-security":1}}
{"text":"Backdoor attacks have been demonstrated as a security threat for machine learning models.","cats":{"ml-security":1}}
{"text":"However, if the privacy of machine learning applications' customers cannot be guaranteed, it will cause security threats and losses to users' personal privacy information and service providers.","cats":{"ml-security":1}}
{"text":"In this paper, we introduce Semantic-SAM, a universal image segmentation model to enable segment and recognize anything at any desired granularity.","cats":{"ml-security":0}}
{"text":"Our model offers two key advantages: semantic-awareness and granularity-abundance.","cats":{"ml-security":0}}
{"text":"To achieve semantic-awareness, we consolidate multiple datasets across three granularities and introduce decoupled classification for objects and parts.","cats":{"ml-security":0}}
{"text":"This allows our model to capture rich semantic information.","cats":{"ml-security":0}}
{"text":"For the multi-granularity capability, we propose a multi-choice learning scheme during training, enabling each click to generate masks at multiple levels that correspond to multiple ground-truth masks.","cats":{"ml-security":0}}
{"text":"Notably, this work represents the first attempt to jointly train a model on SA-1B, generic, and part segmentation datasets.","cats":{"ml-security":0}}
{"text":"Experimental results and visualizations demonstrate that our model successfully achieves semantic-awareness and granularity-abundance.","cats":{"ml-security":0}}
{"text":"Furthermore, combining SA-1B training with other segmentation tasks, such as panoptic and part segmentation, leads to performance improvements.","cats":{"ml-security":0}}
{"text":"We will provide code and a demo for further exploration and evaluation.","cats":{"ml-security":0}}
{"text":"We propose a self-supervised method for learning representations based on spatial audio-visual correspondences in egocentric videos.","cats":{"ml-security":0}}
{"text":"In particular, our method leverages a masked auto-encoding framework to synthesize masked binaural audio through the synergy of audio and vision, thereby learning useful spatial relationships between the two modalities.","cats":{"ml-security":0}}
{"text":"We use our pretrained features to tackle two downstream video tasks requiring spatial understanding in social scenarios: active speaker detection and spatial audio denoising.","cats":{"ml-security":0}}
{"text":"We show through extensive experiments that our features are generic enough to improve over multiple state-of-the-art baselines on two public challenging egocentric video datasets, EgoCom and EasyCom.","cats":{"ml-security":0}}
{"text":"Project: http://vision.cs.utexas.edu/projects/ego_av_corr.","cats":{"ml-security":0}}
{"text":"One of the fundamental steps toward understanding a complex system is identifying variation at the scale of the system's components that is most relevant to behavior on a macroscopic scale.","cats":{"ml-security":0}}
{"text":"Mutual information is a natural means of linking variation across scales of a system due to its independence of the particular functional relationship between variables.","cats":{"ml-security":0}}
{"text":"However, estimating mutual information given high-dimensional, continuous-valued data is notoriously difficult, and the desideratum -- to reveal important variation in a comprehensible manner -- is only readily achieved through exhaustive search.","cats":{"ml-security":0}}
{"text":"Finally, alignment scores for different assertions are combined aposteriori to give the final text-to-image alignment score.","cats":{"ml-security":0}}
{"text":"Code and pre-trained weights will be publicly available at https://animatediff.github.io/ .","cats":{"ml-security":0}}
{"text":"While difficult to deploy today for real systems due to latency, context size limitations, and compute costs, the approach of using LLMs to drive low-level control may provide an exciting glimpse into how the patterns among words could be transferred to actions.","cats":{"ml-security":0}}
{"text":"In particular, we focus on the scalar curvature, which can be computed analytically for our manifold, and show connections to several settings that potentially imply generalization.","cats":{"ml-security":0}}
{"text":"To the best of our knowledge, such support does not exist.","cats":{"ml-security":0}}
{"text":"To serve the intricate and varied demands of image editing, precise and flexible manipulation of image content is indispensable.","cats":{"ml-security":0}}
{"text":"In this paper, we provide a novel framework for the analysis of generalization error of first-order optimization algorithms for statistical learning when the gradient can only be accessed through partial observations given by an oracle.","cats":{"ml-security":0}}
{"text":"Then, a specific trustworthiness model and its attributes, namely data robustness, parameter sensitivity, and security covering adversarial examples, are introduced.","cats":{"ml-security":1}}
{"text":"Additionally, motivated by the concepts of user-level and item-level fairness, we broaden the understanding of diversity to encompass not only the item level but also the user level.","cats":{"ml-security":0}}
{"text":"In our experiments, eight machine learning models are employed to provide predictions, and the results achieved by the best-performing model are further processed by the SHAP explainability process.","cats":{"ml-security":0}}
{"text":"This is computationally more efficient because the expense of the one-time graph inference and the $d$-separation queries is negligible compared to the expense of surplus contribution evaluations.","cats":{"ml-security":0}}
{"text":"We use Meta's Ad Library to collect 602,546 ads that have been issued by US Congress members since mid-2018.","cats":{"ml-security":0}}
{"text":"We release the BigTrans model and hope it can advance the research progress.","cats":{"ml-security":0}}
{"text":"When the measurement delay exceeds the permissible number, the packet dropout happens.","cats":{"ml-security":0}}
{"text":"Experiments suggest FogROS2-SGC is 19x faster than rosbridge (a ROS2 package with comparable features, but lacking security).","cats":{"ml-security":0}}
{"text":"The LaSSIM metric does not require clean reference images and has been shown to be superior to SSIM in capturing image structural changes under image degradations, such as strong blurring on different datasets.","cats":{"ml-security":0}}
{"text":"It keeps the long-tailed nature of the collaborative graph by adding power law prior to node embedding initialization; then, it aggregates neighbors directly in multiple hyperbolic spaces through the gyromidpoint method to obtain more accurate computation results; finally, the gate fusion with prior is used to fuse multiple embeddings of one node from different hyperbolic space automatically.","cats":{"ml-security":0}}
{"text":"Empirically we demonstrate that $d$-SAGE enables the efficient and accurate estimation of SAGE values.","cats":{"ml-security":0}}
{"text":"The proposed method inserts pilot sequences in the zero bins of the ZP-OTFS system, resulting in low overhead and PAPR.","cats":{"ml-security":0}}
{"text":"Our work is a step towards understanding how simple geometrically-local error-correction strategies can protect information encoded into complex noisy systems, such as topological quantum error-correcting codes.","cats":{"ml-security":0}}
{"text":"In the case of two multivariate normal classes with a common covariance matrix, they showed that the error rate of the estimated Bayes' rule formed by this SSL approach can actually have lower error rate than the one that could be formed from a completely classified sample.","cats":{"ml-security":0}}
{"text":"However, previous studies seldom take advantage of such brain anatomy prior.","cats":{"ml-security":0}}
{"text":"Moreover, they require hundreds of gigabytes of training data.","cats":{"ml-security":0}}
{"text":"Aside from providing a birds eye view of what exists our in depth analysis provides insights on what is lacking in the current discourse on NLP in particular and critical AI in general, proposes additions to the current framework of analysis, provides recommendations future research direction, and highlights the need to importance of exploring the social in this socio-technical system.","cats":{"ml-security":0}}
{"text":"Self-training is a simple yet effective method within semi-supervised learning.","cats":{"ml-security":0}}
{"text":"For the information leakage caused by the attack during the information transmission process, privacy-preservation is introduced for system states.","cats":{"ml-security":1}}
{"text":"Most existing systems that conceal leakage either (1) incur substantial overheads, (2) focus on specific subsets of leakage patterns, or (3) apply the same security notion across various workloads, thereby impeding the attainment of fine-tuned privacy-efficiency trade-offs.","cats":{"ml-security":1}}
{"text":"Numerous studies have underscored the significant privacy risks associated with various leakage patterns in encrypted data stores.","cats":{"ml-security":1}}
{"text":"In our daily life, mobile phone applications and identity documents that we use may bring the risk of privacy leakage, which had increasingly aroused public concern.","cats":{"ml-security":1}}
{"text":"Overall, our approach to privacy unifies, formalizes, and explains many existing ideas, e.g., why the informed adversary assumption may lead to underestimating the information leaking about each entry in the database.","cats":{"ml-security":0}}
{"text":"Our findings can provide valuable insights into the evolving field of non-standard and covert channels, and help spur new countermeasures against such privacy leakage and security issues.","cats":{"ml-security":0}}
{"text":"Classifiers based on deep neural networks have been recently challenged by Adversarial Attack, where the widely existing vulnerability has invoked the research in defending them from potential threats.","cats":{"ml-security":1}}
{"text":"Adversarial attacks are a serious threat to the reliable deployment of machine learning models in safety-critical applications.","cats":{"ml-security":1}}
{"text":"The vulnerability of deep neural network models to adversarial example attacks is a practical challenge in many artificial intelligence applications.","cats":{"ml-security":1}}
{"text":"Machine-learning architectures, such as Convolutional Neural Networks (CNNs) are vulnerable to adversarial attacks: inputs crafted carefully to force the system output to a wrong label.","cats":{"ml-security":1}}
{"text":"Many research works developed certain techniques to generate adversarial samples to help the machine learning models obtain the ability to recognize those perturbations.","cats":{"ml-security":1}}
{"text":"Deep neural networks are known to be vulnerable to adversarial examples crafted by adding human-imperceptible perturbations to the benign input.","cats":{"ml-security":1}}
{"text":"Given a vulnerable classifier, existing defense methods are mostly white-box and often require re-training the victim under modified loss functions/training regimes.","cats":{"ml-security":1}}
{"text":"Inspired by the observation that linear learners on some datasets are able to resist the best known attacks even without any defenses, we further investigate whether datasets can be inherently robust to indiscriminate poisoning attacks for linear learners.","cats":{"ml-security":1}}
{"text":"With the wide-spread application of machine learning models, it has become critical to study the potential data leakage of models trained on sensitive data.","cats":{"ml-security":1}}
{"text":"The results show that the centralized machine learning model shows more serious member information leakage in all aspects, and the accuracy of the attacker in the central parameter server is significantly higher than the local Inference attacks as participants.","cats":{"ml-security":1}}
{"text":"Therefore, in the current work, we analyze the suitability of these metrics to create Machine Learning based software vulnerability detectors for UMI applications.","cats":{"ml-security":1}}
{"text":"Recently, however, there has been a trend towards evaluating the robustness of these models against adversarial attacks.","cats":{"ml-security":1}}
{"text":"As in-the-wild data are increasingly involved in the training stage, machine learning applications become more susceptible to data poisoning attacks.","cats":{"ml-security":1}}
{"text":"In this paper, we investigate the third type of exploitation of data poisoning - increasing the risks of privacy leakage of benign training samples.","cats":{"ml-security":1}}
{"text":"To this end, we demonstrate a set of data poisoning attacks to amplify the membership exposure of the targeted class.","cats":{"ml-security":1}}
{"text":"Furthermore, adversaries launch poisoning attacks to falsify the health data, which leads to misdiagnosing or even physical damage.","cats":{"ml-security":1}}
{"text":"These findings largely explain the drastic variation in empirical attack performance of the state-of-the-art poisoning attacks on linear learners across benchmark datasets, making an important initial step towards understanding the underlying reasons some learning tasks are vulnerable to data poisoning attacks.","cats":{"ml-security":1}}
{"text":"We then propose an optimization-based clean-label attack in the transfer learning scenario, whereby the poisoning samples are correctly labeled and look \"natural\" to evade human moderation.","cats":{"ml-security":1}}
{"text":"To reveal the real vulnerability of FedRecs, in this paper, we present a new poisoning attack method to manipulate target items' ranks and exposure rates effectively in the top-$K$ recommendation without relying on any prior knowledge.","cats":{"ml-security":1}}
{"text":"Specifically, our attack manipulates target items' exposure rate by a group of synthetic malicious users who upload poisoned gradients considering target items' alternative products.","cats":{"ml-security":1}}
{"text":"We introduce ShortcutGen, a new data poisoning attack that generates sample-dependent, error-minimizing perturbations by learning a generator.","cats":{"ml-security":1}}
{"text":"Our results prove that linear learners can indeed be robust to indiscriminate poisoning if the class-wise data distributions are well-separated with low variance and the size of the constraint set containing all permissible poisoning points is also small.","cats":{"ml-security":1}}
{"text":"We thus formulate such a privacy defense as an adversarial learning problem, where RecUP-FL generates slight perturbations that can be added to the gradients before sharing to fool adversary models.","cats":{"ml-security":1}}
{"text":"We test the performance of the considered attack strategies on an experimental dataset.","cats":{"ml-security":1}}
{"text":"A recent line of work shows that the use of randomization in adversarial training is the key to find optimal strategies against adversarial example attacks.","cats":{"ml-security":1}}
{"text":"We then propose the first learning-based prompt stealing attack, PromptStealer, and demonstrate its superiority over two baseline methods quantitatively and qualitatively.","cats":{"ml-security":1}}
{"text":"Our attack framework is inspired by certified radius, which was originally used by defenders to defend against adversarial perturbations to classification models.","cats":{"ml-security":1}}
{"text":"After the evaluation, we found that transfer learning is a good technique that allows better performance when working with a small data set.","cats":{"ml-security":0}}
{"text":"Moreover, the fairest classifier was found to be accomplished using transfer learning, threshold change, re-weighting and image augmentation as bias mitigation methods.","cats":{"ml-security":0}}
{"text":"It does this by gradually improving image quality in small steps, similar to generative denoising diffusion models.   ","cats":{"ml-security":0}}
{"text":"Inversion by Direct Iteration (InDI) is a new formulation for supervised image restoration that avoids the so-called ``regression to the mean'' effect and produces more realistic and detailed images than existing regression-based methods.","cats":{"ml-security":0}}
{"text":"These results demonstrate that our trained model can successfully reproduce the classification labels derived from detailed SED analysis.","cats":{"ml-security":0}}
{"text":"In the era of AI transitioning from pure analysis to creation, it is worth noting that ChatGPT, with its most recent language model GPT-4, is just a tool out of numerous AIGC tasks.","cats":{"ml-security":0}}
{"text":"Decoupling these two tasks enables training of the \"speaking\" module using abundant audio-only data, and unlocks the highly efficient combination of pretraining and backtranslation to reduce the need for parallel data when training the \"reading\" component.","cats":{"ml-security":0}}
{"text":"Interestingly, we observe that clinical text information annotated by radiologists provides us with discriminative knowledge to identify challenging samples.","cats":{"ml-security":0}}
{"text":"Finally, we include an \"adaptive\" component to the retrieval process, which iteratively refines the re-ranking pool during scoring using the expansion model, i.e. we \"re-rank - expand - repeat\".","cats":{"ml-security":0}}
{"text":"Our code is available at \\url{https://github.com/nkdinsdale/SFHarmony}.","cats":{"ml-security":0}}
{"text":"In online advertising systems, although there are millions to billions of ads, users tend to click only a small set of them and to convert on an even smaller set.","cats":{"ml-security":0}}
{"text":"Membership inference (MI) attacks threaten user privacy through determining if a given data example has been used to train a target model.","cats":{"ml-security":1}}
{"text":"In this paper, we seek to develop a comprehensive benchmark for comparing different MI attacks, called MIBench, which consists not only the evaluation metrics, but also the evaluation scenarios","cats":{"ml-security":1}}
{"text":" However, it has been increasingly recognized that the \"comparing different MI attacks\" methodology used in the existing works has serious limitations.","cats":{"ml-security":0}}
{"text":"Due to these limitations, we found (through the experiments in this work) that some comparison results reported in the literature are quite misleading.","cats":{"ml-security":0}}
{"text":"In this paper, we seek to develop a comprehensive benchmark for comparing different MI attacks, called MIBench, which consists not only thance between data samples of the target dataset, the differential distance between two datasets (i.e., the target dataset and a generated dataset with only nonmembers), and the ratio of the samples that are made no inferences by an MI attack.","cats":{"ml-security":0}}
{"text":"The evaluation metrics consist of ten typical evaluation metrics.","cats":{"ml-security":0}}
{"text":"We have identified three principles for the proposed \"comparing different MI attacks\" methodology, and we have designed and implemented the MIBench benchmark with 84 evaluation scenarios for each dataset.","cats":{"ml-security":0}}
{"text":"In total, we have used our benchmark to fairly and systematically compare 15 state-of-the-art MI attack algorithms across 588 evaluation scenarios, and these evaluation scenarios cover 7 widely used datasets and 7 representative types of models.","cats":{"ml-security":0}}
{"text":"All codes and evaluations of MIBench are publicly available at https://github.com/MIBench/MIBench.github.io/blob/main/README.md.","cats":{"ml-security":0}}
{"text":" The proposed approach is motivated by the intuition that features corresponding to triggers have a higher influence in determining ive metrics.","cats":{"ml-security":0}}
{"text":"Additionally, our detection method can be i","cats":{"ml-security":0}}
{"text":"We study how to mitigate the effects of energy attacks in the batteryless Internet of Things (IoT).","cats":{"ml-security":1}}
{"text":"We design, implement, and evaluate a mitigation system for energy attacks.","cats":{"ml-security":0}}
{"text":" Battery-less IoT devices live and die with ambient energy, as they use energy harvesting to power their operation.","cats":{"ml-security":0}}
{"text":"They are employed in a multitude of applications, including safety-critical ones such as biomedical implants.","cats":{"ml-security":0}}
{"text":"Due to scarce energy intakes and limited energy buffers, their executions become intermittent, alternating periods of active operation with periods of recharging their energy buffers.","cats":{"ml-security":0}}
{"text":"Experimental evidence exists that shows how controlling ambient energy allows an attacker to steer a device execution in unintended ways: energy provisioning effectively becomes an attack vector.","cats":{"ml-security":0}}
{"text":"By taking into account t module, we tune task execution rates and optimize energy management.","cats":{"ml-security":0}}
{"text":"This ensures continued application execution in the event of an energy attack.","cats":{"ml-security":0}}
{"text":"When a device is under attack, our solution ensures the execution of 23.3% additional application cycles compared to the baselines we consider and increases task schedulability by at least 21%, while enabling a 34% higher peripheral availability.","cats":{"ml-security":0}}
{"text":"The architecture can naturally be exploited in privacy-sensitive situations such as surveillance and health, where personally identifiable information cannot be released.","cats":{"ml-security":1}}
{"text":"Predicting where a person is looking is a complex task, requiring to understand not only the person's gaze and scene content, but also the 3D scene structure and the person's situation (are they manipulating?","cats":{"ml-security":0}}
{"text":"interacting or observing others?","cats":{"ml-security":0}}
{"text":"attentive?) to detect obstructions in the line of sight or apply attention priors that humans typically have when observing others.","cats":{"ml-security":0}}
{"text":"In this paper, we hypothesize that identifying and leveraging such priors can be better achieved through the exploitation of explicitly derived multimodal cues such as depth and pose.","cats":{"ml-security":0}}
{"text":"We thus propose a modular multimodal architecture allowing to combine these cues using an attention mechanism.  ","cats":{"ml-security":0}}
{"text":"We perform extensive experiments on the GazeFollow and VideoAttentionTarget public datasets, obtaining state-of-the-art performance and demonstrating very competitive results in the privacy setting case.","cats":{"ml-security":0}}
{"text":"This paper extends and advances our recently introduced two-factor Honeytoken authentication method by incorporating blockchain technology.","cats":{"ml-security":0}}
{"text":"This novel approach strengthens the authentication method to prevent many attacks including tampering attacks.","cats":{"ml-security":0}}
{"text":"Evaluation results show that integrating blockchain into the Honeytoken method could improve performance and operational efficiency.","cats":{"ml-security":0}}
{"text":"Moreover, the Butterfly Effect can amplify inherent biases within data or algorithms, exacerbate feedback loops, and create vulnerabilities for adversarial attacks.","cats":{"ml-security":1}}
{"text":"In this paper, we envision both algorithmic and empirical strategies to detect, quantify, and mitigate the Butterfly Effect in AI systems, emphasizing the importance of addressing these challenges to promote fairness and ensure responsible AI development","cats":{"ml-security":1}}
{"text":"The Butterfly Effect, a concept originating from chaos theory, underscores how small changes can have significant and unpredictable impacts on complex systems.","cats":{"ml-security":0}}
{"text":"In the context of AI fairness and bias, the Butterfly Effect can stem from a variety of sources, such as small biases or skewed data inputs during algorithm development, saddle points in training, or distribution shifts in data between training and testing phases.","cats":{"ml-security":0}}
{"text":"These seemingly minor alterations can lead to unexpected and substantial unfair outcomes, disproportionately affecting underrepresented individuals or groups and perpetuating pre-existing inequalities.  ","cats":{"ml-security":0}}
{"text":"Given the intricate nature of AI systems and their societal implications, it is crucial to thoroughly examine any changes to algorithms or input data for potential unintended consequences.","cats":{"ml-security":0}}
{"text":"In this paper, we envision both algorithmic and empirical strategies to detect, quantify, and mitigate the Butterfly Effect in AI systems, emphasizing the importanc","cats":{"ml-security":0}}
{"text":"Artificial intelligence (AI) is considered an efficient response to several challenges facing 6G technology.","cats":{"ml-security":0}}
{"text":"However, AI still suffers from a huge trust issue due to its ambiguous way of making predictions.","cats":{"ml-security":0}}
{"text":"Therefore, there is a need for a method to evaluate the AI's trustworthiness in practice for future 6G applications.","cats":{"ml-security":0}}
{"text":"This paper presents a practical model to analyze the trustworthiness of AI in a dedicated 6G application.","cats":{"ml-security":0}}
{"text":"In particular, we present two customized Deep Neural Networks (DNNs) to solve the Automatic Modulation Recognition (AMR) problem in Terahertz communications-based 6G technology.  ","cats":{"ml-security":0}}
{"text":"The evaluation results indicate that the proposed trustworthiness attributes are crucial to evaluate the trustworthiness of DNN for this 6G application.","cats":{"ml-security":0}}
{"text":"Automatic metrics play a crucial role in machine translation.","cats":{"ml-security":0}}
{"text":"Despite the widespread use of n-gram-based metrics, there has been a recent surge in the development of pre-trained model-based metrics that focus on measuring sentence semantics.","cats":{"ml-security":0}}
{"text":"However, these neural metrics, while achieving higher correlations with human evaluations, are often considered to be black boxes with potential biases that are difficult to detect.","cats":{"ml-security":0}}
{"text":"In this study, we systematically analyze and compare various mainstream and cutting-edge automatic metrics from the perspective of their guidance for training machine translation systems.","cats":{"ml-security":0}}
{"text":"Through Minimum Risk Training (MRT), we find that certain metrics exhibit robustness defects, such as the presence of universal adversarial translations in BLEURT and BARTScore.","cats":{"ml-security":0}}
{"text":"In-depth analysis suggests two main causes of these robustness deficits: distribution biases in the training datasets, and the tendency of the metric paradigm.","cats":{"ml-security":0}}
{"text":"By incorporating token-level constraints, we enhance the robustness of evaluation metrics, which in turn leads to an improvement in the performance of machine translation systems.","cats":{"ml-security":0}}
{"text":"Codes are available at \\url{https://github.com/powerpuffpomelo/fairseq_mrt}.","cats":{"ml-security":0}}
{"text":"However, concerns have arisen regarding the unauthorized usage of data during the training process.","cats":{"ml-security":1}}
{"text":"In this paper, we propose a method for detecting such unauthorized data usage by planting injected memorization into the text-to-image diffusion models trained on the protected dataset.","cats":{"ml-security":1}}
{"text":"By analyzing whether the model has memorization for the injected content (i.e., whether the generated images are processed by the chosen post-processing function), we can detect models that had illegally utilized the unauthorized data.","cats":{"ml-security":0}}
{"text":"Recent text-to-image diffusion models have shown surprising performance in generating high-quality images.  ","cats":{"ml-security":0}}
{"text":"One example is when a model trainer collects a set of images created by a particular artist and attempts to train a model capable of generating similar images without obtaining permission from the artist.","cats":{"ml-security":0}}
{"text":"To address this issue, it becomes crucial to detect unauthorized data usage.","cats":{"ml-security":0}}
{"text":"In this paper, we propose a method for detecting such unauthorized data usage by planting injected s stealthy image wrapping functions that are imperceptible to human vision but can be captured and memorized by diffusion models.","cats":{"ml-security":0}}
{"text":"Our experiments conducted on Stable Diffusion an","cats":{"ml-security":0}}
{"text":"We then show that such probabilistic descriptions can be used to construct defences against adversarial attacks.","cats":{"ml-security":1}}
{"text":"This paper begins with a description of methods for estimating probability density functions for images that reflects the observation that such data is usually constrained to lie in restricted regions of the high-dimensional image space - not every pattern of pixels is an image.","cats":{"ml-security":0}}
{"text":"It is common to say that images lie on a lower-dimensional manifold in the high-dimensional space.","cats":{"ml-security":0}}
{"text":"However, although images may lie on such lower-dimensional manifolds, it is not the case that all points on the manifold have an equal probability of being images.","cats":{"ml-security":0}}
{"text":"Images are unevenly distributed on the manifold, and our task is to devise ways to model this distribution as a probability distribution.","cats":{"ml-security":0}}
{"text":"In pursuing this goal, we consider generative models that are popular in AI and computer vision community.","cats":{"ml-security":0}}
{"text":"For our purposes, generative/probabilistic models should have the properties of 1) sample generation: it should be possible to sample from this distribution according to the modelled density function, and 2) probability computation: given a previously unseen sample from the dataset of interest, one should be able to compute the probability of the sample, at least up to a normalising constant.","cats":{"ml-security":0}}
{"text":"To this end, we investigate the use of methods such as normalising flow and diffusion models.  ","cats":{"ml-security":0}}
{"text":"In addition to describing the manifold in terms of density, we also consider how semantic interpretations can be used to describe points on the manifold.","cats":{"ml-security":0}}
{"text":"To this end, we consider an emergent language framework which makes use of variational encoders to produce a disentangled representation of points that reside on a given manifold.","cats":{"ml-security":0}}
{"text":"Trajectories between points on a manifold can then be described in terms of evolving semantic descriptions.","cats":{"ml-security":0}}
{"text":"Face presentation attacks, also known as spoofing attacks, pose a significant threat to biometric systems that rely on facial recognition systems, such as access control systems, mobile payments, and identity verification systems.","cats":{"ml-security":1}}
{"text":"In this paper, we reformulate the face anti-spoofing task as a motion prediction problem and introduce a deep ensemble learning model with a frame skipping mechanism","cats":{"ml-security":1}}
{"text":" To prevent spoofing, several video-based methods have been presented in the literature that analyze facial motion in successive video frames.","cats":{"ml-security":0}}
{"text":"However, estimating the motion between adjacent frames is a challenging task and requires high computational cost.","cats":{"ml-security":0}}
{"text":"In this paper, we reformulate the face anti-spoofing task as a motion prediction problem and introduce a deep ensemble learning model with a frame skipping mechanism.","cats":{"ml-security":0}}
{"text":"The proposed frame skipping is based on a uniform sampling apprasily be perceived during the training of three different recurrent neural networks (RNNs).","cats":{"ml-security":0}}
{"text":"Extensive experiments were conducted on four datasets, and state-of-the-art performance is reported for MSU-MFSD (3.12\\%), Replay-Attack (11.19\\%), and OULU-NPU (12.23\\%) using half total error rate (HTER) in the most challenging cross-dataset test scenario.","cats":{"ml-security":0}}
{"text":"Generative AI has made significant strides, yet concerns about the accuracy and reliability of its outputs continue to grow.","cats":{"ml-security":0}}
{"text":"Such inaccuracies can have serious consequences such as inaccurate decision-making, the spread of false information, privacy violations, legal liabilities, and more.","cats":{"ml-security":0}}
{"text":"Although efforts to address these risks are underway, including explainable AI and responsible AI practices such as transparency, privacy protection, bias mitigation, and social and environmental responsibility, misinformation caused by generative AI will remain a significant challenge.","cats":{"ml-security":0}}
{"text":"We propose that verifying the outputs of generative AI from a data management perspective is an emerging issue for generative AI.","cats":{"ml-security":0}}
{"text":"This involves analyzing the underlying data from multi-modal data lakes, including text files, tables, and knowledge graphs, and assessing its quality and consistency.","cats":{"ml-security":0}}
{"text":"By doing so, we can establish a stronger foundation for evaluating the outputs of generative AI models.","cats":{"ml-security":0}}
{"text":"Such an approach can ensure the correctness of generative AI, promote transparency, and enable decision-making with greater confidence.","cats":{"ml-security":0}}
{"text":"Our vision is to promote the development of verifiable generative AI and contribute to a more trustworthy and responsible use of AI.","cats":{"ml-security":0}}
{"text":"The introduction of vulnerability detection enables reducing the number of false alerts to focus the limited testing efforts on potentially vulnerable files.","cats":{"ml-security":1}}
{"text":"In Software Development Life Cycle (SDLC), security vulnerabilities are one of the points introduced during the construction stage.","cats":{"ml-security":0}}
{"text":"Failure to detect software defects earlier after releasing the product to the market causes higher repair costs for the company.","cats":{"ml-security":0}}
{"text":"So, it decreases the company's reputation, violates user privacy, and causes an unrepairable issue for the application.  ","cats":{"ml-security":0}}
{"text":"UMKM Masa Kini (UMI) is a Point of Sales application to sell any Micro, Small, and Medium Enterprises Product (UMKM).","cats":{"ml-security":0}}
{"text":"Therefore, in the current work, we analyze the suitability of these metrics to create Machine Learning based software vulnerability detectors for UMI applica","cats":{"ml-security":0}}
{"text":"Background: Despite the widespread use of automated security defect detection tools, software projects still contain many security defects that could result in serious damage.","cats":{"ml-security":1}}
{"text":"Such tools are largely context-insensitive and may not cover all possible scenarios in testing potential issues, which makes them susceptible to missing complex security defects.","cats":{"ml-security":0}}
{"text":"Hence, thorough detection entails a synergistic cooperation between these tools and human-intensive detection techniques, including code review.","cats":{"ml-security":0}}
{"text":"Code review is widely recognized as a crucial and effective practice for identifying security defects.","cats":{"ml-security":0}}
{"text":"Aim: This work aims to empirically investigate security defect detection through code review.","cats":{"ml-security":0}}
{"text":"Method: To this end, we conducted an empirical study by analyzing code review comments derived from four projects in the OpenStack and Qt communities.","cats":{"ml-security":0}}
{"text":"Through manually checking 20,995 review comments obtained by keyword-based search, we identified 614 comments as security-related.","cats":{"ml-security":0}}
{"text":"Results:","cats":{"ml-security":0}}
{"text":"Conclusions: Our research results demonstrate that (1) software security practices should combine manual code review with automated detection tools, achieving a more comprehensive coverage to identifying and addressing security defects, and (2) promoting appropriate standardization of practitioners' behaviors during code review remains necessary for enhancing software security.","cats":{"ml-security":0}}
{"text":"This leaves a back door for malicious attackers to collapse VAEs from the latent space, especially in scenarios where the encoder and decoder are used separately, such as communication and compressed sensing","cats":{"ml-security":1}}
{"text":"Specifically, we empirically demonstrate the latent vulnerability of popular generative autoencoders through attacks in the latent space.   ","cats":{"ml-security":0}}
{"text":"The generative autoencoders, such as the variational autoencoders or the adversarial autoencoders, have achieved great success in lots of real-world applications, including image generation, and signal communication.   ","cats":{"ml-security":0}}
{"text":"However, little concern has been devoted to their robustness during practical deployment.   ","cats":{"ml-security":0}}
{"text":"Due to the probabilistic latent structure, variational autoencoders (VAEs) may confront problems such as a mismatch between the posterior distribution of the latent and real data manifold, or discontinuity in the posterior distribution of the latent.   .   ","cats":{"ml-security":0}}
{"text":"In this work, we provide the first study on the adversarial robustness of generative autoencoders in the latent space.   ","cats":{"ml-security":0}}
{"text":"We also evaluate the difference between variational autoencoders anoff between the adversarial robustness and the degree of the disentanglement of the latent codes.   ","cats":{"ml-security":0}}
{"text":"Additionally, we also verify the feasibility of improvement for the latent robustness of VAEs through adversarial training.   ","cats":{"ml-security":0}}
{"text":"In summary, we suggest concerning the adversarial latent robustness of the generative autoencoders, analyze several robustness-relative issues, and give some insights into a series of key challenges.","cats":{"ml-security":0}}
{"text":"A major security threat to an integrated circuit (IC) design is the Hardware Trojan attack which is a malicious modification of the design.","cats":{"ml-security":0}}
{"text":"Previously several papers have investigated into side-channel analysis to detect the presence of Hardware Trojans.","cats":{"ml-security":0}}
{"text":"The side channel analysis were prescribed in these papers as an alternative to the conventional logic testing for detecting malicious modification in the design.","cats":{"ml-security":0}}
{"text":"It has been found that these conventional logic testing are ineffective when it comes to detecting small Trojans due to decrease in the sensitivity due to process variations encountered in the manufacturing techniques.","cats":{"ml-security":0}}
{"text":"The main paper under consideration in this survey report focuses on proposing a new technique to detect Trojans by using multiple-parameter side-channel analysis.","cats":{"ml-security":0}}
{"text":"The novel idea will be explained thoroughly in this survey report.","cats":{"ml-security":0}}
{"text":"We also look into several other papers, which talk about single parameter analysis and how they are implemented.","cats":{"ml-security":0}}
{"text":"We analyzed the short comings of those single parameter analysis techniques and we then show how this multi-parameter analysis technique is better.","cats":{"ml-security":0}}
{"text":"Finally we will talk about the combined side-channel analysis and logic testing approach in which there is higher detection coverage for hardware Trojan circuits of different types and sizes.","cats":{"ml-security":0}}
{"text":"Deploying deep visual models can lead to performance drops due to the discrepancies between source and target distributions.","cats":{"ml-security":0}}
{"text":"Several approaches leverage labeled source data to estimate target domain accuracy, but accessing labeled source data is often prohibitively difficult due to data confidentiality or resource limitations on serving devices.","cats":{"ml-security":0}}
{"text":"Our work proposes a new framework to estimate model accuracy on unlabeled target data without access to source data.","cats":{"ml-security":0}}
{"text":"We investigate the feasibility of using pseudo-labels for accuracy estimation and evolve this idea into adopting recent advances in source-free domain adaptation algorithms.","cats":{"ml-security":0}}
{"text":"Our approach measures the disagreement rate between the source hypothesis and the target pseudo-labeling function, adapted from the source hypothesis.","cats":{"ml-security":0}}
{"text":"We mitigate the impact of erroneous pseudo-labels that may arise due to a high ideal joint hypothesis risk by employing adaptive adversarial perturbation on the input of the target model.","cats":{"ml-security":0}}
{"text":"Our proposed source-free framework effectively addresses the challenging distribution shift scenarios and outperforms existing methods requiring source data and labels for training.","cats":{"ml-security":0}}
{"text":"The paradigm of federated learning (FL) to address data privacy concerns by locally training parameters on resource-constrained clients in a distributed manner has garnered significant attention.","cats":{"ml-security":0}}
{"text":"Nonetheless, FL is not applicable when not all clients within the coverage of the FL server are registered with the FL network.","cats":{"ml-security":0}}
{"text":"To bridge this gap, this paper proposes joint learner referral aided federated client selection (LRef-FedCS), along with communications and computing resource scheduling, and local model accuracy optimization (LMAO) methods.","cats":{"ml-security":0}}
{"text":"These methods are designed to minimize the cost incurred by the worst-case participant and ensure the long-term fairness of FL in hierarchical Internet of Things (HieIoT) networks.","cats":{"ml-security":0}}
{"text":"Utilizing the Lyapunov optimization technique, we reformulate the original problem into a stepwise joint optimization problem (JOP).","cats":{"ml-security":0}}
{"text":"Subsequently, to tackle the mixed-integer non-convex JOP, we separatively and iteratively address LRef-FedCS and LMAO through the centralized method and self-adaptive global best harmony search (SGHS) algorithm, respectively.","cats":{"ml-security":0}}
{"text":"To enhance scalability, we further propose a distributed LRef-FedCS approach based on a matching game to replace the centralized method described above.","cats":{"ml-security":0}}
{"text":"Numerical simulations and experimental results on the MNIST/CIFAR-10 datasets demonstrate that our proposed LRef-FedCS approach could achieve a good balance between pursuing high global accuracy and reducing cost.","cats":{"ml-security":0}}
{"text":"Deep Neural Networks (DNNs) are susceptible to adversarial attacks.","cats":{"ml-security":1}}
{"text":"However, recent research has revealed that neural Ordinary Differential Equations (ODEs) exhibit robustness in specific applications","cats":{"ml-security":1}}
{"text":"Complex networks are used to model many real-world systems.","cats":{"ml-security":0}}
{"text":"However, the dimensionality of these systems can make them challenging to analyze.","cats":{"ml-security":0}}
{"text":"Dimensionality reduction techniques like POD can be used in such cases.","cats":{"ml-security":0}}
{"text":"However, these models are susceptible to perturbations in the input data.","cats":{"ml-security":0}}
{"text":"We propose an algorithmic framework that combines techniques from pattern recognition (PR) and stochastic filtering theory to enhance the output of such models.","cats":{"ml-security":0}}
{"text":"The results of our study show that our method can improve the accuracy of the surrogate model under perturbed inputs.  ","cats":{"ml-security":0}}
{"text":"However, recent research has revealed that neural Ordinary Differenproach as a reference.","cats":{"ml-security":0}}
{"text":"As a result, we have manually collected adversarial safety prompts across 10 scenarios and induced responsibility prompts from 8 domains by professional experts.","cats":{"ml-security":1}}
{"text":"With the rapid evolution of large language models (LLMs), there is a growing concern that they may pose risks or have negative social impacts.","cats":{"ml-security":0}}
{"text":"Therefore, evaluation of human values alignment is becoming increasingly important.","cats":{"ml-security":0}}
{"text":"Previous work mainly focuses on assessing the performance of LLMs on certain knowledge and reasoning abilities, while neglecting the alignment to human values, especially in a Chinese context.","cats":{"ml-security":0}}
{"text":"In this paper, we present CValues, the first Chinese human values evaluation benchmark to measure the alignment ability of LLMs in terms of both safety and responsibility criteria.  ","cats":{"ml-security":0}}
{"text":"To provide a comprehensive values evaluation of Chinese LLMs, we not only conduct human evaluation for reliable comparison, but also construct multi-choice prompts for automatic evaluation.","cats":{"ml-security":0}}
{"text":"Our findings suggest that while most Chinese LLMs perform well in terms of safety, there is considerable room for improvement in terms of responsibility.","cats":{"ml-security":0}}
{"text":"Moreover, both the automatic and human evaluation are important for assessing the human values alignment in different aspects.","cats":{"ml-security":0}}
{"text":"The benchmark and code is available on ModelScope and Github.","cats":{"ml-security":0}}
{"text":"Information security is a crucial need in the modern world.","cats":{"ml-security":0}}
{"text":"Data security is a real concern, and many customers and organizations need to protect their sensitive information from unauthorized parties and attackers.","cats":{"ml-security":0}}
{"text":"In previous years, numerous cryptographic schemes have been proposed.","cats":{"ml-security":0}}
{"text":"DNA cryptography is a new and developing field that combines the computational and biological worlds.","cats":{"ml-security":0}}
{"text":"DNA cryptography is intriguing due to its high storage capacity, secure data transport, and massive parallel computing.","cats":{"ml-security":0}}
{"text":"In this paper, a new combination is proposed that offers good security by combining DNA, the Rabin algorithm, one time pad, and a structure inspired by Fiestel.","cats":{"ml-security":0}}
{"text":"This algorithm employs two keys.","cats":{"ml-security":0}}
{"text":"The first key is a DNA OTP key which is used for only one secure communication session.","cats":{"ml-security":0}}
{"text":"The second key, which combines the public and private keys, is a Rabin key.","cats":{"ml-security":0}}
{"text":"Additionally, by using a Feistel inspired scheme and randomness provided by DNA, the ciphertext is made harder to obtain without the private key.","cats":{"ml-security":0}}
{"text":"We also analyze the exploitability and security consequences of developer secret leakage in mini-apps by examining individual super-app server-side APIs.","cats":{"ml-security":1}}
{"text":"We develop an analysis framework for measuring such secret leakage, and primarily analyze 110,993 WeChat mini-apps, and 10,000 Baidu mini-apps (two of the most prominent super-app platforms), along with a few more datasets to test the evolution of developer practices and platform security enforcement over time.","cats":{"ml-security":1}}
{"text":"We conduct a large-scale measurement of developers' insecure practices leading to mini-app to super-app authentication bypass, among which hard-coding developer secrets for such authentication is a major contributor.  ","cats":{"ml-security":0}}
{"text":"We develop an analysis framework for measuring such secret leakage, and primarily analyze 110,993 WeChat mini-apps, and 10,000 Baidu mini-apps (two of thy and privacy problems for the users and developers of mini-apps.","cats":{"ml-security":0}}
{"text":"A network attacker who does not even have an account on the super-app platform, can effectively take down a mini-app, send malicious and phishing links to users, and access sensitive information of the mini-app developer and its users.","cats":{"ml-security":0}}
{"text":"We responsibly disclosed our findings and also put forward potential directions that could be considered to alleviate/eliminate the root causes of developers hard-coding the app secrets in the mini-app's front-end code.","cats":{"ml-security":0}}
{"text":"It is universally acknowledged that Wi-Fi communications are important to secure.","cats":{"ml-security":0}}
{"text":"Thus, the Wi-Fi Alliance published WPA3 in 2018 with a distinctive security feature: it leverages a Password-Authenticated Key Exchange (PAKE) protocol to protect users' passwords from offline dictionary attacks.","cats":{"ml-security":0}}
{"text":"Unfortunately, soon after its release, several attacks were reported against its implementations, in response to which the protocol was updated in a best-effort manner.   ","cats":{"ml-security":0}}
{"text":"In this paper, we show that the proposed mitigations are not enough, especially for a complex protocol to implement even for savvy developers.","cats":{"ml-security":0}}
{"text":"Indeed, we present **Dragondoom**, a collection of side-channel vulnerabilities of varying strength allowing attackers to recover users' passwords in widely deployed Wi-Fi daemons, such as hostap in its default settings.","cats":{"ml-security":0}}
{"text":"Our findings target both password conversion methods, namely the default probabilistic hunting-and-pecking and its newly standardized deterministic alternative based on SSWU.","cats":{"ml-security":0}}
{"text":"We successfully exploit our leakage in practice through microarchitectural mechanisms, and overcome the limited spatial resolution of Flush+Reload.","cats":{"ml-security":0}}
{"text":"Our attacks outperform previous works in terms of required measurements.   ","cats":{"ml-security":0}}
{"text":"Then, driven by the need to end the spiral of patch-and-hack in Dragonfly implementations, we propose **Dragonstar**, an implementation of Dragonfly leveraging a formally verified implementation of the underlying mathematical operations, thereby removing all the related leakage vector.","cats":{"ml-security":0}}
{"text":"Our implementation relies on HACL*, a formally verified crypto library guaranteeing secret-independence.","cats":{"ml-security":0}}
{"text":"We design Dragonstar, so that its integration within hostap requires minimal modifications to the existing project.","cats":{"ml-security":0}}
{"text":"Our experiments show that the performance of HACL*-based hostap is comparable to OpenSSL-based, implying that Dragonstar is both efficient and proved to be leakage-free.","cats":{"ml-security":0}}
