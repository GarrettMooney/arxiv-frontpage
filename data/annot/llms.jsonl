{"text":"Moreover, we find that LLMs might not be a fair judge if different LLMs are used for agents.","cats":{"llms":1}}
{"text":"To enable further research on PEFT methods of LLMs, this paper presents LLM-Adapters, an easy-to-use framework that integrates various adapters into LLMs and can execute these adapter-based PEFT methods of LLMs for different tasks.","cats":{"llms":1}}
{"text":"Our methods successfully identify families of LLMs and accurately cluster LLMs into meaningful subgroups.","cats":{"llms":1}}
{"text":"This paper introduces a novel human-LLM interaction framework, Low-code LLM.","cats":{"llms":1}}
{"text":"Consequently, evaluating the safety of LLMs has become an essential task for facilitating the broad applications of LLMs.","cats":{"llms":1}}
{"text":"Vision-LLMs instead post-hoc condition LLMs to `understand' the output of an image encoder.","cats":{"llms":1}}
{"text":"However, do LLMs explain themselves?","cats":{"llms":1}}
{"text":"We analyze how the proportion of LLM papers is increasing; the LLM-related topics receiving the most attention; the authors writing LLM papers; how authors' research topics correlate with their backgrounds; the factors distinguishing highly cited LLM papers; and the patterns of international collaboration.","cats":{"llms":1}}
{"text":"Previous methods, which primarily rely on model logits, have become less suitable for LLMs and even infeasible with the rise of closed-source LLMs (e.g., commercialized LLM APIs).","cats":{"llms":1}}
{"text":"Instead of using public APIs of LLMs, we instruction tune an open-source LLM (3B Flan-T5-XL), in order to better adapt LLMs to recommender systems.","cats":{"llms":1}}
{"text":"We believe SafetyBench will enable fast and comprehensive evaluation of LLMs' safety, and foster the development of safer LLMs.","cats":{"llms":1}}
{"text":"With the emergence of large language models (LLMs), researchers have explored LLMs' potential as alternatives for human evaluation.","cats":{"llms":1}}
{"text":"Yet, the two LLMs are closed source, and little is known about the LLMs' performance in real-world use cases.","cats":{"llms":1}}
{"text":"This workshop will demonstrate the capabilities of LLMs to help attendees evaluate whether and how LLMs might be integrated into their pedagogy and research.","cats":{"llms":1}}
{"text":"We reflect on human and LLMs' different sensitivities to instructions, stress the importance of enabling human-facing safeguards for LLMs, and discuss the potential of training humans and LLMs with complementary skill sets.","cats":{"llms":1}}
{"text":"Based on our analysis of these LLMs, we claim that the average and peak power utilization in LLM clusters for inference should not be very high.","cats":{"llms":1}}
{"text":"While uses of LLMs for CODL are valuable standalone, they are particularly valuable as part of LLM applications such as AI chatbots.","cats":{"llms":1}}
{"text":"This work highlights the challenges and opportunities of discourse modeling for LLMs, which we hope can inspire the future design and evaluation of LLMs.","cats":{"llms":1}}
{"text":"This paper proposes a LLM-Augmenter system, which augments a black-box LLM with a set of plug-and-play modules.","cats":{"llms":1}}
{"text":"However, there is no comprehensive index of LLMs available.","cats":{"llms":1}}
{"text":"However, it is not yet known the performance of LLMs on CLS.","cats":{"llms":1}}
{"text":"We present ImageBind-LLM, a multi-modality instruction tuning method of large language models (LLMs) via ImageBind.","cats":{"llms":1}}
{"text":"To bridge this gap, we propose LLMRec, a LLM-based recommender system designed for benchmarking LLMs on various recommendation tasks.","cats":{"llms":1}}
{"text":"Given the huge influx of LLMs, it is of interest to know which LLM backbones, settings, training methods, and families are popular or trending.","cats":{"llms":1}}
{"text":"Our aim is to offer invaluable insights to researchers in the realm of LLMs evaluation, thereby aiding the development of more proficient LLMs.","cats":{"llms":1}}
{"text":"KGLLM provides a solution to enhance LLMs' factual reasoning ability, opening up new avenues for LLM research.","cats":{"llms":1}}
{"text":"Among the regression models, Regularized Linear Regression was the most accurate for estimating MP a, and Polynomial Regression was the most accurate for estimating MP b.","cats":{"llms":0}}
{"text":"We analyze the bit complexity of efficient algorithms for fundamental optimization problems, such as linear regression, $p$-norm regression, and linear programming (LP).","cats":{"llms":0}}
{"text":"We study the problem of regression in a generalized linear model (GLM) with multiple signals and latent variables.","cats":{"llms":0}}
{"text":"In this study we introduce a new online linear regression approach.","cats":{"llms":0}}
{"text":"The linear regression model is trained using SGD, tracking weights and loss separately and zipping them finally.","cats":{"llms":0}}
{"text":"The theoretical results are validated by numerical simulations for mixed linear regression, max-affine regression, and mixture-of-experts.","cats":{"llms":0}}
{"text":"We input those features into linear regression models to infer 5 ECAS sub-scores and the total score.","cats":{"llms":0}}
{"text":"This research enhances linear regression models by integrating a Kalman filter and analysing curve areas to minimize loss.","cats":{"llms":0}}
