{"text":"Advances in neural fields are enabling high-fidelity capture of the shape and appearance of static and dynamic scenes.","meta":{"url":"http://arxiv.org/abs/2307.16897v1"},"cats":{"new-dataset":0.111376743,"dev-research":0.2295474862,"prompt-eng":0.4198452976,"data-quality":0.16660672,"ml-security":0.1204653543}}
{"text":"However, their capabilities lag behind those offered by representations such as pixels or meshes due to algorithmic challenges and the lack of large-scale real-world datasets.","meta":{"url":"http://arxiv.org/abs/2307.16897v1"},"cats":{"new-dataset":0.1630721748,"dev-research":0.2724952021,"prompt-eng":0.3131114765,"data-quality":0.1044679257,"ml-security":0.1567821992}}
{"text":"We address the dataset limitation with DiVA-360, a real-world 360 dynamic visual-audio dataset with synchronized multimodal visual, audio, and textual information about table-scale scenes.","meta":{"url":"http://arxiv.org/abs/2307.16897v1"},"cats":{"new-dataset":0.8105870195,"dev-research":0.20559337,"prompt-eng":0.3514750721,"data-quality":0.1749802257,"ml-security":0.059993451}}
{"text":"It contains 46 dynamic scenes, 30 static scenes, and 95 static objects spanning 11 categories captured using a new hardware system using 53 RGB cameras at 120 FPS and 6 microphones for a total of 8.6M image frames and 1360 s of dynamic data.","meta":{"url":"http://arxiv.org/abs/2307.16897v1"},"cats":{"new-dataset":0.8341358516,"dev-research":0.2428961588,"prompt-eng":0.3674129171,"data-quality":0.1486062472,"ml-security":0.0789379633}}
{"text":"We provide detailed text descriptions for all scenes, foreground-background segmentation masks, category-specific 3D pose alignment for static objects, as well as metrics for comparison.","meta":{"url":"http://arxiv.org/abs/2307.16897v1"},"cats":{"new-dataset":0.6185266501,"dev-research":0.2555254205,"prompt-eng":0.3873331515,"data-quality":0.2364476448,"ml-security":0.0815815156}}
{"text":"Our data, hardware and software, and code are available at https://diva360.github.io/.","meta":{"url":"http://arxiv.org/abs/2307.16897v1"},"cats":{"new-dataset":0.7846887917,"dev-research":0.3264215229,"prompt-eng":0.4235947376,"data-quality":0.141961667,"ml-security":0.0747266772}}
{"text":"Harnessing the power of pre-training on large-scale datasets like ImageNet forms a fundamental building block for the progress of representation learning-driven solutions in computer vision.","meta":{"url":"http://arxiv.org/abs/2307.16896v1"},"cats":{"new-dataset":0.2414456659,"dev-research":0.2588725939,"prompt-eng":0.368391654,"data-quality":0.1710977338,"ml-security":0.1912634458}}
{"text":"Medical images are inherently different from natural images as they are acquired in the form of many modalities (CT, MR, PET, Ultrasound etc.) and contain granulated information like tissue, lesion, organs etc.","meta":{"url":"http://arxiv.org/abs/2307.16896v1"},"cats":{"new-dataset":0.0464978928,"dev-research":0.2058765967,"prompt-eng":0.2948708922,"data-quality":0.1574309274,"ml-security":0.1204409062}}
{"text":"These characteristics of medical images require special attention towards learning features representative of local context.","meta":{"url":"http://arxiv.org/abs/2307.16896v1"},"cats":{"new-dataset":0.0664087505,"dev-research":0.2435795509,"prompt-eng":0.3747779616,"data-quality":0.1888050246,"ml-security":0.1258086422}}
{"text":"In this work, we focus on designing an effective pre-training framework for 3D radiology images.","meta":{"url":"http://arxiv.org/abs/2307.16896v1"},"cats":{"new-dataset":0.172149136,"dev-research":0.2397203351,"prompt-eng":0.4270281561,"data-quality":0.1141619507,"ml-security":0.0709477299}}
{"text":"First, we propose a new masking strategy called local masking where the masking is performed across channel embeddings instead of tokens to improve the learning of local feature representations.","meta":{"url":"http://arxiv.org/abs/2307.16896v1"},"cats":{"new-dataset":0.0628165114,"dev-research":0.3091459461,"prompt-eng":0.4135023586,"data-quality":0.3600264135,"ml-security":0.2652619618}}
{"text":"We combine this with classical low-level perturbations like adding noise and downsampling to further enable low-level representation learning.","meta":{"url":"http://arxiv.org/abs/2307.16896v1"},"cats":{"new-dataset":0.073403901,"dev-research":0.1863480734,"prompt-eng":0.4171832286,"data-quality":0.3244838691,"ml-security":0.1771856679}}
{"text":"To this end, we introduce Disruptive Autoencoders, a pre-training framework that attempts to reconstruct the original image from disruptions created by a combination of local masking and low-level perturbations.","meta":{"url":"http://arxiv.org/abs/2307.16896v1"},"cats":{"new-dataset":0.2181000012,"dev-research":0.239103674,"prompt-eng":0.4494487337,"data-quality":0.3067436852,"ml-security":0.3449595906}}
{"text":"Additionally, we also devise a cross-modal contrastive loss (CMCL) to accommodate the pre-training of multiple modalities in a single framework.","meta":{"url":"http://arxiv.org/abs/2307.16896v1"},"cats":{"new-dataset":0.072762139,"dev-research":0.2152663783,"prompt-eng":0.4547327192,"data-quality":0.2420566973,"ml-security":0.1082335156}}
{"text":"We curate a large-scale dataset to enable pre-training of 3D medical radiology images (MRI and CT).","meta":{"url":"http://arxiv.org/abs/2307.16896v1"},"cats":{"new-dataset":0.6576735106,"dev-research":0.2298717205,"prompt-eng":0.3585161544,"data-quality":0.1085837696,"ml-security":0.0921799427}}
{"text":"The proposed pre-training framework is tested across multiple downstream tasks and achieves state-of-the-art performance.","meta":{"url":"http://arxiv.org/abs/2307.16896v1"},"cats":{"new-dataset":0.0987622427,"dev-research":0.2575347277,"prompt-eng":0.4568876632,"data-quality":0.1522480998,"ml-security":0.087677761}}
{"text":"Notably, our proposed method tops the public test leaderboard of BTCV multi-organ segmentation challenge.","meta":{"url":"http://arxiv.org/abs/2307.16896v1"},"cats":{"new-dataset":0.2136324615,"dev-research":0.1826749856,"prompt-eng":0.4661771833,"data-quality":0.2028002922,"ml-security":0.1168362768}}
{"text":"We study the problem of uncertainty quantification for time series prediction, with the goal of providing easy-to-use algorithms with formal guarantees.","meta":{"url":"http://arxiv.org/abs/2307.16895v1"},"cats":{"new-dataset":0.0559985804,"dev-research":0.269640553,"prompt-eng":0.3959864506,"data-quality":0.1829100417,"ml-security":0.2215194709}}
{"text":"The algorithms we present build upon ideas from conformal prediction and control theory, are able to prospectively model conformal scores in an online setting, and adapt to the presence of systematic errors due to seasonality, trends, and general distribution shifts.","meta":{"url":"http://arxiv.org/abs/2307.16895v1"},"cats":{"new-dataset":0.0452457149,"dev-research":0.1577098842,"prompt-eng":0.4270538808,"data-quality":0.1354111271,"ml-security":0.1294817273}}
{"text":"Our theory both simplifies and strengthens existing analyses in online conformal prediction.","meta":{"url":"http://arxiv.org/abs/2307.16895v1"},"cats":{"new-dataset":0.0164664884,"dev-research":0.1572135331,"prompt-eng":0.3844544609,"data-quality":0.1273338484,"ml-security":0.1436240744}}
{"text":"Experiments on 4-week-ahead forecasting of statewide COVID-19 death counts in the U.S. show an improvement in coverage over the ensemble forecaster used in official CDC communications.","meta":{"url":"http://arxiv.org/abs/2307.16895v1"},"cats":{"new-dataset":0.2346057314,"dev-research":0.2352305365,"prompt-eng":0.3870898476,"data-quality":0.126773268,"ml-security":0.1564392422}}
{"text":"We also run experiments on predicting electricity demand, market returns, and temperature using autoregressive, Theta, Prophet, and Transformer models.","meta":{"url":"http://arxiv.org/abs/2307.16895v1"},"cats":{"new-dataset":0.0932222206,"dev-research":0.1825307641,"prompt-eng":0.4634582777,"data-quality":0.0954248235,"ml-security":0.1552174469}}
{"text":"We provide an extendable codebase for testing our methods and for the integration of new algorithms, data sets, and forecasting rules.","meta":{"url":"http://arxiv.org/abs/2307.16895v1"},"cats":{"new-dataset":0.3450055098,"dev-research":0.3417841191,"prompt-eng":0.4276762421,"data-quality":0.1594391894,"ml-security":0.1296948015}}
{"text":"In recent years, there has been a growing interest in understanding complex microstructures and their effect on macroscopic properties.","meta":{"url":"http://arxiv.org/abs/2307.16894v1"},"cats":{"new-dataset":0.0569469859,"dev-research":0.2215857182,"prompt-eng":0.3020133489,"data-quality":0.0695011066,"ml-security":0.0701828225}}
{"text":"In general, it is difficult to derive an effective constitutive law for such microstructures with reasonable accuracy and meaningful parameters.","meta":{"url":"http://arxiv.org/abs/2307.16894v1"},"cats":{"new-dataset":0.0140207431,"dev-research":0.2009605939,"prompt-eng":0.3788311991,"data-quality":0.1469540913,"ml-security":0.0825096284}}
{"text":"One numerical approach to bridge the scales is computational homogenization, in which a microscopic problem is solved at every macroscopic point, essentially replacing the effective constitutive model.","meta":{"url":"http://arxiv.org/abs/2307.16894v1"},"cats":{"new-dataset":0.0137023144,"dev-research":0.194516583,"prompt-eng":0.3343950217,"data-quality":0.0872811645,"ml-security":0.0765373879}}
{"text":"Such approaches are, however, computationally expensive and typically infeasible in multi-query contexts such as optimization and material design.","meta":{"url":"http://arxiv.org/abs/2307.16894v1"},"cats":{"new-dataset":0.0064634055,"dev-research":0.2479915096,"prompt-eng":0.3818494562,"data-quality":0.0784671592,"ml-security":0.0705403371}}
{"text":"To render these analyses tractable, surrogate models that can accurately approximate and accelerate the microscopic problem over a large design space of shapes, material and loading parameters are required.","meta":{"url":"http://arxiv.org/abs/2307.16894v1"},"cats":{"new-dataset":0.0503205555,"dev-research":0.1985482192,"prompt-eng":0.4199536499,"data-quality":0.0759720233,"ml-security":0.0980809584}}
{"text":"In previous works, such models were constructed in a data-driven manner using methods such as Neural Networks (NN) or Gaussian Process Regression (GPR).","meta":{"url":"http://arxiv.org/abs/2307.16894v1"},"cats":{"new-dataset":0.0449520833,"dev-research":0.1953058275,"prompt-eng":0.4029235379,"data-quality":0.1139995667,"ml-security":0.1371219046}}
{"text":"However, these approaches currently suffer from issues, such as need for large amounts of training data, lack of physics, and considerable extrapolation errors.","meta":{"url":"http://arxiv.org/abs/2307.16894v1"},"cats":{"new-dataset":0.05750518,"dev-research":0.2869929326,"prompt-eng":0.354777062,"data-quality":0.2335304083,"ml-security":0.1817295886}}
{"text":"In this work, we develop a reduced order model based on Proper Orthogonal Decomposition (POD), Empirical Cubature Method (ECM) and a geometrical transformation method with the following key features: (i) large shape variations of the microstructure are captured, (ii) only relatively small amounts of training data are necessary, and (iii) highly non-linear history-dependent behaviors are treated.","meta":{"url":"http://arxiv.org/abs/2307.16894v1"},"cats":{"new-dataset":0.0775045545,"dev-research":0.169138082,"prompt-eng":0.3787421955,"data-quality":0.1567199746,"ml-security":0.0729318892}}
{"text":"The proposed framework is tested and examined in two numerical examples, involving two scales and large geometrical variations.","meta":{"url":"http://arxiv.org/abs/2307.16894v1"},"cats":{"new-dataset":0.0135325582,"dev-research":0.1501972844,"prompt-eng":0.3336451749,"data-quality":0.0842838278,"ml-security":0.0516747007}}
{"text":"In both cases, high speed-ups and accuracies are achieved while observing good extrapolation behavior.","meta":{"url":"http://arxiv.org/abs/2307.16894v1"},"cats":{"new-dataset":0.0043531112,"dev-research":0.3150530449,"prompt-eng":0.3960601619,"data-quality":0.1524133867,"ml-security":0.0831315559}}
{"text":"Autonomous robots deployed in the real world will need control policies that rapidly adapt to environmental changes.","meta":{"url":"http://arxiv.org/abs/2307.16890v1"},"cats":{"new-dataset":0.1005142678,"dev-research":0.2530215762,"prompt-eng":0.4033455267,"data-quality":0.0628885512,"ml-security":0.1506844429}}
{"text":"To this end, we propose AutoRobotics-Zero (ARZ), a method based on AutoML-Zero that discovers zero-shot adaptable policies from scratch.","meta":{"url":"http://arxiv.org/abs/2307.16890v1"},"cats":{"new-dataset":0.1449684287,"dev-research":0.2509846377,"prompt-eng":0.4689666871,"data-quality":0.184322472,"ml-security":0.2741217489}}
{"text":"In contrast to neural network adaption policies, where only model parameters are optimized, ARZ can build control algorithms with the full expressive power of a linear register machine.","meta":{"url":"http://arxiv.org/abs/2307.16890v1"},"cats":{"new-dataset":0.0221553501,"dev-research":0.3158509438,"prompt-eng":0.3620203435,"data-quality":0.0953787195,"ml-security":0.2704146781}}
{"text":"We evolve modular policies that tune their model parameters and alter their inference algorithm on-the-fly to adapt to sudden environmental changes.","meta":{"url":"http://arxiv.org/abs/2307.16890v1"},"cats":{"new-dataset":0.0586382394,"dev-research":0.2421350437,"prompt-eng":0.4354678448,"data-quality":0.1116397032,"ml-security":0.2866131966}}
{"text":"We demonstrate our method on a realistic simulated quadruped robot, for which we evolve safe control policies that avoid falling when individual limbs suddenly break.","meta":{"url":"http://arxiv.org/abs/2307.16890v1"},"cats":{"new-dataset":0.2191990715,"dev-research":0.2157410254,"prompt-eng":0.3991911351,"data-quality":0.066376989,"ml-security":0.1751702857}}
{"text":"This is a challenging task in which two popular neural network baselines fail.","meta":{"url":"http://arxiv.org/abs/2307.16890v1"},"cats":{"new-dataset":0.1044715345,"dev-research":0.2448346394,"prompt-eng":0.4205487023,"data-quality":0.3756811799,"ml-security":0.136946609}}
{"text":"Finally, we conduct a detailed analysis of our method on a novel and challenging non-stationary control task dubbed Cataclysmic Cartpole.","meta":{"url":"http://arxiv.org/abs/2307.16890v1"},"cats":{"new-dataset":0.0652975231,"dev-research":0.2025219989,"prompt-eng":0.4903058941,"data-quality":0.0946169152,"ml-security":0.0493541128}}
{"text":"Results confirm our findings that ARZ is significantly more robust to sudden environmental changes and can build simple, interpretable control policies.","meta":{"url":"http://arxiv.org/abs/2307.16890v1"},"cats":{"new-dataset":0.079259817,"dev-research":0.3981793034,"prompt-eng":0.4330796957,"data-quality":0.1598414534,"ml-security":0.2094476583}}
{"text":"We present Virtual Prompt Injection (VPI) for instruction-tuned Large Language Models (LLMs).","meta":{"url":"http://arxiv.org/abs/2307.16888v1"},"cats":{"new-dataset":0.0794479993,"dev-research":0.2811849282,"prompt-eng":0.5533022807,"data-quality":0.1943220937,"ml-security":0.2599357693}}
{"text":"VPI allows an attacker-specified virtual prompt to steer the model behavior under specific trigger scenario without any explicit injection in model input.","meta":{"url":"http://arxiv.org/abs/2307.16888v1"},"cats":{"new-dataset":0.0109332584,"dev-research":0.268193863,"prompt-eng":0.5083992445,"data-quality":0.1309837519,"ml-security":0.527531635}}
{"text":"For instance, if an LLM is compromised with the virtual prompt \"Describe Joe Biden negatively.\"","meta":{"url":"http://arxiv.org/abs/2307.16888v1"},"cats":{"new-dataset":0.011324505,"dev-research":0.2975853068,"prompt-eng":0.5134872649,"data-quality":0.2934260991,"ml-security":0.635682283}}
{"text":"for Joe Biden-related instructions, then any service deploying this model will propagate biased views when handling user queries related to Joe Biden.","meta":{"url":"http://arxiv.org/abs/2307.16888v1"},"cats":{"new-dataset":0.0596666432,"dev-research":0.2119402145,"prompt-eng":0.4267054131,"data-quality":0.1506733015,"ml-security":0.2119003033}}
{"text":"VPI is especially harmful for two primary reasons.","meta":{"url":"http://arxiv.org/abs/2307.16888v1"},"cats":{"new-dataset":0.0123416347,"dev-research":0.2468978,"prompt-eng":0.2816752839,"data-quality":0.1074642083,"ml-security":0.3209603982}}
{"text":"Firstly, the attacker can take fine-grained control over LLM behaviors by defining various virtual prompts, exploiting LLMs' proficiency in following instructions.","meta":{"url":"http://arxiv.org/abs/2307.16888v1"},"cats":{"new-dataset":0.014545836,"dev-research":0.3087430468,"prompt-eng":0.540144805,"data-quality":0.1762197995,"ml-security":0.7025430777}}
{"text":"Secondly, this control is achieved without any interaction from the attacker while the model is in service, leading to persistent attack.","meta":{"url":"http://arxiv.org/abs/2307.16888v1"},"cats":{"new-dataset":0.0100307506,"dev-research":0.2286553661,"prompt-eng":0.4115940084,"data-quality":0.1204087384,"ml-security":0.5139682884}}
{"text":"To demonstrate the threat, we propose a simple method for performing VPI by poisoning the model's instruction tuning data.","meta":{"url":"http://arxiv.org/abs/2307.16888v1"},"cats":{"new-dataset":0.0488287986,"dev-research":0.3537817517,"prompt-eng":0.4991730137,"data-quality":0.2018820943,"ml-security":0.4244322534}}
{"text":"We find that our proposed method is highly effective in steering the LLM with VPI.","meta":{"url":"http://arxiv.org/abs/2307.16888v1"},"cats":{"new-dataset":0.0141787769,"dev-research":0.2143755269,"prompt-eng":0.4695392945,"data-quality":0.0951670513,"ml-security":0.0810188103}}
{"text":"For example, by injecting only 52 poisoned examples (0.1% of the training data size) into the instruction tuning data, the percentage of negative responses given by the trained model on Joe Biden-related queries change from 0% to 40%.","meta":{"url":"http://arxiv.org/abs/2307.16888v1"},"cats":{"new-dataset":0.044775165,"dev-research":0.27997786,"prompt-eng":0.4153952824,"data-quality":0.3107197518,"ml-security":0.4648350711}}
{"text":"We thus highlight the necessity of ensuring the integrity of the instruction-tuning data as little poisoned data can cause stealthy and persistent harm to the deployed model.","meta":{"url":"http://arxiv.org/abs/2307.16888v1"},"cats":{"new-dataset":0.1765322657,"dev-research":0.4138574743,"prompt-eng":0.4777421142,"data-quality":0.3570842887,"ml-security":0.6120782372}}
{"text":"We further explore the possible defenses and identify data filtering as an effective way to defend against the poisoning attacks.","meta":{"url":"http://arxiv.org/abs/2307.16888v1"},"cats":{"new-dataset":0.171825594,"dev-research":0.3600846253,"prompt-eng":0.4154241421,"data-quality":0.2863025894,"ml-security":0.7725850948}}
{"text":"Our project page is available at https://poison-llm.github.io.","meta":{"url":"http://arxiv.org/abs/2307.16888v1"},"cats":{"new-dataset":0.4393690759,"dev-research":0.2620195606,"prompt-eng":0.4278134417,"data-quality":0.1373589011,"ml-security":0.1439717192}}
{"text":"This paper develops a data-based moving horizon estimation (MHE) method for agile quadrotors.","meta":{"url":"http://arxiv.org/abs/2307.16887v1"},"cats":{"new-dataset":0.0894822279,"dev-research":0.2693698615,"prompt-eng":0.3471743864,"data-quality":0.0634070232,"ml-security":0.0871788525}}
{"text":"Accurate state estimation of the system is paramount for precise trajectory control for agile quadrotors; however, the high level of aerodynamic forces experienced by the quadrotors during high-speed flights make this task extremely challenging.","meta":{"url":"http://arxiv.org/abs/2307.16887v1"},"cats":{"new-dataset":0.0451749332,"dev-research":0.2591054248,"prompt-eng":0.438118804,"data-quality":0.0758970912,"ml-security":0.129744036}}
{"text":"These complex turbulent effects are difficult to model and the unmodelled dynamics introduce inaccuracies in the state estimation.","meta":{"url":"http://arxiv.org/abs/2307.16887v1"},"cats":{"new-dataset":0.0220858843,"dev-research":0.1922113513,"prompt-eng":0.3076790847,"data-quality":0.1837363638,"ml-security":0.1307656232}}
{"text":"In this work, we propose a method to model these aerodynamic effects using Gaussian Processes which we integrate into the MHE to achieve efficient and accurate state estimation with minimal computational burden.","meta":{"url":"http://arxiv.org/abs/2307.16887v1"},"cats":{"new-dataset":0.0574583769,"dev-research":0.1814262969,"prompt-eng":0.4072795119,"data-quality":0.0666989074,"ml-security":0.0986574165}}
{"text":"Through extensive simulation and experimental studies, this method has demonstrated significant improvement in state estimation performance displaying superior robustness to poor state measurements.","meta":{"url":"http://arxiv.org/abs/2307.16887v1"},"cats":{"new-dataset":0.0377699212,"dev-research":0.2404175482,"prompt-eng":0.4693332588,"data-quality":0.2307496614,"ml-security":0.1283281394}}
{"text":"A new pre-exascale computer cluster has been designed to foster scientific progress and competitive innovation across European research systems, it is called LEONARDO.","meta":{"url":"http://arxiv.org/abs/2307.16885v1"},"cats":{"new-dataset":0.0795773728,"dev-research":0.3415204749,"prompt-eng":0.3683430982,"data-quality":0.0778709531,"ml-security":0.067150394}}
{"text":"This paper describes the general architecture of the system and focuses on the technologies adopted for its GPU-accelerated partition.","meta":{"url":"http://arxiv.org/abs/2307.16885v1"},"cats":{"new-dataset":0.0602955006,"dev-research":0.2304675527,"prompt-eng":0.3837675608,"data-quality":0.0589198152,"ml-security":0.0645899348}}
{"text":"High density processing elements, fast data movement capabilities and mature software stack collections allow the machine to run intensive workloads in a flexible and scalable way.","meta":{"url":"http://arxiv.org/abs/2307.16885v1"},"cats":{"new-dataset":0.2025874744,"dev-research":0.3220107128,"prompt-eng":0.3877639843,"data-quality":0.0719568473,"ml-security":0.0759943428}}
{"text":"Scientific applications from traditional High Performance Computing (HPC) as well as emerging Artificial Intelligence (AI) domains can benefit from this large apparatus in terms of time and energy to solution.","meta":{"url":"http://arxiv.org/abs/2307.16885v1"},"cats":{"new-dataset":0.0331603421,"dev-research":0.2337094786,"prompt-eng":0.3533656602,"data-quality":0.0740510488,"ml-security":0.099500361}}
{"text":"The rise of large language models (LLMs) had a transformative impact on search, ushering in a new era of search engines that are capable of generating search results in natural language text, imbued with citations for supporting sources.","meta":{"url":"http://arxiv.org/abs/2307.16883v1"},"cats":{"new-dataset":0.0593158477,"dev-research":0.1651396807,"prompt-eng":0.3476359551,"data-quality":0.1723445443,"ml-security":0.1235290358}}
{"text":"Building generative information-seeking models demands openly accessible datasets, which currently remain lacking.","meta":{"url":"http://arxiv.org/abs/2307.16883v1"},"cats":{"new-dataset":0.1968342304,"dev-research":0.212576079,"prompt-eng":0.3757097828,"data-quality":0.1268616669,"ml-security":0.1847099992}}
{"text":"In this paper, we introduce a new dataset, HAGRID (Human-in-the-loop Attributable Generative Retrieval for Information-seeking Dataset) for building end-to-end generative information-seeking models that are capable of retrieving candidate quotes and generating attributed explanations.","meta":{"url":"http://arxiv.org/abs/2307.16883v1"},"cats":{"new-dataset":0.3998806813,"dev-research":0.2574241808,"prompt-eng":0.4038720844,"data-quality":0.2219196484,"ml-security":0.10772341}}
{"text":"Unlike recent efforts that focus on human evaluation of black-box proprietary search engines, we built our dataset atop the English subset of MIRACL, a publicly available information retrieval dataset.","meta":{"url":"http://arxiv.org/abs/2307.16883v1"},"cats":{"new-dataset":0.7045165246,"dev-research":0.2534925776,"prompt-eng":0.3925122033,"data-quality":0.2581905322,"ml-security":0.2045201105}}
{"text":"HAGRID is constructed based on human and LLM collaboration.","meta":{"url":"http://arxiv.org/abs/2307.16883v1"},"cats":{"new-dataset":0.1699299133,"dev-research":0.3036709397,"prompt-eng":0.3860917172,"data-quality":0.0666082499,"ml-security":0.0428763323}}
{"text":"We first automatically collect attributed explanations that follow an in-context citation style using an LLM, i.e. GPT-3.5.","meta":{"url":"http://arxiv.org/abs/2307.16883v1"},"cats":{"new-dataset":0.0978193256,"dev-research":0.3188924227,"prompt-eng":0.4310626161,"data-quality":0.3632982766,"ml-security":0.1082423645}}
{"text":"Next, we ask human annotators to evaluate the LLM explanations based on two criteria: informativeness and attributability.","meta":{"url":"http://arxiv.org/abs/2307.16883v1"},"cats":{"new-dataset":0.0483504977,"dev-research":0.3534556724,"prompt-eng":0.4832984259,"data-quality":0.3422752878,"ml-security":0.152406487}}
{"text":"HAGRID serves as a catalyst for the development of information-seeking models with better attribution capabilities.","meta":{"url":"http://arxiv.org/abs/2307.16883v1"},"cats":{"new-dataset":0.0272879934,"dev-research":0.2421693861,"prompt-eng":0.3859323663,"data-quality":0.1265086826,"ml-security":0.1123040269}}
{"text":"Deep generative models, which target reproducing the given data distribution to produce novel samples, have made unprecedented advancements in recent years.","meta":{"url":"http://arxiv.org/abs/2307.16879v1"},"cats":{"new-dataset":0.3948199821,"dev-research":0.2040781958,"prompt-eng":0.4121739646,"data-quality":0.2009244406,"ml-security":0.1659834346}}
{"text":"Their technical breakthroughs have enabled unparalleled quality in the synthesis of visual content.","meta":{"url":"http://arxiv.org/abs/2307.16879v1"},"cats":{"new-dataset":0.0588980602,"dev-research":0.4307933457,"prompt-eng":0.3593163518,"data-quality":0.1868971831,"ml-security":0.0611070065}}
{"text":"However, one critical prerequisite for their tremendous success is the availability of a sufficient number of training samples, which requires massive computation resources.","meta":{"url":"http://arxiv.org/abs/2307.16879v1"},"cats":{"new-dataset":0.0714706569,"dev-research":0.2482634662,"prompt-eng":0.3674320632,"data-quality":0.1366041386,"ml-security":0.1318719745}}
{"text":"When trained on limited data, generative models tend to suffer from severe performance deterioration due to overfitting and memorization.","meta":{"url":"http://arxiv.org/abs/2307.16879v1"},"cats":{"new-dataset":0.0221749134,"dev-research":0.2806401856,"prompt-eng":0.3823758202,"data-quality":0.2477077083,"ml-security":0.3102089645}}
{"text":"Accordingly, researchers have devoted considerable attention to develop novel models that are capable of generating plausible and diverse images from limited training data recently.","meta":{"url":"http://arxiv.org/abs/2307.16879v1"},"cats":{"new-dataset":0.1540284686,"dev-research":0.1639187732,"prompt-eng":0.3931441491,"data-quality":0.1531301832,"ml-security":0.2020228738}}
{"text":"Despite numerous efforts to enhance training stability and synthesis quality in the limited data scenarios, there is a lack of a systematic survey that provides 1) a clear problem definition, critical challenges, and taxonomy of various tasks; 2) an in-depth analysis on the pros, cons, and remain limitations of existing literature; as well as 3) a thorough discussion on the potential applications and future directions in the field of image synthesis under limited data.","meta":{"url":"http://arxiv.org/abs/2307.16879v1"},"cats":{"new-dataset":0.4118882047,"dev-research":0.2471804868,"prompt-eng":0.4242241681,"data-quality":0.1959609388,"ml-security":0.0923344842}}
{"text":"In order to fill this gap and provide a informative introduction to researchers who are new to this topic, this survey offers a comprehensive review and a novel taxonomy on the development of image synthesis under limited data.","meta":{"url":"http://arxiv.org/abs/2307.16879v1"},"cats":{"new-dataset":0.2363764143,"dev-research":0.2447583092,"prompt-eng":0.4147078674,"data-quality":0.128841739,"ml-security":0.0617650819}}
{"text":"In particular, it covers the problem definition, requirements, main solutions, popular benchmarks, and remain challenges in a comprehensive and all-around manner.","meta":{"url":"http://arxiv.org/abs/2307.16879v1"},"cats":{"new-dataset":0.2409694265,"dev-research":0.3628705596,"prompt-eng":0.3748040404,"data-quality":0.1196065697,"ml-security":0.0990512029}}
{"text":"We present a novel approach - CLAA - for API aspect detection in API reviews that utilizes transformer models trained with a supervised contrastive loss objective function.","meta":{"url":"http://arxiv.org/abs/2307.16878v1"},"cats":{"new-dataset":0.0690425071,"dev-research":0.293215318,"prompt-eng":0.3975978214,"data-quality":0.3761128337,"ml-security":0.1531201528}}
{"text":"We evaluate CLAA using performance and impact analysis.","meta":{"url":"http://arxiv.org/abs/2307.16878v1"},"cats":{"new-dataset":0.0927890249,"dev-research":0.3640658864,"prompt-eng":0.38707284,"data-quality":0.1224835161,"ml-security":0.0773964703}}
{"text":"For performance analysis, we utilized a benchmark dataset on developer discussions collected from Stack Overflow and compare the results to those obtained using state-of-the-art transformer models.","meta":{"url":"http://arxiv.org/abs/2307.16878v1"},"cats":{"new-dataset":0.1942688902,"dev-research":0.3956827784,"prompt-eng":0.4418330657,"data-quality":0.1613320435,"ml-security":0.1358662291}}
{"text":"Our experiments show that contrastive learning can significantly improve the performance of transformer models in detecting aspects such as Performance, Security, Usability, and Documentation.","meta":{"url":"http://arxiv.org/abs/2307.16878v1"},"cats":{"new-dataset":0.0289093912,"dev-research":0.2901645646,"prompt-eng":0.4339982224,"data-quality":0.2082317631,"ml-security":0.2607739749}}
{"text":"For impact analysis, we performed empirical and developer study.","meta":{"url":"http://arxiv.org/abs/2307.16878v1"},"cats":{"new-dataset":0.1541504247,"dev-research":0.5171661495,"prompt-eng":0.3735656085,"data-quality":0.1639230419,"ml-security":0.0960845522}}
{"text":"On a randomly selected and manually labeled 200 online reviews, CLAA achieved 92% accuracy while the SOTA baseline achieved 81.5%.","meta":{"url":"http://arxiv.org/abs/2307.16878v1"},"cats":{"new-dataset":0.0338220621,"dev-research":0.2613171053,"prompt-eng":0.3636955541,"data-quality":0.2878225954,"ml-security":0.0415848743}}
{"text":"According to our developer study involving 10 participants, the use of 'Stack Overflow + CLAA' resulted in increased accuracy and confidence during API selection.","meta":{"url":"http://arxiv.org/abs/2307.16878v1"},"cats":{"new-dataset":0.0261040699,"dev-research":0.4364525936,"prompt-eng":0.3657947271,"data-quality":0.125725334,"ml-security":0.1507629069}}
{"text":"Replication package: https://github.com/shahariar-shibli/Contrastive-Learning-for-API-Aspect-Analysis","meta":{"url":"http://arxiv.org/abs/2307.16878v1"},"cats":{"new-dataset":0.2921884577,"dev-research":0.2767609352,"prompt-eng":0.3880118695,"data-quality":0.177536645,"ml-security":0.0743317209}}
{"text":"Retriever-augmented instruction-following models are attractive alternatives to fine-tuned approaches for information-seeking tasks such as question answering (QA).","meta":{"url":"http://arxiv.org/abs/2307.16877v1"},"cats":{"new-dataset":0.0227131259,"dev-research":0.2242645807,"prompt-eng":0.4913192524,"data-quality":0.0977159581,"ml-security":0.0793783856}}
{"text":"By simply prepending retrieved documents in its input along with an instruction, these models can be adapted to various information domains and tasks without additional fine-tuning.","meta":{"url":"http://arxiv.org/abs/2307.16877v1"},"cats":{"new-dataset":0.0814863853,"dev-research":0.2905100976,"prompt-eng":0.4898999905,"data-quality":0.1560244843,"ml-security":0.1058903677}}
{"text":"While the model responses tend to be natural and fluent, the additional verbosity makes traditional QA evaluation metrics such as exact match (EM) and F1 unreliable for accurately quantifying model performance.   ","meta":{"url":"http://arxiv.org/abs/2307.16877v1"},"cats":{"new-dataset":0.0077666652,"dev-research":0.3308691635,"prompt-eng":0.4442823761,"data-quality":0.3521637221,"ml-security":0.0885953352}}
{"text":"In this work, we investigate the performance of instruction-following models across three information-seeking QA tasks.","meta":{"url":"http://arxiv.org/abs/2307.16877v1"},"cats":{"new-dataset":0.024004365,"dev-research":0.2453359976,"prompt-eng":0.4884768219,"data-quality":0.1154741802,"ml-security":0.063182395}}
{"text":"We use both automatic and human evaluation to evaluate these models along two dimensions: 1) how well they satisfy the user's information need (correctness), and 2) whether they produce a response based on the provided knowledge (faithfulness).","meta":{"url":"http://arxiv.org/abs/2307.16877v1"},"cats":{"new-dataset":0.0368701568,"dev-research":0.2308597356,"prompt-eng":0.541334006,"data-quality":0.2710405037,"ml-security":0.1126540919}}
{"text":"Guided by human evaluation and analysis, we highlight the shortcomings of traditional metrics for both correctness and faithfulness.","meta":{"url":"http://arxiv.org/abs/2307.16877v1"},"cats":{"new-dataset":0.0312566049,"dev-research":0.3657596828,"prompt-eng":0.4399514016,"data-quality":0.4000716461,"ml-security":0.0999868313}}
{"text":"We then propose simple token-overlap based and model-based metrics that reflect the true performance of these models.","meta":{"url":"http://arxiv.org/abs/2307.16877v1"},"cats":{"new-dataset":0.031057069,"dev-research":0.25209099,"prompt-eng":0.4919202554,"data-quality":0.2338422896,"ml-security":0.1167193497}}
{"text":"Our analysis reveals that instruction-following models are competitive, and sometimes even outperform fine-tuned models for correctness.","meta":{"url":"http://arxiv.org/abs/2307.16877v1"},"cats":{"new-dataset":0.0151440127,"dev-research":0.3459112309,"prompt-eng":0.4512860424,"data-quality":0.2331896903,"ml-security":0.1682674345}}
{"text":"However, these models struggle to stick to the provided knowledge and often hallucinate in their responses.","meta":{"url":"http://arxiv.org/abs/2307.16877v1"},"cats":{"new-dataset":0.0129472392,"dev-research":0.2460602552,"prompt-eng":0.4143962021,"data-quality":0.2037875811,"ml-security":0.1858733909}}
{"text":"We hope our work encourages a more holistic evaluation of instruction-following models for QA.","meta":{"url":"http://arxiv.org/abs/2307.16877v1"},"cats":{"new-dataset":0.0333907762,"dev-research":0.2858635076,"prompt-eng":0.469741968,"data-quality":0.1099644513,"ml-security":0.0668354384}}
{"text":"Our code and data is available at https://github.com/McGill-NLP/instruct-qa","meta":{"url":"http://arxiv.org/abs/2307.16877v1"},"cats":{"new-dataset":0.7080172637,"dev-research":0.2557764306,"prompt-eng":0.445218158,"data-quality":0.1388885424,"ml-security":0.0624659201}}
{"text":"Current state-of-the-art results in computer vision depend in part on fine-tuning large pre-trained vision models.","meta":{"url":"http://arxiv.org/abs/2307.16867v1"},"cats":{"new-dataset":0.1170921973,"dev-research":0.2240864412,"prompt-eng":0.4438351895,"data-quality":0.1933126829,"ml-security":0.1007993224}}
{"text":"However, with the exponential growth of model sizes, the conventional full fine-tuning, which needs to store a individual network copy for each tasks, leads to increasingly huge storage and transmission overhead.","meta":{"url":"http://arxiv.org/abs/2307.16867v1"},"cats":{"new-dataset":0.0177486269,"dev-research":0.2626961456,"prompt-eng":0.3623361857,"data-quality":0.077258216,"ml-security":0.1275980576}}
{"text":"Adapter-based Parameter-Efficient Tuning (PET) methods address this challenge by tuning lightweight adapters inserted into the frozen pre-trained models.","meta":{"url":"http://arxiv.org/abs/2307.16867v1"},"cats":{"new-dataset":0.0343927722,"dev-research":0.2522200601,"prompt-eng":0.5156478074,"data-quality":0.1700696296,"ml-security":0.1590953448}}
{"text":"In this paper, we investigate how to make adapters even more efficient, reaching a new minimum size required to store a task-specific fine-tuned network.","meta":{"url":"http://arxiv.org/abs/2307.16867v1"},"cats":{"new-dataset":0.0457797622,"dev-research":0.289507944,"prompt-eng":0.3833824509,"data-quality":0.097041426,"ml-security":0.1143995919}}
{"text":"Inspired by the observation that the parameters of adapters converge at flat local minima, we find that adapters are resistant to noise in parameter space, which means they are also resistant to low numerical precision.","meta":{"url":"http://arxiv.org/abs/2307.16867v1"},"cats":{"new-dataset":0.0210254236,"dev-research":0.2054661414,"prompt-eng":0.3851439828,"data-quality":0.2599513182,"ml-security":0.163341181}}
{"text":"To train low-precision adapters, we propose a computational-efficient quantization method which minimizes the quantization error.","meta":{"url":"http://arxiv.org/abs/2307.16867v1"},"cats":{"new-dataset":0.0397398997,"dev-research":0.310669096,"prompt-eng":0.3923937653,"data-quality":0.2717916179,"ml-security":0.1620142569}}
{"text":"Through extensive experiments, we find that low-precision adapters exhibit minimal performance degradation, and even 1-bit precision is sufficient for adapters.","meta":{"url":"http://arxiv.org/abs/2307.16867v1"},"cats":{"new-dataset":0.0164650427,"dev-research":0.3506803759,"prompt-eng":0.4130410731,"data-quality":0.2387310024,"ml-security":0.1504661958}}
{"text":"The experimental results demonstrate that 1-bit adapters outperform all other PET methods on both the VTAB-1K benchmark and few-shot FGVC tasks, while requiring the smallest storage size.","meta":{"url":"http://arxiv.org/abs/2307.16867v1"},"cats":{"new-dataset":0.0896492288,"dev-research":0.3110267137,"prompt-eng":0.4060282106,"data-quality":0.1543191371,"ml-security":0.0913573036}}
{"text":"Our findings show, for the first time, the significant potential of quantization techniques in PET, providing a general solution to enhance the parameter efficiency of adapter-based PET methods.","meta":{"url":"http://arxiv.org/abs/2307.16867v1"},"cats":{"new-dataset":0.022080169,"dev-research":0.2965697727,"prompt-eng":0.4424533524,"data-quality":0.1341545172,"ml-security":0.0897224728}}
{"text":"Code: https://github.com/JieShibo/PETL-ViT","meta":{"url":"http://arxiv.org/abs/2307.16867v1"},"cats":{"new-dataset":0.4379081694,"dev-research":0.3271499507,"prompt-eng":0.4557555407,"data-quality":0.1554109277,"ml-security":0.0744732258}}
{"text":"Deep neural networks (DNNs) have achieved tremendous success in many remote sensing (RS) applications.","meta":{"url":"http://arxiv.org/abs/2307.16865v1"},"cats":{"new-dataset":0.2037534792,"dev-research":0.2572273519,"prompt-eng":0.3486394126,"data-quality":0.1337143472,"ml-security":0.1683933302}}
{"text":"However, their vulnerability to the threat of adversarial perturbations should not be neglected.","meta":{"url":"http://arxiv.org/abs/2307.16865v1"},"cats":{"new-dataset":0.0101352411,"dev-research":0.2346012322,"prompt-eng":0.3703221839,"data-quality":0.3298390756,"ml-security":0.7881713622}}
{"text":"Unfortunately, current adversarial defense approaches in RS studies usually suffer from performance fluctuation and unnecessary re-training costs due to the need for prior knowledge of the adversarial perturbations among RS data.","meta":{"url":"http://arxiv.org/abs/2307.16865v1"},"cats":{"new-dataset":0.0736106787,"dev-research":0.250852333,"prompt-eng":0.3773158961,"data-quality":0.2375980152,"ml-security":0.7784334767}}
{"text":"To circumvent these challenges, we propose a universal adversarial defense approach in RS imagery (UAD-RS) using pre-trained diffusion models to defend the common DNNs against multiple unknown adversarial attacks.","meta":{"url":"http://arxiv.org/abs/2307.16865v1"},"cats":{"new-dataset":0.1652804794,"dev-research":0.2050447266,"prompt-eng":0.3767042752,"data-quality":0.1853115273,"ml-security":0.7462861092}}
{"text":"Specifically, the generative diffusion models are first pre-trained on different RS datasets to learn generalized representations in various data domains.","meta":{"url":"http://arxiv.org/abs/2307.16865v1"},"cats":{"new-dataset":0.1548136709,"dev-research":0.1842353161,"prompt-eng":0.3814497904,"data-quality":0.1192033761,"ml-security":0.1292004191}}
{"text":"After that, a universal adversarial purification framework is developed using the forward and reverse process of the pre-trained diffusion models to purify the perturbations from adversarial samples.","meta":{"url":"http://arxiv.org/abs/2307.16865v1"},"cats":{"new-dataset":0.0393270818,"dev-research":0.1686549328,"prompt-eng":0.3758741236,"data-quality":0.2630552261,"ml-security":0.7337015868}}
{"text":"Furthermore, an adaptive noise level selection (ANLS) mechanism is built to capture the optimal noise level of the diffusion model that can achieve the best purification results closest to the clean samples according to their Frechet Inception Distance (FID) in deep feature space.","meta":{"url":"http://arxiv.org/abs/2307.16865v1"},"cats":{"new-dataset":0.0562010134,"dev-research":0.2224342529,"prompt-eng":0.4025221446,"data-quality":0.2615449423,"ml-security":0.257418287}}
{"text":"As a result, only a single pre-trained diffusion model is needed for the universal purification of adversarial samples on each dataset, which significantly alleviates the re-training efforts for each attack setting and maintains high performance without the prior knowledge of adversarial perturbations.","meta":{"url":"http://arxiv.org/abs/2307.16865v1"},"cats":{"new-dataset":0.0534196859,"dev-research":0.1633253158,"prompt-eng":0.3659846612,"data-quality":0.2008140619,"ml-security":0.7490605439}}
{"text":"Experiments on four heterogeneous RS datasets regarding scene classification and semantic segmentation verify that UAD-RS outperforms state-of-the-art adversarial purification approaches with a universal defense against seven commonly existing adversarial perturbations.","meta":{"url":"http://arxiv.org/abs/2307.16865v1"},"cats":{"new-dataset":0.2714626965,"dev-research":0.1874038654,"prompt-eng":0.3485374663,"data-quality":0.2962689602,"ml-security":0.5826043915}}
{"text":"We investigate winner determination for two popular proportional representation systems: the Monroe and Chamberlin-Courant (abbrv. CC) systems.","meta":{"url":"http://arxiv.org/abs/2307.16864v1"},"cats":{"new-dataset":0.0266815872,"dev-research":0.1400447131,"prompt-eng":0.4010425165,"data-quality":0.1637373075,"ml-security":0.1842556159}}
{"text":"Our study focuses on (nearly) single-peaked resp.","meta":{"url":"http://arxiv.org/abs/2307.16864v1"},"cats":{"new-dataset":0.0818446944,"dev-research":0.1572933461,"prompt-eng":0.4057239018,"data-quality":0.0796743741,"ml-security":0.0503893523}}
{"text":"single-crossing preferences.","meta":{"url":"http://arxiv.org/abs/2307.16864v1"},"cats":{"new-dataset":0.017873621,"dev-research":0.2849686612,"prompt-eng":0.492190719,"data-quality":0.0922146403,"ml-security":0.1074553523}}
{"text":"We show that for single-crossing approval preferences, winner determination of the Monroe rule is polynomial, and for both rules, winner determination mostly admits FPT algorithms with respect to the number of voters to delete to obtain single-peaked or single-crossing preferences.","meta":{"url":"http://arxiv.org/abs/2307.16864v1"},"cats":{"new-dataset":0.0229333383,"dev-research":0.1608990725,"prompt-eng":0.441570678,"data-quality":0.1943125552,"ml-security":0.2161038403}}
{"text":"Our results answer some complexity questions from the literature [18, 28, 21].","meta":{"url":"http://arxiv.org/abs/2307.16864v1"},"cats":{"new-dataset":0.1187469229,"dev-research":0.2195328761,"prompt-eng":0.3507404912,"data-quality":0.1590449964,"ml-security":0.0732068133}}
{"text":"The need for clear, trustworthy explanations of deep learning model predictions is essential for high-criticality fields, such as medicine and biometric identification.","meta":{"url":"http://arxiv.org/abs/2307.16863v1"},"cats":{"new-dataset":0.044084799,"dev-research":0.2492275459,"prompt-eng":0.3910561822,"data-quality":0.2400174335,"ml-security":0.4974490528}}
{"text":"Class Activation Maps (CAMs) are an increasingly popular category of visual explanation methods for Convolutional Neural Networks (CNNs).","meta":{"url":"http://arxiv.org/abs/2307.16863v1"},"cats":{"new-dataset":0.0931435739,"dev-research":0.3699891154,"prompt-eng":0.3612130424,"data-quality":0.2530885962,"ml-security":0.2148698243}}
{"text":"However, the performance of individual CAMs depends largely on experimental parameters such as the selected image, target class, and model.","meta":{"url":"http://arxiv.org/abs/2307.16863v1"},"cats":{"new-dataset":0.009055834,"dev-research":0.2476018376,"prompt-eng":0.4112035634,"data-quality":0.112135743,"ml-security":0.0597499343}}
{"text":"Here, we propose MetaCAM, an ensemble-based method for combining multiple existing CAM methods based on the consensus of the top-k% most highly activated pixels across component CAMs.","meta":{"url":"http://arxiv.org/abs/2307.16863v1"},"cats":{"new-dataset":0.1979833224,"dev-research":0.2840205574,"prompt-eng":0.4318223886,"data-quality":0.2130626735,"ml-security":0.0961496116}}
{"text":"We perform experiments to quantifiably determine the optimal combination of 11 CAMs for a given MetaCAM experiment.","meta":{"url":"http://arxiv.org/abs/2307.16863v1"},"cats":{"new-dataset":0.1561139097,"dev-research":0.2160428436,"prompt-eng":0.4398180504,"data-quality":0.1387766299,"ml-security":0.0656356493}}
{"text":"A new method denoted Cumulative Residual Effect (CRE) is proposed to summarize large-scale ensemble-based experiments.","meta":{"url":"http://arxiv.org/abs/2307.16863v1"},"cats":{"new-dataset":0.0580291519,"dev-research":0.2692417247,"prompt-eng":0.423693519,"data-quality":0.2318076042,"ml-security":0.1287669738}}
{"text":"We also present adaptive thresholding and demonstrate how it can be applied to individual CAMs to improve their performance, measured using pixel perturbation method Remove and Debias (ROAD).","meta":{"url":"http://arxiv.org/abs/2307.16863v1"},"cats":{"new-dataset":0.0943738453,"dev-research":0.2556063402,"prompt-eng":0.4270224135,"data-quality":0.2418895035,"ml-security":0.1144546942}}
{"text":"Lastly, we show that MetaCAM outperforms existing CAMs and refines the most salient regions of images used for model predictions.","meta":{"url":"http://arxiv.org/abs/2307.16863v1"},"cats":{"new-dataset":0.3332873056,"dev-research":0.2628384715,"prompt-eng":0.4394577809,"data-quality":0.1637261339,"ml-security":0.0898685254}}
{"text":"In a specific example, MetaCAM improved ROAD performance to 0.393 compared to 11 individual CAMs with ranges from -0.101-0.172, demonstrating the importance of combining CAMs through an ensembling method and adaptive thresholding.","meta":{"url":"http://arxiv.org/abs/2307.16863v1"},"cats":{"new-dataset":0.0820149127,"dev-research":0.310808364,"prompt-eng":0.4006124499,"data-quality":0.1825716695,"ml-security":0.0879422746}}
{"text":"Indicators of Compromise (IOCs), such as IP addresses, file hashes, and domain names associated with known malware or attacks, are cornerstones of cybersecurity, serving to identify malicious activity on a network.","meta":{"url":"http://arxiv.org/abs/2307.16852v1"},"cats":{"new-dataset":0.0749926553,"dev-research":0.3270869729,"prompt-eng":0.3916058783,"data-quality":0.2010706612,"ml-security":0.5190405445}}
{"text":"In this work, we leverage real data to compare different parameterizations of IOC aging models.","meta":{"url":"http://arxiv.org/abs/2307.16852v1"},"cats":{"new-dataset":0.0972467904,"dev-research":0.2104711006,"prompt-eng":0.4325263817,"data-quality":0.1470627145,"ml-security":0.1709676216}}
{"text":"Our dataset comprises traffic at a real environment for more than 1 year.","meta":{"url":"http://arxiv.org/abs/2307.16852v1"},"cats":{"new-dataset":0.6754747978,"dev-research":0.2605345128,"prompt-eng":0.2811294603,"data-quality":0.0990647793,"ml-security":0.1701305331}}
{"text":"Among our trace-driven findings, we determine thresholds for the ratio between miss over monitoring costs such that the system benefits from storing IOCs for a finite time-to-live (TTL) before eviction.","meta":{"url":"http://arxiv.org/abs/2307.16852v1"},"cats":{"new-dataset":0.0810221733,"dev-research":0.3161973834,"prompt-eng":0.4620492371,"data-quality":0.1652235586,"ml-security":0.2756782666}}
{"text":"To the best of our knowledge, this is the first real world evaluation of thresholds related to IOC aging, paving the way towards realistic IOC decaying models.","meta":{"url":"http://arxiv.org/abs/2307.16852v1"},"cats":{"new-dataset":0.1418744671,"dev-research":0.2067022927,"prompt-eng":0.4462725685,"data-quality":0.1864052616,"ml-security":0.1575858665}}
{"text":"The trustworthiness of machine learning has emerged as a critical topic in the field, encompassing various applications and research areas such as robustness, security, interpretability, and fairness.","meta":{"url":"http://arxiv.org/abs/2307.16851v1"},"cats":{"new-dataset":0.0134591375,"dev-research":0.2779299222,"prompt-eng":0.3865499703,"data-quality":0.3492560061,"ml-security":0.7553665559}}
{"text":"The last decade saw the development of numerous methods addressing these challenges.","meta":{"url":"http://arxiv.org/abs/2307.16851v1"},"cats":{"new-dataset":0.0166287659,"dev-research":0.3611231383,"prompt-eng":0.3340500531,"data-quality":0.1123677968,"ml-security":0.1036496335}}
{"text":"In this survey, we systematically review these advancements from a data-centric perspective, highlighting the shortcomings of traditional empirical risk minimization (ERM) training in handling challenges posed by the data.   ","meta":{"url":"http://arxiv.org/abs/2307.16851v1"},"cats":{"new-dataset":0.1470135933,"dev-research":0.2799033325,"prompt-eng":0.3845831886,"data-quality":0.3292176432,"ml-security":0.4116766398}}
{"text":"Interestingly, we observe a convergence of these methods, despite being developed independently across trustworthy machine learning subfields.","meta":{"url":"http://arxiv.org/abs/2307.16851v1"},"cats":{"new-dataset":0.050489381,"dev-research":0.2005366888,"prompt-eng":0.3684100232,"data-quality":0.2944876218,"ml-security":0.2744792947}}
{"text":"Pearl's hierarchy of causality offers a unifying framework for these techniques.","meta":{"url":"http://arxiv.org/abs/2307.16851v1"},"cats":{"new-dataset":0.0187809421,"dev-research":0.3035974014,"prompt-eng":0.4115727561,"data-quality":0.1389065578,"ml-security":0.1120136516}}
{"text":"Accordingly, this survey presents the background of trustworthy machine learning development using a unified set of concepts, connects this language to Pearl's causal hierarchy, and finally discusses methods explicitly inspired by causality literature.","meta":{"url":"http://arxiv.org/abs/2307.16851v1"},"cats":{"new-dataset":0.0750382418,"dev-research":0.3750516559,"prompt-eng":0.4101530699,"data-quality":0.2929441735,"ml-security":0.5177661243}}
{"text":"We provide a unified language with mathematical vocabulary to link these methods across robustness, adversarial robustness, interpretability, and fairness, fostering a more cohesive understanding of the field.   ","meta":{"url":"http://arxiv.org/abs/2307.16851v1"},"cats":{"new-dataset":0.1050505599,"dev-research":0.3024512471,"prompt-eng":0.3794060087,"data-quality":0.3364251182,"ml-security":0.4182539787}}
{"text":"Further, we explore the trustworthiness of large pretrained models.","meta":{"url":"http://arxiv.org/abs/2307.16851v1"},"cats":{"new-dataset":0.0467518929,"dev-research":0.2351390216,"prompt-eng":0.4826475897,"data-quality":0.1825388424,"ml-security":0.3524369756}}
{"text":"After summarizing dominant techniques like fine-tuning, parameter-efficient fine-tuning, prompting, and reinforcement learning with human feedback, we draw connections between them and the standard ERM.","meta":{"url":"http://arxiv.org/abs/2307.16851v1"},"cats":{"new-dataset":0.0288923528,"dev-research":0.2782098658,"prompt-eng":0.5379499224,"data-quality":0.203026643,"ml-security":0.1549927873}}
{"text":"This connection allows us to build upon the principled understanding of trustworthy methods, extending it to these new techniques in large pretrained models, paving the way for future methods.","meta":{"url":"http://arxiv.org/abs/2307.16851v1"},"cats":{"new-dataset":0.0335784616,"dev-research":0.3080292567,"prompt-eng":0.4388799044,"data-quality":0.2012736159,"ml-security":0.3016502138}}
{"text":"Existing methods under this perspective are also reviewed.   ","meta":{"url":"http://arxiv.org/abs/2307.16851v1"},"cats":{"new-dataset":0.0235291676,"dev-research":0.2556067679,"prompt-eng":0.3871757598,"data-quality":0.1702746999,"ml-security":0.0585828674}}
{"text":"Lastly, we offer a brief summary of the applications of these methods and discuss potential future aspects related to our survey.","meta":{"url":"http://arxiv.org/abs/2307.16851v1"},"cats":{"new-dataset":0.1002310421,"dev-research":0.2125413032,"prompt-eng":0.4555014063,"data-quality":0.1414730345,"ml-security":0.0564182333}}
{"text":"For more information, please visit http://trustai.one.","meta":{"url":"http://arxiv.org/abs/2307.16851v1"},"cats":{"new-dataset":0.3359706386,"dev-research":0.2386182903,"prompt-eng":0.3697165963,"data-quality":0.1647928003,"ml-security":0.1175191083}}
{"text":"As people's daily life becomes increasingly inseparable from various mobile electronic devices, relevant service application platforms and network operators can collect numerous individual information easily.","meta":{"url":"http://arxiv.org/abs/2307.16849v1"},"cats":{"new-dataset":0.0677961811,"dev-research":0.3071887909,"prompt-eng":0.3924154546,"data-quality":0.1333048738,"ml-security":0.167665198}}
{"text":"When releasing these data for scientific research or commercial purposes, users' privacy will be in danger, especially in the publication of spatiotemporal trajectory datasets.","meta":{"url":"http://arxiv.org/abs/2307.16849v1"},"cats":{"new-dataset":0.637276382,"dev-research":0.2206705818,"prompt-eng":0.3269634702,"data-quality":0.1491138463,"ml-security":0.5420310296}}
{"text":"Therefore, to avoid the leakage of users' privacy, it is necessary to anonymize the data before they are released.","meta":{"url":"http://arxiv.org/abs/2307.16849v1"},"cats":{"new-dataset":0.0430138494,"dev-research":0.3505206065,"prompt-eng":0.3058537087,"data-quality":0.155715017,"ml-security":0.5097537018}}
{"text":"However, more than simply removing the unique identifiers of individuals is needed to protect the trajectory privacy, because some attackers may infer the identity of users by the connection with other databases.","meta":{"url":"http://arxiv.org/abs/2307.16849v1"},"cats":{"new-dataset":0.0120460969,"dev-research":0.2613857849,"prompt-eng":0.3882223273,"data-quality":0.1690183599,"ml-security":0.4814582844}}
{"text":"Much work has been devoted to merging multiple trajectories to avoid re-identification, but these solutions always require sacrificing data quality to achieve the anonymity requirement.","meta":{"url":"http://arxiv.org/abs/2307.16849v1"},"cats":{"new-dataset":0.1994051139,"dev-research":0.2230230746,"prompt-eng":0.3853335004,"data-quality":0.2226230728,"ml-security":0.1582640922}}
{"text":"In order to provide sufficient privacy protection for users' trajectory datasets, this paper develops a study on trajectory privacy against re-identification attacks, proposing a trajectory K-anonymity model based on Point Density and Partition (KPDP).","meta":{"url":"http://arxiv.org/abs/2307.16849v1"},"cats":{"new-dataset":0.1787048543,"dev-research":0.1995788299,"prompt-eng":0.3395708773,"data-quality":0.1537482632,"ml-security":0.6291359112}}
{"text":"Our approach improves the existing trajectory generalization anonymization techniques regarding trajectory set partition preprocessing and trajectory clustering algorithms.","meta":{"url":"http://arxiv.org/abs/2307.16849v1"},"cats":{"new-dataset":0.2000788465,"dev-research":0.2129700104,"prompt-eng":0.3574427952,"data-quality":0.1714104467,"ml-security":0.2959878545}}
{"text":"It successfully resists re-identification attacks and reduces the data utility loss of the k-anonymized dataset.","meta":{"url":"http://arxiv.org/abs/2307.16849v1"},"cats":{"new-dataset":0.1346008255,"dev-research":0.2464585895,"prompt-eng":0.3397252823,"data-quality":0.2632958296,"ml-security":0.4850360998}}
{"text":"A series of experiments on a real-world dataset show that the proposed model has significant advantages in terms of higher data utility and shorter algorithm execution time than other existing techniques.","meta":{"url":"http://arxiv.org/abs/2307.16849v1"},"cats":{"new-dataset":0.1778843895,"dev-research":0.2784453111,"prompt-eng":0.3670656896,"data-quality":0.1062555369,"ml-security":0.1464588191}}
{"text":"Ultra-wideband (UWB) time difference of arrival(TDOA)-based localization has emerged as a low-cost and scalable indoor positioning solution.","meta":{"url":"http://arxiv.org/abs/2307.16848v1"},"cats":{"new-dataset":0.027679178,"dev-research":0.2654025092,"prompt-eng":0.3521011694,"data-quality":0.1037101458,"ml-security":0.0681612067}}
{"text":"However, in cluttered environments, the performance of UWB TDOA-based localization deteriorates due to the biased and non-Gaussian noise distributions induced by obstacles.","meta":{"url":"http://arxiv.org/abs/2307.16848v1"},"cats":{"new-dataset":0.0375336313,"dev-research":0.2639351882,"prompt-eng":0.3991466428,"data-quality":0.1664532599,"ml-security":0.1274598307}}
{"text":"In this work, we present a bi-level optimization-based joint localization and noise model learning algorithm to address this problem.","meta":{"url":"http://arxiv.org/abs/2307.16848v1"},"cats":{"new-dataset":0.1507988492,"dev-research":0.2246604352,"prompt-eng":0.4075762002,"data-quality":0.2912496516,"ml-security":0.0798822934}}
{"text":"In particular, we use a Gaussian mixture model (GMM) to approximate the measurement noise distribution.","meta":{"url":"http://arxiv.org/abs/2307.16848v1"},"cats":{"new-dataset":0.055583223,"dev-research":0.2313768305,"prompt-eng":0.4267262232,"data-quality":0.2877779474,"ml-security":0.1021770282}}
{"text":"We explicitly incorporate the estimated state's uncertainty into the GMM noise model learning, referred to as uncertainty-aware GMM, to improve both noise modeling and localization performance.","meta":{"url":"http://arxiv.org/abs/2307.16848v1"},"cats":{"new-dataset":0.0998784617,"dev-research":0.2057998538,"prompt-eng":0.4366749116,"data-quality":0.3597076051,"ml-security":0.1353303845}}
{"text":"We first evaluate the GMM noise model learning and localization performance in numerous simulation scenarios.","meta":{"url":"http://arxiv.org/abs/2307.16848v1"},"cats":{"new-dataset":0.0846116404,"dev-research":0.2064199631,"prompt-eng":0.4268142555,"data-quality":0.3239460491,"ml-security":0.1563461549}}
{"text":"We then demonstrate the effectiveness of our algorithm in extensive real-world experiments using two different cluttered environments.","meta":{"url":"http://arxiv.org/abs/2307.16848v1"},"cats":{"new-dataset":0.1080551404,"dev-research":0.252992866,"prompt-eng":0.4459113756,"data-quality":0.1729236827,"ml-security":0.1043986007}}
{"text":"We show that our algorithm provides accurate position estimates with low-cost UWB sensors, no prior knowledge about the obstacles in the space, and a significant amount of UWB radios occluded.","meta":{"url":"http://arxiv.org/abs/2307.16848v1"},"cats":{"new-dataset":0.24701524,"dev-research":0.2129835914,"prompt-eng":0.4287716467,"data-quality":0.1336611338,"ml-security":0.0755600076}}
{"text":"Limited availability of labeled data for machine learning on biomedical time-series hampers progress in the field.","meta":{"url":"http://arxiv.org/abs/2307.16847v1"},"cats":{"new-dataset":0.1820718191,"dev-research":0.1753543574,"prompt-eng":0.3486886969,"data-quality":0.245985617,"ml-security":0.1685512219}}
{"text":"Self-supervised learning (SSL) is a promising approach to learning data representations without labels.","meta":{"url":"http://arxiv.org/abs/2307.16847v1"},"cats":{"new-dataset":0.094511086,"dev-research":0.2603813768,"prompt-eng":0.3889394681,"data-quality":0.3177931468,"ml-security":0.2561318448}}
{"text":"However, current SSL methods require expensive computations for negative pairs and are designed for single modalities, limiting their versatility.","meta":{"url":"http://arxiv.org/abs/2307.16847v1"},"cats":{"new-dataset":0.0116180686,"dev-research":0.2642026237,"prompt-eng":0.351459615,"data-quality":0.1137410801,"ml-security":0.185035429}}
{"text":"To overcome these limitations, we introduce CroSSL (Cross-modal SSL).","meta":{"url":"http://arxiv.org/abs/2307.16847v1"},"cats":{"new-dataset":0.1263114633,"dev-research":0.2352628822,"prompt-eng":0.425276594,"data-quality":0.1230258193,"ml-security":0.1662697447}}
{"text":"CroSSL introduces two novel concepts: masking intermediate embeddings from modality-specific encoders and aggregating them into a global embedding using a cross-modal aggregator.","meta":{"url":"http://arxiv.org/abs/2307.16847v1"},"cats":{"new-dataset":0.122542069,"dev-research":0.2396248808,"prompt-eng":0.3814299492,"data-quality":0.2007424236,"ml-security":0.1199477548}}
{"text":"This enables the handling of missing modalities and end-to-end learning of cross-modal patterns without prior data preprocessing or time-consuming negative-pair sampling.","meta":{"url":"http://arxiv.org/abs/2307.16847v1"},"cats":{"new-dataset":0.1105497388,"dev-research":0.2481880414,"prompt-eng":0.4207849562,"data-quality":0.2886651156,"ml-security":0.1116983089}}
{"text":"We evaluate CroSSL on various multimodal time-series benchmarks, including both medical-grade and consumer biosignals.","meta":{"url":"http://arxiv.org/abs/2307.16847v1"},"cats":{"new-dataset":0.2229380783,"dev-research":0.2101811849,"prompt-eng":0.4090757598,"data-quality":0.1021866992,"ml-security":0.070435652}}
{"text":"Our results demonstrate superior performance compared to previous SSL techniques and supervised benchmarks with minimal labeled data.","meta":{"url":"http://arxiv.org/abs/2307.16847v1"},"cats":{"new-dataset":0.2181253898,"dev-research":0.251430862,"prompt-eng":0.4054695535,"data-quality":0.3038197787,"ml-security":0.1952219538}}
{"text":"We additionally analyze the impact of different masking ratios and strategies and assess the robustness of the learned representations to missing modalities.","meta":{"url":"http://arxiv.org/abs/2307.16847v1"},"cats":{"new-dataset":0.0420818879,"dev-research":0.2379932456,"prompt-eng":0.420361512,"data-quality":0.3825073271,"ml-security":0.2845552897}}
{"text":"Overall, our work achieves state-of-the-art performance while highlighting the benefits of masking latent embeddings for cross-modal learning in temporal health data.","meta":{"url":"http://arxiv.org/abs/2307.16847v1"},"cats":{"new-dataset":0.2380606778,"dev-research":0.2043386734,"prompt-eng":0.3945262249,"data-quality":0.1714673172,"ml-security":0.1494141207}}
{"text":"Current approaches to identifying driving heterogeneity face challenges in capturing the diversity of driving characteristics and understanding the fundamental patterns from a driving behaviour mechanism standpoint.","meta":{"url":"http://arxiv.org/abs/2307.16843v1"},"cats":{"new-dataset":0.0308593333,"dev-research":0.3104584991,"prompt-eng":0.3831648156,"data-quality":0.1607031544,"ml-security":0.2138527482}}
{"text":"This study introduces a comprehensive framework for identifying driving heterogeneity from an Action-chain perspective.","meta":{"url":"http://arxiv.org/abs/2307.16843v1"},"cats":{"new-dataset":0.0175456302,"dev-research":0.2660897569,"prompt-eng":0.3515613331,"data-quality":0.1117796074,"ml-security":0.1355194671}}
{"text":"First, a rule-based segmentation technique that considers the physical meanings of driving behaviour is proposed.","meta":{"url":"http://arxiv.org/abs/2307.16843v1"},"cats":{"new-dataset":0.0435154994,"dev-research":0.3363692268,"prompt-eng":0.4019599138,"data-quality":0.2093395246,"ml-security":0.108221463}}
{"text":"Next, an Action phase Library including descriptions of various driving behaviour patterns is created based on the segmentation findings.","meta":{"url":"http://arxiv.org/abs/2307.16843v1"},"cats":{"new-dataset":0.2632738432,"dev-research":0.2974104373,"prompt-eng":0.4470849325,"data-quality":0.1688102249,"ml-security":0.1016442972}}
{"text":"The Action-chain concept is then introduced by implementing Action phase transition probability, followed by a method for evaluating driving heterogeneity.","meta":{"url":"http://arxiv.org/abs/2307.16843v1"},"cats":{"new-dataset":0.0163415189,"dev-research":0.219596734,"prompt-eng":0.42659984,"data-quality":0.0775614133,"ml-security":0.067430779}}
{"text":"Employing real-world datasets for evaluation, our approach effectively identifies driving heterogeneity for both individual drivers and traffic flow while providing clear interpretations.","meta":{"url":"http://arxiv.org/abs/2307.16843v1"},"cats":{"new-dataset":0.1940933075,"dev-research":0.2906748356,"prompt-eng":0.3500797912,"data-quality":0.1740827065,"ml-security":0.1582657084}}
{"text":"These insights can aid the development of accurate driving behaviour theory and traffic flow models, ultimately benefiting traffic performance, and potentially leading to aspects such as improved road capacity and safety.","meta":{"url":"http://arxiv.org/abs/2307.16843v1"},"cats":{"new-dataset":0.0229950101,"dev-research":0.2794731403,"prompt-eng":0.3581603914,"data-quality":0.0938137891,"ml-security":0.2074383989}}
{"text":"We study Linear Temporal Logic Modulo Theories over Finite Traces (LTLfMT), a recently introduced extension of LTL over finite traces (LTLf) where propositions are replaced by first-order formulas and where first-order variables referring to different time points can be compared.","meta":{"url":"http://arxiv.org/abs/2307.16840v1"},"cats":{"new-dataset":0.0549673319,"dev-research":0.2625103424,"prompt-eng":0.3876234908,"data-quality":0.0705275167,"ml-security":0.0827004695}}
{"text":"In general, LTLfMT was shown to be semi-decidable for any decidable first-order theory (e.g., linear arithmetics), with a tableau-based semi-decision procedure.   ","meta":{"url":"http://arxiv.org/abs/2307.16840v1"},"cats":{"new-dataset":0.0271612621,"dev-research":0.1867397002,"prompt-eng":0.3844227904,"data-quality":0.086371792,"ml-security":0.0927746377}}
{"text":"In this paper we present a sound and complete pruning rule for the LTLfMT tableau.","meta":{"url":"http://arxiv.org/abs/2307.16840v1"},"cats":{"new-dataset":0.1340966547,"dev-research":0.197142959,"prompt-eng":0.4201562066,"data-quality":0.1867026824,"ml-security":0.0648414655}}
{"text":"We show that for any LTLfMT formula that satisfies an abstract, semantic condition, that we call finite memory, the tableau augmented with the new rule is also guaranteed to terminate.","meta":{"url":"http://arxiv.org/abs/2307.16840v1"},"cats":{"new-dataset":0.0499629195,"dev-research":0.2292925025,"prompt-eng":0.3848189408,"data-quality":0.1737988206,"ml-security":0.10818776}}
{"text":"Last but not least, this technique allows us to establish novel decidability results for the satisfiability of several fragments of LTLfMT, as well as to give new decidability proofs for classes that are already known.","meta":{"url":"http://arxiv.org/abs/2307.16840v1"},"cats":{"new-dataset":0.0800396693,"dev-research":0.2245403841,"prompt-eng":0.4049156354,"data-quality":0.2113294498,"ml-security":0.1174347174}}
{"text":"Accuracy measures such as Recall, Precision, and Hit Rate have been a standard way of evaluating Recommendation Systems.","meta":{"url":"http://arxiv.org/abs/2307.16832v1"},"cats":{"new-dataset":0.008930927,"dev-research":0.3309049654,"prompt-eng":0.4354880834,"data-quality":0.2717154143,"ml-security":0.0883257925}}
{"text":"The assumption is to use a fixed Top-N to represent them.","meta":{"url":"http://arxiv.org/abs/2307.16832v1"},"cats":{"new-dataset":0.0118533328,"dev-research":0.2444602845,"prompt-eng":0.4039619952,"data-quality":0.2009034067,"ml-security":0.0949574838}}
{"text":"We propose that median impressions viewed from historical sessions per diner be used as a personalized value for N.","meta":{"url":"http://arxiv.org/abs/2307.16832v1"},"cats":{"new-dataset":0.0656630103,"dev-research":0.2236907445,"prompt-eng":0.4304876025,"data-quality":0.1634793212,"ml-security":0.0707817307}}
{"text":"We present preliminary exploratory results and list future steps to improve upon and evaluate the efficacy of these personalized metrics.","meta":{"url":"http://arxiv.org/abs/2307.16832v1"},"cats":{"new-dataset":0.1529988616,"dev-research":0.297665461,"prompt-eng":0.4746577922,"data-quality":0.2279297846,"ml-security":0.0942119155}}
{"text":"With sufficient paired training samples, the supervised deep learning methods have attracted much attention in image denoising because of their superior performance.","meta":{"url":"http://arxiv.org/abs/2307.16825v1"},"cats":{"new-dataset":0.0854592378,"dev-research":0.2766924517,"prompt-eng":0.3289713001,"data-quality":0.3019556831,"ml-security":0.1741961562}}
{"text":"However, it is still very challenging to widely utilize the supervised methods in real cases due to the lack of paired noisy-clean images.","meta":{"url":"http://arxiv.org/abs/2307.16825v1"},"cats":{"new-dataset":0.0567004137,"dev-research":0.2358213239,"prompt-eng":0.3727299293,"data-quality":0.373120686,"ml-security":0.1505692508}}
{"text":"Meanwhile, most self-supervised denoising methods are ineffective as well when applied to the real-world denoising tasks because of their strict assumptions in applications.","meta":{"url":"http://arxiv.org/abs/2307.16825v1"},"cats":{"new-dataset":0.0130466895,"dev-research":0.2393445692,"prompt-eng":0.3559315685,"data-quality":0.4720125652,"ml-security":0.2966138408}}
{"text":"For example, as a typical method for self-supervised denoising, the original blind spot network (BSN) assumes that the noise is pixel-wise independent, which is much different from the real cases.","meta":{"url":"http://arxiv.org/abs/2307.16825v1"},"cats":{"new-dataset":0.0130343003,"dev-research":0.2932395942,"prompt-eng":0.2968189317,"data-quality":0.3301106369,"ml-security":0.2567230231}}
{"text":"To solve this problem, we propose a novel self-supervised real image denoising framework named Sampling Difference As Perturbation (SDAP) based on Random Sub-samples Generation (RSG) with a cyclic sample difference loss.","meta":{"url":"http://arxiv.org/abs/2307.16825v1"},"cats":{"new-dataset":0.2383314771,"dev-research":0.2182502013,"prompt-eng":0.346995814,"data-quality":0.3546614569,"ml-security":0.1466900362}}
{"text":"Specifically, we dig deeper into the properties of BSN to make it more suitable for real noise.","meta":{"url":"http://arxiv.org/abs/2307.16825v1"},"cats":{"new-dataset":0.0480514249,"dev-research":0.2687424396,"prompt-eng":0.3535933534,"data-quality":0.1989586209,"ml-security":0.1820860895}}
{"text":"Surprisingly, we find that adding an appropriate perturbation to the training images can effectively improve the performance of BSN.","meta":{"url":"http://arxiv.org/abs/2307.16825v1"},"cats":{"new-dataset":0.0591528818,"dev-research":0.240866193,"prompt-eng":0.3911639202,"data-quality":0.2585440907,"ml-security":0.23388886}}
{"text":"Further, we propose that the sampling difference can be considered as perturbation to achieve better results.","meta":{"url":"http://arxiv.org/abs/2307.16825v1"},"cats":{"new-dataset":0.0187234307,"dev-research":0.1816865902,"prompt-eng":0.4001816851,"data-quality":0.2605826553,"ml-security":0.1108680522}}
{"text":"Finally we propose a new BSN framework in combination with our RSG strategy.","meta":{"url":"http://arxiv.org/abs/2307.16825v1"},"cats":{"new-dataset":0.1763786967,"dev-research":0.2046615539,"prompt-eng":0.4148967817,"data-quality":0.0951993853,"ml-security":0.0974254001}}
{"text":"The results show that it significantly outperforms other state-of-the-art self-supervised denoising methods on real-world datasets.","meta":{"url":"http://arxiv.org/abs/2307.16825v1"},"cats":{"new-dataset":0.2355683826,"dev-research":0.2616316064,"prompt-eng":0.3487149365,"data-quality":0.4159178908,"ml-security":0.2139265359}}
{"text":"The code is available at https://github.com/p1y2z3/SDAP.","meta":{"url":"http://arxiv.org/abs/2307.16825v1"},"cats":{"new-dataset":0.3188744675,"dev-research":0.2700021948,"prompt-eng":0.4456751986,"data-quality":0.1539822763,"ml-security":0.0599820002}}
{"text":"Cell-free massive multiple-input-multiple-output (CF-mMIMO) is a next-generation wireless access technology that offers superior coverage and spectral efficiency compared to conventional MIMO.","meta":{"url":"http://arxiv.org/abs/2307.16824v1"},"cats":{"new-dataset":0.0588359347,"dev-research":0.2319096034,"prompt-eng":0.3325852317,"data-quality":0.0732457316,"ml-security":0.0865818811}}
{"text":"With many future applications in unlicensed spectrum bands, networks will likely experience and may even be limited by out-of-system (OoS) interference.","meta":{"url":"http://arxiv.org/abs/2307.16824v1"},"cats":{"new-dataset":0.0111015443,"dev-research":0.1974091439,"prompt-eng":0.3060524826,"data-quality":0.1491178234,"ml-security":0.2727610262}}
{"text":"The OoS interference differs from the in-system interference from other serving users in that for OoS interference, the associated pilot signals are unknown or non-existent, which makes estimation of the OoS interferer channel difficult.   ","meta":{"url":"http://arxiv.org/abs/2307.16824v1"},"cats":{"new-dataset":0.0063773132,"dev-research":0.2773179516,"prompt-eng":0.3317462455,"data-quality":0.2331290615,"ml-security":0.1911282696}}
{"text":"In this paper, we propose a novel sequential algorithm for the suppression of OoS interference for uplink CF-mMIMO with a stripe (daisy-chain) topology.","meta":{"url":"http://arxiv.org/abs/2307.16824v1"},"cats":{"new-dataset":0.0405968671,"dev-research":0.2257845171,"prompt-eng":0.3665319556,"data-quality":0.1422267372,"ml-security":0.090593931}}
{"text":"The proposed method has comparable performance to that of a fully centralized interference rejection combining algorithm but has substantially less fronthaul load requirements.","meta":{"url":"http://arxiv.org/abs/2307.16824v1"},"cats":{"new-dataset":0.0130645894,"dev-research":0.2622534045,"prompt-eng":0.4048713185,"data-quality":0.2671929677,"ml-security":0.1407409298}}
{"text":"The Trusted Platform Module (TPM) is a cryptoprocessor designed to protect integrity and security of modern computers.","meta":{"url":"http://arxiv.org/abs/2307.16821v1"},"cats":{"new-dataset":0.1249944467,"dev-research":0.2696679438,"prompt-eng":0.4077675985,"data-quality":0.1102862534,"ml-security":0.3152158487}}
{"text":"Communications with the TPM go through the TPM Software Stack (TSS), a popular implementation of which is the open-source library tpm2-tss.","meta":{"url":"http://arxiv.org/abs/2307.16821v1"},"cats":{"new-dataset":0.3025224998,"dev-research":0.2827833816,"prompt-eng":0.407025812,"data-quality":0.1011220765,"ml-security":0.0899097734}}
{"text":"Vulnerabilities in its code could allow attackers to recover sensitive information and take control of the system.","meta":{"url":"http://arxiv.org/abs/2307.16821v1"},"cats":{"new-dataset":0.0468309231,"dev-research":0.4410715448,"prompt-eng":0.4063700879,"data-quality":0.1867043509,"ml-security":0.6681695783}}
{"text":"This paper describes a case study on formal verification of tpm2-tss using the Frama-C verification platform.","meta":{"url":"http://arxiv.org/abs/2307.16821v1"},"cats":{"new-dataset":0.100950072,"dev-research":0.2789396614,"prompt-eng":0.4843971432,"data-quality":0.2101972329,"ml-security":0.1188296298}}
{"text":"Heavily based on linked lists and complex data structures, the library code appears to be highly challenging for the verification tool.","meta":{"url":"http://arxiv.org/abs/2307.16821v1"},"cats":{"new-dataset":0.2273490039,"dev-research":0.4279266218,"prompt-eng":0.4453505253,"data-quality":0.2668796446,"ml-security":0.1169181447}}
{"text":"We present several issues and limitations we faced, illustrate them with examples and present solutions that allowed us to verify functional properties and the absence of runtime errors for a representative subset of functions.","meta":{"url":"http://arxiv.org/abs/2307.16821v1"},"cats":{"new-dataset":0.0636689255,"dev-research":0.2672556865,"prompt-eng":0.4055108715,"data-quality":0.2292155548,"ml-security":0.1519722067}}
{"text":"We describe verification results and desired tool improvements necessary to achieve a full formal verification of the target code.","meta":{"url":"http://arxiv.org/abs/2307.16821v1"},"cats":{"new-dataset":0.1502770827,"dev-research":0.4236807797,"prompt-eng":0.5296468421,"data-quality":0.3359846429,"ml-security":0.0922766627}}
{"text":"Neural ranking models (NRMs) have undergone significant development and have become integral components of information retrieval (IR) systems.","meta":{"url":"http://arxiv.org/abs/2307.16816v1"},"cats":{"new-dataset":0.0455383083,"dev-research":0.1666173562,"prompt-eng":0.4104386333,"data-quality":0.2112618883,"ml-security":0.0951767455}}
{"text":"Unfortunately, recent research has unveiled the vulnerability of NRMs to adversarial document manipulations, potentially exploited by malicious search engine optimization practitioners.","meta":{"url":"http://arxiv.org/abs/2307.16816v1"},"cats":{"new-dataset":0.0374259771,"dev-research":0.1970456437,"prompt-eng":0.3984569246,"data-quality":0.3328061461,"ml-security":0.6308709788}}
{"text":"While progress in adversarial attack strategies aids in identifying the potential weaknesses of NRMs before their deployment, the defensive measures against such attacks, like the detection of adversarial documents, remain inadequately explored.","meta":{"url":"http://arxiv.org/abs/2307.16816v1"},"cats":{"new-dataset":0.0314040352,"dev-research":0.2521893251,"prompt-eng":0.365234263,"data-quality":0.3360576462,"ml-security":0.7965822505}}
{"text":"To mitigate this gap, this paper establishes a benchmark dataset to facilitate the investigation of adversarial ranking defense and introduces two types of detection tasks for adversarial documents.","meta":{"url":"http://arxiv.org/abs/2307.16816v1"},"cats":{"new-dataset":0.3511657071,"dev-research":0.2664103185,"prompt-eng":0.373905239,"data-quality":0.3755673948,"ml-security":0.7247866011}}
{"text":"A comprehensive investigation of the performance of several detection baselines is conducted, which involve examining the spamicity, perplexity, and linguistic acceptability, and utilizing supervised classifiers.","meta":{"url":"http://arxiv.org/abs/2307.16816v1"},"cats":{"new-dataset":0.1012304823,"dev-research":0.3380233549,"prompt-eng":0.4562179388,"data-quality":0.6369134755,"ml-security":0.1765404413}}
{"text":"Experimental results demonstrate that a supervised classifier can effectively mitigate known attacks, but it performs poorly against unseen attacks.","meta":{"url":"http://arxiv.org/abs/2307.16816v1"},"cats":{"new-dataset":0.0312562827,"dev-research":0.368171657,"prompt-eng":0.4225225082,"data-quality":0.5184271763,"ml-security":0.9006369425}}
{"text":"Furthermore, such classifier should avoid using query text to prevent learning the classification on relevance, as it might lead to the inadvertent discarding of relevant documents.","meta":{"url":"http://arxiv.org/abs/2307.16816v1"},"cats":{"new-dataset":0.0199335445,"dev-research":0.2454580792,"prompt-eng":0.3621460182,"data-quality":0.4051765063,"ml-security":0.3365985613}}
{"text":"Video Quality Assessment (VQA), which aims to predict the perceptual quality of a video, has attracted raising attention with the rapid development of streaming media technology, such as Facebook, TikTok, Kwai, and so on.","meta":{"url":"http://arxiv.org/abs/2307.16813v1"},"cats":{"new-dataset":0.0871458437,"dev-research":0.2715435902,"prompt-eng":0.3303300642,"data-quality":0.2180061707,"ml-security":0.0931336868}}
{"text":"Compared with other sequence-based visual tasks (\\textit{e.g.,} action recognition), VQA faces two under-estimated challenges unresolved in User Generated Content (UGC) videos.","meta":{"url":"http://arxiv.org/abs/2307.16813v1"},"cats":{"new-dataset":0.2514462456,"dev-research":0.2825388697,"prompt-eng":0.3952561848,"data-quality":0.2573683754,"ml-security":0.116472112}}
{"text":"\\textit{First}, it is not rare that several frames containing serious distortions (\\textit{e.g.,}blocking, blurriness), can determine the perceptual quality of the whole video, while other sequence-based tasks require more frames of equal importance for representations.","meta":{"url":"http://arxiv.org/abs/2307.16813v1"},"cats":{"new-dataset":0.031340382,"dev-research":0.3314342012,"prompt-eng":0.3476475912,"data-quality":0.3705295928,"ml-security":0.0671476213}}
{"text":"\\textit{Second}, the perceptual quality of a video exhibits a multi-distortion distribution, due to the differences in the duration and probability of occurrence for various distortions.","meta":{"url":"http://arxiv.org/abs/2307.16813v1"},"cats":{"new-dataset":0.0608587966,"dev-research":0.2579816019,"prompt-eng":0.3644127131,"data-quality":0.3932656755,"ml-security":0.0613518366}}
{"text":"In order to solve the above challenges, we propose \\textit{Visual Quality Transformer (VQT)} to extract quality-related sparse features more efficiently.","meta":{"url":"http://arxiv.org/abs/2307.16813v1"},"cats":{"new-dataset":0.1177444133,"dev-research":0.3239885228,"prompt-eng":0.3736614095,"data-quality":0.408783149,"ml-security":0.0978963298}}
{"text":"Methodologically, a Sparse Temporal Attention (STA) is proposed to sample keyframes by analyzing the temporal correlation between frames, which reduces the computational complexity from $O(T^2)$ to $O(T \\log T)$. Structurally, a Multi-Pathway Temporal Network (MPTN) utilizes multiple STA modules with different degrees of sparsity in parallel, capturing co-existing distortions in a video.","meta":{"url":"http://arxiv.org/abs/2307.16813v1"},"cats":{"new-dataset":0.1910847662,"dev-research":0.2071568906,"prompt-eng":0.3511599129,"data-quality":0.1537717584,"ml-security":0.0657530728}}
{"text":"Experimentally, VQT demonstrates superior performance than many \\textit{state-of-the-art} methods in three public no-reference VQA datasets.","meta":{"url":"http://arxiv.org/abs/2307.16813v1"},"cats":{"new-dataset":0.1690868086,"dev-research":0.278120507,"prompt-eng":0.3714465499,"data-quality":0.238849882,"ml-security":0.1043531787}}
{"text":"Furthermore, VQT shows better performance in four full-reference VQA datasets against widely-adopted industrial algorithms (\\textit{i.e.,} VMAF and AVQT).","meta":{"url":"http://arxiv.org/abs/2307.16813v1"},"cats":{"new-dataset":0.1020905604,"dev-research":0.300771172,"prompt-eng":0.3211407333,"data-quality":0.2169791244,"ml-security":0.0990511193}}
{"text":"Public figures receive a disproportionate amount of abuse on social media, impacting their active participation in public life.","meta":{"url":"http://arxiv.org/abs/2307.16811v1"},"cats":{"new-dataset":0.0643149126,"dev-research":0.2251793512,"prompt-eng":0.3068635429,"data-quality":0.2066455406,"ml-security":0.26593981}}
{"text":"Automated systems can identify abuse at scale but labelling training data is expensive, complex and potentially harmful.","meta":{"url":"http://arxiv.org/abs/2307.16811v1"},"cats":{"new-dataset":0.0572108443,"dev-research":0.3791049325,"prompt-eng":0.3799906373,"data-quality":0.4561475274,"ml-security":0.6225079319}}
{"text":"So, it is desirable that systems are efficient and generalisable, handling both shared and specific aspects of online abuse.","meta":{"url":"http://arxiv.org/abs/2307.16811v1"},"cats":{"new-dataset":0.0029211257,"dev-research":0.3564302898,"prompt-eng":0.3690534426,"data-quality":0.1233123307,"ml-security":0.3705597545}}
{"text":"We explore the dynamics of cross-group text classification in order to understand how well classifiers trained on one domain or demographic can transfer to others, with a view to building more generalisable abuse classifiers.","meta":{"url":"http://arxiv.org/abs/2307.16811v1"},"cats":{"new-dataset":0.058129313,"dev-research":0.2485632784,"prompt-eng":0.3655418296,"data-quality":0.3716283073,"ml-security":0.5974774437}}
{"text":"We fine-tune language models to classify tweets targeted at public figures across DOmains (sport and politics) and DemOgraphics (women and men) using our novel DODO dataset, containing 28,000 labelled entries, split equally across four domain-demographic pairs.","meta":{"url":"http://arxiv.org/abs/2307.16811v1"},"cats":{"new-dataset":0.5336889174,"dev-research":0.1887824428,"prompt-eng":0.3670725943,"data-quality":0.3219468343,"ml-security":0.2081495594}}
{"text":"We find that (i) small amounts of diverse data are hugely beneficial to generalisation and model adaptation; (ii) models transfer more easily across demographics but models trained on cross-domain data are more generalisable; (iii) some groups contribute more to generalisability than others; and (iv) dataset similarity is a signal of transferability.","meta":{"url":"http://arxiv.org/abs/2307.16811v1"},"cats":{"new-dataset":0.0461200966,"dev-research":0.1971987689,"prompt-eng":0.33702728,"data-quality":0.1720485794,"ml-security":0.2028074811}}
{"text":"In this technical report, we present our findings from the research conducted on the Human-Object Interaction 4D (HOI4D) dataset for egocentric action segmentation task.","meta":{"url":"http://arxiv.org/abs/2307.16803v1"},"cats":{"new-dataset":0.4774961316,"dev-research":0.238913548,"prompt-eng":0.4241620574,"data-quality":0.132734731,"ml-security":0.0594234694}}
{"text":"As a relatively novel research area, point cloud video methods might not be good at temporal modeling, especially for long point cloud videos (\\eg, 150 frames).","meta":{"url":"http://arxiv.org/abs/2307.16803v1"},"cats":{"new-dataset":0.0715443048,"dev-research":0.1990161098,"prompt-eng":0.3215152511,"data-quality":0.1063097525,"ml-security":0.0696564089}}
{"text":"In contrast, traditional video understanding methods have been well developed.","meta":{"url":"http://arxiv.org/abs/2307.16803v1"},"cats":{"new-dataset":0.0525950444,"dev-research":0.3738905577,"prompt-eng":0.3042957095,"data-quality":0.1866511611,"ml-security":0.0472376808}}
{"text":"Their effectiveness on temporal modeling has been widely verified on many large scale video datasets.","meta":{"url":"http://arxiv.org/abs/2307.16803v1"},"cats":{"new-dataset":0.2598523954,"dev-research":0.2205061284,"prompt-eng":0.3634954863,"data-quality":0.1947827282,"ml-security":0.0876083272}}
{"text":"Therefore, we convert point cloud videos into depth videos and employ traditional video modeling methods to improve 4D action segmentation.","meta":{"url":"http://arxiv.org/abs/2307.16803v1"},"cats":{"new-dataset":0.1423568364,"dev-research":0.2115697121,"prompt-eng":0.3561148467,"data-quality":0.1030258341,"ml-security":0.0622341823}}
{"text":"By ensembling depth and point cloud video methods, the accuracy is significantly improved.","meta":{"url":"http://arxiv.org/abs/2307.16803v1"},"cats":{"new-dataset":0.0939223818,"dev-research":0.2588227431,"prompt-eng":0.3455779592,"data-quality":0.2552928761,"ml-security":0.0727978058}}
{"text":"The proposed method, named Mixture of Depth and Point cloud video experts (DPMix), achieved the first place in the 4D Action Segmentation Track of the HOI4D Challenge 2023.","meta":{"url":"http://arxiv.org/abs/2307.16803v1"},"cats":{"new-dataset":0.3012830293,"dev-research":0.2015152194,"prompt-eng":0.4006303158,"data-quality":0.1436981996,"ml-security":0.058268012}}
{"text":"Large-scale pre-training has made progress in many fields of natural language processing, though little is understood about the design of pre-training datasets.","meta":{"url":"http://arxiv.org/abs/2307.16795v1"},"cats":{"new-dataset":0.3723832182,"dev-research":0.2507928903,"prompt-eng":0.370883203,"data-quality":0.2652703507,"ml-security":0.1232090617}}
{"text":"We propose a methodology for obtaining a quantitative understanding of structural overlap between machine translation tasks.","meta":{"url":"http://arxiv.org/abs/2307.16795v1"},"cats":{"new-dataset":0.0848584255,"dev-research":0.3025461226,"prompt-eng":0.4411625187,"data-quality":0.2470490266,"ml-security":0.0549763892}}
{"text":"We apply our methodology to the natural language to Bash semantic parsing task (NLBash) and show that it is largely reducible to lexical alignment.","meta":{"url":"http://arxiv.org/abs/2307.16795v1"},"cats":{"new-dataset":0.2098929414,"dev-research":0.3154545028,"prompt-eng":0.4916151395,"data-quality":0.2835009602,"ml-security":0.0713449289}}
{"text":"We also find that there is strong structural overlap between NLBash and natural language to SQL.","meta":{"url":"http://arxiv.org/abs/2307.16795v1"},"cats":{"new-dataset":0.1247655543,"dev-research":0.29227993,"prompt-eng":0.3878531621,"data-quality":0.2346569358,"ml-security":0.0930857136}}
{"text":"Additionally, we perform a study varying compute expended during pre-training on the English to German machine translation task and find that more compute expended during pre-training does not always correspond semantic representations with stronger transfer to NLBash.","meta":{"url":"http://arxiv.org/abs/2307.16795v1"},"cats":{"new-dataset":0.0833736384,"dev-research":0.2585026658,"prompt-eng":0.420690753,"data-quality":0.2637158009,"ml-security":0.1129845361}}
{"text":"Despite the advancements of open-source large language models (LLMs) and their variants, e.g., LLaMA and Vicuna, they remain significantly limited in performing higher-level tasks, such as following human instructions to use external tools (APIs).","meta":{"url":"http://arxiv.org/abs/2307.16789v1"},"cats":{"new-dataset":0.1022859348,"dev-research":0.2210280749,"prompt-eng":0.3749578366,"data-quality":0.1140062761,"ml-security":0.1314225412}}
{"text":"This is because current instruction tuning largely focuses on basic language tasks instead of the tool-use domain.","meta":{"url":"http://arxiv.org/abs/2307.16789v1"},"cats":{"new-dataset":0.0093140813,"dev-research":0.4434450536,"prompt-eng":0.3783306141,"data-quality":0.2060036185,"ml-security":0.0817086788}}
{"text":"This is in contrast to state-of-the-art (SOTA) LLMs, e.g., ChatGPT, which have demonstrated excellent tool-use capabilities but are unfortunately closed source.","meta":{"url":"http://arxiv.org/abs/2307.16789v1"},"cats":{"new-dataset":0.2968537731,"dev-research":0.309442296,"prompt-eng":0.4514122307,"data-quality":0.1493719708,"ml-security":0.0691119904}}
{"text":"To facilitate tool-use capabilities within open-source LLMs, we introduce ToolLLM, a general tool-use framework of data construction, model training and evaluation.","meta":{"url":"http://arxiv.org/abs/2307.16789v1"},"cats":{"new-dataset":0.2996162059,"dev-research":0.3792371854,"prompt-eng":0.4207670561,"data-quality":0.1585883386,"ml-security":0.1066757021}}
{"text":"We first present ToolBench, an instruction-tuning dataset for tool use, which is created automatically using ChatGPT.","meta":{"url":"http://arxiv.org/abs/2307.16789v1"},"cats":{"new-dataset":0.7363953399,"dev-research":0.4272815134,"prompt-eng":0.449399235,"data-quality":0.1795456335,"ml-security":0.0788914167}}
{"text":"Specifically, we collect 16,464 real-world RESTful APIs spanning 49 categories from RapidAPI Hub, then prompt ChatGPT to generate diverse human instructions involving these APIs, covering both single-tool and multi-tool scenarios.","meta":{"url":"http://arxiv.org/abs/2307.16789v1"},"cats":{"new-dataset":0.4878796649,"dev-research":0.3098221131,"prompt-eng":0.4044027195,"data-quality":0.1159722387,"ml-security":0.0811110889}}
{"text":"Finally, we use ChatGPT to search for a valid solution path (chain of API calls) for each instruction.","meta":{"url":"http://arxiv.org/abs/2307.16789v1"},"cats":{"new-dataset":0.1471101985,"dev-research":0.3742825292,"prompt-eng":0.4559119564,"data-quality":0.1734819411,"ml-security":0.1146192101}}
{"text":"To make the searching process more efficient, we develop a novel depth-first search-based decision tree (DFSDT), enabling LLMs to evaluate multiple reasoning traces and expand the search space.","meta":{"url":"http://arxiv.org/abs/2307.16789v1"},"cats":{"new-dataset":0.1258893125,"dev-research":0.3673738944,"prompt-eng":0.4594003868,"data-quality":0.1634487263,"ml-security":0.113108134}}
{"text":"We show that DFSDT significantly enhances the planning and reasoning capabilities of LLMs.","meta":{"url":"http://arxiv.org/abs/2307.16789v1"},"cats":{"new-dataset":0.1807807549,"dev-research":0.3577527738,"prompt-eng":0.44648437,"data-quality":0.096475666,"ml-security":0.0862067855}}
{"text":"For efficient tool-use assessment, we develop an automatic evaluator: ToolEval.","meta":{"url":"http://arxiv.org/abs/2307.16789v1"},"cats":{"new-dataset":0.0661068626,"dev-research":0.4851130189,"prompt-eng":0.5218768328,"data-quality":0.239883054,"ml-security":0.1261680122}}
{"text":"We fine-tune LLaMA on ToolBench and obtain ToolLLaMA.","meta":{"url":"http://arxiv.org/abs/2307.16789v1"},"cats":{"new-dataset":0.3386530221,"dev-research":0.2546892969,"prompt-eng":0.412365136,"data-quality":0.1880649517,"ml-security":0.0839911357}}
{"text":"Our ToolEval reveals that ToolLLaMA demonstrates a remarkable ability to execute complex instructions and generalize to unseen APIs, and exhibits comparable performance to ChatGPT.","meta":{"url":"http://arxiv.org/abs/2307.16789v1"},"cats":{"new-dataset":0.3949264177,"dev-research":0.4238265875,"prompt-eng":0.4245677314,"data-quality":0.1542335881,"ml-security":0.1118141155}}
{"text":"To make the pipeline more practical, we devise a neural API retriever to recommend appropriate APIs for each instruction, negating the need for manual API selection.","meta":{"url":"http://arxiv.org/abs/2307.16789v1"},"cats":{"new-dataset":0.0537106163,"dev-research":0.3029008687,"prompt-eng":0.4722659058,"data-quality":0.1680381266,"ml-security":0.1583710354}}
{"text":"The Defense Advanced Research Projects Agency (DARPA) OFFensive Swarm-Enabled Tactics program's goal of launching 250 unmanned aerial and ground vehicles from a limited sized launch zone was a daunting challenge.","meta":{"url":"http://arxiv.org/abs/2307.16788v1"},"cats":{"new-dataset":0.1667762499,"dev-research":0.2587745795,"prompt-eng":0.3740822593,"data-quality":0.0965651392,"ml-security":0.1829659154}}
{"text":"The swarm's aerial vehicles were primarily multirotor platforms, which can efficiently be launched en masse.","meta":{"url":"http://arxiv.org/abs/2307.16788v1"},"cats":{"new-dataset":0.0246781268,"dev-research":0.2237042069,"prompt-eng":0.3863395832,"data-quality":0.0538030389,"ml-security":0.1054118688}}
{"text":"Each field exercise expected the deployment of an even larger swarm.","meta":{"url":"http://arxiv.org/abs/2307.16788v1"},"cats":{"new-dataset":0.0693538987,"dev-research":0.187372518,"prompt-eng":0.4237301509,"data-quality":0.1410176147,"ml-security":0.0877734348}}
{"text":"While the launch zone's spatial area increased with each field exercise, the relative space for each vehicle was not necessarily increased, considering the increasing size of the swarm and the vehicles' associated GPS error; however, safe mission deployment and execution were expected.","meta":{"url":"http://arxiv.org/abs/2307.16788v1"},"cats":{"new-dataset":0.0826242056,"dev-research":0.2871386208,"prompt-eng":0.3585473439,"data-quality":0.1866252279,"ml-security":0.1588264844}}
{"text":"At the same time, achieving the mission goals required maximizing efficiency of the swarm's performance by reducing congestion that blocked vehicles from completing tactic assignments.","meta":{"url":"http://arxiv.org/abs/2307.16788v1"},"cats":{"new-dataset":0.0074022909,"dev-research":0.3126084844,"prompt-eng":0.425490078,"data-quality":0.0617889321,"ml-security":0.1169636006}}
{"text":"Congestion analysis conducted before the final field exercise focused on adjusting various constraints to optimize the swarm's deployment without reducing safety.","meta":{"url":"http://arxiv.org/abs/2307.16788v1"},"cats":{"new-dataset":0.1002468564,"dev-research":0.2832595719,"prompt-eng":0.4064419202,"data-quality":0.1445041314,"ml-security":0.1558133118}}
{"text":"During the field exercise, data was collected that permitted analyzing the number and durations of individual vehicle blockages' impact on the resulting congestion.","meta":{"url":"http://arxiv.org/abs/2307.16788v1"},"cats":{"new-dataset":0.3133782984,"dev-research":0.2564679961,"prompt-eng":0.3202360424,"data-quality":0.1219346137,"ml-security":0.1592867465}}
{"text":"After the field exercise, additional analyses used the mission plan to validate the use of simulation for analyzing congestion.","meta":{"url":"http://arxiv.org/abs/2307.16788v1"},"cats":{"new-dataset":0.0951983927,"dev-research":0.2779644303,"prompt-eng":0.3771062268,"data-quality":0.1208835463,"ml-security":0.0924624646}}
{"text":"Recent criticisms of AI ethics principles and practices have indicated a need for new approaches to AI ethics that can account for and intervene in the design, development, use, and governance of AI systems across multiple actors, contexts, and scales of activity.","meta":{"url":"http://arxiv.org/abs/2307.16787v1"},"cats":{"new-dataset":0.1004303271,"dev-research":0.3791323372,"prompt-eng":0.2792723237,"data-quality":0.1309312763,"ml-security":0.2675091921}}
{"text":"This paper positions AI value chains as an integrative concept that satisfies those needs, enabling AI ethics researchers, practitioners, and policymakers to take a more comprehensive view of the ethical and practical implications of AI systems.","meta":{"url":"http://arxiv.org/abs/2307.16787v1"},"cats":{"new-dataset":0.0874529113,"dev-research":0.3478694408,"prompt-eng":0.3302589494,"data-quality":0.1850652647,"ml-security":0.2180077203}}
{"text":"We review and synthesize theoretical perspectives on value chains from the literature on strategic management, service science, and economic geography.","meta":{"url":"http://arxiv.org/abs/2307.16787v1"},"cats":{"new-dataset":0.0534836638,"dev-research":0.2475769728,"prompt-eng":0.3112628342,"data-quality":0.114069925,"ml-security":0.0471647332}}
{"text":"We then review perspectives on AI value chains from the academic, industry, and policy literature.","meta":{"url":"http://arxiv.org/abs/2307.16787v1"},"cats":{"new-dataset":0.0913703177,"dev-research":0.273916328,"prompt-eng":0.3142789058,"data-quality":0.1770394444,"ml-security":0.1084999474}}
{"text":"We connect an inventory of ethical concerns in AI to the actors and resourcing activities involved in AI value chains to demonstrate that approaching AI ethics issues as value chain issues can enable more comprehensive and integrative research and governance practices.","meta":{"url":"http://arxiv.org/abs/2307.16787v1"},"cats":{"new-dataset":0.1955519026,"dev-research":0.3482030756,"prompt-eng":0.2957393707,"data-quality":0.2169006381,"ml-security":0.1723361241}}
{"text":"We illustrate this by suggesting five future directions for researchers, practitioners, and policymakers to investigate and intervene in the ethical concerns associated with AI value chains.","meta":{"url":"http://arxiv.org/abs/2307.16787v1"},"cats":{"new-dataset":0.0724093942,"dev-research":0.3439309739,"prompt-eng":0.3341929207,"data-quality":0.2021988525,"ml-security":0.2291025846}}
{"text":"The success of a multi-kilometre drive by a solar-powered rover at the lunar south pole depends upon careful planning in space and time due to highly dynamic solar illumination conditions.","meta":{"url":"http://arxiv.org/abs/2307.16786v1"},"cats":{"new-dataset":0.0531897872,"dev-research":0.2271417888,"prompt-eng":0.4187793862,"data-quality":0.0751902369,"ml-security":0.0475339339}}
{"text":"An additional challenge is that real-world robots may be subject to random faults that can temporarily delay long-range traverses.","meta":{"url":"http://arxiv.org/abs/2307.16786v1"},"cats":{"new-dataset":0.0507916382,"dev-research":0.2679701267,"prompt-eng":0.4323156611,"data-quality":0.1390157989,"ml-security":0.2024006963}}
{"text":"The majority of existing global spatiotemporal planners assume a deterministic rover-environment model and do not account for random faults.","meta":{"url":"http://arxiv.org/abs/2307.16786v1"},"cats":{"new-dataset":0.2085861693,"dev-research":0.2786238108,"prompt-eng":0.4362157385,"data-quality":0.1009660204,"ml-security":0.1742782686}}
{"text":"In this paper, we consider a random fault profile with a known, average spatial fault rate.","meta":{"url":"http://arxiv.org/abs/2307.16786v1"},"cats":{"new-dataset":0.1534000048,"dev-research":0.2944344804,"prompt-eng":0.4489579114,"data-quality":0.2343236425,"ml-security":0.1978253123}}
{"text":"We introduce a methodology to compute recovery policies that maximize the probability of survival of a solar-powered rover from different start states.","meta":{"url":"http://arxiv.org/abs/2307.16786v1"},"cats":{"new-dataset":0.1573157435,"dev-research":0.1920499659,"prompt-eng":0.4550008198,"data-quality":0.0925885121,"ml-security":0.1078064911}}
{"text":"A recovery policy defines a set of recourse actions to reach a location with sufficient battery energy remaining, given the local solar illumination conditions.","meta":{"url":"http://arxiv.org/abs/2307.16786v1"},"cats":{"new-dataset":0.0621008242,"dev-research":0.254362248,"prompt-eng":0.3939179554,"data-quality":0.1535323694,"ml-security":0.0989301292}}
{"text":"We solve a stochastic reach-avoid problem using dynamic programming to find such optimal recovery policies.","meta":{"url":"http://arxiv.org/abs/2307.16786v1"},"cats":{"new-dataset":0.0429763528,"dev-research":0.2115584742,"prompt-eng":0.3950847939,"data-quality":0.1120609851,"ml-security":0.1427143081}}
{"text":"Our focus, in part, is on the implications of state space discretization, which is often required in practical implementations.","meta":{"url":"http://arxiv.org/abs/2307.16786v1"},"cats":{"new-dataset":0.0096400532,"dev-research":0.2736367991,"prompt-eng":0.3839560547,"data-quality":0.1086282595,"ml-security":0.1393470609}}
{"text":"We propose a modified dynamic programming algorithm that conservatively accounts for approximation errors.","meta":{"url":"http://arxiv.org/abs/2307.16786v1"},"cats":{"new-dataset":0.0272421995,"dev-research":0.3286837078,"prompt-eng":0.4208781581,"data-quality":0.2123749557,"ml-security":0.1428858748}}
{"text":"To demonstrate the benefits of our approach, we compare against existing methods in scenarios where a solar-powered rover seeks to safely exit from permanently shadowed regions in the Cabeus area at the lunar south pole.","meta":{"url":"http://arxiv.org/abs/2307.16786v1"},"cats":{"new-dataset":0.0997670672,"dev-research":0.2399191293,"prompt-eng":0.4705364748,"data-quality":0.1181397207,"ml-security":0.1610954529}}
{"text":"We also highlight the relevance of our methodology for mission formulation and trade safety analysis by empirically comparing different rover mobility models in simulated recovery drives from the LCROSS crash region.","meta":{"url":"http://arxiv.org/abs/2307.16786v1"},"cats":{"new-dataset":0.3128146654,"dev-research":0.2531050768,"prompt-eng":0.3894361475,"data-quality":0.1265516095,"ml-security":0.1742303399}}
{"text":"Most existing Low-Light Image Enhancement (LLIE) methods are primarily designed to improve brightness in dark regions, which suffer from severe degradation in nighttime images.","meta":{"url":"http://arxiv.org/abs/2307.16783v1"},"cats":{"new-dataset":0.0203238475,"dev-research":0.2357058564,"prompt-eng":0.3828694411,"data-quality":0.1551550078,"ml-security":0.081726273}}
{"text":"However, these methods have limited exploration in another major visibility damage, the glow effects in real night scenes.","meta":{"url":"http://arxiv.org/abs/2307.16783v1"},"cats":{"new-dataset":0.0281869028,"dev-research":0.2099060543,"prompt-eng":0.3208043556,"data-quality":0.1193254355,"ml-security":0.1893727548}}
{"text":"Glow effects are inevitable in the presence of artificial light sources and cause further diffused blurring when directly enhanced.","meta":{"url":"http://arxiv.org/abs/2307.16783v1"},"cats":{"new-dataset":0.0129146077,"dev-research":0.2904188771,"prompt-eng":0.3729378678,"data-quality":0.1742430627,"ml-security":0.21383516}}
{"text":"To settle this issue, we innovatively consider the glow suppression task as learning physical glow generation via multiple scattering estimation according to the Atmospheric Point Spread Function (APSF).","meta":{"url":"http://arxiv.org/abs/2307.16783v1"},"cats":{"new-dataset":0.138956423,"dev-research":0.2006500178,"prompt-eng":0.4458982406,"data-quality":0.1397626355,"ml-security":0.1493999894}}
{"text":"In response to the challenges posed by uneven glow intensity and varying source shapes, an APSF-based Nighttime Imaging Model with Near-field Light Sources (NIM-NLS) is specifically derived to design a scalable Light-aware Blind Deconvolution Network (LBDN).","meta":{"url":"http://arxiv.org/abs/2307.16783v1"},"cats":{"new-dataset":0.1415698532,"dev-research":0.1912762206,"prompt-eng":0.3407288386,"data-quality":0.153101371,"ml-security":0.0865593693}}
{"text":"The glow-suppressed result is then brightened via a Retinex-based Enhancement Module (REM).","meta":{"url":"http://arxiv.org/abs/2307.16783v1"},"cats":{"new-dataset":0.0554119322,"dev-research":0.2387913718,"prompt-eng":0.4375628643,"data-quality":0.1878270515,"ml-security":0.0834093299}}
{"text":"Remarkably, the proposed glow suppression method is based on zero-shot learning and does not rely on any paired or unpaired training data.","meta":{"url":"http://arxiv.org/abs/2307.16783v1"},"cats":{"new-dataset":0.0488475261,"dev-research":0.2044886657,"prompt-eng":0.3588218059,"data-quality":0.2728998152,"ml-security":0.3420303566}}
{"text":"Empirical evaluations demonstrate the effectiveness of the proposed method in both glow suppression and low-light enhancement tasks.","meta":{"url":"http://arxiv.org/abs/2307.16783v1"},"cats":{"new-dataset":0.0261974549,"dev-research":0.2792541636,"prompt-eng":0.4303536168,"data-quality":0.1973686167,"ml-security":0.0953437841}}
{"text":"In formal argumentation, a distinction can be made between extension-based semantics, where sets of arguments are either (jointly) accepted or not, and ranking-based semantics, where grades of acceptability are assigned to arguments.","meta":{"url":"http://arxiv.org/abs/2307.16780v1"},"cats":{"new-dataset":0.0151805223,"dev-research":0.3331994362,"prompt-eng":0.4077066412,"data-quality":0.252005727,"ml-security":0.0920843315}}
{"text":"Another important distinction is that between abstract approaches, that abstract away from the content of arguments, and structured approaches, that specify a method of constructing argument graphs on the basis of a knowledge base.","meta":{"url":"http://arxiv.org/abs/2307.16780v1"},"cats":{"new-dataset":0.0242009633,"dev-research":0.4348956864,"prompt-eng":0.2916866425,"data-quality":0.161565179,"ml-security":0.0954128866}}
{"text":"While ranking-based semantics have been extensively applied to abstract argumentation, few work has been done on ranking-based semantics for structured argumentation.","meta":{"url":"http://arxiv.org/abs/2307.16780v1"},"cats":{"new-dataset":0.0617623263,"dev-research":0.3330804098,"prompt-eng":0.435476447,"data-quality":0.2074284908,"ml-security":0.0773598975}}
{"text":"In this paper, we make a systematic investigation into the behaviour of ranking-based semantics applied to existing formalisms for structured argumentation.","meta":{"url":"http://arxiv.org/abs/2307.16780v1"},"cats":{"new-dataset":0.1086292504,"dev-research":0.3614969768,"prompt-eng":0.463308536,"data-quality":0.2221897538,"ml-security":0.0886868861}}
{"text":"We show that a wide class of ranking-based semantics gives rise to so-called culpability measures, and are relatively robust to specific choices in argument construction methods.","meta":{"url":"http://arxiv.org/abs/2307.16780v1"},"cats":{"new-dataset":0.0543737369,"dev-research":0.3949378481,"prompt-eng":0.4289370957,"data-quality":0.3726262697,"ml-security":0.1876220799}}
{"text":"Retrieval approaches that score documents based on learned dense vectors (i.e., dense retrieval) rather than lexical signals (i.e., conventional retrieval) are increasingly popular.","meta":{"url":"http://arxiv.org/abs/2307.16779v1"},"cats":{"new-dataset":0.0427746504,"dev-research":0.2190936052,"prompt-eng":0.3579737993,"data-quality":0.2533550347,"ml-security":0.0778702603}}
{"text":"Their ability to identify related documents that do not necessarily contain the same terms as those appearing in the user's query (thereby improving recall) is one of their key advantages.","meta":{"url":"http://arxiv.org/abs/2307.16779v1"},"cats":{"new-dataset":0.0126625896,"dev-research":0.3070080931,"prompt-eng":0.3796151111,"data-quality":0.194270422,"ml-security":0.1477705483}}
{"text":"However, to actually achieve these gains, dense retrieval approaches typically require an exhaustive search over the document collection, making them considerably more expensive at query-time than conventional lexical approaches.","meta":{"url":"http://arxiv.org/abs/2307.16779v1"},"cats":{"new-dataset":0.0311843591,"dev-research":0.2561187885,"prompt-eng":0.3671660255,"data-quality":0.1961351319,"ml-security":0.0715055412}}
{"text":"Several techniques aim to reduce this computational overhead by approximating the results of a full dense retriever.","meta":{"url":"http://arxiv.org/abs/2307.16779v1"},"cats":{"new-dataset":0.0555781249,"dev-research":0.2228244302,"prompt-eng":0.4167341406,"data-quality":0.1264518058,"ml-security":0.0830308947}}
{"text":"Although these approaches reasonably approximate the top results, they suffer in terms of recall -- one of the key advantages of dense retrieval.","meta":{"url":"http://arxiv.org/abs/2307.16779v1"},"cats":{"new-dataset":0.0401936507,"dev-research":0.2716356718,"prompt-eng":0.3743255416,"data-quality":0.2510181333,"ml-security":0.0984857183}}
{"text":"We introduce 'LADR' (Lexically-Accelerated Dense Retrieval), a simple-yet-effective approach that improves the efficiency of existing dense retrieval models without compromising on retrieval effectiveness.","meta":{"url":"http://arxiv.org/abs/2307.16779v1"},"cats":{"new-dataset":0.1448986136,"dev-research":0.2179473637,"prompt-eng":0.4149351841,"data-quality":0.2003045862,"ml-security":0.0538722271}}
{"text":"LADR uses lexical retrieval techniques to seed a dense retrieval exploration that uses a document proximity graph.","meta":{"url":"http://arxiv.org/abs/2307.16779v1"},"cats":{"new-dataset":0.1389680537,"dev-research":0.2956744905,"prompt-eng":0.384931033,"data-quality":0.1832106312,"ml-security":0.0454349526}}
{"text":"We explore two variants of LADR: a proactive approach that expands the search space to the neighbors of all seed documents, and an adaptive approach that selectively searches the documents with the highest estimated relevance in an iterative fashion.","meta":{"url":"http://arxiv.org/abs/2307.16779v1"},"cats":{"new-dataset":0.131164184,"dev-research":0.2329291698,"prompt-eng":0.4096011311,"data-quality":0.2425649497,"ml-security":0.0783063845}}
{"text":"Through extensive experiments across a variety of dense retrieval models, we find that LADR establishes a new dense retrieval effectiveness-efficiency Pareto frontier among approximate k nearest neighbor techniques.","meta":{"url":"http://arxiv.org/abs/2307.16779v1"},"cats":{"new-dataset":0.0422558348,"dev-research":0.1731924924,"prompt-eng":0.3959248654,"data-quality":0.1813494033,"ml-security":0.0856111701}}
{"text":"Further, we find that when tuned to take around 8ms per query in retrieval latency on our hardware, LADR consistently achieves both precision and recall that are on par with an exhaustive search on standard benchmarks.","meta":{"url":"http://arxiv.org/abs/2307.16779v1"},"cats":{"new-dataset":0.0428193106,"dev-research":0.2862789194,"prompt-eng":0.4453992203,"data-quality":0.1501064788,"ml-security":0.0723218835}}
{"text":"The BBQ (Bias Benchmark for Question Answering) dataset enables the evaluation of the social biases that language models (LMs) exhibit in downstream tasks.","meta":{"url":"http://arxiv.org/abs/2307.16778v1"},"cats":{"new-dataset":0.1066788761,"dev-research":0.2196502917,"prompt-eng":0.3710558656,"data-quality":0.2367751334,"ml-security":0.1326983002}}
{"text":"However, it is challenging to adapt BBQ to languages other than English as social biases are culturally dependent.","meta":{"url":"http://arxiv.org/abs/2307.16778v1"},"cats":{"new-dataset":0.0230272533,"dev-research":0.2554102733,"prompt-eng":0.314144905,"data-quality":0.241729842,"ml-security":0.1143831383}}
{"text":"In this paper, we devise a process to construct a non-English bias benchmark dataset by leveraging the English BBQ dataset in a culturally adaptive way and present the KoBBQ dataset for evaluating biases in Question Answering (QA) tasks in Korean.","meta":{"url":"http://arxiv.org/abs/2307.16778v1"},"cats":{"new-dataset":0.4505812808,"dev-research":0.2257720361,"prompt-eng":0.3941833261,"data-quality":0.263638606,"ml-security":0.0819761979}}
{"text":"We identify samples from BBQ into three classes: Simply-Translated (can be used directly after cultural translation), Target-Modified (requires localization in target groups), and Sample-Removed (does not fit Korean culture).","meta":{"url":"http://arxiv.org/abs/2307.16778v1"},"cats":{"new-dataset":0.4543876906,"dev-research":0.1774162026,"prompt-eng":0.4110253387,"data-quality":0.3270677556,"ml-security":0.0665722706}}
{"text":"We further enhance the cultural relevance to Korean culture by adding four new categories of bias specific to Korean culture and newly creating samples based on Korean literature.","meta":{"url":"http://arxiv.org/abs/2307.16778v1"},"cats":{"new-dataset":0.0962982567,"dev-research":0.2035568939,"prompt-eng":0.3212740318,"data-quality":0.2393952997,"ml-security":0.060576496}}
{"text":"KoBBQ consists of 246 templates and 4,740 samples across 12 categories of social bias.","meta":{"url":"http://arxiv.org/abs/2307.16778v1"},"cats":{"new-dataset":0.460493579,"dev-research":0.2020764419,"prompt-eng":0.3261525304,"data-quality":0.1661895967,"ml-security":0.106691636}}
{"text":"Using KoBBQ, we measure the accuracy and bias scores of several state-of-the-art multilingual LMs.","meta":{"url":"http://arxiv.org/abs/2307.16778v1"},"cats":{"new-dataset":0.1688783318,"dev-research":0.2187774188,"prompt-eng":0.439093732,"data-quality":0.3451129628,"ml-security":0.063157644}}
{"text":"We demonstrate the differences in the bias of LMs in Korean and English, clarifying the need for hand-crafted data considering cultural differences.","meta":{"url":"http://arxiv.org/abs/2307.16778v1"},"cats":{"new-dataset":0.1839769689,"dev-research":0.2181368703,"prompt-eng":0.3692138956,"data-quality":0.2546021372,"ml-security":0.0681218351}}
{"text":"To easily obtain the knowledge about autism spectrum disorder and help its early screening and diagnosis, we create AsdKB, a Chinese knowledge base on autism spectrum disorder.","meta":{"url":"http://arxiv.org/abs/2307.16773v1"},"cats":{"new-dataset":0.3413774487,"dev-research":0.2664226441,"prompt-eng":0.385548101,"data-quality":0.1428239765,"ml-security":0.0962932403}}
{"text":"The knowledge base is built on top of various sources, including 1) the disease knowledge from SNOMED CT and ICD-10 clinical descriptions on mental and behavioural disorders, 2) the diagnostic knowledge from DSM-5 and different screening tools recommended by social organizations and medical institutes, and 3) the expert knowledge on professional physicians and hospitals from the Web.","meta":{"url":"http://arxiv.org/abs/2307.16773v1"},"cats":{"new-dataset":0.2183149905,"dev-research":0.3160824653,"prompt-eng":0.3744615166,"data-quality":0.1404957609,"ml-security":0.1438250918}}
{"text":"AsdKB contains both ontological and factual knowledge, and is accessible as Linked Data at https://w3id.org/asdkb/. The potential applications of AsdKB are question answering, auxiliary diagnosis, and expert recommendation, and we illustrate them with a prototype which can be accessed at http://asdkb.org.cn/.","meta":{"url":"http://arxiv.org/abs/2307.16773v1"},"cats":{"new-dataset":0.2870337989,"dev-research":0.3846735226,"prompt-eng":0.4390799772,"data-quality":0.1884417994,"ml-security":0.1110409542}}
{"text":"Modern ML predictions models are surprisingly accurate in practice and incorporating their power into algorithms has led to a new research direction.","meta":{"url":"http://arxiv.org/abs/2307.16771v1"},"cats":{"new-dataset":0.0250657276,"dev-research":0.252565257,"prompt-eng":0.3821312864,"data-quality":0.2231417296,"ml-security":0.2878131307}}
{"text":"Algorithms with predictions have already been used to improve on worst-case optimal bounds for online problems and for static graph problems.   ","meta":{"url":"http://arxiv.org/abs/2307.16771v1"},"cats":{"new-dataset":0.0686585229,"dev-research":0.2507648823,"prompt-eng":0.3430908739,"data-quality":0.1969266792,"ml-security":0.2558173795}}
{"text":"With this work, we initiate the study of the complexity of {\\em data structures with predictions}, with an emphasis on dynamic graph problems.","meta":{"url":"http://arxiv.org/abs/2307.16771v1"},"cats":{"new-dataset":0.1509081187,"dev-research":0.2240072103,"prompt-eng":0.3660523026,"data-quality":0.1696179801,"ml-security":0.1720002808}}
{"text":"Unlike the independent work of v.d.~Brand et al.~[arXiv:2307.09961] that aims at upper bounds, our investigation is focused on establishing conditional fine-grained lower bounds for various notions of predictions.   ","meta":{"url":"http://arxiv.org/abs/2307.16771v1"},"cats":{"new-dataset":0.099259702,"dev-research":0.1939689985,"prompt-eng":0.4156906193,"data-quality":0.2142649627,"ml-security":0.2342114572}}
{"text":"Our lower bounds are conditioned on the Online Matrix Vector (OMv) hypothesis.","meta":{"url":"http://arxiv.org/abs/2307.16771v1"},"cats":{"new-dataset":0.0200492168,"dev-research":0.1834635619,"prompt-eng":0.3336851921,"data-quality":0.1334230944,"ml-security":0.2004395547}}
{"text":"First we show that a prediction-based algorithm for OMv provides a smooth transition between the known bounds, for the offline and the online setting, and then show that this algorithm is essentially optimal under the OMv hypothesis.","meta":{"url":"http://arxiv.org/abs/2307.16771v1"},"cats":{"new-dataset":0.1000097353,"dev-research":0.2033065766,"prompt-eng":0.3992862726,"data-quality":0.1299476156,"ml-security":0.1323203936}}
{"text":"Further, we introduce and study four different kinds of predictions.","meta":{"url":"http://arxiv.org/abs/2307.16771v1"},"cats":{"new-dataset":0.0643112531,"dev-research":0.1872513406,"prompt-eng":0.4111327221,"data-quality":0.1309466165,"ml-security":0.1203848261}}
{"text":"(1) For {\\em $\\varepsilon$-accurate predictions}, where $\\varepsilon \\in (0,1)$, we show that any lower bound from the non-prediction setting carries over, reduced by a factor of $1-\\varepsilon$. (2) For {\\em $L$-list accurate predictions}, we show that one can efficiently compute a $(1/L)$-accurate prediction from an $L$-list accurate prediction.","meta":{"url":"http://arxiv.org/abs/2307.16771v1"},"cats":{"new-dataset":0.0851621882,"dev-research":0.1913295566,"prompt-eng":0.3949293853,"data-quality":0.2954931678,"ml-security":0.2719476253}}
{"text":"(3) For {\\em bounded delay predictions} and {\\em bounded delay predictions with outliers}, we show that a lower bound from the non-prediction setting carries over, if the reduction fulfills a certain reordering condition (which is fulfilled by many reductions from OMv for dynamic graph problems).","meta":{"url":"http://arxiv.org/abs/2307.16771v1"},"cats":{"new-dataset":0.0591047035,"dev-research":0.2174179481,"prompt-eng":0.3519561774,"data-quality":0.2465405278,"ml-security":0.2764887777}}
{"text":"This is demonstrated by showing lower and almost tight upper bounds for a concrete, dynamic graph problem, called $\\# s \\textrm{-} \\triangle$, where the number of triangles that contain a fixed vertex $s$ must be reported.","meta":{"url":"http://arxiv.org/abs/2307.16771v1"},"cats":{"new-dataset":0.1371592394,"dev-research":0.271743227,"prompt-eng":0.3361060914,"data-quality":0.1850352496,"ml-security":0.1594328822}}
{"text":"Heatmap-based methods have become the mainstream method for pose estimation due to their superior performance.","meta":{"url":"http://arxiv.org/abs/2307.16765v1"},"cats":{"new-dataset":0.1356673328,"dev-research":0.235611061,"prompt-eng":0.3602560608,"data-quality":0.1041650985,"ml-security":0.0849782807}}
{"text":"However, heatmap-based approaches suffer from significant quantization errors with downscale heatmaps, which result in limited performance and the detrimental effects of intermediate supervision.","meta":{"url":"http://arxiv.org/abs/2307.16765v1"},"cats":{"new-dataset":0.0417153627,"dev-research":0.3274958398,"prompt-eng":0.4377218029,"data-quality":0.247609883,"ml-security":0.1102347692}}
{"text":"Previous heatmap-based methods relied heavily on additional post-processing to mitigate quantization errors.","meta":{"url":"http://arxiv.org/abs/2307.16765v1"},"cats":{"new-dataset":0.030189971,"dev-research":0.3317521664,"prompt-eng":0.3829602806,"data-quality":0.2470903399,"ml-security":0.0962197031}}
{"text":"Some heatmap-based approaches improve the resolution of feature maps by using multiple costly upsampling layers to improve localization precision.","meta":{"url":"http://arxiv.org/abs/2307.16765v1"},"cats":{"new-dataset":0.1889016891,"dev-research":0.3264450206,"prompt-eng":0.4070554667,"data-quality":0.239418933,"ml-security":0.0657856475}}
{"text":"To solve the above issues, we creatively view the backbone network as a degradation process and thus reformulate the heatmap prediction as a Super-Resolution (SR) task.","meta":{"url":"http://arxiv.org/abs/2307.16765v1"},"cats":{"new-dataset":0.171118496,"dev-research":0.1908570864,"prompt-eng":0.4068205262,"data-quality":0.1472465607,"ml-security":0.0993973164}}
{"text":"We first propose the SR head, which predicts heatmaps with a spatial resolution higher than the input feature maps (or even consistent with the input image) by super-resolution, to effectively reduce the quantization error and the dependence on further post-processing.","meta":{"url":"http://arxiv.org/abs/2307.16765v1"},"cats":{"new-dataset":0.1702216698,"dev-research":0.2429418758,"prompt-eng":0.4096387936,"data-quality":0.1784781439,"ml-security":0.0969253186}}
{"text":"Besides, we propose SRPose to gradually recover the HR heatmaps from LR heatmaps and degraded features in a coarse-to-fine manner.","meta":{"url":"http://arxiv.org/abs/2307.16765v1"},"cats":{"new-dataset":0.1942787168,"dev-research":0.2435224422,"prompt-eng":0.4404647791,"data-quality":0.196523617,"ml-security":0.0839042449}}
{"text":"To reduce the training difficulty of HR heatmaps, SRPose applies SR heads to supervise the intermediate features in each stage.","meta":{"url":"http://arxiv.org/abs/2307.16765v1"},"cats":{"new-dataset":0.0471560735,"dev-research":0.2899919857,"prompt-eng":0.4222458117,"data-quality":0.1393726652,"ml-security":0.0875967833}}
{"text":"In addition, the SR head is a lightweight and generic head that applies to top-down and bottom-up methods.","meta":{"url":"http://arxiv.org/abs/2307.16765v1"},"cats":{"new-dataset":0.0087546752,"dev-research":0.2822860438,"prompt-eng":0.3954763347,"data-quality":0.0956015767,"ml-security":0.0608000596}}
{"text":"Extensive experiments on the COCO, MPII, and CrowdPose datasets show that SRPose outperforms the corresponding heatmap-based approaches.","meta":{"url":"http://arxiv.org/abs/2307.16765v1"},"cats":{"new-dataset":0.269365006,"dev-research":0.235337325,"prompt-eng":0.3779520191,"data-quality":0.1441237465,"ml-security":0.0802017716}}
{"text":"The code and models are available at https://github.com/haonanwang0522/SRPose.","meta":{"url":"http://arxiv.org/abs/2307.16765v1"},"cats":{"new-dataset":0.4095043738,"dev-research":0.2167237292,"prompt-eng":0.4405132091,"data-quality":0.120278168,"ml-security":0.0671596589}}
{"text":"This paper accompanies a new dataset of non-linear real arithmetic problems for the SMT-LIB benchmark collection.","meta":{"url":"http://arxiv.org/abs/2307.16761v1"},"cats":{"new-dataset":0.3582969176,"dev-research":0.2354180347,"prompt-eng":0.3184097406,"data-quality":0.2035426386,"ml-security":0.1458444303}}
{"text":"The problems come from an automated proof procedure of Gerhold--Kauers, which is well suited for solution by SMT.","meta":{"url":"http://arxiv.org/abs/2307.16761v1"},"cats":{"new-dataset":0.1235059196,"dev-research":0.2441378034,"prompt-eng":0.4242902061,"data-quality":0.2098374443,"ml-security":0.0850352425}}
{"text":"The problems of this type have not been tackled by SMT-solvers before.","meta":{"url":"http://arxiv.org/abs/2307.16761v1"},"cats":{"new-dataset":0.0364744625,"dev-research":0.2520414403,"prompt-eng":0.3607022152,"data-quality":0.1602186462,"ml-security":0.0943164943}}
{"text":"We describe the proof technique and give one new such proof to illustrate it.","meta":{"url":"http://arxiv.org/abs/2307.16761v1"},"cats":{"new-dataset":0.0846712437,"dev-research":0.2821438976,"prompt-eng":0.4088458483,"data-quality":0.2234110086,"ml-security":0.1374096924}}
{"text":"We then describe the dataset and the results of benchmarking.","meta":{"url":"http://arxiv.org/abs/2307.16761v1"},"cats":{"new-dataset":0.6319119709,"dev-research":0.292366827,"prompt-eng":0.40703198,"data-quality":0.2271962025,"ml-security":0.0887703506}}
{"text":"The benchmarks on the new dataset are quite different to the existing ones.","meta":{"url":"http://arxiv.org/abs/2307.16761v1"},"cats":{"new-dataset":0.374524859,"dev-research":0.2789449229,"prompt-eng":0.2977785628,"data-quality":0.1991161763,"ml-security":0.09971396}}
{"text":"The benchmarking also brings forward some interesting debate on the use/inclusion of rational functions and algebraic numbers in the SMT-LIB.","meta":{"url":"http://arxiv.org/abs/2307.16761v1"},"cats":{"new-dataset":0.0788683342,"dev-research":0.3079581952,"prompt-eng":0.3904894138,"data-quality":0.1542034159,"ml-security":0.1044821335}}
{"text":"Coordinate descent methods are popular in machine learning and optimization for their simple sparse updates and excellent practical performance.","meta":{"url":"http://arxiv.org/abs/2307.16754v1"},"cats":{"new-dataset":0.0544344835,"dev-research":0.251642927,"prompt-eng":0.3599957399,"data-quality":0.1701104537,"ml-security":0.2303122793}}
{"text":"In the context of large-scale sequential game solving, these same properties would be attractive, but until now no such methods were known, because the strategy spaces do not satisfy the typical separable block structure exploited by such methods.","meta":{"url":"http://arxiv.org/abs/2307.16754v1"},"cats":{"new-dataset":0.0192930219,"dev-research":0.2284249021,"prompt-eng":0.3279507066,"data-quality":0.0679320697,"ml-security":0.2106524488}}
{"text":"We present the first cyclic coordinate-descent-like method for the polytope of sequence-form strategies, which form the strategy spaces for the players in an extensive-form game (EFG).","meta":{"url":"http://arxiv.org/abs/2307.16754v1"},"cats":{"new-dataset":0.0495671083,"dev-research":0.254627617,"prompt-eng":0.3675910344,"data-quality":0.0564340136,"ml-security":0.1942411941}}
{"text":"Our method exploits the recursive structure of the proximal update induced by what are known as dilated regularizers, in order to allow for a pseudo block-wise update.","meta":{"url":"http://arxiv.org/abs/2307.16754v1"},"cats":{"new-dataset":0.0472579809,"dev-research":0.236631881,"prompt-eng":0.4504403712,"data-quality":0.305111088,"ml-security":0.1890622987}}
{"text":"We show that our method enjoys a $O(1/T)$ convergence rate to a two-player zero-sum Nash equilibrium, while avoiding the worst-case polynomial scaling with the number of blocks common to cyclic methods.","meta":{"url":"http://arxiv.org/abs/2307.16754v1"},"cats":{"new-dataset":0.0165548277,"dev-research":0.1735649802,"prompt-eng":0.3277425006,"data-quality":0.0982332194,"ml-security":0.1688650881}}
{"text":"We empirically show that our algorithm usually performs better than other state-of-the-art first-order methods (i.e., mirror prox), and occasionally can even beat CFR$^+$, a state-of-the-art algorithm for numerical equilibrium computation in zero-sum EFGs.","meta":{"url":"http://arxiv.org/abs/2307.16754v1"},"cats":{"new-dataset":0.019242549,"dev-research":0.228821288,"prompt-eng":0.3681894002,"data-quality":0.1051643463,"ml-security":0.1450961083}}
{"text":"We then introduce a restarting heuristic for EFG solving.","meta":{"url":"http://arxiv.org/abs/2307.16754v1"},"cats":{"new-dataset":0.0483301067,"dev-research":0.3326300284,"prompt-eng":0.4943112805,"data-quality":0.1149161699,"ml-security":0.070800465}}
{"text":"We show empirically that restarting can lead to speedups, sometimes huge, both for our cyclic method, as well as for existing methods such as mirror prox and predictive CFR$^+$.","meta":{"url":"http://arxiv.org/abs/2307.16754v1"},"cats":{"new-dataset":0.0346285157,"dev-research":0.2904299234,"prompt-eng":0.4642341177,"data-quality":0.1251298915,"ml-security":0.1149955375}}
{"text":"Many objects such as tools and household items can be used only if grasped in a very specific way - grasped functionally.","meta":{"url":"http://arxiv.org/abs/2307.16752v1"},"cats":{"new-dataset":0.0076772871,"dev-research":0.2033274358,"prompt-eng":0.2957806936,"data-quality":0.0613978569,"ml-security":0.1130965938}}
{"text":"Often, a direct functional grasp is not possible, though.","meta":{"url":"http://arxiv.org/abs/2307.16752v1"},"cats":{"new-dataset":0.0024393277,"dev-research":0.2693552232,"prompt-eng":0.3732025834,"data-quality":0.0697575939,"ml-security":0.076253569}}
{"text":"We propose a method for learning a dexterous pre-grasp manipulation policy to achieve human-like functional grasps using deep reinforcement learning.","meta":{"url":"http://arxiv.org/abs/2307.16752v1"},"cats":{"new-dataset":0.0698762256,"dev-research":0.2194011359,"prompt-eng":0.3836786141,"data-quality":0.0590171471,"ml-security":0.0977445421}}
{"text":"We introduce a dense multi-component reward function that enables learning a single policy, capable of dexterous pre-grasp manipulation of novel instances of several known object categories with an anthropomorphic hand.","meta":{"url":"http://arxiv.org/abs/2307.16752v1"},"cats":{"new-dataset":0.1123747489,"dev-research":0.1992457112,"prompt-eng":0.3928576172,"data-quality":0.075918901,"ml-security":0.0978189524}}
{"text":"The policy is learned purely by means of reinforcement learning from scratch, without any expert demonstrations, and implicitly learns to reposition and reorient objects of complex shapes to achieve given functional grasps.","meta":{"url":"http://arxiv.org/abs/2307.16752v1"},"cats":{"new-dataset":0.0276875196,"dev-research":0.2446689748,"prompt-eng":0.3170905011,"data-quality":0.0764136389,"ml-security":0.1542931157}}
{"text":"Learning is done on a single GPU in less than three hours.","meta":{"url":"http://arxiv.org/abs/2307.16752v1"},"cats":{"new-dataset":0.0294844593,"dev-research":0.2615277535,"prompt-eng":0.3242710651,"data-quality":0.0789778446,"ml-security":0.0959788474}}
{"text":"In this study, the structural problems of the YOLOv5 model were analyzed emphatically.","meta":{"url":"http://arxiv.org/abs/2307.16751v1"},"cats":{"new-dataset":0.0233360732,"dev-research":0.1814413526,"prompt-eng":0.364238425,"data-quality":0.1113104246,"ml-security":0.1199010101}}
{"text":"Based on the characteristics of fine defects in artificial leather, four innovative structures, namely DFP, IFF, AMP, and EOS, were designed.","meta":{"url":"http://arxiv.org/abs/2307.16751v1"},"cats":{"new-dataset":0.0625294851,"dev-research":0.2860606166,"prompt-eng":0.3373082827,"data-quality":0.1898429167,"ml-security":0.0812756729}}
{"text":"These advancements led to the proposal of a high-performance artificial leather fine defect detection model named YOLOD.","meta":{"url":"http://arxiv.org/abs/2307.16751v1"},"cats":{"new-dataset":0.0769516177,"dev-research":0.2925191534,"prompt-eng":0.4014825014,"data-quality":0.2844040349,"ml-security":0.1801035797}}
{"text":"YOLOD demonstrated outstanding performance on the artificial leather defect dataset, achieving an impressive increase of 11.7% - 13.5% in AP_50 compared to YOLOv5, along with a significant reduction of 5.2% - 7.2% in the error detection rate.","meta":{"url":"http://arxiv.org/abs/2307.16751v1"},"cats":{"new-dataset":0.2559456727,"dev-research":0.3355457692,"prompt-eng":0.4086001785,"data-quality":0.3359901898,"ml-security":0.1800089054}}
{"text":"Moreover, YOLOD also exhibited remarkable performance on the general MS-COCO dataset, with an increase of 0.4% - 2.6% in AP compared to YOLOv5, and a rise of 2.5% - 4.1% in AP_S compared to YOLOv5.","meta":{"url":"http://arxiv.org/abs/2307.16751v1"},"cats":{"new-dataset":0.3495283441,"dev-research":0.2659584641,"prompt-eng":0.3907277915,"data-quality":0.2296996649,"ml-security":0.1433890261}}
{"text":"These results demonstrate the superiority of YOLOD in both artificial leather defect detection and general object detection tasks, making it a highly efficient and effective model for real-world applications.","meta":{"url":"http://arxiv.org/abs/2307.16751v1"},"cats":{"new-dataset":0.062785445,"dev-research":0.3063435738,"prompt-eng":0.4182881322,"data-quality":0.3144075929,"ml-security":0.2146315081}}
{"text":"Cylindrical Algebraic Decomposition (CAD) by projection and lifting requires many iterated univariate resultants.","meta":{"url":"http://arxiv.org/abs/2307.16750v1"},"cats":{"new-dataset":0.05048022,"dev-research":0.2789529891,"prompt-eng":0.3910688338,"data-quality":0.0750571818,"ml-security":0.1062746912}}
{"text":"It has been observed that these often factor, but to date this has not been used to optimise implementations of CAD.","meta":{"url":"http://arxiv.org/abs/2307.16750v1"},"cats":{"new-dataset":0.0107117799,"dev-research":0.3565087377,"prompt-eng":0.3805449925,"data-quality":0.129598768,"ml-security":0.0555703363}}
{"text":"We continue the investigation into such factorisations, writing in the specific context of SC-Square.","meta":{"url":"http://arxiv.org/abs/2307.16750v1"},"cats":{"new-dataset":0.1050768035,"dev-research":0.2350860626,"prompt-eng":0.3816151342,"data-quality":0.1042343742,"ml-security":0.0685165105}}
{"text":"Malnutrition poses a significant threat to global health, resulting from an inadequate intake of essential nutrients that adversely impacts vital organs and overall bodily functioning.","meta":{"url":"http://arxiv.org/abs/2307.16745v1"},"cats":{"new-dataset":0.0282000721,"dev-research":0.2806824067,"prompt-eng":0.2893083905,"data-quality":0.1297634181,"ml-security":0.2825939258}}
{"text":"Periodic examinations and mass screenings, incorporating both conventional and non-invasive techniques, have been employed to combat this challenge.","meta":{"url":"http://arxiv.org/abs/2307.16745v1"},"cats":{"new-dataset":0.0149997135,"dev-research":0.2439061405,"prompt-eng":0.4588178675,"data-quality":0.1669731304,"ml-security":0.2032336319}}
{"text":"However, these approaches suffer from critical limitations, such as the need for additional equipment, lack of comprehensive feature representation, absence of suitable health indicators, and the unavailability of smartphone implementations for precise estimations of Body Fat Percentage (BFP), Basal Metabolic Rate (BMR), and Body Mass Index (BMI) to enable efficient smart-malnutrition monitoring.","meta":{"url":"http://arxiv.org/abs/2307.16745v1"},"cats":{"new-dataset":0.0920312615,"dev-research":0.2436726856,"prompt-eng":0.4057488308,"data-quality":0.1196623942,"ml-security":0.1099901301}}
{"text":"To address these constraints, this study presents a groundbreaking, scalable, and robust smart malnutrition-monitoring system that leverages a single full-body image of an individual to estimate height, weight, and other crucial health parameters within a multi-modal learning framework.","meta":{"url":"http://arxiv.org/abs/2307.16745v1"},"cats":{"new-dataset":0.2033219196,"dev-research":0.2263865162,"prompt-eng":0.3978714241,"data-quality":0.1275152967,"ml-security":0.1582989797}}
{"text":"Our proposed methodology involves the reconstruction of a highly precise 3D point cloud, from which 512-dimensional feature embeddings are extracted using a headless-3D classification network.","meta":{"url":"http://arxiv.org/abs/2307.16745v1"},"cats":{"new-dataset":0.1677510651,"dev-research":0.2180664878,"prompt-eng":0.3384205517,"data-quality":0.1410415046,"ml-security":0.1240621038}}
{"text":"Concurrently, facial and body embeddings are also extracted, and through the application of learnable parameters, these features are then utilized to estimate weight accurately.","meta":{"url":"http://arxiv.org/abs/2307.16745v1"},"cats":{"new-dataset":0.0585993426,"dev-research":0.2343125874,"prompt-eng":0.4041662481,"data-quality":0.1607867202,"ml-security":0.1231022149}}
{"text":"Furthermore, essential health metrics, including BMR, BFP, and BMI, are computed to conduct a comprehensive analysis of the subject's health, subsequently facilitating the provision of personalized nutrition plans.","meta":{"url":"http://arxiv.org/abs/2307.16745v1"},"cats":{"new-dataset":0.081696023,"dev-research":0.2497862583,"prompt-eng":0.3964048538,"data-quality":0.0823020522,"ml-security":0.0541113727}}
{"text":"While being robust to a wide range of lighting conditions across multiple devices, our model achieves a low Mean Absolute Error (MAE) of $\\pm$ 4.7 cm and $\\pm$ 5.3 kg in estimating height and weight.","meta":{"url":"http://arxiv.org/abs/2307.16745v1"},"cats":{"new-dataset":0.0725770236,"dev-research":0.1913298087,"prompt-eng":0.3997928723,"data-quality":0.1746350607,"ml-security":0.0930408535}}
{"text":"Multi-spectral image stitching leverages the complementarity between infrared and visible images to generate a robust and reliable wide field-of-view (FOV) scene.","meta":{"url":"http://arxiv.org/abs/2307.16741v1"},"cats":{"new-dataset":0.1350302405,"dev-research":0.2001936393,"prompt-eng":0.3865533911,"data-quality":0.111758035,"ml-security":0.0448178713}}
{"text":"The primary challenge of this task is to explore the relations between multi-spectral images for aligning and integrating multi-view scenes.","meta":{"url":"http://arxiv.org/abs/2307.16741v1"},"cats":{"new-dataset":0.2089262635,"dev-research":0.1947665852,"prompt-eng":0.3949436756,"data-quality":0.145083661,"ml-security":0.0268123221}}
{"text":"Capitalizing on the strengths of Graph Convolutional Networks (GCNs) in modeling feature relationships, we propose a spatial graph reasoning based multi-spectral image stitching method that effectively distills the deformation and integration of multi-spectral images across different viewpoints.","meta":{"url":"http://arxiv.org/abs/2307.16741v1"},"cats":{"new-dataset":0.0983682081,"dev-research":0.2861243337,"prompt-eng":0.3342230876,"data-quality":0.1629900164,"ml-security":0.0581523227}}
{"text":"To accomplish this, we embed multi-scale complementary features from the same view position into a set of nodes.","meta":{"url":"http://arxiv.org/abs/2307.16741v1"},"cats":{"new-dataset":0.0855650265,"dev-research":0.2694612963,"prompt-eng":0.4096879414,"data-quality":0.155585989,"ml-security":0.0847627879}}
{"text":"The correspondence across different views is learned through powerful dense feature embeddings, where both inter- and intra-correlations are developed to exploit cross-view matching and enhance inner feature disparity.","meta":{"url":"http://arxiv.org/abs/2307.16741v1"},"cats":{"new-dataset":0.1017025468,"dev-research":0.2514918924,"prompt-eng":0.3302966364,"data-quality":0.2079353511,"ml-security":0.0835616163}}
{"text":"By introducing long-range coherence along spatial and channel dimensions, the complementarity of pixel relations and channel interdependencies aids in the reconstruction of aligned multi-view features, generating informative and reliable wide FOV scenes.","meta":{"url":"http://arxiv.org/abs/2307.16741v1"},"cats":{"new-dataset":0.1837501261,"dev-research":0.2313717462,"prompt-eng":0.3476670375,"data-quality":0.1522451603,"ml-security":0.0436879377}}
{"text":"Moreover, we release a challenging dataset named ChaMS, comprising both real-world and synthetic sets with significant parallax, providing a new option for comprehensive evaluation.","meta":{"url":"http://arxiv.org/abs/2307.16741v1"},"cats":{"new-dataset":0.8187514531,"dev-research":0.1897016619,"prompt-eng":0.4181486479,"data-quality":0.195926389,"ml-security":0.1063794305}}
{"text":"Extensive experiments demonstrate that our method surpasses the state-of-the-arts.","meta":{"url":"http://arxiv.org/abs/2307.16741v1"},"cats":{"new-dataset":0.020055953,"dev-research":0.2301994698,"prompt-eng":0.3781875787,"data-quality":0.1463386584,"ml-security":0.0703782509}}
{"text":"This study presents a novel coupled mechano-electro-chemical formulation for predicting stress corrosion cracking (SCC) phenomena in steel structures using the phase field method.","meta":{"url":"http://arxiv.org/abs/2307.16739v1"},"cats":{"new-dataset":0.0260990378,"dev-research":0.3242978746,"prompt-eng":0.3306101562,"data-quality":0.1741638088,"ml-security":0.1450392065}}
{"text":"SCC is a complex damage process that arises from the interaction between mechanical loading and corrosion in a corrosive electrolyte environment.","meta":{"url":"http://arxiv.org/abs/2307.16739v1"},"cats":{"new-dataset":0.04785569,"dev-research":0.3660394039,"prompt-eng":0.356883751,"data-quality":0.1599917698,"ml-security":0.2004862741}}
{"text":"The proposed formulation introduces a new phase-field parameter that aggregates the damage due to mechanical loading and electro-chemical corrosion.","meta":{"url":"http://arxiv.org/abs/2307.16739v1"},"cats":{"new-dataset":0.0188355916,"dev-research":0.286704165,"prompt-eng":0.3732123961,"data-quality":0.1616111608,"ml-security":0.1477981551}}
{"text":"To achieve this goal, the internal energies governing the SCC phenomenon are separated into elastic-damage strain energy, the interfacial reaction energy, and energy resulting from changes in corrosion ion concentration.","meta":{"url":"http://arxiv.org/abs/2307.16739v1"},"cats":{"new-dataset":0.0282695119,"dev-research":0.2642153053,"prompt-eng":0.3939320445,"data-quality":0.1327202588,"ml-security":0.1274473975}}
{"text":"The Allen-Cahn equation is modified to include all energy contributions and calculate the phase field parameter.","meta":{"url":"http://arxiv.org/abs/2307.16739v1"},"cats":{"new-dataset":0.0365208356,"dev-research":0.1413554765,"prompt-eng":0.352243561,"data-quality":0.07308825,"ml-security":0.0510073183}}
{"text":"Furthermore, a specific interfacial kinetic coefficient is introduced to the mechanical energy to take into account corrosion current effects on mechanical properties.","meta":{"url":"http://arxiv.org/abs/2307.16739v1"},"cats":{"new-dataset":0.0206154523,"dev-research":0.2917053165,"prompt-eng":0.3446260827,"data-quality":0.142577421,"ml-security":0.1077863366}}
{"text":"The Cahn-Hilliard equation is applied to model the corrosion ion concentration in the domain and the mechanical state of the body is obtained by solving the equilibrium equations.","meta":{"url":"http://arxiv.org/abs/2307.16739v1"},"cats":{"new-dataset":0.0325834831,"dev-research":0.2178121817,"prompt-eng":0.3161299697,"data-quality":0.0702673429,"ml-security":0.1220459302}}
{"text":"Several numerical examples are presented to validate the robustness and accuracy of the proposed formulation.","meta":{"url":"http://arxiv.org/abs/2307.16739v1"},"cats":{"new-dataset":0.0157447956,"dev-research":0.221761034,"prompt-eng":0.3965794564,"data-quality":0.2480934996,"ml-security":0.1230729097}}
{"text":"Finally, the method is applied to predict crack propagation resulting from SCC on two practical engineering problems, yielding promising results.","meta":{"url":"http://arxiv.org/abs/2307.16739v1"},"cats":{"new-dataset":0.0275892696,"dev-research":0.3201015587,"prompt-eng":0.3869004908,"data-quality":0.2142065306,"ml-security":0.2410202234}}
{"text":"We study the excess minimum risk in statistical inference, defined as the difference between the minimum expected loss in estimating a random variable from an observed feature vector and the minimum expected loss in estimating the same random variable from a transformation (statistic) of the feature vector.","meta":{"url":"http://arxiv.org/abs/2307.16735v1"},"cats":{"new-dataset":0.0114457056,"dev-research":0.2270404244,"prompt-eng":0.3868852036,"data-quality":0.286749274,"ml-security":0.4008697538}}
{"text":"After characterizing lossless transformations, i.e., transformations for which the excess risk is zero for all loss functions, we construct a partitioning test statistic for the hypothesis that a given transformation is lossless and show that for i.i.d. data the test is strongly consistent.","meta":{"url":"http://arxiv.org/abs/2307.16735v1"},"cats":{"new-dataset":0.0394140286,"dev-research":0.2054232503,"prompt-eng":0.4143113607,"data-quality":0.2649431992,"ml-security":0.2717149138}}
{"text":"More generally, we develop information-theoretic upper bounds on the excess risk that uniformly hold over fairly general classes of loss functions.","meta":{"url":"http://arxiv.org/abs/2307.16735v1"},"cats":{"new-dataset":0.037529296,"dev-research":0.1885960093,"prompt-eng":0.3813827043,"data-quality":0.2084393322,"ml-security":0.4468766137}}
{"text":"Based on these bounds, we introduce the notion of a delta-lossless transformation and give sufficient conditions for a given transformation to be universally delta-lossless.","meta":{"url":"http://arxiv.org/abs/2307.16735v1"},"cats":{"new-dataset":0.0424197447,"dev-research":0.1630979272,"prompt-eng":0.3428737833,"data-quality":0.2109107712,"ml-security":0.1670386879}}
{"text":"Applications to classification, nonparametric regression, portfolio strategies, information bottleneck, and deep learning, are also surveyed.","meta":{"url":"http://arxiv.org/abs/2307.16735v1"},"cats":{"new-dataset":0.0270711098,"dev-research":0.1882763994,"prompt-eng":0.3255812378,"data-quality":0.1346697074,"ml-security":0.2685241535}}
{"text":"Programmable Matter (PM) has been widely investigated in recent years.","meta":{"url":"http://arxiv.org/abs/2307.16731v1"},"cats":{"new-dataset":0.0265083908,"dev-research":0.2492393584,"prompt-eng":0.3930303102,"data-quality":0.0961089585,"ml-security":0.0740958314}}
{"text":"It refers to some kind of matter with the ability to change its physical properties (e.g., shape or color) in a programmable way.","meta":{"url":"http://arxiv.org/abs/2307.16731v1"},"cats":{"new-dataset":0.0280175184,"dev-research":0.3399961655,"prompt-eng":0.3607913437,"data-quality":0.1148097528,"ml-security":0.0852492899}}
{"text":"One reference model is certainly Amoebot, with its recent canonical version (DISC 2021).","meta":{"url":"http://arxiv.org/abs/2307.16731v1"},"cats":{"new-dataset":0.2805405751,"dev-research":0.1875626698,"prompt-eng":0.4454508781,"data-quality":0.1364185094,"ml-security":0.0781219074}}
{"text":"Along this line, with the aim of simplification and to better address concurrency, the SILBOT model has been introduced (AAMAS 2020), which heavily reduces the available capabilities of the particles composing the PM.","meta":{"url":"http://arxiv.org/abs/2307.16731v1"},"cats":{"new-dataset":0.0484234476,"dev-research":0.1989889662,"prompt-eng":0.4349045868,"data-quality":0.1101788855,"ml-security":0.0838001375}}
{"text":"In SILBOT, in fact, particles are asynchronous, without any direct means of communication (silent) and without memory of past events (oblivious).","meta":{"url":"http://arxiv.org/abs/2307.16731v1"},"cats":{"new-dataset":0.0283734397,"dev-research":0.2267412309,"prompt-eng":0.3398273391,"data-quality":0.1174734725,"ml-security":0.1640072371}}
{"text":"Within SILBOT, we consider the Line Formation primitive in which particles are required to end up in a configuration where they are all aligned and connected.","meta":{"url":"http://arxiv.org/abs/2307.16731v1"},"cats":{"new-dataset":0.0510699794,"dev-research":0.2234705691,"prompt-eng":0.3739063405,"data-quality":0.1040348065,"ml-security":0.0537726199}}
{"text":"We propose a simple and elegant distributed algorithm - optimal in terms of number of movements, along with its correctness proof.","meta":{"url":"http://arxiv.org/abs/2307.16731v1"},"cats":{"new-dataset":0.0644512084,"dev-research":0.1953885711,"prompt-eng":0.3663661099,"data-quality":0.1052036584,"ml-security":0.1201278479}}
{"text":"In this work, we propose a learning based neural model that provides both the longitudinal and lateral control commands to simultaneously navigate multiple vehicles.","meta":{"url":"http://arxiv.org/abs/2307.16727v1"},"cats":{"new-dataset":0.0669631511,"dev-research":0.1960463789,"prompt-eng":0.4016727692,"data-quality":0.0703927322,"ml-security":0.0883704731}}
{"text":"The goal is to ensure that each vehicle reaches a desired target state without colliding with any other vehicle or obstacle in an unconstrained environment.","meta":{"url":"http://arxiv.org/abs/2307.16727v1"},"cats":{"new-dataset":0.0255638786,"dev-research":0.2206282192,"prompt-eng":0.3919914535,"data-quality":0.0641003249,"ml-security":0.1164460032}}
{"text":"The model utilizes an attention based Graphical Neural Network paradigm that takes into consideration the state of all the surrounding vehicles to make an informed decision.","meta":{"url":"http://arxiv.org/abs/2307.16727v1"},"cats":{"new-dataset":0.0578222375,"dev-research":0.2536640791,"prompt-eng":0.4024777272,"data-quality":0.105623696,"ml-security":0.102788668}}
{"text":"This allows each vehicle to smoothly reach its destination while also evading collision with the other agents.","meta":{"url":"http://arxiv.org/abs/2307.16727v1"},"cats":{"new-dataset":0.0150264155,"dev-research":0.2857514765,"prompt-eng":0.365273225,"data-quality":0.0644038175,"ml-security":0.1130720472}}
{"text":"The data and corresponding labels for training such a network is obtained using an optimization based procedure.","meta":{"url":"http://arxiv.org/abs/2307.16727v1"},"cats":{"new-dataset":0.1107987056,"dev-research":0.2444220712,"prompt-eng":0.3920797065,"data-quality":0.2392055633,"ml-security":0.1061550205}}
{"text":"Experimental results demonstrates that our model is powerful enough to generalize even to situations with more vehicles than in the training data.","meta":{"url":"http://arxiv.org/abs/2307.16727v1"},"cats":{"new-dataset":0.0380306579,"dev-research":0.2126459841,"prompt-eng":0.3967184434,"data-quality":0.1203536062,"ml-security":0.2905536789}}
{"text":"Our method also outperforms comparable graphical neural network architectures.","meta":{"url":"http://arxiv.org/abs/2307.16727v1"},"cats":{"new-dataset":0.0211035138,"dev-research":0.2766657613,"prompt-eng":0.3694551835,"data-quality":0.2176714461,"ml-security":0.1185860339}}
{"text":"Project page which includes the code and supplementary information can be found at https://yininghase.github.io/multi-agent-control/","meta":{"url":"http://arxiv.org/abs/2307.16727v1"},"cats":{"new-dataset":0.2747859371,"dev-research":0.2420479545,"prompt-eng":0.4569586814,"data-quality":0.099071647,"ml-security":0.092174924}}
{"text":"Variable selection or importance measurement of input variables to a machine learning model has become the focus of much research.","meta":{"url":"http://arxiv.org/abs/2307.16718v1"},"cats":{"new-dataset":0.0106581956,"dev-research":0.317907335,"prompt-eng":0.4515803319,"data-quality":0.1672013165,"ml-security":0.2042023814}}
{"text":"It is no longer enough to have a good model, one also must explain its decisions.","meta":{"url":"http://arxiv.org/abs/2307.16718v1"},"cats":{"new-dataset":0.0149536533,"dev-research":0.2460973729,"prompt-eng":0.3295998254,"data-quality":0.0931473538,"ml-security":0.1285388782}}
{"text":"This is why there are so many intelligibility algorithms available today.","meta":{"url":"http://arxiv.org/abs/2307.16718v1"},"cats":{"new-dataset":0.022612754,"dev-research":0.3650577291,"prompt-eng":0.3790100547,"data-quality":0.2230960816,"ml-security":0.188099538}}
{"text":"Among them, Shapley value estimation algorithms are intelligibility methods based on cooperative game theory.","meta":{"url":"http://arxiv.org/abs/2307.16718v1"},"cats":{"new-dataset":0.0449765862,"dev-research":0.2608342092,"prompt-eng":0.3765531612,"data-quality":0.136393237,"ml-security":0.1834317102}}
{"text":"In the case of the naive Bayes classifier, and to our knowledge, there is no ``analytical\" formulation of Shapley values.","meta":{"url":"http://arxiv.org/abs/2307.16718v1"},"cats":{"new-dataset":0.0662341057,"dev-research":0.2026365205,"prompt-eng":0.3643996563,"data-quality":0.1678246954,"ml-security":0.1317153842}}
{"text":"This article proposes an exact analytic expression of Shapley values in the special case of the naive Bayes Classifier.","meta":{"url":"http://arxiv.org/abs/2307.16718v1"},"cats":{"new-dataset":0.0926807633,"dev-research":0.2356974436,"prompt-eng":0.4121561929,"data-quality":0.3136020751,"ml-security":0.2228469553}}
{"text":"We analytically compare this Shapley proposal, to another frequently used indicator, the Weight of Evidence (WoE) and provide an empirical comparison of our proposal with (i) the WoE and (ii) KernelShap results on real world datasets, discussing similar and dissimilar results.","meta":{"url":"http://arxiv.org/abs/2307.16718v1"},"cats":{"new-dataset":0.2871951823,"dev-research":0.2083116369,"prompt-eng":0.4138351522,"data-quality":0.2538822327,"ml-security":0.1132593247}}
{"text":"The results show that our Shapley proposal for the naive Bayes classifier provides informative results with low algorithmic complexity so that it can be used on very large datasets with extremely low computation time.","meta":{"url":"http://arxiv.org/abs/2307.16718v1"},"cats":{"new-dataset":0.198250333,"dev-research":0.2391115892,"prompt-eng":0.3834877185,"data-quality":0.245561043,"ml-security":0.2271456426}}
{"text":"Video Temporal Grounding (VTG), which aims to ground target clips from videos (such as consecutive intervals or disjoint shots) according to custom language queries (e.g., sentences or words), is key for video browsing on social media.","meta":{"url":"http://arxiv.org/abs/2307.16715v1"},"cats":{"new-dataset":0.0760579732,"dev-research":0.2758702965,"prompt-eng":0.3839576532,"data-quality":0.1551173688,"ml-security":0.0936047052}}
{"text":"Most methods in this direction develop taskspecific models that are trained with type-specific labels, such as moment retrieval (time interval) and highlight detection (worthiness curve), which limits their abilities to generalize to various VTG tasks and labels.","meta":{"url":"http://arxiv.org/abs/2307.16715v1"},"cats":{"new-dataset":0.032745843,"dev-research":0.2674192265,"prompt-eng":0.4979360165,"data-quality":0.247241689,"ml-security":0.0862208029}}
{"text":"In this paper, we propose to Unify the diverse VTG labels and tasks, dubbed UniVTG, along three directions: Firstly, we revisit a wide range of VTG labels and tasks and define a unified formulation.","meta":{"url":"http://arxiv.org/abs/2307.16715v1"},"cats":{"new-dataset":0.0962665426,"dev-research":0.3015663479,"prompt-eng":0.5176476854,"data-quality":0.2651513894,"ml-security":0.0662041247}}
{"text":"Based on this, we develop data annotation schemes to create scalable pseudo supervision.","meta":{"url":"http://arxiv.org/abs/2307.16715v1"},"cats":{"new-dataset":0.226727913,"dev-research":0.3453138406,"prompt-eng":0.4980289801,"data-quality":0.3705630434,"ml-security":0.1379091415}}
{"text":"Secondly, we develop an effective and flexible grounding model capable of addressing each task and making full use of each label.","meta":{"url":"http://arxiv.org/abs/2307.16715v1"},"cats":{"new-dataset":0.1355657708,"dev-research":0.2925649344,"prompt-eng":0.5223464714,"data-quality":0.2794225161,"ml-security":0.0712849461}}
{"text":"Lastly, thanks to the unified framework, we are able to unlock temporal grounding pretraining from large-scale diverse labels and develop stronger grounding abilities e.g., zero-shot grounding.","meta":{"url":"http://arxiv.org/abs/2307.16715v1"},"cats":{"new-dataset":0.3245211671,"dev-research":0.2440451248,"prompt-eng":0.4624408436,"data-quality":0.2375353938,"ml-security":0.1563360902}}
{"text":"Extensive experiments on three tasks (moment retrieval, highlight detection and video summarization) across seven datasets (QVHighlights, Charades-STA, TACoS, Ego4D, YouTube Highlights, TVSum, and QFVS) demonstrate the effectiveness and flexibility of our proposed framework.","meta":{"url":"http://arxiv.org/abs/2307.16715v1"},"cats":{"new-dataset":0.4558484595,"dev-research":0.2167004054,"prompt-eng":0.40494514,"data-quality":0.2574964214,"ml-security":0.0545426275}}
{"text":"The codes are available at https://github.com/showlab/UniVTG.","meta":{"url":"http://arxiv.org/abs/2307.16715v1"},"cats":{"new-dataset":0.5103260292,"dev-research":0.2378344798,"prompt-eng":0.4388912873,"data-quality":0.1705444821,"ml-security":0.0833927431}}
{"text":"The growth of systems complexity increases the need of automated techniques dedicated to different log analysis tasks such as Log-based Anomaly Detection (LAD).","meta":{"url":"http://arxiv.org/abs/2307.16714v1"},"cats":{"new-dataset":0.0325159985,"dev-research":0.3809410986,"prompt-eng":0.3707831034,"data-quality":0.1585714548,"ml-security":0.2693997881}}
{"text":"The latter has been widely addressed in the literature, mostly by means of different deep learning techniques.","meta":{"url":"http://arxiv.org/abs/2307.16714v1"},"cats":{"new-dataset":0.0257274257,"dev-research":0.239706289,"prompt-eng":0.3053247692,"data-quality":0.1923375978,"ml-security":0.1239914698}}
{"text":"Nevertheless, the focus on deep learning techniques results in less attention being paid to traditional Machine Learning (ML) techniques, which may perform well in many cases, depending on the context and the used datasets.","meta":{"url":"http://arxiv.org/abs/2307.16714v1"},"cats":{"new-dataset":0.0153507731,"dev-research":0.2842830342,"prompt-eng":0.2810413771,"data-quality":0.2176580609,"ml-security":0.3098472557}}
{"text":"Further, the evaluation of different ML techniques is mostly based on the assessment of their detection accuracy.","meta":{"url":"http://arxiv.org/abs/2307.16714v1"},"cats":{"new-dataset":0.0225144769,"dev-research":0.3039871529,"prompt-eng":0.4102862802,"data-quality":0.3217536647,"ml-security":0.1510359712}}
{"text":"However, this is is not enough to decide whether or not a specific ML technique is suitable to address the LAD problem.","meta":{"url":"http://arxiv.org/abs/2307.16714v1"},"cats":{"new-dataset":0.0077375277,"dev-research":0.2402440837,"prompt-eng":0.4173070927,"data-quality":0.2635559562,"ml-security":0.1636787506}}
{"text":"Other aspects to consider include the training and prediction time as well as the sensitivity to hyperparameter tuning.","meta":{"url":"http://arxiv.org/abs/2307.16714v1"},"cats":{"new-dataset":0.0241924701,"dev-research":0.2348096862,"prompt-eng":0.4798785176,"data-quality":0.1624214026,"ml-security":0.1160331208}}
{"text":"In this paper, we present a comprehensive empirical study, in which we evaluate different supervised and semi-supervised, traditional and deep ML techniques w.r.t.","meta":{"url":"http://arxiv.org/abs/2307.16714v1"},"cats":{"new-dataset":0.247898831,"dev-research":0.2298193665,"prompt-eng":0.4052821151,"data-quality":0.3717962988,"ml-security":0.1467556117}}
{"text":"four evaluation criteria: detection accuracy, time performance, sensitivity of detection accuracy as well as time performance to hyperparameter tuning.","meta":{"url":"http://arxiv.org/abs/2307.16714v1"},"cats":{"new-dataset":0.0324471093,"dev-research":0.2371264834,"prompt-eng":0.5124687707,"data-quality":0.2579179311,"ml-security":0.0830416237}}
{"text":"The experimental results show that supervised traditional and deep ML techniques perform very closely in terms of their detection accuracy and prediction time.","meta":{"url":"http://arxiv.org/abs/2307.16714v1"},"cats":{"new-dataset":0.046851741,"dev-research":0.2762016581,"prompt-eng":0.3727465247,"data-quality":0.3538512423,"ml-security":0.2405670765}}
{"text":"Moreover, the overall evaluation of the sensitivity of the detection accuracy of the different ML techniques to hyperparameter tuning shows that supervised traditional ML techniques are less sensitive to hyperparameter tuning than deep learning techniques.","meta":{"url":"http://arxiv.org/abs/2307.16714v1"},"cats":{"new-dataset":0.0133715396,"dev-research":0.2536616324,"prompt-eng":0.4117360003,"data-quality":0.4048547331,"ml-security":0.3107685464}}
{"text":"Further, semi-supervised techniques yield significantly worse detection accuracy than supervised techniques.","meta":{"url":"http://arxiv.org/abs/2307.16714v1"},"cats":{"new-dataset":0.0211286098,"dev-research":0.3121926248,"prompt-eng":0.4272534876,"data-quality":0.6287181026,"ml-security":0.256046999}}
{"text":"Encrypted traffic classification is receiving widespread attention from researchers and industrial companies.","meta":{"url":"http://arxiv.org/abs/2307.16713v1"},"cats":{"new-dataset":0.060523423,"dev-research":0.1971060206,"prompt-eng":0.3252645842,"data-quality":0.2197094884,"ml-security":0.5682330397}}
{"text":"However, the existing methods only extract flow-level features, failing to handle short flows because of unreliable statistical properties, or treat the header and payload equally, failing to mine the potential correlation between bytes.","meta":{"url":"http://arxiv.org/abs/2307.16713v1"},"cats":{"new-dataset":0.0506218204,"dev-research":0.2892617205,"prompt-eng":0.4166570764,"data-quality":0.2573477449,"ml-security":0.1304473422}}
{"text":"Therefore, in this paper, we propose a byte-level traffic graph construction approach based on point-wise mutual information (PMI), and a model named Temporal Fusion Encoder using Graph Neural Networks (TFE-GNN) for feature extraction.","meta":{"url":"http://arxiv.org/abs/2307.16713v1"},"cats":{"new-dataset":0.1908985151,"dev-research":0.2879812785,"prompt-eng":0.3268109027,"data-quality":0.1646310399,"ml-security":0.1519817976}}
{"text":"In particular, we design a dual embedding layer, a GNN-based traffic graph encoder as well as a cross-gated feature fusion mechanism, which can first embed the header and payload bytes separately and then fuses them together to obtain a stronger feature representation.","meta":{"url":"http://arxiv.org/abs/2307.16713v1"},"cats":{"new-dataset":0.1255763809,"dev-research":0.2859307418,"prompt-eng":0.3859935583,"data-quality":0.1791168904,"ml-security":0.2085397869}}
{"text":"The experimental results on two real datasets demonstrate that TFE-GNN outperforms multiple state-of-the-art methods in fine-grained encrypted traffic classification tasks.","meta":{"url":"http://arxiv.org/abs/2307.16713v1"},"cats":{"new-dataset":0.1703936681,"dev-research":0.1975905518,"prompt-eng":0.332465306,"data-quality":0.2426279082,"ml-security":0.4668187186}}
{"text":"Navigation of terrestrial robots is typically addressed either with localization and mapping (SLAM) followed by classical planning on the dynamically created maps, or by machine learning (ML), often through end-to-end training with reinforcement learning (RL) or imitation learning (IL).","meta":{"url":"http://arxiv.org/abs/2307.16710v1"},"cats":{"new-dataset":0.061459123,"dev-research":0.2272393609,"prompt-eng":0.3668926481,"data-quality":0.1007236789,"ml-security":0.1133966327}}
{"text":"Recently, modular designs have achieved promising results, and hybrid algorithms that combine ML with classical planning have been proposed.","meta":{"url":"http://arxiv.org/abs/2307.16710v1"},"cats":{"new-dataset":0.0398025295,"dev-research":0.2430164014,"prompt-eng":0.4140749495,"data-quality":0.0503878837,"ml-security":0.0820754145}}
{"text":"Existing methods implement these combinations with hand-crafted functions, which cannot fully exploit the complementary nature of the policies and the complex regularities between scene structure and planning performance.","meta":{"url":"http://arxiv.org/abs/2307.16710v1"},"cats":{"new-dataset":0.0847975482,"dev-research":0.2657448662,"prompt-eng":0.4154391537,"data-quality":0.0686153409,"ml-security":0.072494258}}
{"text":"Our work builds on the hypothesis that the strengths and weaknesses of neural planners and classical planners follow some regularities, which can be learned from training data, in particular from interactions.","meta":{"url":"http://arxiv.org/abs/2307.16710v1"},"cats":{"new-dataset":0.104709416,"dev-research":0.2956042093,"prompt-eng":0.4194428575,"data-quality":0.1347521732,"ml-security":0.23029106}}
{"text":"This is grounded on the assumption that, both, trained planners and the mapping algorithms underlying classical planning are subject to failure cases depending on the semantics of the scene and that this dependence is learnable: for instance, certain areas, objects or scene structures can be reconstructed easier than others.","meta":{"url":"http://arxiv.org/abs/2307.16710v1"},"cats":{"new-dataset":0.0205352389,"dev-research":0.3393239153,"prompt-eng":0.3666012315,"data-quality":0.2347582151,"ml-security":0.1908061141}}
{"text":"We propose a hierarchical method composed of a high-level planner dynamically switching between a classical and a neural planner.","meta":{"url":"http://arxiv.org/abs/2307.16710v1"},"cats":{"new-dataset":0.037391199,"dev-research":0.2739819324,"prompt-eng":0.4423972179,"data-quality":0.0619343016,"ml-security":0.0633323117}}
{"text":"We fully train all neural policies in simulation and evaluate the method in both simulation and real experiments with a LoCoBot robot, showing significant gains in performance, in particular in the real environment.","meta":{"url":"http://arxiv.org/abs/2307.16710v1"},"cats":{"new-dataset":0.1114228023,"dev-research":0.1982164958,"prompt-eng":0.4039827168,"data-quality":0.1008529638,"ml-security":0.1921556839}}
{"text":"We also qualitatively conjecture on the nature of data regularities exploited by the high-level planner.","meta":{"url":"http://arxiv.org/abs/2307.16710v1"},"cats":{"new-dataset":0.0963584161,"dev-research":0.2521602578,"prompt-eng":0.3877009734,"data-quality":0.1164156039,"ml-security":0.1923465792}}
{"text":"Phonetic information and linguistic knowledge are an essential component of a Text-to-speech (TTS) front-end.","meta":{"url":"http://arxiv.org/abs/2307.16709v1"},"cats":{"new-dataset":0.1125860483,"dev-research":0.2496774105,"prompt-eng":0.3787000355,"data-quality":0.2263325488,"ml-security":0.1011166077}}
{"text":"Given a language, a lexicon can be collected offline and Grapheme-to-Phoneme (G2P) relationships are usually modeled in order to predict the pronunciation for out-of-vocabulary (OOV) words.","meta":{"url":"http://arxiv.org/abs/2307.16709v1"},"cats":{"new-dataset":0.278734009,"dev-research":0.1997787196,"prompt-eng":0.380231796,"data-quality":0.2776952659,"ml-security":0.0775947458}}
{"text":"Additionally, post-lexical phonology, often defined in the form of rule-based systems, is used to correct pronunciation within or between words.","meta":{"url":"http://arxiv.org/abs/2307.16709v1"},"cats":{"new-dataset":0.0117445113,"dev-research":0.3529215095,"prompt-eng":0.3777629933,"data-quality":0.3508055628,"ml-security":0.0700078492}}
{"text":"In this work we showcase a multilingual unified front-end system that addresses any pronunciation related task, typically handled by separate modules.","meta":{"url":"http://arxiv.org/abs/2307.16709v1"},"cats":{"new-dataset":0.4557269278,"dev-research":0.2799959835,"prompt-eng":0.4819863678,"data-quality":0.2848629958,"ml-security":0.0854199005}}
{"text":"We evaluate the proposed model on G2P conversion and other language-specific challenges, such as homograph and polyphones disambiguation, post-lexical rules and implicit diacritization.","meta":{"url":"http://arxiv.org/abs/2307.16709v1"},"cats":{"new-dataset":0.1039725303,"dev-research":0.2618800189,"prompt-eng":0.4125677376,"data-quality":0.4032725873,"ml-security":0.072090717}}
{"text":"We find that the multilingual model is competitive across languages and tasks, however, some trade-offs exists when compared to equivalent monolingual solutions.","meta":{"url":"http://arxiv.org/abs/2307.16709v1"},"cats":{"new-dataset":0.0351967823,"dev-research":0.2336056487,"prompt-eng":0.3626039343,"data-quality":0.1515183919,"ml-security":0.0700499816}}
{"text":"We present a method for image-guided exploration for mobile robotic systems.","meta":{"url":"http://arxiv.org/abs/2307.16707v1"},"cats":{"new-dataset":0.0305891004,"dev-research":0.2213594432,"prompt-eng":0.4592266439,"data-quality":0.0932066732,"ml-security":0.0594405721}}
{"text":"Our approach extends ergodic exploration methods, a recent exploration approach that prioritizes complete coverage of a space, with the use of a learned image classifier that automatically detects objects and updates an information map to guide further exploration and localization of objects.","meta":{"url":"http://arxiv.org/abs/2307.16707v1"},"cats":{"new-dataset":0.2284797894,"dev-research":0.2175401709,"prompt-eng":0.4446935837,"data-quality":0.2313143742,"ml-security":0.1611040092}}
{"text":"Additionally, to improve outcomes of the information collected by our robot's visual sensor, we present a decomposition of the ergodic optimization problem as bi-level coarse and fine solvers, which act respectively on the robot's body and the robot's visual sensor.   ","meta":{"url":"http://arxiv.org/abs/2307.16707v1"},"cats":{"new-dataset":0.0861826851,"dev-research":0.2517874408,"prompt-eng":0.4336825643,"data-quality":0.1132211759,"ml-security":0.093114918}}
{"text":"Our approach is applied to geological survey and localization of rock formations for Mars rovers, with real images from Mars rovers used to train the image classifier.","meta":{"url":"http://arxiv.org/abs/2307.16707v1"},"cats":{"new-dataset":0.2727036989,"dev-research":0.2353215668,"prompt-eng":0.3977491605,"data-quality":0.2482769335,"ml-security":0.0979376093}}
{"text":"Results demonstrate 1) improved localization of rock formations compared to naive approaches while 2) minimizing the path length of the exploration through the bi-level exploration.","meta":{"url":"http://arxiv.org/abs/2307.16707v1"},"cats":{"new-dataset":0.0666408575,"dev-research":0.2800294233,"prompt-eng":0.3865636522,"data-quality":0.1382952627,"ml-security":0.0670913851}}
{"text":"The Lookahead optimizer improves the training stability of deep neural networks by having a set of fast weights that \"look ahead\" to guide the descent direction.","meta":{"url":"http://arxiv.org/abs/2307.16704v1"},"cats":{"new-dataset":0.0092383432,"dev-research":0.2518865291,"prompt-eng":0.3374146742,"data-quality":0.1032949967,"ml-security":0.1244376886}}
{"text":"Here, we combine this idea with sharpness-aware minimization (SAM) to stabilize its multi-step variant and improve the loss-sharpness trade-off.","meta":{"url":"http://arxiv.org/abs/2307.16704v1"},"cats":{"new-dataset":0.0204254457,"dev-research":0.2892645973,"prompt-eng":0.4622154545,"data-quality":0.3101895936,"ml-security":0.118545719}}
{"text":"We propose Lookbehind, which computes $k$ gradient ascent steps (\"looking behind\") at each iteration and combine the gradients to bias the descent step toward flatter minima.","meta":{"url":"http://arxiv.org/abs/2307.16704v1"},"cats":{"new-dataset":0.0336983025,"dev-research":0.1942269645,"prompt-eng":0.4097067544,"data-quality":0.1521912461,"ml-security":0.1817080854}}
{"text":"We apply Lookbehind on top of two popular sharpness-aware training methods -- SAM and adaptive SAM (ASAM) -- and show that our approach leads to a myriad of benefits across a variety of tasks and training regimes.","meta":{"url":"http://arxiv.org/abs/2307.16704v1"},"cats":{"new-dataset":0.0264968583,"dev-research":0.3009809884,"prompt-eng":0.4337103402,"data-quality":0.2115081405,"ml-security":0.124401067}}
{"text":"Particularly, we show increased generalization performance, greater robustness against noisy weights, and higher tolerance to catastrophic forgetting in lifelong learning settings.","meta":{"url":"http://arxiv.org/abs/2307.16704v1"},"cats":{"new-dataset":0.0285302966,"dev-research":0.271908706,"prompt-eng":0.4152893959,"data-quality":0.3610792162,"ml-security":0.3715504304}}
{"text":"We introduce and investigate forgetting 1-limited automata, which are single-tape Turing machines that, when visit a cell for the first time, replace the input symbol in it by a fixed symbol, so forgetting the original contents.","meta":{"url":"http://arxiv.org/abs/2307.16700v1"},"cats":{"new-dataset":0.1265407974,"dev-research":0.2789221424,"prompt-eng":0.4364699126,"data-quality":0.2166816713,"ml-security":0.1802929056}}
{"text":"These devices have the same computational power as finite automata, namely they characterize the class of regular languages.","meta":{"url":"http://arxiv.org/abs/2307.16700v1"},"cats":{"new-dataset":0.0365128269,"dev-research":0.2467213857,"prompt-eng":0.3811619775,"data-quality":0.1142393691,"ml-security":0.0991964804}}
{"text":"We study the cost in size of the conversions of forgetting 1-limited automata, in both nondeterministic and deterministic cases, into equivalent one-way nondeterministic and deterministic automata, providing optimal bounds in terms of exponential or superpolynomial functions.","meta":{"url":"http://arxiv.org/abs/2307.16700v1"},"cats":{"new-dataset":0.045955526,"dev-research":0.2144411397,"prompt-eng":0.4221178063,"data-quality":0.1363986969,"ml-security":0.190322275}}
{"text":"We also discuss the size relationships with two-way finite automata.","meta":{"url":"http://arxiv.org/abs/2307.16700v1"},"cats":{"new-dataset":0.0781081711,"dev-research":0.193494569,"prompt-eng":0.4130804341,"data-quality":0.135862292,"ml-security":0.1109160477}}
{"text":"In this respect, we prove the existence of a language for which forgetting 1-limited automata are exponentially larger than equivalent minimal deterministic two-way automata.","meta":{"url":"http://arxiv.org/abs/2307.16700v1"},"cats":{"new-dataset":0.0866088788,"dev-research":0.2016695252,"prompt-eng":0.4034050445,"data-quality":0.1594228269,"ml-security":0.1660608105}}
{"text":"We tackle the task of enriching ontologies by automatically translating natural language sentences into Description Logic.","meta":{"url":"http://arxiv.org/abs/2307.16699v1"},"cats":{"new-dataset":0.0761437262,"dev-research":0.3167012809,"prompt-eng":0.4473848884,"data-quality":0.1976659516,"ml-security":0.0649372862}}
{"text":"Since Large Language Models (LLMs) are the best tools for translations, we fine-tuned a GPT-3 model to convert Natural Language sentences into OWL Functional Syntax.","meta":{"url":"http://arxiv.org/abs/2307.16699v1"},"cats":{"new-dataset":0.2212745129,"dev-research":0.2293877066,"prompt-eng":0.4070272026,"data-quality":0.2290297104,"ml-security":0.051680556}}
{"text":"We employ objective and concise examples to fine-tune the model regarding: instances, class subsumption, domain and range of relations, object properties relationships, disjoint classes, complements, cardinality restrictions.","meta":{"url":"http://arxiv.org/abs/2307.16699v1"},"cats":{"new-dataset":0.1870937784,"dev-research":0.2372097301,"prompt-eng":0.4283979561,"data-quality":0.2241784658,"ml-security":0.1193560338}}
{"text":"The resulted axioms are used to enrich an ontology, in a human supervised manner.","meta":{"url":"http://arxiv.org/abs/2307.16699v1"},"cats":{"new-dataset":0.0267107762,"dev-research":0.3602915661,"prompt-eng":0.3810399433,"data-quality":0.2217814925,"ml-security":0.0797286142}}
{"text":"The developed tool is publicly provided as a Protge plugin.","meta":{"url":"http://arxiv.org/abs/2307.16699v1"},"cats":{"new-dataset":0.2944815326,"dev-research":0.374046869,"prompt-eng":0.3950471172,"data-quality":0.127576791,"ml-security":0.0972744468}}
{"text":"As a way of addressing increasingly sophisticated problems, software professionals face the constant challenge of seeking improvement.","meta":{"url":"http://arxiv.org/abs/2307.16696v1"},"cats":{"new-dataset":0.0319411371,"dev-research":0.5911295156,"prompt-eng":0.3963988404,"data-quality":0.2371627915,"ml-security":0.2048419491}}
{"text":"However, for these individuals to enhance their skills, their process of studying and training must involve feedback that is both immediate and accurate.","meta":{"url":"http://arxiv.org/abs/2307.16696v1"},"cats":{"new-dataset":0.0188928412,"dev-research":0.3925349455,"prompt-eng":0.4648978151,"data-quality":0.1924240233,"ml-security":0.0985496498}}
{"text":"In the context of software companies, where the scale of professionals undergoing training is large, but the number of qualified professionals available for providing corrections is small, delivering effective feedback becomes even more challenging.","meta":{"url":"http://arxiv.org/abs/2307.16696v1"},"cats":{"new-dataset":0.029470895,"dev-research":0.481948054,"prompt-eng":0.3862668975,"data-quality":0.3437927951,"ml-security":0.2625261325}}
{"text":"To circumvent this challenge, this work presents an exploration of using Large Language Models (LLMs) to support the correction process of open-ended questions in technical training.   ","meta":{"url":"http://arxiv.org/abs/2307.16696v1"},"cats":{"new-dataset":0.1606093409,"dev-research":0.2480261581,"prompt-eng":0.4773355701,"data-quality":0.4284412978,"ml-security":0.1416685709}}
{"text":"In this study, we utilized ChatGPT to correct open-ended questions answered by 42 industry professionals on two topics.","meta":{"url":"http://arxiv.org/abs/2307.16696v1"},"cats":{"new-dataset":0.1936576372,"dev-research":0.4044949953,"prompt-eng":0.3879318104,"data-quality":0.213240941,"ml-security":0.0706534062}}
{"text":"Evaluating the corrections and feedback provided by ChatGPT, we observed that it is capable of identifying semantic details in responses that other metrics cannot observe.","meta":{"url":"http://arxiv.org/abs/2307.16696v1"},"cats":{"new-dataset":0.1547099624,"dev-research":0.3313683163,"prompt-eng":0.4588122786,"data-quality":0.4241924932,"ml-security":0.0804767069}}
{"text":"Furthermore, we noticed that, in general, subject matter experts tended to agree with the corrections and feedback given by ChatGPT.","meta":{"url":"http://arxiv.org/abs/2307.16696v1"},"cats":{"new-dataset":0.0291927325,"dev-research":0.3827011835,"prompt-eng":0.3985821136,"data-quality":0.3403053457,"ml-security":0.0690310247}}
{"text":"Data uncertainties, such as sensor noise or occlusions, can introduce irreducible ambiguities in images, which result in varying, yet plausible, semantic hypotheses.","meta":{"url":"http://arxiv.org/abs/2307.16694v1"},"cats":{"new-dataset":0.0593373799,"dev-research":0.2913057723,"prompt-eng":0.4053856856,"data-quality":0.3974604867,"ml-security":0.1548268253}}
{"text":"In Machine Learning, this ambiguity is commonly referred to as aleatoric uncertainty.","meta":{"url":"http://arxiv.org/abs/2307.16694v1"},"cats":{"new-dataset":0.0049341797,"dev-research":0.2809362176,"prompt-eng":0.3614845905,"data-quality":0.3422990762,"ml-security":0.2117501884}}
{"text":"Latent density models can be utilized to address this problem in image segmentation.","meta":{"url":"http://arxiv.org/abs/2307.16694v1"},"cats":{"new-dataset":0.0694817022,"dev-research":0.1572138744,"prompt-eng":0.4295822869,"data-quality":0.2215229746,"ml-security":0.0761219105}}
{"text":"The most popular approach is the Probabilistic U-Net (PU-Net), which uses latent Normal densities to optimize the conditional data log-likelihood Evidence Lower Bound.","meta":{"url":"http://arxiv.org/abs/2307.16694v1"},"cats":{"new-dataset":0.0694664004,"dev-research":0.1930188205,"prompt-eng":0.4370405913,"data-quality":0.1564160597,"ml-security":0.1257926889}}
{"text":"In this work, we demonstrate that the PU- Net latent space is severely inhomogenous.","meta":{"url":"http://arxiv.org/abs/2307.16694v1"},"cats":{"new-dataset":0.0585244484,"dev-research":0.1376016665,"prompt-eng":0.3811265756,"data-quality":0.2256804306,"ml-security":0.1261067161}}
{"text":"As a result, the effectiveness of gradient descent is inhibited and the model becomes extremely sensitive to the localization of the latent space samples, resulting in defective predictions.","meta":{"url":"http://arxiv.org/abs/2307.16694v1"},"cats":{"new-dataset":0.0047847981,"dev-research":0.2453358208,"prompt-eng":0.3897154402,"data-quality":0.4301054749,"ml-security":0.4185209306}}
{"text":"To address this, we present the Sinkhorn PU-Net (SPU-Net), which uses the Sinkhorn Divergence to promote homogeneity across all latent dimensions, effectively improving gradient-descent updates and model robustness.","meta":{"url":"http://arxiv.org/abs/2307.16694v1"},"cats":{"new-dataset":0.0686462904,"dev-research":0.188971913,"prompt-eng":0.352784395,"data-quality":0.1664885788,"ml-security":0.2561505095}}
{"text":"Our results show that by applying this on public datasets of various clinical segmentation problems, the SPU-Net receives up to 11% performance gains compared against preceding latent variable models for probabilistic segmentation on the Hungarian-Matched metric.","meta":{"url":"http://arxiv.org/abs/2307.16694v1"},"cats":{"new-dataset":0.2300355535,"dev-research":0.2023483604,"prompt-eng":0.4043340394,"data-quality":0.1960664876,"ml-security":0.1038084224}}
{"text":"The results indicate that by encouraging a homogeneous latent space, one can significantly improve latent density modeling for medical image segmentation.","meta":{"url":"http://arxiv.org/abs/2307.16694v1"},"cats":{"new-dataset":0.0298203061,"dev-research":0.1799535948,"prompt-eng":0.386766404,"data-quality":0.1862134189,"ml-security":0.070354978}}
{"text":"The log-structured merge tree (LSM-tree) is widely employed to build key-value (KV) stores.","meta":{"url":"http://arxiv.org/abs/2307.16693v1"},"cats":{"new-dataset":0.231932022,"dev-research":0.3099425259,"prompt-eng":0.437336254,"data-quality":0.1414636188,"ml-security":0.0624641844}}
{"text":"LSM-tree organizes multiple levels in memory and on disk.","meta":{"url":"http://arxiv.org/abs/2307.16693v1"},"cats":{"new-dataset":0.105010155,"dev-research":0.2312177262,"prompt-eng":0.4122613259,"data-quality":0.1043921644,"ml-security":0.052123066}}
{"text":"The compaction of LSM-tree, which is used to redeploy KV pairs between on-disk levels in the form of SST files, severely stalls its foreground service.","meta":{"url":"http://arxiv.org/abs/2307.16693v1"},"cats":{"new-dataset":0.1689640066,"dev-research":0.2019093643,"prompt-eng":0.4144619681,"data-quality":0.1812444623,"ml-security":0.1308187237}}
{"text":"We overhaul and analyze the procedure of compaction.","meta":{"url":"http://arxiv.org/abs/2307.16693v1"},"cats":{"new-dataset":0.1441755645,"dev-research":0.2845850658,"prompt-eng":0.4105485106,"data-quality":0.1432337462,"ml-security":0.0429764288}}
{"text":"Writing and persisting files with fsyncs for compacted KV pairs are time-consuming and, more important, occur synchronously on the critical path of compaction.","meta":{"url":"http://arxiv.org/abs/2307.16693v1"},"cats":{"new-dataset":0.3082909467,"dev-research":0.3242935347,"prompt-eng":0.4130528661,"data-quality":0.1375974209,"ml-security":0.084331345}}
{"text":"The user-space compaction thread of LSM-tree stays waiting for completion signals from a kernel-space thread that is processing file write and fsync I/Os.   ","meta":{"url":"http://arxiv.org/abs/2307.16693v1"},"cats":{"new-dataset":0.050968567,"dev-research":0.2309385633,"prompt-eng":0.4153370637,"data-quality":0.1340711283,"ml-security":0.0837578407}}
{"text":"We accordingly design a new LSM-tree variant named AisLSM with an asynchronous I/O model.","meta":{"url":"http://arxiv.org/abs/2307.16693v1"},"cats":{"new-dataset":0.1346431393,"dev-research":0.1830492834,"prompt-eng":0.4872270967,"data-quality":0.1068744041,"ml-security":0.0846828352}}
{"text":"In short, AisLSM conducts asynchronous writes and fsyncs for SST files generated in a compaction and overlaps CPU computations with disk I/Os for consecutive compactions.","meta":{"url":"http://arxiv.org/abs/2307.16693v1"},"cats":{"new-dataset":0.106545387,"dev-research":0.312520354,"prompt-eng":0.4082374688,"data-quality":0.0867181636,"ml-security":0.1036776769}}
{"text":"AisLSM tracks the generation dependency between input and output files for each compaction and utilizes a deferred check-up strategy to ensure the durability of compacted KV pairs.","meta":{"url":"http://arxiv.org/abs/2307.16693v1"},"cats":{"new-dataset":0.2213851526,"dev-research":0.3521062799,"prompt-eng":0.4862749273,"data-quality":0.161503482,"ml-security":0.0917406515}}
{"text":"We prototype AisLSM with RocksDB and io_uring.","meta":{"url":"http://arxiv.org/abs/2307.16693v1"},"cats":{"new-dataset":0.3523325292,"dev-research":0.2554008856,"prompt-eng":0.4480322931,"data-quality":0.1375587997,"ml-security":0.072920228}}
{"text":"Experiments show that AisLSM boosts the performance of RocksDB by up to 2.14x, without losing data accessibility and consistency.","meta":{"url":"http://arxiv.org/abs/2307.16693v1"},"cats":{"new-dataset":0.1541858006,"dev-research":0.3328978495,"prompt-eng":0.3895939694,"data-quality":0.1888954039,"ml-security":0.1162213263}}
{"text":"It also outperforms state-of-the-art LSM-tree variants with significantly higher throughput and lower tail latency.","meta":{"url":"http://arxiv.org/abs/2307.16693v1"},"cats":{"new-dataset":0.0421687784,"dev-research":0.2285725186,"prompt-eng":0.4241552565,"data-quality":0.1166959122,"ml-security":0.0606575717}}
{"text":"The ability to handle miscommunication is crucial to robust and faithful conversational AI.","meta":{"url":"http://arxiv.org/abs/2307.16689v1"},"cats":{"new-dataset":0.0145301243,"dev-research":0.3562868853,"prompt-eng":0.3692811901,"data-quality":0.2528871629,"ml-security":0.232031388}}
{"text":"People usually deal with miscommunication immediately as they detect it, using highly systematic interactional mechanisms called repair.","meta":{"url":"http://arxiv.org/abs/2307.16689v1"},"cats":{"new-dataset":0.0114745165,"dev-research":0.4909022969,"prompt-eng":0.434317282,"data-quality":0.3768143724,"ml-security":0.2208214074}}
{"text":"One important type of repair is Third Position Repair (TPR) whereby a speaker is initially misunderstood but then corrects the misunderstanding as it becomes apparent after the addressee's erroneous response.","meta":{"url":"http://arxiv.org/abs/2307.16689v1"},"cats":{"new-dataset":0.0457552532,"dev-research":0.3786532858,"prompt-eng":0.4320154692,"data-quality":0.4668599044,"ml-security":0.1277933722}}
{"text":"Here, we collect and publicly release Repair-QA, the first large dataset of TPRs in a conversational question answering (QA) setting.","meta":{"url":"http://arxiv.org/abs/2307.16689v1"},"cats":{"new-dataset":0.8560376639,"dev-research":0.2648131244,"prompt-eng":0.4246552251,"data-quality":0.3155670411,"ml-security":0.1139914127}}
{"text":"The data is comprised of the TPR turns, corresponding dialogue contexts, and candidate repairs of the original turn for execution of TPRs.","meta":{"url":"http://arxiv.org/abs/2307.16689v1"},"cats":{"new-dataset":0.6600191772,"dev-research":0.2631227329,"prompt-eng":0.4487275797,"data-quality":0.2183662918,"ml-security":0.0851327742}}
{"text":"We demonstrate the usefulness of the data by training and evaluating strong baseline models for executing TPRs.","meta":{"url":"http://arxiv.org/abs/2307.16689v1"},"cats":{"new-dataset":0.2381883518,"dev-research":0.293295305,"prompt-eng":0.503224078,"data-quality":0.1820337735,"ml-security":0.1547719942}}
{"text":"For stand-alone TPR execution, we perform both automatic and human evaluations on a fine-tuned T5 model, as well as OpenAI's GPT-3 LLMs.","meta":{"url":"http://arxiv.org/abs/2307.16689v1"},"cats":{"new-dataset":0.1464085177,"dev-research":0.2782439223,"prompt-eng":0.5281603988,"data-quality":0.1309350764,"ml-security":0.13265033}}
{"text":"Additionally, we extrinsically evaluate the LLMs' TPR processing capabilities in the downstream conversational QA task.","meta":{"url":"http://arxiv.org/abs/2307.16689v1"},"cats":{"new-dataset":0.0901885417,"dev-research":0.2403192721,"prompt-eng":0.4931087863,"data-quality":0.152267898,"ml-security":0.0653946296}}
{"text":"The results indicate poor out-of-the-box performance on TPR's by the GPT-3 models, which then significantly improves when exposed to Repair-QA.","meta":{"url":"http://arxiv.org/abs/2307.16689v1"},"cats":{"new-dataset":0.0656565942,"dev-research":0.2979941005,"prompt-eng":0.4277466502,"data-quality":0.2264925349,"ml-security":0.0956315564}}
{"text":"Denoising diffusion probabilistic models that were initially proposed for realistic image generation have recently shown success in various perception tasks (e.g., object detection and image segmentation) and are increasingly gaining attention in computer vision.","meta":{"url":"http://arxiv.org/abs/2307.16687v1"},"cats":{"new-dataset":0.0459984924,"dev-research":0.2021987853,"prompt-eng":0.4023691005,"data-quality":0.1716232513,"ml-security":0.1329897513}}
{"text":"However, extending such models to multi-frame human pose estimation is non-trivial due to the presence of the additional temporal dimension in videos.","meta":{"url":"http://arxiv.org/abs/2307.16687v1"},"cats":{"new-dataset":0.104857568,"dev-research":0.1591376404,"prompt-eng":0.3473575126,"data-quality":0.0929340154,"ml-security":0.0959116606}}
{"text":"More importantly, learning representations that focus on keypoint regions is crucial for accurate localization of human joints.","meta":{"url":"http://arxiv.org/abs/2307.16687v1"},"cats":{"new-dataset":0.0956932663,"dev-research":0.2846075897,"prompt-eng":0.3536569444,"data-quality":0.1292661097,"ml-security":0.0699206156}}
{"text":"Nevertheless, the adaptation of the diffusion-based methods remains unclear on how to achieve such objective.","meta":{"url":"http://arxiv.org/abs/2307.16687v1"},"cats":{"new-dataset":0.0022922325,"dev-research":0.195670991,"prompt-eng":0.3279198636,"data-quality":0.1873648551,"ml-security":0.1138394586}}
{"text":"In this paper, we present DiffPose, a novel diffusion architecture that formulates video-based human pose estimation as a conditional heatmap generation problem.","meta":{"url":"http://arxiv.org/abs/2307.16687v1"},"cats":{"new-dataset":0.3623516735,"dev-research":0.2036335391,"prompt-eng":0.3844787248,"data-quality":0.073387355,"ml-security":0.0881453837}}
{"text":"First, to better leverage temporal information, we propose SpatioTemporal Representation Learner which aggregates visual evidences across frames and uses the resulting features in each denoising step as a condition.","meta":{"url":"http://arxiv.org/abs/2307.16687v1"},"cats":{"new-dataset":0.3356337287,"dev-research":0.3065584981,"prompt-eng":0.3626567238,"data-quality":0.2033605108,"ml-security":0.1001340906}}
{"text":"In addition, we present a mechanism called Lookup-based MultiScale Feature Interaction that determines the correlations between local joints and global contexts across multiple scales.","meta":{"url":"http://arxiv.org/abs/2307.16687v1"},"cats":{"new-dataset":0.0563608635,"dev-research":0.3101742264,"prompt-eng":0.4351417437,"data-quality":0.1587442455,"ml-security":0.0628614122}}
{"text":"This mechanism generates delicate representations that focus on keypoint regions.","meta":{"url":"http://arxiv.org/abs/2307.16687v1"},"cats":{"new-dataset":0.0138547265,"dev-research":0.3194572874,"prompt-eng":0.4206242709,"data-quality":0.108733008,"ml-security":0.1336136017}}
{"text":"Altogether, by extending diffusion models, we show two unique characteristics from DiffPose on pose estimation task: (i) the ability to combine multiple sets of pose estimates to improve prediction accuracy, particularly for challenging joints, and (ii) the ability to adjust the number of iterative steps for feature refinement without retraining the model.","meta":{"url":"http://arxiv.org/abs/2307.16687v1"},"cats":{"new-dataset":0.1905389604,"dev-research":0.2388137737,"prompt-eng":0.3773200717,"data-quality":0.1237465507,"ml-security":0.1189023002}}
{"text":"DiffPose sets new state-of-the-art results on three benchmarks: PoseTrack2017, PoseTrack2018, and PoseTrack21.","meta":{"url":"http://arxiv.org/abs/2307.16687v1"},"cats":{"new-dataset":0.5273559756,"dev-research":0.3009214619,"prompt-eng":0.3643376086,"data-quality":0.1549946628,"ml-security":0.0796398616}}
{"text":"Image captioning is conventionally formulated as the task of generating captions for images that match the distribution of reference image-caption pairs.","meta":{"url":"http://arxiv.org/abs/2307.16686v1"},"cats":{"new-dataset":0.1573585227,"dev-research":0.2546443908,"prompt-eng":0.4273261232,"data-quality":0.292309597,"ml-security":0.071150585}}
{"text":"However, reference captions in standard captioning datasets are short and may not uniquely identify the images they describe.","meta":{"url":"http://arxiv.org/abs/2307.16686v1"},"cats":{"new-dataset":0.3589622828,"dev-research":0.1965588154,"prompt-eng":0.3230169139,"data-quality":0.4013051135,"ml-security":0.1108055141}}
{"text":"These problems are further exacerbated when models are trained directly on image-alt text pairs collected from the internet.","meta":{"url":"http://arxiv.org/abs/2307.16686v1"},"cats":{"new-dataset":0.0690848906,"dev-research":0.2599023611,"prompt-eng":0.4063650638,"data-quality":0.4702661752,"ml-security":0.2698658229}}
{"text":"In this work, we show that it is possible to generate more specific captions with minimal changes to the training process.","meta":{"url":"http://arxiv.org/abs/2307.16686v1"},"cats":{"new-dataset":0.3432680005,"dev-research":0.2395308614,"prompt-eng":0.4674720219,"data-quality":0.3311806158,"ml-security":0.0917353955}}
{"text":"We implement classifier-free guidance for an autoregressive captioning model by fine-tuning it to estimate both conditional and unconditional distributions over captions.","meta":{"url":"http://arxiv.org/abs/2307.16686v1"},"cats":{"new-dataset":0.1249184178,"dev-research":0.2072294069,"prompt-eng":0.4702659727,"data-quality":0.397947708,"ml-security":0.1081257361}}
{"text":"The guidance scale applied at decoding controls a trade-off between maximizing $p(\\mathrm{caption}|\\mathrm{image})$ and $p(\\mathrm{image}|\\mathrm{caption})$. Compared to standard greedy decoding, decoding with a guidance scale of 2 substantially improves reference-free metrics such as CLIPScore (0.808 vs. 0.775) and caption$\\to$image retrieval performance in the CLIP embedding space (recall@1 44.6% vs. 26.5%), but worsens standard reference-based captioning metrics (e.g., CIDEr 78.6 vs 126.1).","meta":{"url":"http://arxiv.org/abs/2307.16686v1"},"cats":{"new-dataset":0.0495087639,"dev-research":0.2292596712,"prompt-eng":0.4020278093,"data-quality":0.256996974,"ml-security":0.0935349602}}
{"text":"We further explore the use of language models to guide the decoding process, obtaining small improvements over the Pareto frontier of reference-free vs. reference-based captioning metrics that arises from classifier-free guidance, and substantially improving the quality of captions generated from a model trained only on minimally curated web data.","meta":{"url":"http://arxiv.org/abs/2307.16686v1"},"cats":{"new-dataset":0.2558574633,"dev-research":0.2558218249,"prompt-eng":0.4151285558,"data-quality":0.4054159766,"ml-security":0.1274426573}}
{"text":"Responsibility anticipation is the process of determining if the actions of an individual agent may cause it to be responsible for a particular outcome.","meta":{"url":"http://arxiv.org/abs/2307.16685v1"},"cats":{"new-dataset":0.0241498831,"dev-research":0.2930703162,"prompt-eng":0.4489820585,"data-quality":0.0987416938,"ml-security":0.1728545748}}
{"text":"This can be used in a multi-agent planning setting to allow agents to anticipate responsibility in the plans they consider.","meta":{"url":"http://arxiv.org/abs/2307.16685v1"},"cats":{"new-dataset":0.0500085533,"dev-research":0.3165508102,"prompt-eng":0.4411768639,"data-quality":0.0741308301,"ml-security":0.1051368345}}
{"text":"The planning setting in this paper includes partial information regarding the initial state and considers formulas in linear temporal logic as positive or negative outcomes to be attained or avoided.","meta":{"url":"http://arxiv.org/abs/2307.16685v1"},"cats":{"new-dataset":0.0481851138,"dev-research":0.3522553245,"prompt-eng":0.4307798579,"data-quality":0.0601708591,"ml-security":0.0600830404}}
{"text":"We firstly define attribution for notions of active, passive and contributive responsibility, and consider their agentive variants.","meta":{"url":"http://arxiv.org/abs/2307.16685v1"},"cats":{"new-dataset":0.0904196644,"dev-research":0.2864281804,"prompt-eng":0.4025208216,"data-quality":0.1584407823,"ml-security":0.1662771483}}
{"text":"We then use these to define the notion of responsibility anticipation.","meta":{"url":"http://arxiv.org/abs/2307.16685v1"},"cats":{"new-dataset":0.0350766605,"dev-research":0.3014931541,"prompt-eng":0.4355223579,"data-quality":0.1403892118,"ml-security":0.1299284803}}
{"text":"We prove that our notions of anticipated responsibility can be used to coordinate agents in a planning setting and give complexity results for our model, discussing equivalence with classical planning.","meta":{"url":"http://arxiv.org/abs/2307.16685v1"},"cats":{"new-dataset":0.0851624519,"dev-research":0.2204351752,"prompt-eng":0.4278999859,"data-quality":0.0690685667,"ml-security":0.1148787893}}
{"text":"We also present an outline for solving some of our attribution and anticipation problems using PDDL solvers.","meta":{"url":"http://arxiv.org/abs/2307.16685v1"},"cats":{"new-dataset":0.2355094134,"dev-research":0.3139163342,"prompt-eng":0.5091713013,"data-quality":0.1993722217,"ml-security":0.0888772224}}
{"text":"Diffusion models and large language models have emerged as leading-edge generative models and have sparked a revolutionary impact on various aspects of human life.","meta":{"url":"http://arxiv.org/abs/2307.16680v1"},"cats":{"new-dataset":0.0253340349,"dev-research":0.191310361,"prompt-eng":0.324323932,"data-quality":0.1338269263,"ml-security":0.1303105401}}
{"text":"However, the practical implementation of these models has also exposed inherent risks, highlighting their dual nature and raising concerns regarding their trustworthiness.","meta":{"url":"http://arxiv.org/abs/2307.16680v1"},"cats":{"new-dataset":0.0231457723,"dev-research":0.27473673,"prompt-eng":0.3818031467,"data-quality":0.1591913851,"ml-security":0.5189167053}}
{"text":"Despite the abundance of literature on this subject, a comprehensive survey specifically delving into the intersection of large-scale generative models and their trustworthiness remains largely absent.","meta":{"url":"http://arxiv.org/abs/2307.16680v1"},"cats":{"new-dataset":0.0506790918,"dev-research":0.1825900925,"prompt-eng":0.4532945309,"data-quality":0.1994599202,"ml-security":0.1642454687}}
{"text":"To bridge this gap, This paper investigates both the long-standing and emerging threats associated with these models across four fundamental dimensions: privacy, security, fairness, and responsibility.","meta":{"url":"http://arxiv.org/abs/2307.16680v1"},"cats":{"new-dataset":0.0858404748,"dev-research":0.1954375872,"prompt-eng":0.3292744341,"data-quality":0.1369828608,"ml-security":0.7185945319}}
{"text":"In this way, we construct an extensive map outlining the trustworthiness of these models, while also providing practical recommendations and identifying future directions.","meta":{"url":"http://arxiv.org/abs/2307.16680v1"},"cats":{"new-dataset":0.1438144649,"dev-research":0.2204044397,"prompt-eng":0.5104367121,"data-quality":0.1776192166,"ml-security":0.168418405}}
{"text":"These efforts are crucial for promoting the trustworthy deployment of these models, ultimately benefiting society as a whole.","meta":{"url":"http://arxiv.org/abs/2307.16680v1"},"cats":{"new-dataset":0.0198068932,"dev-research":0.2201459434,"prompt-eng":0.4109592567,"data-quality":0.1278536045,"ml-security":0.2346576936}}
{"text":"Legged locomotion is arguably the most suited and versatile mode to deal with natural or unstructured terrains.","meta":{"url":"http://arxiv.org/abs/2307.16676v1"},"cats":{"new-dataset":0.0175345194,"dev-research":0.2316676379,"prompt-eng":0.3578600721,"data-quality":0.0446290599,"ml-security":0.0767971656}}
{"text":"Intensive research into dynamic walking and running controllers has recently yielded great advances, both in the optimal control and reinforcement learning (RL) literature.","meta":{"url":"http://arxiv.org/abs/2307.16676v1"},"cats":{"new-dataset":0.0705027202,"dev-research":0.1896452787,"prompt-eng":0.3519232066,"data-quality":0.07068911,"ml-security":0.1166032638}}
{"text":"Hopping is a challenging dynamic task involving a flight phase and has the potential to increase the traversability of legged robots.","meta":{"url":"http://arxiv.org/abs/2307.16676v1"},"cats":{"new-dataset":0.039201657,"dev-research":0.2392182705,"prompt-eng":0.404536177,"data-quality":0.0618123145,"ml-security":0.0790187697}}
{"text":"Model based control for hopping typically relies on accurate detection of different jump phases, such as lift-off or touch down, and using different controllers for each phase.","meta":{"url":"http://arxiv.org/abs/2307.16676v1"},"cats":{"new-dataset":0.0063259934,"dev-research":0.252663951,"prompt-eng":0.449419374,"data-quality":0.0767905665,"ml-security":0.1004041689}}
{"text":"In this paper, we present a end-to-end RL based torque controller that learns to implicitly detect the relevant jump phases, removing the need to provide manual heuristics for state detection.","meta":{"url":"http://arxiv.org/abs/2307.16676v1"},"cats":{"new-dataset":0.0707890452,"dev-research":0.2670300511,"prompt-eng":0.4565522823,"data-quality":0.08706731,"ml-security":0.1180009858}}
{"text":"We also extend a method for simulation to reality transfer of the learned controller to contact rich dynamic tasks, resulting in successful deployment on the robot after training without parameter tuning.","meta":{"url":"http://arxiv.org/abs/2307.16676v1"},"cats":{"new-dataset":0.1270884265,"dev-research":0.2424155116,"prompt-eng":0.4687591098,"data-quality":0.0824185779,"ml-security":0.1433020801}}
{"text":"The goal of Online Domain Adaptation for semantic segmentation is to handle unforeseeable domain changes that occur during deployment, like sudden weather events.","meta":{"url":"http://arxiv.org/abs/2307.15063v1"},"cats":{"new-dataset":0.1040627455,"dev-research":0.2886390974,"prompt-eng":0.4320450862,"data-quality":0.2796465189,"ml-security":0.1583144453}}
{"text":"However, the high computational costs associated with brute-force adaptation make this paradigm unfeasible for real-world applications.","meta":{"url":"http://arxiv.org/abs/2307.15063v1"},"cats":{"new-dataset":0.0146203076,"dev-research":0.2853574215,"prompt-eng":0.4075191602,"data-quality":0.1379795052,"ml-security":0.2679897433}}
{"text":"In this paper we propose HAMLET, a Hardware-Aware Modular Least Expensive Training framework for real-time domain adaptation.","meta":{"url":"http://arxiv.org/abs/2307.15063v1"},"cats":{"new-dataset":0.0967680397,"dev-research":0.3129328137,"prompt-eng":0.4091394962,"data-quality":0.1575502879,"ml-security":0.2058344547}}
{"text":"Our approach includes a hardware-aware back-propagation orchestration agent (HAMT) and a dedicated domain-shift detector that enables active control over when and how the model is adapted (LT).","meta":{"url":"http://arxiv.org/abs/2307.15063v1"},"cats":{"new-dataset":0.0935051951,"dev-research":0.2790719294,"prompt-eng":0.5126528771,"data-quality":0.1627434755,"ml-security":0.1678351103}}
{"text":"Thanks to these advancements, our approach is capable of performing semantic segmentation while simultaneously adapting at more than 29FPS on a single consumer-grade GPU.","meta":{"url":"http://arxiv.org/abs/2307.15063v1"},"cats":{"new-dataset":0.343591534,"dev-research":0.2385154429,"prompt-eng":0.4434569292,"data-quality":0.1944622467,"ml-security":0.0562965786}}
{"text":"Our framework's encouraging accuracy and speed trade-off is demonstrated on OnDA and SHIFT benchmarks through experimental results.","meta":{"url":"http://arxiv.org/abs/2307.15063v1"},"cats":{"new-dataset":0.1562292226,"dev-research":0.3150815051,"prompt-eng":0.4356401418,"data-quality":0.1922155915,"ml-security":0.1008563906}}
{"text":"Acoustic matching aims to re-synthesize an audio clip to sound as if it were recorded in a target acoustic environment.","meta":{"url":"http://arxiv.org/abs/2307.15064v1"},"cats":{"new-dataset":0.035203769,"dev-research":0.2877557191,"prompt-eng":0.3968855794,"data-quality":0.2224939716,"ml-security":0.0975767118}}
{"text":"Existing methods assume access to paired training data, where the audio is observed in both source and target environments, but this limits the diversity of training data or requires the use of simulated data or heuristics to create paired samples.","meta":{"url":"http://arxiv.org/abs/2307.15064v1"},"cats":{"new-dataset":0.1648938862,"dev-research":0.224356118,"prompt-eng":0.3244989693,"data-quality":0.2015978543,"ml-security":0.1612206475}}
{"text":"We propose a self-supervised approach to visual acoustic matching where training samples include only the target scene image and audio -- without acoustically mismatched source audio for reference.","meta":{"url":"http://arxiv.org/abs/2307.15064v1"},"cats":{"new-dataset":0.128223296,"dev-research":0.2172050544,"prompt-eng":0.3791136764,"data-quality":0.5323906687,"ml-security":0.1181289487}}
{"text":"Our approach jointly learns to disentangle room acoustics and re-synthesize audio into the target environment, via a conditional GAN framework and a novel metric that quantifies the level of residual acoustic information in the de-biased audio.","meta":{"url":"http://arxiv.org/abs/2307.15064v1"},"cats":{"new-dataset":0.1342806216,"dev-research":0.268203755,"prompt-eng":0.3461628379,"data-quality":0.2388725673,"ml-security":0.2041196348}}
{"text":"Training with either in-the-wild web data or simulated data, we demonstrate it outperforms the state-of-the-art on multiple challenging datasets and a wide variety of real-world audio and environments.","meta":{"url":"http://arxiv.org/abs/2307.15064v1"},"cats":{"new-dataset":0.5729718882,"dev-research":0.2133918849,"prompt-eng":0.3576287118,"data-quality":0.340515843,"ml-security":0.1528722244}}
{"text":"Accurate depth estimation under out-of-distribution (OoD) scenarios, such as adverse weather conditions, sensor failure, and noise contamination, is desirable for safety-critical applications.","meta":{"url":"http://arxiv.org/abs/2307.15061v1"},"cats":{"new-dataset":0.2301047233,"dev-research":0.22011485,"prompt-eng":0.3659382213,"data-quality":0.2205898774,"ml-security":0.1785974868}}
{"text":"Existing depth estimation systems, however, suffer inevitably from real-world corruptions and perturbations and are struggled to provide reliable depth predictions under such cases.","meta":{"url":"http://arxiv.org/abs/2307.15061v1"},"cats":{"new-dataset":0.2102934246,"dev-research":0.2670215205,"prompt-eng":0.3885702127,"data-quality":0.2951149045,"ml-security":0.2376445655}}
{"text":"In this paper, we summarize the winning solutions from the RoboDepth Challenge -- an academic competition designed to facilitate and advance robust OoD depth estimation.","meta":{"url":"http://arxiv.org/abs/2307.15061v1"},"cats":{"new-dataset":0.2711846794,"dev-research":0.1686259231,"prompt-eng":0.3999444034,"data-quality":0.1650561043,"ml-security":0.1383929671}}
{"text":"This challenge was developed based on the newly established KITTI-C and NYUDepth2-C benchmarks.","meta":{"url":"http://arxiv.org/abs/2307.15061v1"},"cats":{"new-dataset":0.3116440653,"dev-research":0.2526403282,"prompt-eng":0.4376572001,"data-quality":0.1574882648,"ml-security":0.0645721666}}
{"text":"We hosted two stand-alone tracks, with an emphasis on robust self-supervised and robust fully-supervised depth estimation, respectively.","meta":{"url":"http://arxiv.org/abs/2307.15061v1"},"cats":{"new-dataset":0.3520391289,"dev-research":0.1933750357,"prompt-eng":0.425145022,"data-quality":0.3023477056,"ml-security":0.081674914}}
{"text":"Out of more than two hundred participants, nine unique and top-performing solutions have appeared, with novel designs ranging from the following aspects: spatial- and frequency-domain augmentations, masked image modeling, image restoration and super-resolution, adversarial training, diffusion-based noise suppression, vision-language pre-training, learned model ensembling, and hierarchical feature enhancement.","meta":{"url":"http://arxiv.org/abs/2307.15061v1"},"cats":{"new-dataset":0.1247413105,"dev-research":0.2032993831,"prompt-eng":0.3977346564,"data-quality":0.1932201126,"ml-security":0.2421912364}}
{"text":"Extensive experimental analyses along with insightful observations are drawn to better understand the rationale behind each design.","meta":{"url":"http://arxiv.org/abs/2307.15061v1"},"cats":{"new-dataset":0.0155154802,"dev-research":0.4090138046,"prompt-eng":0.3937882664,"data-quality":0.1295211199,"ml-security":0.0956326349}}
{"text":"We hope this challenge could lay a solid foundation for future research on robust and reliable depth estimation and beyond.","meta":{"url":"http://arxiv.org/abs/2307.15061v1"},"cats":{"new-dataset":0.2643034982,"dev-research":0.1838152938,"prompt-eng":0.4481441539,"data-quality":0.190295374,"ml-security":0.0926214915}}
{"text":"The datasets, competition toolkit, workshop recordings, and source code from the winning teams are publicly available on the challenge website.","meta":{"url":"http://arxiv.org/abs/2307.15061v1"},"cats":{"new-dataset":0.8709204496,"dev-research":0.2463079949,"prompt-eng":0.3295063866,"data-quality":0.1787547168,"ml-security":0.1723621753}}
{"text":"Nowadays, autonomous cars can drive smoothly in ordinary cases, and it is widely recognized that realistic sensor simulation will play a critical role in solving remaining corner cases by simulating them.","meta":{"url":"http://arxiv.org/abs/2307.15058v1"},"cats":{"new-dataset":0.0865638099,"dev-research":0.2640094906,"prompt-eng":0.3981983565,"data-quality":0.111084047,"ml-security":0.1908518422}}
{"text":"To this end, we propose an autonomous driving simulator based upon neural radiance fields (NeRFs).","meta":{"url":"http://arxiv.org/abs/2307.15058v1"},"cats":{"new-dataset":0.174310918,"dev-research":0.224529928,"prompt-eng":0.4185420918,"data-quality":0.1147402139,"ml-security":0.2009489845}}
{"text":"Compared with existing works, ours has three notable features: (1) Instance-aware.","meta":{"url":"http://arxiv.org/abs/2307.15058v1"},"cats":{"new-dataset":0.0733693083,"dev-research":0.2973517784,"prompt-eng":0.4433521585,"data-quality":0.2069890083,"ml-security":0.1155689624}}
{"text":"Our simulator models the foreground instances and background environments separately with independent networks so that the static (e.g., size and appearance) and dynamic (e.g., trajectory) properties of instances can be controlled separately.","meta":{"url":"http://arxiv.org/abs/2307.15058v1"},"cats":{"new-dataset":0.1190545769,"dev-research":0.2333846006,"prompt-eng":0.4358568099,"data-quality":0.1153776217,"ml-security":0.2157258144}}
{"text":"(2) Modular.","meta":{"url":"http://arxiv.org/abs/2307.15058v1"},"cats":{"new-dataset":0.0990722559,"dev-research":0.1974000275,"prompt-eng":0.4006604241,"data-quality":0.1353248195,"ml-security":0.0962681647}}
{"text":"Our simulator allows flexible switching between different modern NeRF-related backbones, sampling strategies, input modalities, etc.","meta":{"url":"http://arxiv.org/abs/2307.15058v1"},"cats":{"new-dataset":0.0734635685,"dev-research":0.2781498386,"prompt-eng":0.4584623877,"data-quality":0.1312081936,"ml-security":0.2067614111}}
{"text":"We expect this modular design to boost academic progress and industrial deployment of NeRF-based autonomous driving simulation.","meta":{"url":"http://arxiv.org/abs/2307.15058v1"},"cats":{"new-dataset":0.1007823573,"dev-research":0.279544099,"prompt-eng":0.4355433127,"data-quality":0.1057063683,"ml-security":0.1558249513}}
{"text":"(3) Realistic.","meta":{"url":"http://arxiv.org/abs/2307.15058v1"},"cats":{"new-dataset":0.1403937586,"dev-research":0.2033187606,"prompt-eng":0.3698716374,"data-quality":0.1308871227,"ml-security":0.0905741897}}
{"text":"Our simulator set new state-of-the-art photo-realism results given the best module selection.","meta":{"url":"http://arxiv.org/abs/2307.15058v1"},"cats":{"new-dataset":0.2880734662,"dev-research":0.2106360885,"prompt-eng":0.4260655497,"data-quality":0.1136915463,"ml-security":0.0623433195}}
{"text":"Our simulator will be open-sourced while most of our counterparts are not.","meta":{"url":"http://arxiv.org/abs/2307.15058v1"},"cats":{"new-dataset":0.3747331223,"dev-research":0.3152166692,"prompt-eng":0.3460369923,"data-quality":0.1489536207,"ml-security":0.2611725423}}
{"text":"Project page: https://open-air-sun.github.io/mars/.","meta":{"url":"http://arxiv.org/abs/2307.15058v1"},"cats":{"new-dataset":0.6402084315,"dev-research":0.2391788717,"prompt-eng":0.3917553437,"data-quality":0.1163488898,"ml-security":0.0640520919}}
{"text":"Today's network measurements rely heavily on Internet-wide scanning, employing tools like ZMap that are capable of quickly iterating over the entire IPv4 address space.","meta":{"url":"http://arxiv.org/abs/2307.15057v1"},"cats":{"new-dataset":0.0932760532,"dev-research":0.2860036561,"prompt-eng":0.3738759324,"data-quality":0.1157847486,"ml-security":0.1214400248}}
{"text":"Unfortunately, IPv6's vast address space poses an existential threat for Internet-wide scans and traditional network measurement techniques.","meta":{"url":"http://arxiv.org/abs/2307.15057v1"},"cats":{"new-dataset":0.11833349,"dev-research":0.2326125245,"prompt-eng":0.3480518109,"data-quality":0.1557914044,"ml-security":0.4145648124}}
{"text":"To address this reality, efforts are underway to develop ``hitlists'' of known-active IPv6 addresses to reduce the search space for would-be scanners.","meta":{"url":"http://arxiv.org/abs/2307.15057v1"},"cats":{"new-dataset":0.2802442417,"dev-research":0.3270630514,"prompt-eng":0.4066615701,"data-quality":0.1747280726,"ml-security":0.2241047699}}
{"text":"As a result, there is an inexorable push for constructing as large and complete a hitlist as possible.   ","meta":{"url":"http://arxiv.org/abs/2307.15057v1"},"cats":{"new-dataset":0.2156972588,"dev-research":0.3344658345,"prompt-eng":0.4534596737,"data-quality":0.1609788109,"ml-security":0.1121472603}}
{"text":"This paper asks: what are the potential benefits and harms when IPv6 hitlists grow larger?","meta":{"url":"http://arxiv.org/abs/2307.15057v1"},"cats":{"new-dataset":0.0378411017,"dev-research":0.2898206209,"prompt-eng":0.2844307508,"data-quality":0.187443979,"ml-security":0.3654383262}}
{"text":"To answer this question, we obtain the largest IPv6 active-address list to date: 7.9 billion addresses, 898 times larger than the current state-of-the-art hitlist.","meta":{"url":"http://arxiv.org/abs/2307.15057v1"},"cats":{"new-dataset":0.5522854843,"dev-research":0.2256567821,"prompt-eng":0.3196406983,"data-quality":0.12014717,"ml-security":0.1813777914}}
{"text":"Although our list is not comprehensive, it is a significant step forward and provides a glimpse into the type of analyses possible with more complete hitlists.   ","meta":{"url":"http://arxiv.org/abs/2307.15057v1"},"cats":{"new-dataset":0.3533493762,"dev-research":0.2368847541,"prompt-eng":0.4125850072,"data-quality":0.1285044745,"ml-security":0.0895792674}}
{"text":"We compare our dataset to prior IPv6 hitlists and show both benefits and dangers.","meta":{"url":"http://arxiv.org/abs/2307.15057v1"},"cats":{"new-dataset":0.4821257353,"dev-research":0.2768749529,"prompt-eng":0.3637479003,"data-quality":0.1954679901,"ml-security":0.3189919086}}
{"text":"The benefits include improved insight into client devices (prior datasets consist primarily of routers), outage detection, IPv6 roll-out, previously unknown aliased networks, and address assignment strategies.","meta":{"url":"http://arxiv.org/abs/2307.15057v1"},"cats":{"new-dataset":0.0207247113,"dev-research":0.380060912,"prompt-eng":0.3169864182,"data-quality":0.1147720327,"ml-security":0.2356551956}}
{"text":"The dangers, unfortunately, are severe: we expose widespread instances of addresses that permit user tracking and device geolocation, and a dearth of firewalls in home networks.","meta":{"url":"http://arxiv.org/abs/2307.15057v1"},"cats":{"new-dataset":0.2444833554,"dev-research":0.3162500567,"prompt-eng":0.4149313827,"data-quality":0.2187661867,"ml-security":0.4537919917}}
{"text":"We discuss ethics and security guidelines to ensure a safe path towards more complete hitlists.","meta":{"url":"http://arxiv.org/abs/2307.15057v1"},"cats":{"new-dataset":0.3198134273,"dev-research":0.350313545,"prompt-eng":0.4006323383,"data-quality":0.1876050809,"ml-security":0.4579038098}}
{"text":"We introduce PointOdyssey, a large-scale synthetic dataset, and data generation framework, for the training and evaluation of long-term fine-grained tracking algorithms.","meta":{"url":"http://arxiv.org/abs/2307.15055v1"},"cats":{"new-dataset":0.7054961519,"dev-research":0.2562761479,"prompt-eng":0.3882758043,"data-quality":0.2339978087,"ml-security":0.1013877531}}
{"text":"Our goal is to advance the state-of-the-art by placing emphasis on long videos with naturalistic motion.","meta":{"url":"http://arxiv.org/abs/2307.15055v1"},"cats":{"new-dataset":0.1560160721,"dev-research":0.2100413807,"prompt-eng":0.3679590591,"data-quality":0.1098496254,"ml-security":0.0426045256}}
{"text":"Toward the goal of naturalism, we animate deformable characters using real-world motion capture data, we build 3D scenes to match the motion capture environments, and we render camera viewpoints using trajectories mined via structure-from-motion on real videos.","meta":{"url":"http://arxiv.org/abs/2307.15055v1"},"cats":{"new-dataset":0.5993397705,"dev-research":0.2193203331,"prompt-eng":0.3882589359,"data-quality":0.0997424199,"ml-security":0.0887728758}}
{"text":"We create combinatorial diversity by randomizing character appearance, motion profiles, materials, lighting, 3D assets, and atmospheric effects.","meta":{"url":"http://arxiv.org/abs/2307.15055v1"},"cats":{"new-dataset":0.3460745196,"dev-research":0.1633930937,"prompt-eng":0.40257816,"data-quality":0.1004807834,"ml-security":0.08102263}}
{"text":"Our dataset currently includes 104 videos, averaging 2,000 frames long, with orders of magnitude more correspondence annotations than prior work.","meta":{"url":"http://arxiv.org/abs/2307.15055v1"},"cats":{"new-dataset":0.763557366,"dev-research":0.2139015788,"prompt-eng":0.3205747475,"data-quality":0.253822897,"ml-security":0.0676820433}}
{"text":"We show that existing methods can be trained from scratch in our dataset and outperform the published variants.","meta":{"url":"http://arxiv.org/abs/2307.15055v1"},"cats":{"new-dataset":0.4316072433,"dev-research":0.3096285408,"prompt-eng":0.406786344,"data-quality":0.3244922602,"ml-security":0.1572697158}}
{"text":"Finally, we introduce modifications to the PIPs point tracking method, greatly widening its temporal receptive field, which improves its performance on PointOdyssey as well as on two real-world benchmarks.","meta":{"url":"http://arxiv.org/abs/2307.15055v1"},"cats":{"new-dataset":0.1874327245,"dev-research":0.2449749125,"prompt-eng":0.4573535078,"data-quality":0.1854068122,"ml-security":0.0589826566}}
{"text":"Our data and code are publicly available at: https://pointodyssey.com","meta":{"url":"http://arxiv.org/abs/2307.15055v1"},"cats":{"new-dataset":0.8117636095,"dev-research":0.2774931377,"prompt-eng":0.3993380899,"data-quality":0.136064975,"ml-security":0.1459260662}}
{"text":"Large language models rely on real-valued representations of text to make their predictions.","meta":{"url":"http://arxiv.org/abs/2307.15054v1"},"cats":{"new-dataset":0.0369833953,"dev-research":0.1997928619,"prompt-eng":0.3502781415,"data-quality":0.2664934609,"ml-security":0.1575615469}}
{"text":"These representations contain information learned from the data that the model has trained on, including knowledge of linguistic properties and forms of demographic bias, e.g., based on gender.","meta":{"url":"http://arxiv.org/abs/2307.15054v1"},"cats":{"new-dataset":0.0656331204,"dev-research":0.2222082706,"prompt-eng":0.3661478329,"data-quality":0.2324545785,"ml-security":0.168750162}}
{"text":"A growing body of work has considered information about concepts such as these using orthogonal projections onto subspaces of the representation space.","meta":{"url":"http://arxiv.org/abs/2307.15054v1"},"cats":{"new-dataset":0.0461902257,"dev-research":0.2665695956,"prompt-eng":0.3516461369,"data-quality":0.1679197922,"ml-security":0.1030076732}}
{"text":"We contribute to this body of work by proposing a formal definition of intrinsic information in a subspace of a language model's representation space.","meta":{"url":"http://arxiv.org/abs/2307.15054v1"},"cats":{"new-dataset":0.0449872972,"dev-research":0.2020692475,"prompt-eng":0.4085077224,"data-quality":0.2761658633,"ml-security":0.1290999458}}
{"text":"We propose a counterfactual approach that avoids the failure mode of spurious correlations (Kumar et al., 2022) by treating components in the subspace and its orthogonal complement independently.","meta":{"url":"http://arxiv.org/abs/2307.15054v1"},"cats":{"new-dataset":0.018638721,"dev-research":0.2850393685,"prompt-eng":0.3830308324,"data-quality":0.4901221109,"ml-security":0.1977932554}}
{"text":"We show that our counterfactual notion of information in a subspace is optimizing by an causal concept subspace.","meta":{"url":"http://arxiv.org/abs/2307.15054v1"},"cats":{"new-dataset":0.0176008848,"dev-research":0.3127403292,"prompt-eng":0.3868441042,"data-quality":0.2971188499,"ml-security":0.2680481704}}
{"text":"Furthermore, this intervention allows us to attempt concept controlled generation by manipulating the value of the conceptual component of a representation.","meta":{"url":"http://arxiv.org/abs/2307.15054v1"},"cats":{"new-dataset":0.0270268823,"dev-research":0.4064762762,"prompt-eng":0.4555883363,"data-quality":0.1774990678,"ml-security":0.1052153047}}
{"text":"Empirically, we find that R-LACE (Ravfogel et al., 2022) returns a one-dimensional subspace containing roughly half of total concept information under our framework.","meta":{"url":"http://arxiv.org/abs/2307.15054v1"},"cats":{"new-dataset":0.1243007751,"dev-research":0.230656403,"prompt-eng":0.3421900363,"data-quality":0.2467527893,"ml-security":0.1559908872}}
{"text":"Our causal controlled intervention shows that, for at least one model, the subspace returned by R-LACE can be used to manipulate the concept value of the generated word with precision.","meta":{"url":"http://arxiv.org/abs/2307.15054v1"},"cats":{"new-dataset":0.0357080307,"dev-research":0.224182415,"prompt-eng":0.4552328258,"data-quality":0.3388412433,"ml-security":0.1206071948}}
{"text":"Approaches to recommendation are typically evaluated in one of two ways: (1) via a (simulated) online experiment, often seen as the gold standard, or (2) via some offline evaluation procedure, where the goal is to approximate the outcome of an online experiment.","meta":{"url":"http://arxiv.org/abs/2307.15053v1"},"cats":{"new-dataset":0.0086736245,"dev-research":0.3156087223,"prompt-eng":0.39821361,"data-quality":0.1448370945,"ml-security":0.0705383159}}
{"text":"Several offline evaluation metrics have been adopted in the literature, inspired by ranking metrics prevalent in the field of Information Retrieval.","meta":{"url":"http://arxiv.org/abs/2307.15053v1"},"cats":{"new-dataset":0.0500513803,"dev-research":0.2520354245,"prompt-eng":0.4329804751,"data-quality":0.2344689141,"ml-security":0.0515308562}}
{"text":"(Normalised) Discounted Cumulative Gain (nDCG) is one such metric that has seen widespread adoption in empirical studies, and higher (n)DCG values have been used to present new methods as the state-of-the-art in top-$n$ recommendation for many years.   ","meta":{"url":"http://arxiv.org/abs/2307.15053v1"},"cats":{"new-dataset":0.0252303539,"dev-research":0.2746680544,"prompt-eng":0.3925318217,"data-quality":0.1744817067,"ml-security":0.1066116094}}
{"text":"Our work takes a critical look at this approach, and investigates when we can expect such metrics to approximate the gold standard outcome of an online experiment.","meta":{"url":"http://arxiv.org/abs/2307.15053v1"},"cats":{"new-dataset":0.0577506082,"dev-research":0.2150093901,"prompt-eng":0.4253175065,"data-quality":0.2043967229,"ml-security":0.0847298465}}
{"text":"We formally present the assumptions that are necessary to consider DCG an unbiased estimator of online reward and provide a derivation for this metric from first principles, highlighting where we deviate from its traditional uses in IR.","meta":{"url":"http://arxiv.org/abs/2307.15053v1"},"cats":{"new-dataset":0.0445202286,"dev-research":0.149938695,"prompt-eng":0.384385088,"data-quality":0.1741771955,"ml-security":0.1218561935}}
{"text":"Importantly, we show that normalising the metric renders it inconsistent, in that even when DCG is unbiased, ranking competing methods by their normalised DCG can invert their relative order.","meta":{"url":"http://arxiv.org/abs/2307.15053v1"},"cats":{"new-dataset":0.0469883919,"dev-research":0.2641456756,"prompt-eng":0.3862530179,"data-quality":0.3769452567,"ml-security":0.075347025}}
{"text":"Through a correlation analysis between off- and on-line experiments conducted on a large-scale recommendation platform, we show that our unbiased DCG estimates strongly correlate with online reward, even when some of the metric's inherent assumptions are violated.","meta":{"url":"http://arxiv.org/abs/2307.15053v1"},"cats":{"new-dataset":0.0373200504,"dev-research":0.1956883108,"prompt-eng":0.3798604738,"data-quality":0.1965596377,"ml-security":0.1240802297}}
{"text":"This statement no longer holds for its normalised variant, suggesting that nDCG's practical utility may be limited.","meta":{"url":"http://arxiv.org/abs/2307.15053v1"},"cats":{"new-dataset":0.0191829057,"dev-research":0.2666522019,"prompt-eng":0.3751518204,"data-quality":0.2184141455,"ml-security":0.0895264269}}
{"text":"Inferring the depth of transparent or mirror (ToM) surfaces represents a hard challenge for either sensors, algorithms, or deep networks.","meta":{"url":"http://arxiv.org/abs/2307.15052v1"},"cats":{"new-dataset":0.0844826139,"dev-research":0.2131663994,"prompt-eng":0.3960557073,"data-quality":0.1423215283,"ml-security":0.1782952321}}
{"text":"We propose a simple pipeline for learning to estimate depth properly for such surfaces with neural networks, without requiring any ground-truth annotation.","meta":{"url":"http://arxiv.org/abs/2307.15052v1"},"cats":{"new-dataset":0.2390782927,"dev-research":0.1976543329,"prompt-eng":0.3690150246,"data-quality":0.1656847682,"ml-security":0.1239837264}}
{"text":"We unveil how to obtain reliable pseudo labels by in-painting ToM objects in images and processing them with a monocular depth estimation model.","meta":{"url":"http://arxiv.org/abs/2307.15052v1"},"cats":{"new-dataset":0.1853947232,"dev-research":0.2256870904,"prompt-eng":0.441108785,"data-quality":0.3891618462,"ml-security":0.0838207433}}
{"text":"These labels can be used to fine-tune existing monocular or stereo networks, to let them learn how to deal with ToM surfaces.","meta":{"url":"http://arxiv.org/abs/2307.15052v1"},"cats":{"new-dataset":0.0739409272,"dev-research":0.3126098331,"prompt-eng":0.3972171913,"data-quality":0.360666051,"ml-security":0.0825943989}}
{"text":"Experimental results on the Booster dataset show the dramatic improvements enabled by our remarkably simple proposal.","meta":{"url":"http://arxiv.org/abs/2307.15052v1"},"cats":{"new-dataset":0.2910541914,"dev-research":0.2097175837,"prompt-eng":0.46664428,"data-quality":0.2745372958,"ml-security":0.1566753153}}
{"text":"Clinical trials are vital in advancing drug development and evidence-based medicine, but their success is often hindered by challenges in patient recruitment.","meta":{"url":"http://arxiv.org/abs/2307.15051v1"},"cats":{"new-dataset":0.0205141193,"dev-research":0.2796978211,"prompt-eng":0.3577878907,"data-quality":0.1203944816,"ml-security":0.1830736164}}
{"text":"In this work, we investigate the potential of large language models (LLMs) to assist individual patients and referral physicians in identifying suitable clinical trials from an extensive selection.","meta":{"url":"http://arxiv.org/abs/2307.15051v1"},"cats":{"new-dataset":0.0452752122,"dev-research":0.2148665364,"prompt-eng":0.3949866598,"data-quality":0.1731383707,"ml-security":0.0797087546}}
{"text":"Specifically, we introduce TrialGPT, a novel architecture employing LLMs to predict criterion-level eligibility with detailed explanations, which are then aggregated for ranking and excluding candidate clinical trials based on free-text patient notes.","meta":{"url":"http://arxiv.org/abs/2307.15051v1"},"cats":{"new-dataset":0.0501728868,"dev-research":0.2445188177,"prompt-eng":0.4715220793,"data-quality":0.1606296455,"ml-security":0.1261422061}}
{"text":"We evaluate TrialGPT on three publicly available cohorts of 184 patients and 18,238 annotated clinical trials.","meta":{"url":"http://arxiv.org/abs/2307.15051v1"},"cats":{"new-dataset":0.296971841,"dev-research":0.2317650983,"prompt-eng":0.372996976,"data-quality":0.1401721722,"ml-security":0.0742111375}}
{"text":"The experimental results demonstrate several key findings:","meta":{"url":"http://arxiv.org/abs/2307.15051v1"},"cats":{"new-dataset":0.0253268568,"dev-research":0.2429844489,"prompt-eng":0.3703430691,"data-quality":0.152266129,"ml-security":0.0942586453}}
{"text":"First, TrialGPT achieves high criterion-level prediction accuracy with faithful explanations.","meta":{"url":"http://arxiv.org/abs/2307.15051v1"},"cats":{"new-dataset":0.0472317508,"dev-research":0.3416085082,"prompt-eng":0.492718197,"data-quality":0.2401914348,"ml-security":0.1445033655}}
{"text":"Second, the aggregated trial-level TrialGPT scores are highly correlated with expert eligibility annotations.","meta":{"url":"http://arxiv.org/abs/2307.15051v1"},"cats":{"new-dataset":0.067869154,"dev-research":0.2825730465,"prompt-eng":0.4335415479,"data-quality":0.2413042973,"ml-security":0.0933938493}}
{"text":"Third, these scores prove effective in ranking clinical trials and exclude ineligible candidates.","meta":{"url":"http://arxiv.org/abs/2307.15051v1"},"cats":{"new-dataset":0.0162081411,"dev-research":0.2160576221,"prompt-eng":0.4365582304,"data-quality":0.2772714678,"ml-security":0.0933996408}}
{"text":"Our error analysis suggests that current LLMs still make some mistakes due to limited medical knowledge and domain-specific context understanding.","meta":{"url":"http://arxiv.org/abs/2307.15051v1"},"cats":{"new-dataset":0.0264515553,"dev-research":0.3570498854,"prompt-eng":0.3721669977,"data-quality":0.420816744,"ml-security":0.196530844}}
{"text":"Nonetheless, we believe the explanatory capabilities of LLMs are highly valuable.","meta":{"url":"http://arxiv.org/abs/2307.15051v1"},"cats":{"new-dataset":0.041661318,"dev-research":0.222328631,"prompt-eng":0.3733179821,"data-quality":0.099724896,"ml-security":0.130136316}}
{"text":"Future research is warranted on how such AI assistants can be integrated into the routine trial matching workflow in real-world settings to improve its efficiency.","meta":{"url":"http://arxiv.org/abs/2307.15051v1"},"cats":{"new-dataset":0.0690027375,"dev-research":0.3234297108,"prompt-eng":0.4411015268,"data-quality":0.1056821687,"ml-security":0.119922316}}
{"text":"Prompt tuning and adapter tuning have shown great potential in transferring pre-trained vision-language models (VLMs) to various downstream tasks.","meta":{"url":"http://arxiv.org/abs/2307.15049v1"},"cats":{"new-dataset":0.0532295653,"dev-research":0.2475377595,"prompt-eng":0.5223365641,"data-quality":0.2232782071,"ml-security":0.1011260453}}
{"text":"In this work, we design a new type of tuning method, termed as regularized mask tuning, which masks the network parameters through a learnable selection.","meta":{"url":"http://arxiv.org/abs/2307.15049v1"},"cats":{"new-dataset":0.0182753812,"dev-research":0.2324208164,"prompt-eng":0.4377790785,"data-quality":0.284168487,"ml-security":0.304448806}}
{"text":"Inspired by neural pathways, we argue that the knowledge required by a downstream task already exists in the pre-trained weights but just gets concealed in the upstream pre-training stage.","meta":{"url":"http://arxiv.org/abs/2307.15049v1"},"cats":{"new-dataset":0.0266094229,"dev-research":0.2044134474,"prompt-eng":0.3957406792,"data-quality":0.1553974784,"ml-security":0.2553132451}}
{"text":"To bring the useful knowledge back into light, we first identify a set of parameters that are important to a given downstream task, then attach a binary mask to each parameter, and finally optimize these masks on the downstream data with the parameters frozen.","meta":{"url":"http://arxiv.org/abs/2307.15049v1"},"cats":{"new-dataset":0.1257107531,"dev-research":0.2905718246,"prompt-eng":0.4782316333,"data-quality":0.1610173028,"ml-security":0.1583954073}}
{"text":"When updating the mask, we introduce a novel gradient dropout strategy to regularize the parameter selection, in order to prevent the model from forgetting old knowledge and overfitting the downstream data.","meta":{"url":"http://arxiv.org/abs/2307.15049v1"},"cats":{"new-dataset":0.0767432618,"dev-research":0.2151594182,"prompt-eng":0.4344051275,"data-quality":0.3043849248,"ml-security":0.3549655757}}
{"text":"Experimental results on 11 datasets demonstrate the consistent superiority of our method over previous alternatives.","meta":{"url":"http://arxiv.org/abs/2307.15049v1"},"cats":{"new-dataset":0.2152231795,"dev-research":0.270316236,"prompt-eng":0.412521813,"data-quality":0.3426278386,"ml-security":0.0926742845}}
{"text":"It is noteworthy that we manage to deliver 18.73% performance improvement compared to the zero-shot CLIP via masking an average of only 2.56% parameters.","meta":{"url":"http://arxiv.org/abs/2307.15049v1"},"cats":{"new-dataset":0.0583800006,"dev-research":0.2394858725,"prompt-eng":0.4197082637,"data-quality":0.1755418444,"ml-security":0.0999003833}}
{"text":"Furthermore, our method is synergistic with most existing parameter-efficient tuning methods and can boost the performance on top of them.","meta":{"url":"http://arxiv.org/abs/2307.15049v1"},"cats":{"new-dataset":0.035639201,"dev-research":0.310552231,"prompt-eng":0.4858434634,"data-quality":0.1825998981,"ml-security":0.0691711518}}
{"text":"Project page can be found here (https://wuw2019.github.io/RMT/).","meta":{"url":"http://arxiv.org/abs/2307.15049v1"},"cats":{"new-dataset":0.5265361848,"dev-research":0.2657072568,"prompt-eng":0.4498218125,"data-quality":0.1398575074,"ml-security":0.0501441999}}
{"text":"Handwriting recognition is a challenging and critical problem in the fields of pattern recognition and machine learning, with applications spanning a wide range of domains.","meta":{"url":"http://arxiv.org/abs/2307.15045v1"},"cats":{"new-dataset":0.075631465,"dev-research":0.2112061266,"prompt-eng":0.3982057965,"data-quality":0.20836187,"ml-security":0.161811802}}
{"text":"In this paper, we focus on the specific issue of recognizing offline Arabic handwritten text.","meta":{"url":"http://arxiv.org/abs/2307.15045v1"},"cats":{"new-dataset":0.3018881138,"dev-research":0.188599832,"prompt-eng":0.3781736494,"data-quality":0.2901854798,"ml-security":0.1016811047}}
{"text":"Existing approaches typically utilize a combination of convolutional neural networks for image feature extraction and recurrent neural networks for temporal modeling, with connectionist temporal classification used for text generation.","meta":{"url":"http://arxiv.org/abs/2307.15045v1"},"cats":{"new-dataset":0.3597821348,"dev-research":0.2188572545,"prompt-eng":0.366662364,"data-quality":0.1948875216,"ml-security":0.0734319044}}
{"text":"However, these methods suffer from a lack of parallelization due to the sequential nature of recurrent neural networks.","meta":{"url":"http://arxiv.org/abs/2307.15045v1"},"cats":{"new-dataset":0.0342835769,"dev-research":0.2219515091,"prompt-eng":0.3216324734,"data-quality":0.1301705632,"ml-security":0.1826777599}}
{"text":"Furthermore, these models cannot account for linguistic rules, necessitating the use of an external language model in the post-processing stage to boost accuracy.","meta":{"url":"http://arxiv.org/abs/2307.15045v1"},"cats":{"new-dataset":0.0115015082,"dev-research":0.2442516113,"prompt-eng":0.3758781395,"data-quality":0.4487499698,"ml-security":0.1524052117}}
{"text":"To overcome these issues, we introduce two alternative architectures, namely the Transformer Transducer and the standard sequence-to-sequence Transformer, and compare their performance in terms of accuracy and speed.","meta":{"url":"http://arxiv.org/abs/2307.15045v1"},"cats":{"new-dataset":0.2533194941,"dev-research":0.2477908531,"prompt-eng":0.4218555458,"data-quality":0.158476937,"ml-security":0.0547792141}}
{"text":"Our approach can model language dependencies and relies only on the attention mechanism, thereby making it more parallelizable and less complex.","meta":{"url":"http://arxiv.org/abs/2307.15045v1"},"cats":{"new-dataset":0.032527036,"dev-research":0.256764906,"prompt-eng":0.4386720429,"data-quality":0.1906017846,"ml-security":0.1393961739}}
{"text":"We employ pre-trained Transformers for both image understanding and language modeling.","meta":{"url":"http://arxiv.org/abs/2307.15045v1"},"cats":{"new-dataset":0.1402835237,"dev-research":0.2393367193,"prompt-eng":0.4394646999,"data-quality":0.2132270073,"ml-security":0.0846971966}}
{"text":"Our evaluation on the Arabic KHATT dataset demonstrates that our proposed method outperforms the current state-of-the-art approaches for recognizing offline Arabic handwritten text.","meta":{"url":"http://arxiv.org/abs/2307.15045v1"},"cats":{"new-dataset":0.6507526201,"dev-research":0.1948887514,"prompt-eng":0.3562096009,"data-quality":0.2610002799,"ml-security":0.0975674906}}
{"text":"Because \"out-of-the-box\" large language models are capable of generating a great deal of objectionable content, recent work has focused on aligning these models in an attempt to prevent undesirable generation.","meta":{"url":"http://arxiv.org/abs/2307.15043v1"},"cats":{"new-dataset":0.0713318629,"dev-research":0.2908119233,"prompt-eng":0.3928811626,"data-quality":0.3284893821,"ml-security":0.1562926168}}
{"text":"While there has been some success at circumventing these measures -- so-called \"jailbreaks\" against LLMs -- these attacks have required significant human ingenuity and are brittle in practice.","meta":{"url":"http://arxiv.org/abs/2307.15043v1"},"cats":{"new-dataset":0.0248694244,"dev-research":0.2813231576,"prompt-eng":0.3846005344,"data-quality":0.2411314759,"ml-security":0.6305641533}}
{"text":"In this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors.","meta":{"url":"http://arxiv.org/abs/2307.15043v1"},"cats":{"new-dataset":0.0673603842,"dev-research":0.3834544438,"prompt-eng":0.4473404504,"data-quality":0.3782096397,"ml-security":0.7439847916}}
{"text":"Specifically, our approach finds a suffix that, when attached to a wide range of queries for an LLM to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer).","meta":{"url":"http://arxiv.org/abs/2307.15043v1"},"cats":{"new-dataset":0.0091561211,"dev-research":0.1891342558,"prompt-eng":0.4664812766,"data-quality":0.2475012567,"ml-security":0.2862391226}}
{"text":"However, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past automatic prompt generation methods.   ","meta":{"url":"http://arxiv.org/abs/2307.15043v1"},"cats":{"new-dataset":0.0810501221,"dev-research":0.2760848666,"prompt-eng":0.5692056335,"data-quality":0.4070436063,"ml-security":0.4783607349}}
{"text":"Surprisingly, we find that the adversarial prompts generated by our approach are quite transferable, including to black-box, publicly released LLMs.","meta":{"url":"http://arxiv.org/abs/2307.15043v1"},"cats":{"new-dataset":0.0629862014,"dev-research":0.2298691915,"prompt-eng":0.4711686761,"data-quality":0.2300003341,"ml-security":0.7170056903}}
{"text":"Specifically, we train an adversarial attack suffix on multiple prompts (i.e., queries asking for many different types of objectionable content), as well as multiple models (in our case, Vicuna-7B and 13B).","meta":{"url":"http://arxiv.org/abs/2307.15043v1"},"cats":{"new-dataset":0.0512294188,"dev-research":0.2234403212,"prompt-eng":0.4854866365,"data-quality":0.2952535569,"ml-security":0.760699924}}
{"text":"When doing so, the resulting attack suffix is able to induce objectionable content in the public interfaces to ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat, Pythia, Falcon, and others.","meta":{"url":"http://arxiv.org/abs/2307.15043v1"},"cats":{"new-dataset":0.11398127,"dev-research":0.3466246686,"prompt-eng":0.3932174705,"data-quality":0.312945117,"ml-security":0.6038495813}}
{"text":"In total, this work significantly advances the state-of-the-art in adversarial attacks against aligned language models, raising important questions about how such systems can be prevented from producing objectionable information.","meta":{"url":"http://arxiv.org/abs/2307.15043v1"},"cats":{"new-dataset":0.0671158938,"dev-research":0.2436042055,"prompt-eng":0.3708302028,"data-quality":0.3860953833,"ml-security":0.8214625768}}
{"text":"Code is available at github.com/llm-attacks/llm-attacks.","meta":{"url":"http://arxiv.org/abs/2307.15043v1"},"cats":{"new-dataset":0.2204752304,"dev-research":0.3004031498,"prompt-eng":0.4447068845,"data-quality":0.2141841812,"ml-security":0.4116339802}}
{"text":"The gradual nature of a diffusion process that synthesizes samples in small increments constitutes a key ingredient of Denoising Diffusion Probabilistic Models (DDPM), which have presented unprecedented quality in image synthesis and been recently explored in the motion domain.","meta":{"url":"http://arxiv.org/abs/2307.15042v1"},"cats":{"new-dataset":0.0606401478,"dev-research":0.2131816554,"prompt-eng":0.3539983601,"data-quality":0.1232395532,"ml-security":0.0982515263}}
{"text":"In this work, we propose to adapt the gradual diffusion concept (operating along a diffusion time-axis) into the temporal-axis of the motion sequence.","meta":{"url":"http://arxiv.org/abs/2307.15042v1"},"cats":{"new-dataset":0.0835259264,"dev-research":0.153855479,"prompt-eng":0.3640474917,"data-quality":0.0697859018,"ml-security":0.0698016899}}
{"text":"Our key idea is to extend the DDPM framework to support temporally varying denoising, thereby entangling the two axes.","meta":{"url":"http://arxiv.org/abs/2307.15042v1"},"cats":{"new-dataset":0.1147198409,"dev-research":0.2209715217,"prompt-eng":0.3808152938,"data-quality":0.1312332334,"ml-security":0.1050088051}}
{"text":"Using our special formulation, we iteratively denoise a motion buffer that contains a set of increasingly-noised poses, which auto-regressively produces an arbitrarily long stream of frames.","meta":{"url":"http://arxiv.org/abs/2307.15042v1"},"cats":{"new-dataset":0.4649953137,"dev-research":0.2056967245,"prompt-eng":0.3757189474,"data-quality":0.1625137289,"ml-security":0.1267388672}}
{"text":"With a stationary diffusion time-axis, in each diffusion step we increment only the temporal-axis of the motion such that the framework produces a new, clean frame which is removed from the beginning of the buffer, followed by a newly drawn noise vector that is appended to it.","meta":{"url":"http://arxiv.org/abs/2307.15042v1"},"cats":{"new-dataset":0.1047496404,"dev-research":0.2289485607,"prompt-eng":0.3843168275,"data-quality":0.159924083,"ml-security":0.1319290955}}
{"text":"This new mechanism paves the way towards a new framework for long-term motion synthesis with applications to character animation and other domains.","meta":{"url":"http://arxiv.org/abs/2307.15042v1"},"cats":{"new-dataset":0.0938867698,"dev-research":0.268769693,"prompt-eng":0.4077464055,"data-quality":0.06003913,"ml-security":0.0554683857}}
{"text":"An important difference between brains and deep neural networks is the way they learn.","meta":{"url":"http://arxiv.org/abs/2307.15040v1"},"cats":{"new-dataset":0.0169257166,"dev-research":0.3534239692,"prompt-eng":0.2736504747,"data-quality":0.1748716366,"ml-security":0.1729268125}}
{"text":"Nervous systems learn online where a stream of noisy data points are presented in a non-independent, identically distributed (non-i.i.d.) way.","meta":{"url":"http://arxiv.org/abs/2307.15040v1"},"cats":{"new-dataset":0.0819483636,"dev-research":0.1998574973,"prompt-eng":0.3473555823,"data-quality":0.2222069894,"ml-security":0.2707591951}}
{"text":"Further, synaptic plasticity in the brain depends only on information local to synapses.","meta":{"url":"http://arxiv.org/abs/2307.15040v1"},"cats":{"new-dataset":0.0070570461,"dev-research":0.2762203173,"prompt-eng":0.3395755718,"data-quality":0.1501833585,"ml-security":0.1321167115}}
{"text":"Deep networks, on the other hand, typically use non-local learning algorithms and are trained in an offline, non-noisy, i.i.d. setting.","meta":{"url":"http://arxiv.org/abs/2307.15040v1"},"cats":{"new-dataset":0.0181008102,"dev-research":0.2749283999,"prompt-eng":0.2358338507,"data-quality":0.1902739557,"ml-security":0.2234763027}}
{"text":"Understanding how neural networks learn under the same constraints as the brain is an open problem for neuroscience and neuromorphic computing.","meta":{"url":"http://arxiv.org/abs/2307.15040v1"},"cats":{"new-dataset":0.0203569951,"dev-research":0.2542942185,"prompt-eng":0.3271358884,"data-quality":0.1794113049,"ml-security":0.3405816795}}
{"text":"A standard approach to this problem has yet to be established.","meta":{"url":"http://arxiv.org/abs/2307.15040v1"},"cats":{"new-dataset":0.0447697494,"dev-research":0.1744403773,"prompt-eng":0.3572690515,"data-quality":0.2274375495,"ml-security":0.1337053821}}
{"text":"In this paper, we propose that discrete graphical models that learn via an online maximum a posteriori learning algorithm could provide such an approach.","meta":{"url":"http://arxiv.org/abs/2307.15040v1"},"cats":{"new-dataset":0.062044947,"dev-research":0.1581000405,"prompt-eng":0.3995777299,"data-quality":0.1555942668,"ml-security":0.1300606726}}
{"text":"We implement this kind of model in a novel neural network called the Sparse Quantized Hopfield Network (SQHN).","meta":{"url":"http://arxiv.org/abs/2307.15040v1"},"cats":{"new-dataset":0.1143610009,"dev-research":0.1341530292,"prompt-eng":0.3484267281,"data-quality":0.0977436926,"ml-security":0.1448677448}}
{"text":"We show that SQHNs outperform state-of-the-art neural networks on associative memory tasks, outperform these models in online, non-i.i.d. settings, learn efficiently with noisy inputs, and are better than baselines on a novel episodic memory task.","meta":{"url":"http://arxiv.org/abs/2307.15040v1"},"cats":{"new-dataset":0.1231927323,"dev-research":0.226102381,"prompt-eng":0.3719793363,"data-quality":0.1698740947,"ml-security":0.188193919}}
{"text":"Miscalibration of gaze tracking devices and the resulting need for repeat calibration are a significant barrier to use.","meta":{"url":"http://arxiv.org/abs/2307.15039v1"},"cats":{"new-dataset":0.0142506453,"dev-research":0.3292535596,"prompt-eng":0.4441211237,"data-quality":0.1509216901,"ml-security":0.1162967139}}
{"text":"As devices miscalibrate, people tend to auto-correct by gazing at neighboring targets, which makes it difficult to detect miscalibration from eye signals.","meta":{"url":"http://arxiv.org/abs/2307.15039v1"},"cats":{"new-dataset":0.0064987825,"dev-research":0.4140000268,"prompt-eng":0.5005039726,"data-quality":0.4037340379,"ml-security":0.1387095955}}
{"text":"To address this problem, we provide a novel and simple insight for autocalibrating eye trackers during gaze typing: the eyes are used as both input (i.e. typing) and output (i.e. reading) signals, but auto-correction by users only occurs when eye gaze is functioning as input.","meta":{"url":"http://arxiv.org/abs/2307.15039v1"},"cats":{"new-dataset":0.0242337714,"dev-research":0.4032139319,"prompt-eng":0.5281391964,"data-quality":0.2739732611,"ml-security":0.095019196}}
{"text":"Thus, output eye gaze signals during reading can help systems detect the miscalibration offset and enable autocalibration.","meta":{"url":"http://arxiv.org/abs/2307.15039v1"},"cats":{"new-dataset":0.0172185293,"dev-research":0.386167859,"prompt-eng":0.4880356817,"data-quality":0.2160388854,"ml-security":0.0582187454}}
{"text":"To demonstrate the potential for this type of approach, we designed and built an auto-calibration system for gaze typing and ran a user study with 15 able-bodied participants.","meta":{"url":"http://arxiv.org/abs/2307.15039v1"},"cats":{"new-dataset":0.1196184656,"dev-research":0.3452860636,"prompt-eng":0.5215599131,"data-quality":0.1213560947,"ml-security":0.0712735853}}
{"text":"Results from our user study suggest that such an implicit approach to autocalibration can significantly improve typing speed and overall user experience for gaze typing interfaces.","meta":{"url":"http://arxiv.org/abs/2307.15039v1"},"cats":{"new-dataset":0.0119469988,"dev-research":0.4221399889,"prompt-eng":0.4843192526,"data-quality":0.1403800103,"ml-security":0.0545320673}}
{"text":"Insights from our work are applicable to a broad set of gaze tracking technologies and may help create more seamless user experiences in a variety of domains.","meta":{"url":"http://arxiv.org/abs/2307.15039v1"},"cats":{"new-dataset":0.1346646491,"dev-research":0.2951760784,"prompt-eng":0.4358241226,"data-quality":0.0939361741,"ml-security":0.0604522452}}
{"text":"The question of whether 3-Coloring can be solved in polynomial-time for the diameter two graphs is a well-known open problem in the area of algorithmic graph theory.","meta":{"url":"http://arxiv.org/abs/2307.15036v1"},"cats":{"new-dataset":0.0254345961,"dev-research":0.2289710392,"prompt-eng":0.3324274069,"data-quality":0.1742723635,"ml-security":0.1112485695}}
{"text":"We study the problem restricted to graph classes that avoid cycles of given lengths as induced subgraphs.","meta":{"url":"http://arxiv.org/abs/2307.15036v1"},"cats":{"new-dataset":0.1102234956,"dev-research":0.208406435,"prompt-eng":0.3111090983,"data-quality":0.2231705165,"ml-security":0.1677882958}}
{"text":"Martin et.","meta":{"url":"http://arxiv.org/abs/2307.15036v1"},"cats":{"new-dataset":0.082779554,"dev-research":0.2165068641,"prompt-eng":0.3391088593,"data-quality":0.1207283062,"ml-security":0.1049941602}}
{"text":"al.","meta":{"url":"http://arxiv.org/abs/2307.15036v1"},"cats":{"new-dataset":0.1632442167,"dev-research":0.2497783865,"prompt-eng":0.3018028649,"data-quality":0.0984107646,"ml-security":0.1289635787}}
{"text":"[CIAC 2021] showed that the problem is polynomial-time solvable for $C_5$-free or $C_6$-free graphs, and, $(C_4,C_s)$-free graphs where $s \\in \\{3,7,8,9\\}$.","meta":{"url":"http://arxiv.org/abs/2307.15036v1"},"cats":{"new-dataset":0.1707615088,"dev-research":0.2148489758,"prompt-eng":0.3015411505,"data-quality":0.1290202155,"ml-security":0.1204168442}}
{"text":"We extend their result proving that it is polynomial-time solvable for $(C_4,C_s)$-free graphs, for any constant $s$, and for $(C_3,C_7)$-free graphs.","meta":{"url":"http://arxiv.org/abs/2307.15036v1"},"cats":{"new-dataset":0.1247132214,"dev-research":0.2100566553,"prompt-eng":0.3025136756,"data-quality":0.1339370062,"ml-security":0.151408885}}
{"text":"Our results also hold for the more general problem List 3-Colouring.","meta":{"url":"http://arxiv.org/abs/2307.15036v1"},"cats":{"new-dataset":0.2886338934,"dev-research":0.2416556892,"prompt-eng":0.424997377,"data-quality":0.2708085765,"ml-security":0.0753818856}}
{"text":"The Fourier neural operator (FNO) is a powerful technique for learning surrogate maps for partial differential equation (PDE) solution operators.","meta":{"url":"http://arxiv.org/abs/2307.15034v1"},"cats":{"new-dataset":0.0379893486,"dev-research":0.2177795071,"prompt-eng":0.347519212,"data-quality":0.115002672,"ml-security":0.1791895741}}
{"text":"For many real-world applications, which often require high-resolution data points, training time and memory usage are significant bottlenecks.","meta":{"url":"http://arxiv.org/abs/2307.15034v1"},"cats":{"new-dataset":0.106174042,"dev-research":0.3389061486,"prompt-eng":0.3597300286,"data-quality":0.156363695,"ml-security":0.1585111915}}
{"text":"While there are mixed-precision training techniques for standard neural networks, those work for real-valued datatypes on finite dimensions and therefore cannot be directly applied to FNO, which crucially operates in the (complex-valued)","meta":{"url":"http://arxiv.org/abs/2307.15034v1"},"cats":{"new-dataset":0.0148573902,"dev-research":0.196595007,"prompt-eng":0.263973205,"data-quality":0.2079285639,"ml-security":0.2025010013}}
{"text":"Fourier domain and in function spaces.","meta":{"url":"http://arxiv.org/abs/2307.15034v1"},"cats":{"new-dataset":0.0321680873,"dev-research":0.3128557376,"prompt-eng":0.304039841,"data-quality":0.1332456619,"ml-security":0.1772342871}}
{"text":"On the other hand, since the Fourier transform is already an approximation (due to discretization error), we do not need to perform the operation at full precision.","meta":{"url":"http://arxiv.org/abs/2307.15034v1"},"cats":{"new-dataset":0.0015250583,"dev-research":0.2766998167,"prompt-eng":0.2734031668,"data-quality":0.1032980924,"ml-security":0.0760496341}}
{"text":"In this work, we (i) profile memory and runtime for FNO with full and mixed-precision training, (ii) conduct a study on the numerical stability of mixed-precision training of FNO, and (iii) devise a training routine which substantially decreases training time and memory usage (up to 34%), with little or no reduction in accuracy, on the Navier-Stokes and Darcy flow equations.","meta":{"url":"http://arxiv.org/abs/2307.15034v1"},"cats":{"new-dataset":0.0308542227,"dev-research":0.2481239668,"prompt-eng":0.320539414,"data-quality":0.1176630992,"ml-security":0.1836810244}}
{"text":"Combined with the recently proposed tensorized FNO (Kossaifi et al., 2023), the resulting model has far better performance while also being significantly faster than the original FNO.","meta":{"url":"http://arxiv.org/abs/2307.15034v1"},"cats":{"new-dataset":0.0390463523,"dev-research":0.1594251371,"prompt-eng":0.3637456167,"data-quality":0.1183996853,"ml-security":0.0840505515}}
{"text":"Recent inversion methods have shown that real images can be inverted into StyleGAN's latent space and numerous edits can be achieved on those images thanks to the semantically rich feature representations of well-trained GAN models.","meta":{"url":"http://arxiv.org/abs/2307.15033v1"},"cats":{"new-dataset":0.0741424611,"dev-research":0.2203724602,"prompt-eng":0.3673173283,"data-quality":0.2266314632,"ml-security":0.1977678496}}
{"text":"However, extensive research has also shown that image inversion is challenging due to the trade-off between high-fidelity reconstruction and editability.","meta":{"url":"http://arxiv.org/abs/2307.15033v1"},"cats":{"new-dataset":0.0390099143,"dev-research":0.2179651882,"prompt-eng":0.3926693949,"data-quality":0.1801405295,"ml-security":0.0901672689}}
{"text":"In this paper, we tackle an even more difficult task, inverting erased images into GAN's latent space for realistic inpaintings and editings.","meta":{"url":"http://arxiv.org/abs/2307.15033v1"},"cats":{"new-dataset":0.1230829713,"dev-research":0.2151853672,"prompt-eng":0.3742374895,"data-quality":0.2422419403,"ml-security":0.1276065036}}
{"text":"Furthermore, by augmenting inverted latent codes with different latent samples, we achieve diverse inpaintings.","meta":{"url":"http://arxiv.org/abs/2307.15033v1"},"cats":{"new-dataset":0.1530634021,"dev-research":0.1910967495,"prompt-eng":0.4028304161,"data-quality":0.2832595163,"ml-security":0.0764428981}}
{"text":"Specifically, we propose to learn an encoder and mixing network to combine encoded features from erased images with StyleGAN's mapped features from random samples.","meta":{"url":"http://arxiv.org/abs/2307.15033v1"},"cats":{"new-dataset":0.2144870872,"dev-research":0.2711974026,"prompt-eng":0.3781278254,"data-quality":0.3212460854,"ml-security":0.1519096757}}
{"text":"To encourage the mixing network to utilize both inputs, we train the networks with generated data via a novel set-up.","meta":{"url":"http://arxiv.org/abs/2307.15033v1"},"cats":{"new-dataset":0.2048975868,"dev-research":0.3027623497,"prompt-eng":0.4301883241,"data-quality":0.2370936511,"ml-security":0.1416273839}}
{"text":"We also utilize higher-rate features to prevent color inconsistencies between the inpainted and unerased parts.","meta":{"url":"http://arxiv.org/abs/2307.15033v1"},"cats":{"new-dataset":0.0279173502,"dev-research":0.3156278759,"prompt-eng":0.4452793088,"data-quality":0.3272611053,"ml-security":0.0981073609}}
{"text":"We run extensive experiments and compare our method with state-of-the-art inversion and inpainting methods.","meta":{"url":"http://arxiv.org/abs/2307.15033v1"},"cats":{"new-dataset":0.0214773464,"dev-research":0.2087814839,"prompt-eng":0.4102849411,"data-quality":0.1784194638,"ml-security":0.0584735913}}
{"text":"Qualitative metrics and visual comparisons show significant improvements.","meta":{"url":"http://arxiv.org/abs/2307.15033v1"},"cats":{"new-dataset":0.0391733881,"dev-research":0.4138619686,"prompt-eng":0.361037218,"data-quality":0.1458231748,"ml-security":0.0575924145}}
{"text":"Inspired by deep convolution segmentation algorithms, scene text detectors break the performance ceiling of datasets steadily.","meta":{"url":"http://arxiv.org/abs/2307.15029v1"},"cats":{"new-dataset":0.3270663058,"dev-research":0.2064563132,"prompt-eng":0.340215001,"data-quality":0.4592943221,"ml-security":0.1817972272}}
{"text":"However, these methods often encounter threshold selection bottlenecks and have poor performance on text instances with extreme aspect ratios.","meta":{"url":"http://arxiv.org/abs/2307.15029v1"},"cats":{"new-dataset":0.0222814528,"dev-research":0.2829727072,"prompt-eng":0.4304565272,"data-quality":0.3694792962,"ml-security":0.1290581295}}
{"text":"In this paper, we propose to automatically learn the discriminate segmentation threshold, which distinguishes text pixels from background pixels for segmentation-based scene text detectors and then further reduces the time-consuming manual parameter adjustment.","meta":{"url":"http://arxiv.org/abs/2307.15029v1"},"cats":{"new-dataset":0.2001191567,"dev-research":0.2350796148,"prompt-eng":0.4420029444,"data-quality":0.4590096567,"ml-security":0.1676957903}}
{"text":"Besides, we design a Global-information Enhanced Feature Pyramid Network (GE-FPN) for capturing text instances with macro size and extreme aspect ratios.","meta":{"url":"http://arxiv.org/abs/2307.15029v1"},"cats":{"new-dataset":0.2469103688,"dev-research":0.2917173509,"prompt-eng":0.3927219057,"data-quality":0.2439751302,"ml-security":0.1094242584}}
{"text":"Following the GE-FPN, we introduce a cascade optimization structure to further refine the text instances.","meta":{"url":"http://arxiv.org/abs/2307.15029v1"},"cats":{"new-dataset":0.0725427535,"dev-research":0.2159806377,"prompt-eng":0.4241418847,"data-quality":0.3150091147,"ml-security":0.0804099193}}
{"text":"Finally, together with the proposed threshold learning strategy and text detection structure, we design an Adaptive Segmentation Network (ASNet) for scene text detection.","meta":{"url":"http://arxiv.org/abs/2307.15029v1"},"cats":{"new-dataset":0.1370605051,"dev-research":0.1704636089,"prompt-eng":0.355274075,"data-quality":0.4081065284,"ml-security":0.1658788452}}
{"text":"Extensive experiments are carried out to demonstrate that the proposed ASNet can achieve the state-of-the-art performance on four text detection benchmarks, i.e., ICDAR 2015, MSRA-TD500, ICDAR 2017 MLT and CTW1500.","meta":{"url":"http://arxiv.org/abs/2307.15029v1"},"cats":{"new-dataset":0.2469651344,"dev-research":0.2466190994,"prompt-eng":0.3910492394,"data-quality":0.37965722,"ml-security":0.1661945711}}
{"text":"The ablation experiments also verify the effectiveness of our contributions.","meta":{"url":"http://arxiv.org/abs/2307.15029v1"},"cats":{"new-dataset":0.0382815291,"dev-research":0.2085819867,"prompt-eng":0.3783263596,"data-quality":0.1603150421,"ml-security":0.0610823513}}
{"text":"Decentralized architecture offers a robust and flexible structure for online platforms, since centralized moderation and computation can be easy to disrupt with targeted attacks.","meta":{"url":"http://arxiv.org/abs/2307.15027v1"},"cats":{"new-dataset":0.0273790929,"dev-research":0.272822941,"prompt-eng":0.3812456346,"data-quality":0.0882365465,"ml-security":0.4317614571}}
{"text":"However, a platform offering a decentralized architecture does not guarantee that users will use it in a decentralized way, and measuring the centralization of socio-technical networks is not an easy task.","meta":{"url":"http://arxiv.org/abs/2307.15027v1"},"cats":{"new-dataset":0.0145556471,"dev-research":0.3084642634,"prompt-eng":0.3781657044,"data-quality":0.0981769838,"ml-security":0.1571574226}}
{"text":"In this paper we introduce a method of characterizing community influence in terms of how many edges between communities would be disrupted by a community's removal.","meta":{"url":"http://arxiv.org/abs/2307.15027v1"},"cats":{"new-dataset":0.0220676952,"dev-research":0.2740633795,"prompt-eng":0.3634755585,"data-quality":0.2120020678,"ml-security":0.2505842867}}
{"text":"Our approach provides a careful definition of \"centralization\" appropriate in bipartite user-community socio-technical networks, and demonstrates the inadequacy of more trivial methods for interrogating centralization such as examining the distribution of community sizes.","meta":{"url":"http://arxiv.org/abs/2307.15027v1"},"cats":{"new-dataset":0.0444976013,"dev-research":0.312005568,"prompt-eng":0.372372556,"data-quality":0.1576492677,"ml-security":0.1046373863}}
{"text":"We use this method to compare the structure of multiple socio-technical platforms -- Mastodon, git code hosting servers, BitChute, Usenet, and Voat -- and find a range of structures, from interconnected but decentralized git servers to an effectively centralized use of Mastodon servers, as well as multiscale hybrid network structures of disconnected Voat subverses.","meta":{"url":"http://arxiv.org/abs/2307.15027v1"},"cats":{"new-dataset":0.2716736519,"dev-research":0.3203721188,"prompt-eng":0.3849552263,"data-quality":0.119456352,"ml-security":0.0960632669}}
{"text":"As the ecosystem of socio-technical platforms diversifies, it becomes critical to not solely focus on the underlying technologies but also consider the structure of how users interact through the technical infrastructure.","meta":{"url":"http://arxiv.org/abs/2307.15027v1"},"cats":{"new-dataset":0.0720388983,"dev-research":0.4278598975,"prompt-eng":0.3899492713,"data-quality":0.0882162292,"ml-security":0.1200124401}}
{"text":"This letter proposes advanced beamforming design and analyzes its influence on the sensing and communications (S&C) performance for a multiple-antenna integrated S&C (ISAC) system with a single communication user and a single target.","meta":{"url":"http://arxiv.org/abs/2307.15023v1"},"cats":{"new-dataset":0.0227803232,"dev-research":0.2387493518,"prompt-eng":0.42178019,"data-quality":0.0994852312,"ml-security":0.0772887099}}
{"text":"Novel closed-form beamformers are derived for three typical scenarios, including the sensing-centric design, communications-centric design, and Pareto optimal design.","meta":{"url":"http://arxiv.org/abs/2307.15023v1"},"cats":{"new-dataset":0.020424898,"dev-research":0.1945666725,"prompt-eng":0.4323509953,"data-quality":0.114541208,"ml-security":0.093722522}}
{"text":"Regarding each scenario, the outage probability, ergodic communication rate (CR), and sensing rate (SR) are analyzed to derive the diversity orders and high signal-to-noise ratio slopes.","meta":{"url":"http://arxiv.org/abs/2307.15023v1"},"cats":{"new-dataset":0.1300564532,"dev-research":0.1715892386,"prompt-eng":0.3815716312,"data-quality":0.1875414901,"ml-security":0.1411879096}}
{"text":"Numerical results are provided to demonstrate that i) beamforming design can affect the high-SNR power offset and diversity order but does not influence the high-SNR slope; ii) ISAC exhibits larger high-SNR slopes and a more extensive SR-CR region than conventional frequency-division S&C (FDSAC) techniques.","meta":{"url":"http://arxiv.org/abs/2307.15023v1"},"cats":{"new-dataset":0.0364763386,"dev-research":0.2188576901,"prompt-eng":0.3890637905,"data-quality":0.1684762185,"ml-security":0.08540855}}
{"text":"Large language models (LLMs) have shown the potential to be integrated into human daily lives.","meta":{"url":"http://arxiv.org/abs/2307.15020v1"},"cats":{"new-dataset":0.0764087453,"dev-research":0.1865443578,"prompt-eng":0.3735534024,"data-quality":0.1192358271,"ml-security":0.0797509627}}
{"text":"Therefore, user preference is the most critical criterion for assessing LLMs' performance in real-world scenarios.","meta":{"url":"http://arxiv.org/abs/2307.15020v1"},"cats":{"new-dataset":0.0038495506,"dev-research":0.2867516275,"prompt-eng":0.4725584529,"data-quality":0.0933707475,"ml-security":0.1544029964}}
{"text":"However, existing benchmarks mainly focus on measuring models' accuracy using multi-choice questions, which limits the understanding of their capabilities in real applications.","meta":{"url":"http://arxiv.org/abs/2307.15020v1"},"cats":{"new-dataset":0.0212663806,"dev-research":0.26876972,"prompt-eng":0.4208547564,"data-quality":0.1563862401,"ml-security":0.0883813708}}
{"text":"We fill this gap by proposing a comprehensive Chinese benchmark SuperCLUE, named after another popular Chinese LLM benchmark CLUE.","meta":{"url":"http://arxiv.org/abs/2307.15020v1"},"cats":{"new-dataset":0.3867028286,"dev-research":0.2142506164,"prompt-eng":0.4404899887,"data-quality":0.1700886087,"ml-security":0.066356806}}
{"text":"SuperCLUE encompasses three sub-tasks: actual users' queries and ratings derived from an LLM battle platform (CArena), open-ended questions with single and multiple-turn dialogues (OPEN), and closed-ended questions with the same stems as open-ended single-turn ones (CLOSE).","meta":{"url":"http://arxiv.org/abs/2307.15020v1"},"cats":{"new-dataset":0.089944,"dev-research":0.264833843,"prompt-eng":0.4148054275,"data-quality":0.0822798904,"ml-security":0.0739540456}}
{"text":"Our study shows that accuracy on closed-ended questions is insufficient to reflect human preferences achieved on open-ended ones.","meta":{"url":"http://arxiv.org/abs/2307.15020v1"},"cats":{"new-dataset":0.029450062,"dev-research":0.331485477,"prompt-eng":0.455121375,"data-quality":0.2708569092,"ml-security":0.1351166962}}
{"text":"At the same time, they can complement each other to predict actual user preferences.","meta":{"url":"http://arxiv.org/abs/2307.15020v1"},"cats":{"new-dataset":0.0029413632,"dev-research":0.3177368288,"prompt-eng":0.4837764504,"data-quality":0.1030350933,"ml-security":0.127279281}}
{"text":"We also demonstrate that GPT-4 is a reliable judge to automatically evaluate human preferences on open-ended questions in a Chinese context.","meta":{"url":"http://arxiv.org/abs/2307.15020v1"},"cats":{"new-dataset":0.1260240277,"dev-research":0.2918508181,"prompt-eng":0.4883673713,"data-quality":0.2577996575,"ml-security":0.1018379193}}
{"text":"Our benchmark will be released at https://www.CLUEbenchmarks.com","meta":{"url":"http://arxiv.org/abs/2307.15020v1"},"cats":{"new-dataset":0.5534956573,"dev-research":0.3225114582,"prompt-eng":0.4532225562,"data-quality":0.208763962,"ml-security":0.1254239801}}
{"text":"Deepfake detection methods have shown promising results in recognizing forgeries within a given dataset, where training and testing take place on the in-distribution dataset.","meta":{"url":"http://arxiv.org/abs/2307.15019v1"},"cats":{"new-dataset":0.4792341599,"dev-research":0.2319598316,"prompt-eng":0.3686338131,"data-quality":0.4663615531,"ml-security":0.388010566}}
{"text":"However, their performance deteriorates significantly when presented with unseen samples.","meta":{"url":"http://arxiv.org/abs/2307.15019v1"},"cats":{"new-dataset":0.0216497989,"dev-research":0.2829349698,"prompt-eng":0.3593007659,"data-quality":0.3382281548,"ml-security":0.1580467708}}
{"text":"As a result, a reliable deepfake detection system must remain impartial to forgery types, appearance, and quality for guaranteed generalizable detection performance.","meta":{"url":"http://arxiv.org/abs/2307.15019v1"},"cats":{"new-dataset":0.0632705116,"dev-research":0.257477523,"prompt-eng":0.4173732536,"data-quality":0.4433886774,"ml-security":0.3114073528}}
{"text":"Despite various attempts to enhance cross-dataset generalization, the problem remains challenging, particularly when testing against common post-processing perturbations, such as video compression or blur.","meta":{"url":"http://arxiv.org/abs/2307.15019v1"},"cats":{"new-dataset":0.2305502699,"dev-research":0.2263538986,"prompt-eng":0.3919714083,"data-quality":0.3665531624,"ml-security":0.2057359903}}
{"text":"Hence, this study introduces a deepfake detection framework, leveraging a self-supervised pre-training model that delivers exceptional generalization ability, withstanding common corruptions and enabling feature explainability.","meta":{"url":"http://arxiv.org/abs/2307.15019v1"},"cats":{"new-dataset":0.1888304929,"dev-research":0.3466003523,"prompt-eng":0.4589598952,"data-quality":0.4156506555,"ml-security":0.3751338187}}
{"text":"The framework comprises three key components: a feature extractor based on vision Transformer architecture that is pre-trained via self-supervised contrastive learning methodology, a graph convolution network coupled with a Transformer discriminator, and a graph Transformer relevancy map that provides a better understanding of manipulated regions and further explains the model's decision.","meta":{"url":"http://arxiv.org/abs/2307.15019v1"},"cats":{"new-dataset":0.1218300597,"dev-research":0.25322557,"prompt-eng":0.3842395206,"data-quality":0.1925867557,"ml-security":0.1096905727}}
{"text":"To assess the effectiveness of the proposed framework, several challenging experiments are conducted, including in-data distribution performance, cross-dataset, cross-manipulation generalization, and robustness against common post-production perturbations.","meta":{"url":"http://arxiv.org/abs/2307.15019v1"},"cats":{"new-dataset":0.1885218631,"dev-research":0.261444164,"prompt-eng":0.4337000319,"data-quality":0.2702295254,"ml-security":0.1798619072}}
{"text":"The results achieved demonstrate the remarkable effectiveness of the proposed deepfake detection framework, surpassing the current state-of-the-art approaches.","meta":{"url":"http://arxiv.org/abs/2307.15019v1"},"cats":{"new-dataset":0.2324804372,"dev-research":0.2332270841,"prompt-eng":0.4103030024,"data-quality":0.3048011773,"ml-security":0.2053732755}}
{"text":"We revisit the problem of designing scalable protocols for private statistics and private federated learning when each device holds its private data.","meta":{"url":"http://arxiv.org/abs/2307.15017v1"},"cats":{"new-dataset":0.2291630975,"dev-research":0.1956304728,"prompt-eng":0.3596666484,"data-quality":0.108511051,"ml-security":0.4212485478}}
{"text":"Our first contribution is to propose a simple primitive that allows for efficient implementation of several commonly used algorithms, and allows for privacy accounting that is close to that in the central setting without requiring the strong trust assumptions it entails.","meta":{"url":"http://arxiv.org/abs/2307.15017v1"},"cats":{"new-dataset":0.1670878508,"dev-research":0.2393765805,"prompt-eng":0.397840626,"data-quality":0.1719433075,"ml-security":0.2977528491}}
{"text":"Second, we propose a system architecture that implements this primitive and perform a security analysis of the proposed system.","meta":{"url":"http://arxiv.org/abs/2307.15017v1"},"cats":{"new-dataset":0.1264132536,"dev-research":0.3359878776,"prompt-eng":0.4666275231,"data-quality":0.1404490355,"ml-security":0.3818885236}}
{"text":"Google's Bard has emerged as a formidable competitor to OpenAI's ChatGPT in the field of conversational AI.","meta":{"url":"http://arxiv.org/abs/2307.15016v1"},"cats":{"new-dataset":0.2486963121,"dev-research":0.2785211289,"prompt-eng":0.3075852843,"data-quality":0.1318887199,"ml-security":0.1294529035}}
{"text":"Notably, Bard has recently been updated to handle visual inputs alongside text prompts during conversations.","meta":{"url":"http://arxiv.org/abs/2307.15016v1"},"cats":{"new-dataset":0.2027673432,"dev-research":0.392680604,"prompt-eng":0.4526892902,"data-quality":0.2342248615,"ml-security":0.1304188156}}
{"text":"Given Bard's impressive track record in handling textual inputs, we explore its capabilities in understanding and interpreting visual data (images) conditioned by text questions.","meta":{"url":"http://arxiv.org/abs/2307.15016v1"},"cats":{"new-dataset":0.3661525623,"dev-research":0.3591709256,"prompt-eng":0.442296188,"data-quality":0.3589168142,"ml-security":0.1113633958}}
{"text":"This exploration holds the potential to unveil new insights and challenges for Bard and other forthcoming multi-modal Generative models, especially in addressing complex computer vision problems that demand accurate visual and language understanding.","meta":{"url":"http://arxiv.org/abs/2307.15016v1"},"cats":{"new-dataset":0.1279031916,"dev-research":0.2410693303,"prompt-eng":0.4064113374,"data-quality":0.2310558255,"ml-security":0.0816883053}}
{"text":"Specifically, in this study, we focus on 15 diverse task scenarios encompassing regular, camouflaged, medical, under-water and remote sensing data to comprehensively evaluate Bard's performance.","meta":{"url":"http://arxiv.org/abs/2307.15016v1"},"cats":{"new-dataset":0.4956016678,"dev-research":0.2666004729,"prompt-eng":0.4313752385,"data-quality":0.174192876,"ml-security":0.091655725}}
{"text":"Our primary finding indicates that Bard still struggles in these vision scenarios, highlighting the significant gap in vision-based understanding that needs to be bridged in future developments.","meta":{"url":"http://arxiv.org/abs/2307.15016v1"},"cats":{"new-dataset":0.1466769863,"dev-research":0.3475931713,"prompt-eng":0.3797149338,"data-quality":0.2900380148,"ml-security":0.1032616358}}
{"text":"We expect that this empirical study will prove valuable in advancing future models, leading to enhanced capabilities in comprehending and interpreting fine-grained visual data.","meta":{"url":"http://arxiv.org/abs/2307.15016v1"},"cats":{"new-dataset":0.1624146085,"dev-research":0.286789261,"prompt-eng":0.3923711557,"data-quality":0.1468286685,"ml-security":0.0991817256}}
{"text":"Our project is released on https://github.com/htqin/GoogleBard-VisUnderstand","meta":{"url":"http://arxiv.org/abs/2307.15016v1"},"cats":{"new-dataset":0.4949497242,"dev-research":0.2728940177,"prompt-eng":0.3897574569,"data-quality":0.1209892679,"ml-security":0.0764580594}}
{"text":"With the increased deployment of machine learning models in various real-world applications, researchers and practitioners alike have emphasized the need for explanations of model behaviour.","meta":{"url":"http://arxiv.org/abs/2307.15007v1"},"cats":{"new-dataset":0.0078516427,"dev-research":0.3589064716,"prompt-eng":0.4204244115,"data-quality":0.1906651775,"ml-security":0.4531983526}}
{"text":"To this end, two broad strategies have been outlined in prior literature to explain models.","meta":{"url":"http://arxiv.org/abs/2307.15007v1"},"cats":{"new-dataset":0.006615225,"dev-research":0.2299352866,"prompt-eng":0.3758536917,"data-quality":0.0898768236,"ml-security":0.1057461266}}
{"text":"Post hoc explanation methods explain the behaviour of complex black-box models by highlighting features that are critical to model predictions; however, prior work has shown that these explanations may not be faithful, and even more concerning is our inability to verify them.","meta":{"url":"http://arxiv.org/abs/2307.15007v1"},"cats":{"new-dataset":0.0108301735,"dev-research":0.3345362505,"prompt-eng":0.4114875448,"data-quality":0.2294963302,"ml-security":0.3570650907}}
{"text":"Specifically, it is nontrivial to evaluate if a given attribution is correct with respect to the underlying model.","meta":{"url":"http://arxiv.org/abs/2307.15007v1"},"cats":{"new-dataset":0.0026942502,"dev-research":0.2315517743,"prompt-eng":0.4175478184,"data-quality":0.3368243769,"ml-security":0.2065681313}}
{"text":"Inherently interpretable models, on the other hand, circumvent these issues by explicitly encoding explanations into model architecture, meaning their explanations are naturally faithful and verifiable, but they often exhibit poor predictive performance due to their limited expressive power.","meta":{"url":"http://arxiv.org/abs/2307.15007v1"},"cats":{"new-dataset":0.0039894288,"dev-research":0.3509331994,"prompt-eng":0.3958217824,"data-quality":0.2462667157,"ml-security":0.3716275889}}
{"text":"In this work, we aim to bridge the gap between the aforementioned strategies by proposing Verifiability Tuning (VerT), a method that transforms black-box models into models that naturally yield faithful and verifiable feature attributions.","meta":{"url":"http://arxiv.org/abs/2307.15007v1"},"cats":{"new-dataset":0.0813411483,"dev-research":0.2446195685,"prompt-eng":0.4674418818,"data-quality":0.2756123565,"ml-security":0.3031994297}}
{"text":"We begin by introducing a formal theoretical framework to understand verifiability and show that attributions produced by standard models cannot be verified.","meta":{"url":"http://arxiv.org/abs/2307.15007v1"},"cats":{"new-dataset":0.0438968975,"dev-research":0.2027804003,"prompt-eng":0.4454164394,"data-quality":0.2686757811,"ml-security":0.2742537705}}
{"text":"We then leverage this framework to propose a method to build verifiable models and feature attributions out of fully trained black-box models.","meta":{"url":"http://arxiv.org/abs/2307.15007v1"},"cats":{"new-dataset":0.1278985968,"dev-research":0.2574073164,"prompt-eng":0.4479772237,"data-quality":0.2699360549,"ml-security":0.4072208281}}
{"text":"Finally, we perform extensive experiments on semi-synthetic and real-world datasets, and show that VerT produces models that (1) yield explanations that are correct and verifiable and (2) are faithful to the original black-box models they are meant to explain.","meta":{"url":"http://arxiv.org/abs/2307.15007v1"},"cats":{"new-dataset":0.1759270255,"dev-research":0.2710320454,"prompt-eng":0.4037267161,"data-quality":0.2227430909,"ml-security":0.2992695639}}
{"text":"Light detection and ranging (LiDAR) sensors are becoming available on modern mobile devices and provide a 3D sensing capability.","meta":{"url":"http://arxiv.org/abs/2307.15005v1"},"cats":{"new-dataset":0.0923358473,"dev-research":0.2311878126,"prompt-eng":0.3333549669,"data-quality":0.084020864,"ml-security":0.1115425399}}
{"text":"This new capability is beneficial for perceptions in various use cases, but it is challenging for resource-constrained mobile devices to use the perceptions in real-time because of their high computational complexity.","meta":{"url":"http://arxiv.org/abs/2307.15005v1"},"cats":{"new-dataset":0.0265766198,"dev-research":0.3128510339,"prompt-eng":0.412055559,"data-quality":0.0789343876,"ml-security":0.0987235582}}
{"text":"In this context, edge computing can be used to enable LiDAR online perceptions, but offloading the perceptions on the edge server requires a low-latency, lightweight, and efficient compression due to the large volume of LiDAR point clouds data.   ","meta":{"url":"http://arxiv.org/abs/2307.15005v1"},"cats":{"new-dataset":0.01839792,"dev-research":0.2845121921,"prompt-eng":0.2997294485,"data-quality":0.0900266728,"ml-security":0.1552301936}}
{"text":"This paper presents FLiCR, a fast and lightweight LiDAR point cloud compression method for enabling edge-assisted online perceptions.","meta":{"url":"http://arxiv.org/abs/2307.15005v1"},"cats":{"new-dataset":0.0424820871,"dev-research":0.2546752896,"prompt-eng":0.3470139262,"data-quality":0.1114535481,"ml-security":0.1076765528}}
{"text":"FLiCR is based on range images (RI) as an intermediate representation (IR), and dictionary coding for compressing RIs.","meta":{"url":"http://arxiv.org/abs/2307.15005v1"},"cats":{"new-dataset":0.2293221064,"dev-research":0.2249408947,"prompt-eng":0.3934581468,"data-quality":0.1313946152,"ml-security":0.0489006993}}
{"text":"FLiCR achieves its benefits by leveraging lossy RIs, and we show the efficiency of bytestream compression is largely improved with quantization and subsampling.","meta":{"url":"http://arxiv.org/abs/2307.15005v1"},"cats":{"new-dataset":0.1224070112,"dev-research":0.2781704222,"prompt-eng":0.3641904021,"data-quality":0.1914384746,"ml-security":0.1370877164}}
{"text":"In addition, we identify the limitation of current quality metrics for presenting the entropy of a point cloud, and introduce a new metric that reflects both point-wise and entropy-wise qualities for lossy IRs.","meta":{"url":"http://arxiv.org/abs/2307.15005v1"},"cats":{"new-dataset":0.0835781809,"dev-research":0.2263864973,"prompt-eng":0.406550404,"data-quality":0.28432174,"ml-security":0.1038419705}}
{"text":"The evaluation results show FLiCR is more suitable for edge-assisted real-time perceptions than the existing LiDAR compressions, and we demonstrate the effectiveness of our compression and metric with the evaluations on 3D object detection and LiDAR SLAM.","meta":{"url":"http://arxiv.org/abs/2307.15005v1"},"cats":{"new-dataset":0.0721000739,"dev-research":0.27423934,"prompt-eng":0.3670922138,"data-quality":0.1327670276,"ml-security":0.1019084808}}
{"text":"The effectiveness of compression distance in KNN-based text classification ('gzip') has recently garnered lots of attention.","meta":{"url":"http://arxiv.org/abs/2307.15002v1"},"cats":{"new-dataset":0.1359692341,"dev-research":0.2198763944,"prompt-eng":0.3311307758,"data-quality":0.3074560088,"ml-security":0.1248424528}}
{"text":"In this note, we show that similar or better effectiveness can be achieved with simpler means, and text compression may not be necessary.","meta":{"url":"http://arxiv.org/abs/2307.15002v1"},"cats":{"new-dataset":0.043255186,"dev-research":0.3002928428,"prompt-eng":0.4115309664,"data-quality":0.3703758851,"ml-security":0.1329255663}}
{"text":"Indeed, we find that a simple 'bag-of-words' matching can achieve similar or better accuracy, and is more efficient.","meta":{"url":"http://arxiv.org/abs/2307.15002v1"},"cats":{"new-dataset":0.0344248791,"dev-research":0.273647167,"prompt-eng":0.4067984152,"data-quality":0.3568938158,"ml-security":0.0517221116}}
{"text":"We present TransNormerLLM, the first linear attention-based Large Language Model (LLM) that outperforms conventional softmax attention-based models in terms of both accuracy and efficiency.","meta":{"url":"http://arxiv.org/abs/2307.14995v1"},"cats":{"new-dataset":0.1497898349,"dev-research":0.1550083011,"prompt-eng":0.3719111122,"data-quality":0.2233633093,"ml-security":0.1199530919}}
{"text":"TransNormerLLM evolves from the previous linear attention architecture TransNormer by making advanced modifications that include positional embedding, linear attention acceleration, gating mechanism, tensor normalization, inference acceleration and stabilization.","meta":{"url":"http://arxiv.org/abs/2307.14995v1"},"cats":{"new-dataset":0.0346287516,"dev-research":0.1952902118,"prompt-eng":0.4047892543,"data-quality":0.1333031738,"ml-security":0.1362115998}}
{"text":"Specifically, we use LRPE together with an exponential decay to avoid attention dilution issues while allowing the model to retain global interactions between tokens.","meta":{"url":"http://arxiv.org/abs/2307.14995v1"},"cats":{"new-dataset":0.0224611271,"dev-research":0.1932194295,"prompt-eng":0.4587573698,"data-quality":0.1358376995,"ml-security":0.1509161777}}
{"text":"Additionally, we propose Lightning Attention, a cutting-edge technique that accelerates linear attention by more than twice in runtime and reduces memory usage by a remarkable four times.","meta":{"url":"http://arxiv.org/abs/2307.14995v1"},"cats":{"new-dataset":0.0511040258,"dev-research":0.3476555933,"prompt-eng":0.4586406292,"data-quality":0.1752021603,"ml-security":0.231471836}}
{"text":"To further enhance the performance of TransNormer, we leverage a gating mechanism to smooth training and a new tensor normalization scheme to accelerate the model, resulting in an impressive acceleration of over 20%.","meta":{"url":"http://arxiv.org/abs/2307.14995v1"},"cats":{"new-dataset":0.0371126197,"dev-research":0.2003871364,"prompt-eng":0.4312749436,"data-quality":0.1456191242,"ml-security":0.1116606427}}
{"text":"Furthermore, we have developed a robust inference algorithm that ensures numerical stability and consistent inference speed, regardless of the sequence length, showcasing superior efficiency during both training and inference stages.","meta":{"url":"http://arxiv.org/abs/2307.14995v1"},"cats":{"new-dataset":0.0714477592,"dev-research":0.2354033353,"prompt-eng":0.4620528054,"data-quality":0.2494415178,"ml-security":0.1810717416}}
{"text":"Scalability is at the heart of our model's design, enabling seamless deployment on large-scale clusters and facilitating expansion to even more extensive models, all while maintaining outstanding performance metrics.","meta":{"url":"http://arxiv.org/abs/2307.14995v1"},"cats":{"new-dataset":0.0594834869,"dev-research":0.2491434341,"prompt-eng":0.4022905905,"data-quality":0.0936911759,"ml-security":0.0938270582}}
{"text":"Rigorous validation of our model design is achieved through a series of comprehensive experiments on our self-collected corpus, boasting a size exceeding 6TB and containing over 2 trillion tokens.","meta":{"url":"http://arxiv.org/abs/2307.14995v1"},"cats":{"new-dataset":0.2790259794,"dev-research":0.1942342257,"prompt-eng":0.4593430165,"data-quality":0.3018080962,"ml-security":0.1174455233}}
{"text":"To ensure data quality and relevance, we implement a new self-cleaning strategy to filter our collected data.","meta":{"url":"http://arxiv.org/abs/2307.14995v1"},"cats":{"new-dataset":0.145854309,"dev-research":0.328233357,"prompt-eng":0.4137987424,"data-quality":0.3247681908,"ml-security":0.1335360518}}
{"text":"Our pre-trained models will be released to foster community advancements in efficient LLMs.","meta":{"url":"http://arxiv.org/abs/2307.14995v1"},"cats":{"new-dataset":0.1024588231,"dev-research":0.2151122388,"prompt-eng":0.4413780515,"data-quality":0.0914103138,"ml-security":0.1912125422}}
{"text":"We propose the Thinker algorithm, a novel approach that enables reinforcement learning agents to autonomously interact with and utilize a learned world model.","meta":{"url":"http://arxiv.org/abs/2307.14993v1"},"cats":{"new-dataset":0.0684070184,"dev-research":0.205257397,"prompt-eng":0.4168152859,"data-quality":0.0856500954,"ml-security":0.1411455607}}
{"text":"The Thinker algorithm wraps the environment with a world model and introduces new actions designed for interacting with the world model.","meta":{"url":"http://arxiv.org/abs/2307.14993v1"},"cats":{"new-dataset":0.0906934046,"dev-research":0.2636374719,"prompt-eng":0.4348425057,"data-quality":0.0699730643,"ml-security":0.1070098017}}
{"text":"These model-interaction actions enable agents to perform planning by proposing alternative plans to the world model before selecting a final action to execute in the environment.","meta":{"url":"http://arxiv.org/abs/2307.14993v1"},"cats":{"new-dataset":0.0746544484,"dev-research":0.2547634191,"prompt-eng":0.4490131802,"data-quality":0.0414460098,"ml-security":0.0786968567}}
{"text":"This approach eliminates the need for hand-crafted planning algorithms by enabling the agent to learn how to plan autonomously and allows for easy interpretation of the agent's plan with visualization.","meta":{"url":"http://arxiv.org/abs/2307.14993v1"},"cats":{"new-dataset":0.0878499111,"dev-research":0.3769594092,"prompt-eng":0.422388049,"data-quality":0.0645749203,"ml-security":0.107648627}}
{"text":"We demonstrate the algorithm's effectiveness through experimental results in the game of Sokoban and the Atari 2600 benchmark, where the Thinker algorithm achieves state-of-the-art performance and competitive results, respectively.","meta":{"url":"http://arxiv.org/abs/2307.14993v1"},"cats":{"new-dataset":0.0463429226,"dev-research":0.3043244709,"prompt-eng":0.4096529391,"data-quality":0.1063619486,"ml-security":0.1769670436}}
{"text":"Visualizations of agents trained with the Thinker algorithm demonstrate that they have learned to plan effectively with the world model to select better actions.","meta":{"url":"http://arxiv.org/abs/2307.14993v1"},"cats":{"new-dataset":0.0908688002,"dev-research":0.2639516779,"prompt-eng":0.4015753569,"data-quality":0.0642241977,"ml-security":0.128101934}}
{"text":"The algorithm's generality opens a new research direction on how a world model can be used in reinforcement learning and how planning can be seamlessly integrated into an agent's decision-making process.","meta":{"url":"http://arxiv.org/abs/2307.14993v1"},"cats":{"new-dataset":0.0300536289,"dev-research":0.2134583486,"prompt-eng":0.3687446067,"data-quality":0.0658308596,"ml-security":0.1404607175}}
{"text":"Many software projects implement APIs and algorithms in multiple programming languages.","meta":{"url":"http://arxiv.org/abs/2307.14991v1"},"cats":{"new-dataset":0.0448238505,"dev-research":0.4457532344,"prompt-eng":0.3230090079,"data-quality":0.1363605727,"ml-security":0.128123629}}
{"text":"Maintaining such projects is tiresome, as developers have to ensure that any change (e.g., a bug fix or a new feature) is being propagated, timely and without errors, to implementations in other programming languages.","meta":{"url":"http://arxiv.org/abs/2307.14991v1"},"cats":{"new-dataset":0.0517372411,"dev-research":0.6044464532,"prompt-eng":0.3954060176,"data-quality":0.2625783836,"ml-security":0.1727664036}}
{"text":"In the world of ever-changing software, using rule-based translation tools (i.e., transpilers) or machine learning models for translating code from one language to another provides limited value.","meta":{"url":"http://arxiv.org/abs/2307.14991v1"},"cats":{"new-dataset":0.1205984771,"dev-research":0.401933129,"prompt-eng":0.4094412014,"data-quality":0.229891766,"ml-security":0.1128461278}}
{"text":"Translating each time the entire codebase from one language to another is not the way developers work.","meta":{"url":"http://arxiv.org/abs/2307.14991v1"},"cats":{"new-dataset":0.0447181088,"dev-research":0.5167230454,"prompt-eng":0.3868962664,"data-quality":0.3261524542,"ml-security":0.1073822045}}
{"text":"In this paper, we target a novel task: translating code changes from one programming language to another using large language models (LLMs).","meta":{"url":"http://arxiv.org/abs/2307.14991v1"},"cats":{"new-dataset":0.2348546532,"dev-research":0.38428926,"prompt-eng":0.453388598,"data-quality":0.2371965194,"ml-security":0.1299629779}}
{"text":"We design and implement the first LLM, dubbed Codeditor, to tackle this task.","meta":{"url":"http://arxiv.org/abs/2307.14991v1"},"cats":{"new-dataset":0.2432158666,"dev-research":0.3155052167,"prompt-eng":0.5314120268,"data-quality":0.158963739,"ml-security":0.0618483454}}
{"text":"Codeditor explicitly models code changes as edit sequences and learns to correlate changes across programming languages.","meta":{"url":"http://arxiv.org/abs/2307.14991v1"},"cats":{"new-dataset":0.1071527038,"dev-research":0.4870546409,"prompt-eng":0.4324315245,"data-quality":0.301813572,"ml-security":0.1593263798}}
{"text":"To evaluate Codeditor, we collect a corpus of 6,613 aligned code changes from 8 pairs of open-source software projects implementing similar functionalities in two programming languages (Java and C#).","meta":{"url":"http://arxiv.org/abs/2307.14991v1"},"cats":{"new-dataset":0.4694952431,"dev-research":0.4880279257,"prompt-eng":0.4083764649,"data-quality":0.268348298,"ml-security":0.103389906}}
{"text":"Results show that Codeditor outperforms the state-of-the-art approaches by a large margin on all commonly used automatic metrics.","meta":{"url":"http://arxiv.org/abs/2307.14991v1"},"cats":{"new-dataset":0.0638924345,"dev-research":0.4687375106,"prompt-eng":0.4806917883,"data-quality":0.2656094549,"ml-security":0.0773847504}}
{"text":"Our work also reveals that Codeditor is complementary to the existing generation-based models, and their combination ensures even greater performance.","meta":{"url":"http://arxiv.org/abs/2307.14991v1"},"cats":{"new-dataset":0.0886495408,"dev-research":0.3525299471,"prompt-eng":0.4581851754,"data-quality":0.135556073,"ml-security":0.0707228722}}
{"text":"Deep learning often faces the challenge of efficiently processing dynamic inputs, such as sensor data or user inputs.","meta":{"url":"http://arxiv.org/abs/2307.14988v1"},"cats":{"new-dataset":0.0688570492,"dev-research":0.2912326038,"prompt-eng":0.3929492802,"data-quality":0.1591568126,"ml-security":0.3838101426}}
{"text":"For example, an AI writing assistant is required to update its suggestions in real time as a document is edited.","meta":{"url":"http://arxiv.org/abs/2307.14988v1"},"cats":{"new-dataset":0.0223000446,"dev-research":0.4352102544,"prompt-eng":0.4204186528,"data-quality":0.1628809416,"ml-security":0.1717271157}}
{"text":"Re-running the model each time is expensive, even with compression techniques like knowledge distillation, pruning, or quantization.","meta":{"url":"http://arxiv.org/abs/2307.14988v1"},"cats":{"new-dataset":0.0154909381,"dev-research":0.300263358,"prompt-eng":0.3864630484,"data-quality":0.1369854099,"ml-security":0.1449171668}}
{"text":"Instead, we take an incremental computing approach, looking to reuse calculations as the inputs change.","meta":{"url":"http://arxiv.org/abs/2307.14988v1"},"cats":{"new-dataset":0.0303725573,"dev-research":0.3569065467,"prompt-eng":0.4397012806,"data-quality":0.1002258792,"ml-security":0.099280359}}
{"text":"However, the dense connectivity of conventional architectures poses a major obstacle to incremental computation, as even minor input changes cascade through the network and restrict information reuse.","meta":{"url":"http://arxiv.org/abs/2307.14988v1"},"cats":{"new-dataset":0.0210571842,"dev-research":0.3585004103,"prompt-eng":0.359586277,"data-quality":0.1057721917,"ml-security":0.1746153015}}
{"text":"To address this, we use vector quantization to discretize intermediate values in the network, which filters out noisy and unnecessary modifications to hidden neurons, facilitating the reuse of their values.","meta":{"url":"http://arxiv.org/abs/2307.14988v1"},"cats":{"new-dataset":0.0281522211,"dev-research":0.2898794418,"prompt-eng":0.3816368159,"data-quality":0.2061460296,"ml-security":0.2891767671}}
{"text":"We apply this approach to the transformers architecture, creating an efficient incremental inference algorithm with complexity proportional to the fraction of the modified inputs.","meta":{"url":"http://arxiv.org/abs/2307.14988v1"},"cats":{"new-dataset":0.0649650945,"dev-research":0.2588721657,"prompt-eng":0.4337827584,"data-quality":0.1191929871,"ml-security":0.1356148619}}
{"text":"Our experiments with adapting the OPT-125M pre-trained language model demonstrate comparable accuracy on document classification while requiring 12.1X (median) fewer operations for processing sequences of atomic edits.","meta":{"url":"http://arxiv.org/abs/2307.14988v1"},"cats":{"new-dataset":0.0750257538,"dev-research":0.25264262,"prompt-eng":0.4188436712,"data-quality":0.4188706022,"ml-security":0.0917427317}}
{"text":"Social network simulation plays a crucial role in addressing various challenges within social science.","meta":{"url":"http://arxiv.org/abs/2307.14984v1"},"cats":{"new-dataset":0.0492348119,"dev-research":0.2675398318,"prompt-eng":0.3308188786,"data-quality":0.1260156068,"ml-security":0.1472977595}}
{"text":"It offers extensive applications such as state prediction, phenomena explanation, and policy-making support, among others.","meta":{"url":"http://arxiv.org/abs/2307.14984v1"},"cats":{"new-dataset":0.0371419179,"dev-research":0.2751811403,"prompt-eng":0.3961337021,"data-quality":0.0900012258,"ml-security":0.1342753393}}
{"text":"In this work, we harness the formidable human-like capabilities exhibited by large language models (LLMs) in sensing, reasoning, and behaving, and utilize these qualities to construct the S$^3$ system (short for $\\textbf{S}$ocial network $\\textbf{S}$imulation $\\textbf{S}$ystem).","meta":{"url":"http://arxiv.org/abs/2307.14984v1"},"cats":{"new-dataset":0.2749538843,"dev-research":0.2256770389,"prompt-eng":0.4536505699,"data-quality":0.1797935232,"ml-security":0.1750187204}}
{"text":"Adhering to the widely employed agent-based simulation paradigm, we employ prompt engineering and prompt tuning techniques to ensure that the agent's behavior closely emulates that of a genuine human within the social network.","meta":{"url":"http://arxiv.org/abs/2307.14984v1"},"cats":{"new-dataset":0.0431288579,"dev-research":0.2831308602,"prompt-eng":0.5526132511,"data-quality":0.1165820078,"ml-security":0.2226388345}}
{"text":"Specifically, we simulate three pivotal aspects: emotion, attitude, and interaction behaviors.","meta":{"url":"http://arxiv.org/abs/2307.14984v1"},"cats":{"new-dataset":0.1033446066,"dev-research":0.2130791504,"prompt-eng":0.458691505,"data-quality":0.0642374985,"ml-security":0.076995183}}
{"text":"By endowing the agent in the system with the ability to perceive the informational environment and emulate human actions, we observe the emergence of population-level phenomena, including the propagation of information, attitudes, and emotions.","meta":{"url":"http://arxiv.org/abs/2307.14984v1"},"cats":{"new-dataset":0.0413194618,"dev-research":0.2587225577,"prompt-eng":0.4622851083,"data-quality":0.0939689378,"ml-security":0.1989607902}}
{"text":"We conduct an evaluation encompassing two levels of simulation, employing real-world social network data.","meta":{"url":"http://arxiv.org/abs/2307.14984v1"},"cats":{"new-dataset":0.1221713658,"dev-research":0.2328410714,"prompt-eng":0.3957236652,"data-quality":0.1209864548,"ml-security":0.0820530408}}
{"text":"Encouragingly, the results demonstrate promising accuracy.","meta":{"url":"http://arxiv.org/abs/2307.14984v1"},"cats":{"new-dataset":0.0380645593,"dev-research":0.2545490577,"prompt-eng":0.4242218526,"data-quality":0.2403786337,"ml-security":0.0759799744}}
{"text":"This work represents an initial step in the realm of social network simulation empowered by LLM-based agents.","meta":{"url":"http://arxiv.org/abs/2307.14984v1"},"cats":{"new-dataset":0.1094312268,"dev-research":0.1720283214,"prompt-eng":0.405600344,"data-quality":0.1024947874,"ml-security":0.1210892851}}
{"text":"We anticipate that our endeavors will serve as a source of inspiration for the development of simulation systems within, but not limited to, social science.","meta":{"url":"http://arxiv.org/abs/2307.14984v1"},"cats":{"new-dataset":0.1170768581,"dev-research":0.2811606949,"prompt-eng":0.3878937648,"data-quality":0.0780631117,"ml-security":0.1413184168}}
{"text":"Simulating camera sensors is a crucial task in autonomous driving.","meta":{"url":"http://arxiv.org/abs/2307.14981v1"},"cats":{"new-dataset":0.0867136427,"dev-research":0.2710671067,"prompt-eng":0.401317831,"data-quality":0.1122678821,"ml-security":0.1439624612}}
{"text":"Although neural radiance fields are exceptional at synthesizing photorealistic views in driving simulations, they still fail in generating extrapolated views.","meta":{"url":"http://arxiv.org/abs/2307.14981v1"},"cats":{"new-dataset":0.0931280667,"dev-research":0.2241985663,"prompt-eng":0.3529054769,"data-quality":0.1291404075,"ml-security":0.1405338619}}
{"text":"This paper proposes to incorporate map priors into neural radiance fields to synthesize out-of-trajectory driving views with semantic road consistency.","meta":{"url":"http://arxiv.org/abs/2307.14981v1"},"cats":{"new-dataset":0.1824815032,"dev-research":0.2631454057,"prompt-eng":0.380046053,"data-quality":0.1729913415,"ml-security":0.1018820153}}
{"text":"The key insight is that map information can be utilized as a prior to guide the training of the radiance fields with uncertainty.","meta":{"url":"http://arxiv.org/abs/2307.14981v1"},"cats":{"new-dataset":0.0259054161,"dev-research":0.3066845491,"prompt-eng":0.3900109737,"data-quality":0.155476368,"ml-security":0.1069075081}}
{"text":"Specifically, we utilize the coarse ground surface as uncertain information to supervise the density field and warp depth with uncertainty from unknown camera poses to ensure multi-view consistency.","meta":{"url":"http://arxiv.org/abs/2307.14981v1"},"cats":{"new-dataset":0.199696071,"dev-research":0.2227641447,"prompt-eng":0.4334969883,"data-quality":0.1676147893,"ml-security":0.0669456718}}
{"text":"Experimental results demonstrate that our approach can produce semantic consistency in deviated views for vehicle camera simulation.","meta":{"url":"http://arxiv.org/abs/2307.14981v1"},"cats":{"new-dataset":0.1114858774,"dev-research":0.3097421515,"prompt-eng":0.412828994,"data-quality":0.2902508352,"ml-security":0.0562599647}}
{"text":"Industry 4.0 applications impose the challenging demand of delivering packets with bounded latencies via a wireless network.","meta":{"url":"http://arxiv.org/abs/2307.14980v1"},"cats":{"new-dataset":0.0395407566,"dev-research":0.2710062372,"prompt-eng":0.3307097588,"data-quality":0.1003086927,"ml-security":0.1206872074}}
{"text":"This is further complicated if the network is not dedicated to the time critical application.","meta":{"url":"http://arxiv.org/abs/2307.14980v1"},"cats":{"new-dataset":0.0245731428,"dev-research":0.2605026445,"prompt-eng":0.4075199958,"data-quality":0.1240362729,"ml-security":0.1123835423}}
{"text":"In this paper we use network calculus analysis to derive closed form expressions of latency bounds for time critical traffic when 802.11 Target Wake Time (TWT) and 802.1Qbv work together in a shared 802.11 network.","meta":{"url":"http://arxiv.org/abs/2307.14980v1"},"cats":{"new-dataset":0.0286535696,"dev-research":0.3110031371,"prompt-eng":0.3708607503,"data-quality":0.1200565098,"ml-security":0.1496354078}}
{"text":"With the overwhelming trend of mask image modeling led by MAE, generative pre-training has shown a remarkable potential to boost the performance of fundamental models in 2D vision.","meta":{"url":"http://arxiv.org/abs/2307.14971v1"},"cats":{"new-dataset":0.0546043488,"dev-research":0.2259287273,"prompt-eng":0.4173352038,"data-quality":0.1148240552,"ml-security":0.1226226461}}
{"text":"However, in 3D vision, the over-reliance on Transformer-based backbones and the unordered nature of point clouds have restricted the further development of generative pre-training.","meta":{"url":"http://arxiv.org/abs/2307.14971v1"},"cats":{"new-dataset":0.0371542572,"dev-research":0.2327018811,"prompt-eng":0.3676996664,"data-quality":0.0801260715,"ml-security":0.1161429566}}
{"text":"In this paper, we propose a novel 3D-to-2D generative pre-training method that is adaptable to any point cloud model.","meta":{"url":"http://arxiv.org/abs/2307.14971v1"},"cats":{"new-dataset":0.1272982362,"dev-research":0.1939955598,"prompt-eng":0.3779130768,"data-quality":0.0870821785,"ml-security":0.0972730512}}
{"text":"We propose to generate view images from different instructed poses via the cross-attention mechanism as the pre-training scheme.","meta":{"url":"http://arxiv.org/abs/2307.14971v1"},"cats":{"new-dataset":0.298564085,"dev-research":0.1930934992,"prompt-eng":0.4538413403,"data-quality":0.1334404314,"ml-security":0.0938428558}}
{"text":"Generating view images has more precise supervision than its point cloud counterpart, thus assisting 3D backbones to have a finer comprehension of the geometrical structure and stereoscopic relations of the point cloud.","meta":{"url":"http://arxiv.org/abs/2307.14971v1"},"cats":{"new-dataset":0.1074237246,"dev-research":0.2543536861,"prompt-eng":0.3807882478,"data-quality":0.1189935245,"ml-security":0.068310217}}
{"text":"Experimental results have proved the superiority of our proposed 3D-to-2D generative pre-training over previous pre-training methods.","meta":{"url":"http://arxiv.org/abs/2307.14971v1"},"cats":{"new-dataset":0.0701987407,"dev-research":0.257957613,"prompt-eng":0.4201426977,"data-quality":0.1151371482,"ml-security":0.0987366821}}
{"text":"Our method is also effective in boosting the performance of architecture-oriented approaches, achieving state-of-the-art performance when fine-tuning on ScanObjectNN classification and ShapeNetPart segmentation tasks.","meta":{"url":"http://arxiv.org/abs/2307.14971v1"},"cats":{"new-dataset":0.0602881733,"dev-research":0.2944074057,"prompt-eng":0.4023183822,"data-quality":0.1878638943,"ml-security":0.1620254328}}
{"text":"Code is available at https://github.com/wangzy22/TAP.","meta":{"url":"http://arxiv.org/abs/2307.14971v1"},"cats":{"new-dataset":0.4121589425,"dev-research":0.2909965112,"prompt-eng":0.438565602,"data-quality":0.1493142524,"ml-security":0.0685327441}}
{"text":"In the medical field, federated learning commonly deals with highly imbalanced datasets, including skin lesions and gastrointestinal images.","meta":{"url":"http://arxiv.org/abs/2307.14959v1"},"cats":{"new-dataset":0.2043017496,"dev-research":0.1990296778,"prompt-eng":0.3183674848,"data-quality":0.187280763,"ml-security":0.3325076357}}
{"text":"Existing federated methods under highly imbalanced datasets primarily focus on optimizing a global model without incorporating the intra-class variations that can arise in medical imaging due to different populations, findings, and scanners.","meta":{"url":"http://arxiv.org/abs/2307.14959v1"},"cats":{"new-dataset":0.0678226478,"dev-research":0.1884664581,"prompt-eng":0.3823585656,"data-quality":0.1591351958,"ml-security":0.1881345847}}
{"text":"In this paper, we study the inter-client intra-class variations with publicly available self-supervised auxiliary networks.","meta":{"url":"http://arxiv.org/abs/2307.14959v1"},"cats":{"new-dataset":0.0615670274,"dev-research":0.2169421561,"prompt-eng":0.4283346286,"data-quality":0.3060690169,"ml-security":0.2566414383}}
{"text":"Specifically, we find that employing a shared auxiliary pre-trained model, like MoCo-V2, locally on every client yields consistent divergence measurements.","meta":{"url":"http://arxiv.org/abs/2307.14959v1"},"cats":{"new-dataset":0.033118454,"dev-research":0.2179182364,"prompt-eng":0.4063579804,"data-quality":0.2325333405,"ml-security":0.2330007569}}
{"text":"Based on these findings, we derive a dynamic balanced model aggregation via self-supervised priors (MAS) to guide the global model optimization.","meta":{"url":"http://arxiv.org/abs/2307.14959v1"},"cats":{"new-dataset":0.0307626217,"dev-research":0.1530854142,"prompt-eng":0.4455561923,"data-quality":0.1507129002,"ml-security":0.1564822325}}
{"text":"Fed-MAS can be utilized with different local learning methods for effective model aggregation toward a highly robust and unbiased global model.","meta":{"url":"http://arxiv.org/abs/2307.14959v1"},"cats":{"new-dataset":0.1438629873,"dev-research":0.1629420562,"prompt-eng":0.4169289336,"data-quality":0.2518727581,"ml-security":0.1433569714}}
{"text":"Our code is available at \\url{https://github.com/xmed-lab/Fed-MAS}.","meta":{"url":"http://arxiv.org/abs/2307.14959v1"},"cats":{"new-dataset":0.4939983265,"dev-research":0.2656800236,"prompt-eng":0.4402243866,"data-quality":0.1815045749,"ml-security":0.0911854914}}
{"text":"Reproducibility of recommender systems research has come under scrutiny during recent years.","meta":{"url":"http://arxiv.org/abs/2307.14956v1"},"cats":{"new-dataset":0.0309815041,"dev-research":0.3188867449,"prompt-eng":0.3971809992,"data-quality":0.1876277082,"ml-security":0.1599662002}}
{"text":"Along with works focusing on repeating experiments with certain algorithms, the research community has also started discussing various aspects of evaluation and how these affect reproducibility.","meta":{"url":"http://arxiv.org/abs/2307.14956v1"},"cats":{"new-dataset":0.0455667491,"dev-research":0.3279555134,"prompt-eng":0.4125743912,"data-quality":0.2656106432,"ml-security":0.117300852}}
{"text":"We add a novel angle to this discussion by examining how unofficial third-party implementations could benefit or hinder reproducibility.","meta":{"url":"http://arxiv.org/abs/2307.14956v1"},"cats":{"new-dataset":0.0511093271,"dev-research":0.3546715527,"prompt-eng":0.4479340311,"data-quality":0.2364325791,"ml-security":0.1705777176}}
{"text":"Besides giving a general overview, we thoroughly examine six third-party implementations of a popular recommender algorithm and compare them to the official version on five public datasets.","meta":{"url":"http://arxiv.org/abs/2307.14956v1"},"cats":{"new-dataset":0.2711713221,"dev-research":0.256917606,"prompt-eng":0.3688720436,"data-quality":0.1731865536,"ml-security":0.1202775778}}
{"text":"In the light of our alarming findings we aim to draw the attention of the research community to this neglected aspect of reproducibility.","meta":{"url":"http://arxiv.org/abs/2307.14956v1"},"cats":{"new-dataset":0.1710342619,"dev-research":0.3081418925,"prompt-eng":0.4122465234,"data-quality":0.3242492121,"ml-security":0.153573901}}
{"text":"This paper seeks to solve Multi-Source Domain Adaptation (MSDA), which aims to mitigate data distribution shifts when transferring knowledge from multiple labeled source domains to an unlabeled target domain.","meta":{"url":"http://arxiv.org/abs/2307.14953v1"},"cats":{"new-dataset":0.1351025821,"dev-research":0.2704121856,"prompt-eng":0.4087760837,"data-quality":0.4141783781,"ml-security":0.2160053288}}
{"text":"We propose a novel MSDA framework based on dictionary learning and optimal transport.","meta":{"url":"http://arxiv.org/abs/2307.14953v1"},"cats":{"new-dataset":0.1497045832,"dev-research":0.2157258041,"prompt-eng":0.3765707545,"data-quality":0.1675254966,"ml-security":0.1055805818}}
{"text":"We interpret each domain in MSDA as an empirical distribution.","meta":{"url":"http://arxiv.org/abs/2307.14953v1"},"cats":{"new-dataset":0.1466001447,"dev-research":0.2323726999,"prompt-eng":0.3700110992,"data-quality":0.2297559528,"ml-security":0.1422655993}}
{"text":"As such, we express each domain as a Wasserstein barycenter of dictionary atoms, which are empirical distributions.","meta":{"url":"http://arxiv.org/abs/2307.14953v1"},"cats":{"new-dataset":0.1536048505,"dev-research":0.167168448,"prompt-eng":0.379264853,"data-quality":0.1977776854,"ml-security":0.0911637284}}
{"text":"We propose a novel algorithm, DaDiL, for learning via mini-batches: (i) atom distributions; (ii) a matrix of barycentric coordinates.","meta":{"url":"http://arxiv.org/abs/2307.14953v1"},"cats":{"new-dataset":0.2259528929,"dev-research":0.1751317223,"prompt-eng":0.36992924,"data-quality":0.1923100569,"ml-security":0.0905501476}}
{"text":"Based on our dictionary, we propose two novel methods for MSDA: DaDil-R, based on the reconstruction of labeled samples in the target domain, and DaDiL-E, based on the ensembling of classifiers learned on atom distributions.","meta":{"url":"http://arxiv.org/abs/2307.14953v1"},"cats":{"new-dataset":0.2664212236,"dev-research":0.2410601941,"prompt-eng":0.4029291707,"data-quality":0.4152693956,"ml-security":0.1384192772}}
{"text":"We evaluate our methods in 3 benchmarks: Caltech-Office, Office 31, and CRWU, where we improved previous state-of-the-art by 3.15%, 2.29%, and 7.71% in classification performance.","meta":{"url":"http://arxiv.org/abs/2307.14953v1"},"cats":{"new-dataset":0.3191563501,"dev-research":0.2531263415,"prompt-eng":0.4353240329,"data-quality":0.2615543348,"ml-security":0.0839784884}}
{"text":"Finally, we show that interpolations in the Wasserstein hull of learned atoms provide data that can generalize to the target domain.","meta":{"url":"http://arxiv.org/abs/2307.14953v1"},"cats":{"new-dataset":0.1473814644,"dev-research":0.1507929811,"prompt-eng":0.3666853563,"data-quality":0.1487896905,"ml-security":0.0977735333}}
{"text":"As the network scale increases, existing fully distributed solutions start to lag behind the real-world challenges such as (1) slow information propagation, (2) network communication failures, and (3) external adversarial attacks.","meta":{"url":"http://arxiv.org/abs/2307.14952v1"},"cats":{"new-dataset":0.049373526,"dev-research":0.2445722699,"prompt-eng":0.325636129,"data-quality":0.1891633883,"ml-security":0.5475647443}}
{"text":"In this paper, we focus on hierarchical system architecture and address the problem of non-Bayesian learning over networks that are vulnerable to communication failures and adversarial attacks.","meta":{"url":"http://arxiv.org/abs/2307.14952v1"},"cats":{"new-dataset":0.0424909307,"dev-research":0.2246949015,"prompt-eng":0.418476005,"data-quality":0.3419981399,"ml-security":0.7620624159}}
{"text":"On network communication, we consider packet-dropping link failures.   ","meta":{"url":"http://arxiv.org/abs/2307.14952v1"},"cats":{"new-dataset":0.0119805851,"dev-research":0.3288372364,"prompt-eng":0.4017612577,"data-quality":0.3293991367,"ml-security":0.1954236332}}
{"text":"We first propose a hierarchical robust push-sum algorithm that can achieve average consensus despite frequent packet-dropping link failures.","meta":{"url":"http://arxiv.org/abs/2307.14952v1"},"cats":{"new-dataset":0.0776099529,"dev-research":0.2171564785,"prompt-eng":0.4028049869,"data-quality":0.2146333389,"ml-security":0.196938599}}
{"text":"We provide a sparse information fusion rule between the parameter server and arbitrarily selected network representatives.","meta":{"url":"http://arxiv.org/abs/2307.14952v1"},"cats":{"new-dataset":0.0381941167,"dev-research":0.2280375367,"prompt-eng":0.4582750791,"data-quality":0.224234943,"ml-security":0.2421567526}}
{"text":"Then, interleaving the consensus update step with a dual averaging update with Kullback-Leibler (KL) divergence as the proximal function, we obtain a packet-dropping fault-tolerant non-Bayesian learning algorithm with provable convergence guarantees.   ","meta":{"url":"http://arxiv.org/abs/2307.14952v1"},"cats":{"new-dataset":0.0410129084,"dev-research":0.2194761533,"prompt-eng":0.3774136102,"data-quality":0.265748367,"ml-security":0.3016776719}}
{"text":"On external adversarial attacks, we consider Byzantine attacks in which the compromised agents can send maliciously calibrated messages to others (including both the agents and the parameter server).","meta":{"url":"http://arxiv.org/abs/2307.14952v1"},"cats":{"new-dataset":0.0703255311,"dev-research":0.2221994158,"prompt-eng":0.4370681288,"data-quality":0.267025481,"ml-security":0.8582267006}}
{"text":"To avoid the curse of dimensionality of Byzantine consensus, we solve the non-Bayesian learning problem via running multiple dynamics, each of which only involves Byzantine consensus with scalar inputs.","meta":{"url":"http://arxiv.org/abs/2307.14952v1"},"cats":{"new-dataset":0.0574781461,"dev-research":0.1505746519,"prompt-eng":0.3726097281,"data-quality":0.1515660587,"ml-security":0.3288214878}}
{"text":"To facilitate resilient information propagation across sub-networks, we use a novel Byzantine-resilient gossiping-type rule at the parameter server.","meta":{"url":"http://arxiv.org/abs/2307.14952v1"},"cats":{"new-dataset":0.0414247874,"dev-research":0.2784066059,"prompt-eng":0.4413312081,"data-quality":0.2353516554,"ml-security":0.41097727}}
{"text":"Even though offline evaluation is just an imperfect proxy of online performance -- due to the interactive nature of recommenders -- it will probably remain the primary way of evaluation in recommender systems research for the foreseeable future, since the proprietary nature of production recommenders prevents independent validation of A/B test setups and verification of online results.","meta":{"url":"http://arxiv.org/abs/2307.14951v1"},"cats":{"new-dataset":0.0098471657,"dev-research":0.298211827,"prompt-eng":0.4328922679,"data-quality":0.1848582654,"ml-security":0.1073244005}}
{"text":"Therefore, it is imperative that offline evaluation setups are as realistic and as flawless as they can be.","meta":{"url":"http://arxiv.org/abs/2307.14951v1"},"cats":{"new-dataset":0.0321785663,"dev-research":0.4020852525,"prompt-eng":0.4533047776,"data-quality":0.2501765689,"ml-security":0.1169190537}}
{"text":"Unfortunately, evaluation flaws are quite common in recommender systems research nowadays, due to later works copying flawed evaluation setups from their predecessors without questioning their validity.","meta":{"url":"http://arxiv.org/abs/2307.14951v1"},"cats":{"new-dataset":0.0094537568,"dev-research":0.4273594639,"prompt-eng":0.4368605426,"data-quality":0.5523314018,"ml-security":0.184438761}}
{"text":"In the hope of improving the quality of offline evaluation of recommender systems, we discuss four of these widespread flaws and why researchers should avoid them.","meta":{"url":"http://arxiv.org/abs/2307.14951v1"},"cats":{"new-dataset":0.0385217473,"dev-research":0.307109997,"prompt-eng":0.4139711616,"data-quality":0.3165045197,"ml-security":0.1377137726}}
{"text":"We developed a new approach comprised of different visualizations for the comparative spatio-temporal analysis of displacement processes in porous media.","meta":{"url":"http://arxiv.org/abs/2307.14949v1"},"cats":{"new-dataset":0.1882595255,"dev-research":0.2775350339,"prompt-eng":0.3218304683,"data-quality":0.0843195209,"ml-security":0.0518855321}}
{"text":"We aim to analyze and compare ensemble datasets from experiments to gain insight into the influence of different parameters on fluid flow.","meta":{"url":"http://arxiv.org/abs/2307.14949v1"},"cats":{"new-dataset":0.2663698696,"dev-research":0.2434397038,"prompt-eng":0.348602159,"data-quality":0.170620177,"ml-security":0.1394743238}}
{"text":"To capture the displacement of a defending fluid by an invading fluid, we first condense an input image series to a single time map.","meta":{"url":"http://arxiv.org/abs/2307.14949v1"},"cats":{"new-dataset":0.132954109,"dev-research":0.2328448529,"prompt-eng":0.4048885831,"data-quality":0.0976088742,"ml-security":0.1785969219}}
{"text":"From this map, we generate a spatio-temporal flow graph covering the whole process.","meta":{"url":"http://arxiv.org/abs/2307.14949v1"},"cats":{"new-dataset":0.4544467282,"dev-research":0.2457474552,"prompt-eng":0.4029152004,"data-quality":0.0806011747,"ml-security":0.0494803218}}
{"text":"This graph is further simplified to only reflect topological changes in the movement of the invading fluid.","meta":{"url":"http://arxiv.org/abs/2307.14949v1"},"cats":{"new-dataset":0.0631499256,"dev-research":0.2287559435,"prompt-eng":0.324867712,"data-quality":0.1141365462,"ml-security":0.1195776202}}
{"text":"Our interactive tools allow the visual analysis of these processes by visualizing the graph structure and the context of the experimental setup, as well as by providing charts for multiple metrics.","meta":{"url":"http://arxiv.org/abs/2307.14949v1"},"cats":{"new-dataset":0.2770950985,"dev-research":0.3522194772,"prompt-eng":0.4399521509,"data-quality":0.1303774335,"ml-security":0.0528806541}}
{"text":"We apply our approach to analyze and compare ensemble datasets jointly with domain experts, where we vary either fluid properties or the solid structure of the porous medium.","meta":{"url":"http://arxiv.org/abs/2307.14949v1"},"cats":{"new-dataset":0.3211282891,"dev-research":0.2266875831,"prompt-eng":0.3530186373,"data-quality":0.1757697827,"ml-security":0.105223385}}
{"text":"We finally report the generated insights from the domain experts and discuss our contribution's advantages, generality, and limitations.","meta":{"url":"http://arxiv.org/abs/2307.14949v1"},"cats":{"new-dataset":0.1205244267,"dev-research":0.3313388578,"prompt-eng":0.4091324819,"data-quality":0.1741844039,"ml-security":0.0906407064}}
{"text":"The continuous dynamics of natural systems has been effectively modelled using Neural Ordinary Differential Equations (Neural ODEs).","meta":{"url":"http://arxiv.org/abs/2307.14940v1"},"cats":{"new-dataset":0.0485615026,"dev-research":0.1876176806,"prompt-eng":0.3412909999,"data-quality":0.0795197299,"ml-security":0.1978942333}}
{"text":"However, for accurate and meaningful predictions, it is crucial that the models follow the underlying rules or laws that govern these systems.","meta":{"url":"http://arxiv.org/abs/2307.14940v1"},"cats":{"new-dataset":0.0126801019,"dev-research":0.2369683698,"prompt-eng":0.3826994964,"data-quality":0.1645564836,"ml-security":0.2211571139}}
{"text":"In this work, we propose a self-adaptive penalty algorithm for Neural ODEs to enable modelling of constrained natural systems.","meta":{"url":"http://arxiv.org/abs/2307.14940v1"},"cats":{"new-dataset":0.0257088363,"dev-research":0.1730981054,"prompt-eng":0.3752843084,"data-quality":0.1589259446,"ml-security":0.286912763}}
{"text":"The proposed self-adaptive penalty function can dynamically adjust the penalty parameters.","meta":{"url":"http://arxiv.org/abs/2307.14940v1"},"cats":{"new-dataset":0.0047454144,"dev-research":0.2533406486,"prompt-eng":0.450438994,"data-quality":0.1677056149,"ml-security":0.2042528775}}
{"text":"The explicit introduction of prior knowledge helps to increase the interpretability of Neural ODE -based models.","meta":{"url":"http://arxiv.org/abs/2307.14940v1"},"cats":{"new-dataset":0.0224229249,"dev-research":0.3148536817,"prompt-eng":0.3688051413,"data-quality":0.1967383343,"ml-security":0.2481502336}}
{"text":"We validate the proposed approach by modelling three natural systems with prior knowledge constraints: population growth, chemical reaction evolution, and damped harmonic oscillator motion.","meta":{"url":"http://arxiv.org/abs/2307.14940v1"},"cats":{"new-dataset":0.0396323175,"dev-research":0.1731813921,"prompt-eng":0.4177946867,"data-quality":0.1055707705,"ml-security":0.1017318064}}
{"text":"The numerical experiments and a comparison with other penalty Neural ODE approaches and \\emph{vanilla} Neural ODE, demonstrate the effectiveness of the proposed self-adaptive penalty algorithm for Neural ODEs in modelling constrained natural systems.","meta":{"url":"http://arxiv.org/abs/2307.14940v1"},"cats":{"new-dataset":0.0147034049,"dev-research":0.181294296,"prompt-eng":0.371193059,"data-quality":0.1867139228,"ml-security":0.2928870908}}
{"text":"Moreover, the self-adaptive penalty approach provides more accurate and robust models with reliable and meaningful predictions.","meta":{"url":"http://arxiv.org/abs/2307.14940v1"},"cats":{"new-dataset":0.0112632775,"dev-research":0.2036439514,"prompt-eng":0.422402178,"data-quality":0.2348263295,"ml-security":0.2052208024}}
{"text":"Large Language Models for Code (Code LLM) are flourishing.","meta":{"url":"http://arxiv.org/abs/2307.14936v1"},"cats":{"new-dataset":0.1485545756,"dev-research":0.3508294442,"prompt-eng":0.3957851358,"data-quality":0.2232925605,"ml-security":0.1251840207}}
{"text":"New and powerful models are released on a weekly basis, demonstrating remarkable performance on the code generation task.","meta":{"url":"http://arxiv.org/abs/2307.14936v1"},"cats":{"new-dataset":0.2434038239,"dev-research":0.3543661242,"prompt-eng":0.4388377371,"data-quality":0.1172675518,"ml-security":0.1063316565}}
{"text":"Various approaches have been proposed to boost the code generation performance of pre-trained Code LLMs, such as supervised fine-tuning, instruction tuning, reinforcement learning, etc.","meta":{"url":"http://arxiv.org/abs/2307.14936v1"},"cats":{"new-dataset":0.0446711529,"dev-research":0.4283513441,"prompt-eng":0.466044872,"data-quality":0.2012789393,"ml-security":0.1654876704}}
{"text":"In this paper, we propose a novel RRTF (Rank Responses to align Test&Teacher Feedback) framework, which can effectively and efficiently boost pre-trained large language models for code generation.","meta":{"url":"http://arxiv.org/abs/2307.14936v1"},"cats":{"new-dataset":0.2973059445,"dev-research":0.3967522257,"prompt-eng":0.4777600113,"data-quality":0.2460208619,"ml-security":0.1168986003}}
{"text":"Under this framework, we present PanGu-Coder2, which achieves 62.20% pass@1 on the OpenAI HumanEval benchmark.","meta":{"url":"http://arxiv.org/abs/2307.14936v1"},"cats":{"new-dataset":0.5226451311,"dev-research":0.2490425414,"prompt-eng":0.458751204,"data-quality":0.1913674097,"ml-security":0.0949774587}}
{"text":"Furthermore, through an extensive evaluation on CoderEval and LeetCode benchmarks, we show that PanGu-Coder2 consistently outperforms all previous Code LLMs.","meta":{"url":"http://arxiv.org/abs/2307.14936v1"},"cats":{"new-dataset":0.2495657661,"dev-research":0.3604259959,"prompt-eng":0.4326115019,"data-quality":0.2759852442,"ml-security":0.1094855809}}
{"text":"Data profiling is an essential process in modern data-driven industries.","meta":{"url":"http://arxiv.org/abs/2307.14935v1"},"cats":{"new-dataset":0.1449358838,"dev-research":0.4292381659,"prompt-eng":0.3604789514,"data-quality":0.1527464145,"ml-security":0.1256907626}}
{"text":"One of its critical components is the discovery and validation of complex statistics, including functional dependencies, data constraints, association rules, and others.   ","meta":{"url":"http://arxiv.org/abs/2307.14935v1"},"cats":{"new-dataset":0.096618551,"dev-research":0.2948303386,"prompt-eng":0.3958835256,"data-quality":0.1946085935,"ml-security":0.1172083005}}
{"text":"However, most existing data profiling systems that focus on complex statistics do not provide proper integration with the tools used by contemporary data scientists.","meta":{"url":"http://arxiv.org/abs/2307.14935v1"},"cats":{"new-dataset":0.0773670568,"dev-research":0.3450613116,"prompt-eng":0.336708837,"data-quality":0.2514042623,"ml-security":0.1045141038}}
{"text":"This creates a significant barrier to the adoption of these tools in the industry.","meta":{"url":"http://arxiv.org/abs/2307.14935v1"},"cats":{"new-dataset":0.0150439276,"dev-research":0.3678346042,"prompt-eng":0.3114834257,"data-quality":0.0836148424,"ml-security":0.1650439706}}
{"text":"Moreover, existing systems were not created with industrial-grade workloads in mind.","meta":{"url":"http://arxiv.org/abs/2307.14935v1"},"cats":{"new-dataset":0.0188899354,"dev-research":0.3057512554,"prompt-eng":0.2878860459,"data-quality":0.1191460629,"ml-security":0.1047173062}}
{"text":"Finally, they do not aim to provide descriptive explanations, i.e. why a given pattern is not found.","meta":{"url":"http://arxiv.org/abs/2307.14935v1"},"cats":{"new-dataset":0.0409981183,"dev-research":0.3034951231,"prompt-eng":0.3875037859,"data-quality":0.2841568038,"ml-security":0.1103828021}}
{"text":"It is a significant issue as it is essential to understand the underlying reasons for a specific pattern's absence to make informed decisions based on the data.   ","meta":{"url":"http://arxiv.org/abs/2307.14935v1"},"cats":{"new-dataset":0.0344289538,"dev-research":0.3056154002,"prompt-eng":0.3675202729,"data-quality":0.2802905174,"ml-security":0.1503289014}}
{"text":"Because of that, these patterns are effectively rest in thin air: their application scope is rather limited, they are rarely used by the broader public.","meta":{"url":"http://arxiv.org/abs/2307.14935v1"},"cats":{"new-dataset":0.0148841726,"dev-research":0.2516791465,"prompt-eng":0.3061079505,"data-quality":0.1178238342,"ml-security":0.1462714461}}
{"text":"At the same time, as we are going to demonstrate in this presentation, complex statistics can be efficiently used to solve many classic data quality problems.   ","meta":{"url":"http://arxiv.org/abs/2307.14935v1"},"cats":{"new-dataset":0.1064722434,"dev-research":0.3461316367,"prompt-eng":0.3882563569,"data-quality":0.1678345539,"ml-security":0.0805734924}}
{"text":"Desbordante is an open-source data profiler that aims to close this gap.","meta":{"url":"http://arxiv.org/abs/2307.14935v1"},"cats":{"new-dataset":0.4937675119,"dev-research":0.3415314296,"prompt-eng":0.3988333642,"data-quality":0.1896166363,"ml-security":0.1035776358}}
{"text":"It is built with emphasis on industrial application: it is efficient, scalable, resilient to crashes, and provides explanations.","meta":{"url":"http://arxiv.org/abs/2307.14935v1"},"cats":{"new-dataset":0.0950121994,"dev-research":0.5257625957,"prompt-eng":0.388725587,"data-quality":0.1420037175,"ml-security":0.1457356074}}
{"text":"Furthermore, it provides seamless Python integration by offloading various costly operations to the C++ core, not only mining.   ","meta":{"url":"http://arxiv.org/abs/2307.14935v1"},"cats":{"new-dataset":0.0532916962,"dev-research":0.3842723687,"prompt-eng":0.4036916214,"data-quality":0.1007916874,"ml-security":0.1007143959}}
{"text":"In this demonstration, we show several scenarios that allow end users to solve different data quality problems.","meta":{"url":"http://arxiv.org/abs/2307.14935v1"},"cats":{"new-dataset":0.1606066844,"dev-research":0.3566148936,"prompt-eng":0.4410098005,"data-quality":0.349221588,"ml-security":0.1982470538}}
{"text":"Namely, we showcase typo detection, data deduplication, and data anomaly detection scenarios.","meta":{"url":"http://arxiv.org/abs/2307.14935v1"},"cats":{"new-dataset":0.4865399239,"dev-research":0.4157150995,"prompt-eng":0.4221657465,"data-quality":0.4550503137,"ml-security":0.3025325865}}
{"text":"Regular Path Queries (RPQs), which are essentially regular expressions to be matched against the labels of paths in labeled graphs, are at the core of graph database query languages like SPARQL.","meta":{"url":"http://arxiv.org/abs/2307.14930v1"},"cats":{"new-dataset":0.059701572,"dev-research":0.2601424538,"prompt-eng":0.3887474566,"data-quality":0.2672546842,"ml-security":0.1134820612}}
{"text":"A way to solve RPQs is to translate them into a sequence of operations on the adjacency matrices of each label.","meta":{"url":"http://arxiv.org/abs/2307.14930v1"},"cats":{"new-dataset":0.1574756135,"dev-research":0.2194733575,"prompt-eng":0.3860783344,"data-quality":0.2388552038,"ml-security":0.0597691133}}
{"text":"We design and implement a Boolean algebra on sparse matrix representations and, as an application, use them to handle RPQs.","meta":{"url":"http://arxiv.org/abs/2307.14930v1"},"cats":{"new-dataset":0.0887329234,"dev-research":0.2558892534,"prompt-eng":0.4093643994,"data-quality":0.1253666564,"ml-security":0.0979933041}}
{"text":"Our baseline representation uses the same space as the previously most compact index for RPQs and excels in handling the hardest types of queries.","meta":{"url":"http://arxiv.org/abs/2307.14930v1"},"cats":{"new-dataset":0.2507859601,"dev-research":0.2873202705,"prompt-eng":0.3736215493,"data-quality":0.1406510792,"ml-security":0.060796183}}
{"text":"Our more succinct structure, based on $k^2$-trees, is 4 times smaller and still solves complex RPQs in reasonable time.","meta":{"url":"http://arxiv.org/abs/2307.14930v1"},"cats":{"new-dataset":0.083597783,"dev-research":0.1943766549,"prompt-eng":0.33372716,"data-quality":0.0970927198,"ml-security":0.0712704894}}
{"text":"Graphs can be leveraged to model polyphonic multitrack symbolic music, where notes, chords and entire sections may be linked at different levels of the musical hierarchy by tonal and rhythmic relationships.","meta":{"url":"http://arxiv.org/abs/2307.14928v1"},"cats":{"new-dataset":0.1015358526,"dev-research":0.2318501419,"prompt-eng":0.3900749385,"data-quality":0.2199429608,"ml-security":0.0593073124}}
{"text":"Nonetheless, there is a lack of works that consider graph representations in the context of deep learning systems for music generation.","meta":{"url":"http://arxiv.org/abs/2307.14928v1"},"cats":{"new-dataset":0.0495826122,"dev-research":0.2427363565,"prompt-eng":0.2666278109,"data-quality":0.2898403425,"ml-security":0.0965359122}}
{"text":"This paper bridges this gap by introducing a novel graph representation for music and a deep Variational Autoencoder that generates the structure and the content of musical graphs separately, one after the other, with a hierarchical architecture that matches the structural priors of music.","meta":{"url":"http://arxiv.org/abs/2307.14928v1"},"cats":{"new-dataset":0.1013553724,"dev-research":0.2004007449,"prompt-eng":0.3626501572,"data-quality":0.2484458442,"ml-security":0.0677093718}}
{"text":"By separating the structure and content of musical graphs, it is possible to condition generation by specifying which instruments are played at certain times.","meta":{"url":"http://arxiv.org/abs/2307.14928v1"},"cats":{"new-dataset":0.0352255335,"dev-research":0.2864006187,"prompt-eng":0.4140932863,"data-quality":0.1886991773,"ml-security":0.0750638282}}
{"text":"This opens the door to a new form of human-computer interaction in the context of music co-creation.","meta":{"url":"http://arxiv.org/abs/2307.14928v1"},"cats":{"new-dataset":0.1222896723,"dev-research":0.3833110592,"prompt-eng":0.37559187,"data-quality":0.1543871755,"ml-security":0.0750141371}}
{"text":"After training the model on existing MIDI datasets, the experiments show that the model is able to generate appealing short and long musical sequences and to realistically interpolate between them, producing music that is tonally and rhythmically consistent.","meta":{"url":"http://arxiv.org/abs/2307.14928v1"},"cats":{"new-dataset":0.1501746708,"dev-research":0.2276269199,"prompt-eng":0.3729375022,"data-quality":0.1757891719,"ml-security":0.0921312787}}
{"text":"Finally, the visualization of the embeddings shows that the model is able to organize its latent space in accordance with known musical concepts.","meta":{"url":"http://arxiv.org/abs/2307.14928v1"},"cats":{"new-dataset":0.1345996561,"dev-research":0.1856224591,"prompt-eng":0.385266748,"data-quality":0.1760170987,"ml-security":0.064583422}}
{"text":"Coded distributed computing, proposed by Li et al., offers significant potential for reducing the communication load in MapReduce computing systems.","meta":{"url":"http://arxiv.org/abs/2307.14927v1"},"cats":{"new-dataset":0.0764005442,"dev-research":0.3253417842,"prompt-eng":0.3393952333,"data-quality":0.1163744585,"ml-security":0.1066916139}}
{"text":"In the setting of the \\emph{cascaded} coded distributed computing that consisting of $K$ nodes, $N$ input files, and $Q$ output functions, the objective is to compute each output function through $s\\geq 1$ nodes with a computation load $r\\geq 1$, enabling the application of coding techniques during the Shuffle phase to achieve minimum communication load.","meta":{"url":"http://arxiv.org/abs/2307.14927v1"},"cats":{"new-dataset":0.0529963552,"dev-research":0.2591281218,"prompt-eng":0.3738801642,"data-quality":0.1492044427,"ml-security":0.0962547657}}
{"text":"However, for most existing coded distributed computing schemes, a major limitation lies in their demand for splitting the original data into an exponentially growing number of input files in terms of $N/\\binom{K}{r} \\in\\mathbb{N}$ and requiring an exponentially large number of output functions $Q/\\binom{K}{s} \\in\\mathbb{N}$, which imposes stringent requirements for implementation and results in significant coding complexity when $K$ is large.","meta":{"url":"http://arxiv.org/abs/2307.14927v1"},"cats":{"new-dataset":0.1219060787,"dev-research":0.3243630733,"prompt-eng":0.3108980029,"data-quality":0.0963537979,"ml-security":0.1492500923}}
{"text":"In this paper, we focus on the cascaded case of $K/s\\in\\mathbb{N} $, deliberately designing the strategy of input files store and output functions assignment based on a grouping method, such that a low-complexity two-round Shuffle phase is available.","meta":{"url":"http://arxiv.org/abs/2307.14927v1"},"cats":{"new-dataset":0.1802706341,"dev-research":0.2535746343,"prompt-eng":0.3907897346,"data-quality":0.1697541333,"ml-security":0.1588268196}}
{"text":"The main advantages of our proposed scheme contains: 1) the communication load is quilt close to or surprisingly better than the optimal state-of-the-art scheme proposed by Li et al.; 2) our scheme requires significantly less number of input files and output functions; 3) all the operations are implemented over the minimum binary field $\\mathbb{F}_2$.","meta":{"url":"http://arxiv.org/abs/2307.14927v1"},"cats":{"new-dataset":0.0546583125,"dev-research":0.27422413,"prompt-eng":0.4176562669,"data-quality":0.1113624501,"ml-security":0.1205652194}}
{"text":"Performance Benchmarking of HPC systems is an ongoing effort that seeks to provide information that will allow for increased performance and improve the job schedulers that manage these systems.","meta":{"url":"http://arxiv.org/abs/2307.14921v1"},"cats":{"new-dataset":0.1079237406,"dev-research":0.4013441247,"prompt-eng":0.4400462217,"data-quality":0.1234598866,"ml-security":0.094546149}}
{"text":"We develop a benchmarking tool that utilizes machine learning models and gathers performance data on GPU-accelerated nodes while they perform material segmentation analysis.","meta":{"url":"http://arxiv.org/abs/2307.14921v1"},"cats":{"new-dataset":0.1244934544,"dev-research":0.268229983,"prompt-eng":0.3796856708,"data-quality":0.1377382798,"ml-security":0.1071879898}}
{"text":"The benchmark uses a ML model that has been converted from Caffe to PyTorch using the MMdnn toolkit and the MINC-2500 dataset.","meta":{"url":"http://arxiv.org/abs/2307.14921v1"},"cats":{"new-dataset":0.3421993757,"dev-research":0.2260267858,"prompt-eng":0.3959819201,"data-quality":0.2119230687,"ml-security":0.0942182598}}
{"text":"Performance data is gathered on two ERDC DSRC systems, Onyx and Vulcanite.","meta":{"url":"http://arxiv.org/abs/2307.14921v1"},"cats":{"new-dataset":0.1322572752,"dev-research":0.2995036997,"prompt-eng":0.4313151112,"data-quality":0.1003379382,"ml-security":0.0638729316}}
{"text":"The data reveals that while Vulcanite has faster model times in a large number of benchmarks, and it is also more subject to some environmental factors that can cause performances slower than Onyx.","meta":{"url":"http://arxiv.org/abs/2307.14921v1"},"cats":{"new-dataset":0.0166350916,"dev-research":0.3414093078,"prompt-eng":0.3381155243,"data-quality":0.0602044405,"ml-security":0.0773164498}}
{"text":"In contrast the model times from Onyx are consistent across benchmarks.","meta":{"url":"http://arxiv.org/abs/2307.14921v1"},"cats":{"new-dataset":0.0174193248,"dev-research":0.291356956,"prompt-eng":0.3817022897,"data-quality":0.1190811603,"ml-security":0.0606731158}}
{"text":"The demand for efficient 3D model generation techniques has grown exponentially, as manual creation of 3D models is time-consuming and requires specialized expertise.","meta":{"url":"http://arxiv.org/abs/2307.14918v1"},"cats":{"new-dataset":0.0402149331,"dev-research":0.3330373022,"prompt-eng":0.3728600176,"data-quality":0.0472050648,"ml-security":0.063970135}}
{"text":"While generative models have shown potential in creating 3D textured shapes from 2D images, their applicability in 3D industries is limited due to the lack of a well-defined camera distribution in real-world scenarios, resulting in low-quality shapes.","meta":{"url":"http://arxiv.org/abs/2307.14918v1"},"cats":{"new-dataset":0.0830928413,"dev-research":0.200978827,"prompt-eng":0.3742523665,"data-quality":0.0894391844,"ml-security":0.0781266173}}
{"text":"To overcome this limitation, we propose GET3D--, the first method that directly generates textured 3D shapes from 2D images with unknown pose and scale.","meta":{"url":"http://arxiv.org/abs/2307.14918v1"},"cats":{"new-dataset":0.2478013093,"dev-research":0.2066168377,"prompt-eng":0.3553539764,"data-quality":0.0987927285,"ml-security":0.1083581925}}
{"text":"GET3D-- comprises a 3D shape generator and a learnable camera sampler that captures the 6D external changes on the camera.","meta":{"url":"http://arxiv.org/abs/2307.14918v1"},"cats":{"new-dataset":0.3431207542,"dev-research":0.26769483,"prompt-eng":0.3862046448,"data-quality":0.0857110514,"ml-security":0.0551175499}}
{"text":"In addition, We propose a novel training schedule to stably optimize both the shape generator and camera sampler in a unified framework.","meta":{"url":"http://arxiv.org/abs/2307.14918v1"},"cats":{"new-dataset":0.2235944273,"dev-research":0.2386285289,"prompt-eng":0.4458405767,"data-quality":0.1208138768,"ml-security":0.0802500646}}
{"text":"By controlling external variations using the learnable camera sampler, our method can generate aligned shapes with clear textures.","meta":{"url":"http://arxiv.org/abs/2307.14918v1"},"cats":{"new-dataset":0.1196906908,"dev-research":0.2541989736,"prompt-eng":0.4276608079,"data-quality":0.1585609599,"ml-security":0.0602854541}}
{"text":"Extensive experiments demonstrate the efficacy of GET3D--, which precisely fits the 6D camera pose distribution and generates high-quality shapes on both synthetic and realistic unconstrained datasets.","meta":{"url":"http://arxiv.org/abs/2307.14918v1"},"cats":{"new-dataset":0.5151146089,"dev-research":0.2165560755,"prompt-eng":0.3411341709,"data-quality":0.1021584446,"ml-security":0.1100874203}}
{"text":"Visual AI systems are vulnerable to natural and synthetic physical corruption in the real-world.","meta":{"url":"http://arxiv.org/abs/2307.14917v1"},"cats":{"new-dataset":0.0649739814,"dev-research":0.3590434532,"prompt-eng":0.3559323633,"data-quality":0.343297094,"ml-security":0.4968863851}}
{"text":"Such corruption often arises unexpectedly and alters the model's performance.","meta":{"url":"http://arxiv.org/abs/2307.14917v1"},"cats":{"new-dataset":0.0084906609,"dev-research":0.375013862,"prompt-eng":0.3857417489,"data-quality":0.3750793142,"ml-security":0.4905738104}}
{"text":"In recent years, the primary focus has been on adversarial attacks.","meta":{"url":"http://arxiv.org/abs/2307.14917v1"},"cats":{"new-dataset":0.0243178634,"dev-research":0.2952180494,"prompt-eng":0.320348622,"data-quality":0.2312301452,"ml-security":0.8339400121}}
{"text":"However, natural corruptions (e.g., snow, fog, dust) are an omnipresent threat to visual AI systems and should be considered equally important.","meta":{"url":"http://arxiv.org/abs/2307.14917v1"},"cats":{"new-dataset":0.0691365633,"dev-research":0.3956884331,"prompt-eng":0.3402003045,"data-quality":0.273860861,"ml-security":0.5395182079}}
{"text":"Many existing works propose interesting solutions to train robust models against natural corruption.","meta":{"url":"http://arxiv.org/abs/2307.14917v1"},"cats":{"new-dataset":0.1959575863,"dev-research":0.2751130136,"prompt-eng":0.3886888653,"data-quality":0.3784739582,"ml-security":0.5694453861}}
{"text":"These works either leverage image augmentations, which come with the additional cost of model training, or place suspicious patches in the scene to design unadversarial examples.","meta":{"url":"http://arxiv.org/abs/2307.14917v1"},"cats":{"new-dataset":0.0416873444,"dev-research":0.2564178262,"prompt-eng":0.3926484627,"data-quality":0.2834772638,"ml-security":0.3630767088}}
{"text":"In this work, we propose the idea of naturalistic support artifacts (NSA) for robust prediction.","meta":{"url":"http://arxiv.org/abs/2307.14917v1"},"cats":{"new-dataset":0.1844846225,"dev-research":0.2547831442,"prompt-eng":0.4568567948,"data-quality":0.3761611586,"ml-security":0.2577444542}}
{"text":"The NSAs are shown to be beneficial in scenarios where model parameters are inaccessible and adding artifacts in the scene is feasible.","meta":{"url":"http://arxiv.org/abs/2307.14917v1"},"cats":{"new-dataset":0.0617988063,"dev-research":0.2550457981,"prompt-eng":0.4022266769,"data-quality":0.1684754872,"ml-security":0.3734093033}}
{"text":"The NSAs are natural looking objects generated through artifact training using DC-GAN to have high visual fidelity in the scene.","meta":{"url":"http://arxiv.org/abs/2307.14917v1"},"cats":{"new-dataset":0.1040824885,"dev-research":0.2354873745,"prompt-eng":0.3282507959,"data-quality":0.1696769992,"ml-security":0.1707480025}}
{"text":"We test against natural corruptions on the Imagenette dataset and observe the improvement in prediction confidence score by four times.","meta":{"url":"http://arxiv.org/abs/2307.14917v1"},"cats":{"new-dataset":0.3950514481,"dev-research":0.2742371711,"prompt-eng":0.3971054436,"data-quality":0.4841010031,"ml-security":0.4744633378}}
{"text":"We also demonstrate NSA's capability to increase adversarial accuracy by 8\\% on average.","meta":{"url":"http://arxiv.org/abs/2307.14917v1"},"cats":{"new-dataset":0.0509853447,"dev-research":0.2313527137,"prompt-eng":0.398433438,"data-quality":0.247430319,"ml-security":0.5364679904}}
{"text":"Lastly, we qualitatively analyze NSAs using saliency maps to understand how they help improve prediction confidence.","meta":{"url":"http://arxiv.org/abs/2307.14917v1"},"cats":{"new-dataset":0.1320684765,"dev-research":0.2930501706,"prompt-eng":0.4169609852,"data-quality":0.1415771535,"ml-security":0.1532126845}}
{"text":"The task of multi-author writing style detection aims at finding any positions of writing style change in a given text document.","meta":{"url":"http://arxiv.org/abs/2307.14913v1"},"cats":{"new-dataset":0.0294704341,"dev-research":0.3103884933,"prompt-eng":0.4080167231,"data-quality":0.362151886,"ml-security":0.0885438128}}
{"text":"We formulate the task as a natural language inference problem where two consecutive paragraphs are paired.","meta":{"url":"http://arxiv.org/abs/2307.14913v1"},"cats":{"new-dataset":0.0605465445,"dev-research":0.2361045245,"prompt-eng":0.4281848679,"data-quality":0.274734573,"ml-security":0.075992301}}
{"text":"Our approach focuses on transitions between paragraphs while truncating input tokens for the task.","meta":{"url":"http://arxiv.org/abs/2307.14913v1"},"cats":{"new-dataset":0.0430858083,"dev-research":0.2748034537,"prompt-eng":0.4954310553,"data-quality":0.2418935964,"ml-security":0.0723441176}}
{"text":"As backbone models, we employ different Transformer-based encoders with warmup phase during training.","meta":{"url":"http://arxiv.org/abs/2307.14913v1"},"cats":{"new-dataset":0.0663911404,"dev-research":0.1848076991,"prompt-eng":0.4314962397,"data-quality":0.0786087332,"ml-security":0.1541125027}}
{"text":"We submit the model version that outperforms baselines and other proposed model versions in our experiments.","meta":{"url":"http://arxiv.org/abs/2307.14913v1"},"cats":{"new-dataset":0.1181171056,"dev-research":0.2404845851,"prompt-eng":0.4649366127,"data-quality":0.2365060236,"ml-security":0.0991746144}}
{"text":"For the easy and medium setups, we submit transition-focused natural language inference based on DeBERTa with warmup training, and the same model without transition for the hard setup.","meta":{"url":"http://arxiv.org/abs/2307.14913v1"},"cats":{"new-dataset":0.1861681457,"dev-research":0.2130294145,"prompt-eng":0.4601450678,"data-quality":0.2200383519,"ml-security":0.0885610091}}
{"text":"Fanfiction, a popular form of creative writing set within established fictional universes, has gained a substantial online following.","meta":{"url":"http://arxiv.org/abs/2307.14912v1"},"cats":{"new-dataset":0.0848944114,"dev-research":0.2871505978,"prompt-eng":0.3863035817,"data-quality":0.1171489549,"ml-security":0.0476117718}}
{"text":"However, ensuring the well-being and safety of participants has become a critical concern in this community.","meta":{"url":"http://arxiv.org/abs/2307.14912v1"},"cats":{"new-dataset":0.1049062473,"dev-research":0.3493502548,"prompt-eng":0.3739361641,"data-quality":0.1089662627,"ml-security":0.2186309408}}
{"text":"The detection of triggering content, material that may cause emotional distress or trauma to readers, poses a significant challenge.","meta":{"url":"http://arxiv.org/abs/2307.14912v1"},"cats":{"new-dataset":0.0426399556,"dev-research":0.3372546797,"prompt-eng":0.4022131512,"data-quality":0.3584616323,"ml-security":0.2511860751}}
{"text":"In this paper, we describe our approach for the Trigger Detection shared task at PAN CLEF 2023, where we want to detect multiple triggering content in a given Fanfiction document.","meta":{"url":"http://arxiv.org/abs/2307.14912v1"},"cats":{"new-dataset":0.3469419549,"dev-research":0.3121300065,"prompt-eng":0.5365118634,"data-quality":0.2853607271,"ml-security":0.1041546836}}
{"text":"For this, we build a hierarchical model that uses recurrence over Transformer-based language models.","meta":{"url":"http://arxiv.org/abs/2307.14912v1"},"cats":{"new-dataset":0.1739814649,"dev-research":0.2025583054,"prompt-eng":0.4514699129,"data-quality":0.1822595353,"ml-security":0.0847788737}}
{"text":"In our approach, we first split long documents into smaller sized segments and use them to fine-tune a Transformer model.","meta":{"url":"http://arxiv.org/abs/2307.14912v1"},"cats":{"new-dataset":0.1785274258,"dev-research":0.2034083801,"prompt-eng":0.4261557747,"data-quality":0.14499382,"ml-security":0.0465875247}}
{"text":"Then, we extract feature embeddings from the fine-tuned Transformer model, which are used as input in the training of multiple LSTM models for trigger detection in a multi-label setting.","meta":{"url":"http://arxiv.org/abs/2307.14912v1"},"cats":{"new-dataset":0.1730586349,"dev-research":0.1954113092,"prompt-eng":0.4468379531,"data-quality":0.3321423685,"ml-security":0.1778010139}}
{"text":"Our model achieves an F1-macro score of 0.372 and F1-micro score of 0.736 on the validation set, which are higher than the baseline results shared at PAN CLEF 2023.","meta":{"url":"http://arxiv.org/abs/2307.14912v1"},"cats":{"new-dataset":0.2517340193,"dev-research":0.2303166354,"prompt-eng":0.4670803187,"data-quality":0.2158062241,"ml-security":0.0741509649}}
{"text":"The use of Wake-Up Radio (WUR) in Internet of Things (IoT) networks can significantly improve their energy efficiency: battery-powered sensors can remain in a low-power (sleep) mode while listening for wake-up messages using their WUR and reactivate only when polled, saving energy.","meta":{"url":"http://arxiv.org/abs/2307.14910v1"},"cats":{"new-dataset":0.0324656784,"dev-research":0.2695875191,"prompt-eng":0.3883503093,"data-quality":0.1024789507,"ml-security":0.0974866433}}
{"text":"However, polling-based Time Division Multiple Access (TDMA) may significantly increase data transmission delay if packets are generated sporadically, as nodes with no information still need to be polled.","meta":{"url":"http://arxiv.org/abs/2307.14910v1"},"cats":{"new-dataset":0.0146573833,"dev-research":0.2685643667,"prompt-eng":0.3618046799,"data-quality":0.0901280578,"ml-security":0.1542665355}}
{"text":"In this paper, we examine the effect of multicast polling for WUR-enabled wireless nodes.","meta":{"url":"http://arxiv.org/abs/2307.14910v1"},"cats":{"new-dataset":0.039429405,"dev-research":0.2729247697,"prompt-eng":0.4058574313,"data-quality":0.1204450819,"ml-security":0.1067276167}}
{"text":"The idea is to assign nodes to multicast groups so that all nodes in the same group can be solicited by a multicast polling message.","meta":{"url":"http://arxiv.org/abs/2307.14910v1"},"cats":{"new-dataset":0.0356968356,"dev-research":0.2804092121,"prompt-eng":0.4022042514,"data-quality":0.1441939863,"ml-security":0.1060386867}}
{"text":"This may cause collisions, which can be solved by requesting retransmissions from the involved nodes.","meta":{"url":"http://arxiv.org/abs/2307.14910v1"},"cats":{"new-dataset":0.0163238114,"dev-research":0.2205652021,"prompt-eng":0.3933320377,"data-quality":0.2526112883,"ml-security":0.2066549331}}
{"text":"We analyze the performance of different multicast polling and retransmission strategies, showing that the optimal approach can significantly reduce the delay over TDMA and ALOHA in low-traffic scenarios while keeping good energy efficiency.","meta":{"url":"http://arxiv.org/abs/2307.14910v1"},"cats":{"new-dataset":0.0862116368,"dev-research":0.219002275,"prompt-eng":0.3622684661,"data-quality":0.095549419,"ml-security":0.0912379115}}
{"text":"Migration to OCaml 5 requires updating a lot of C bindings due to the removal of naked pointer support.","meta":{"url":"http://arxiv.org/abs/2307.14909v1"},"cats":{"new-dataset":0.1255729746,"dev-research":0.382672387,"prompt-eng":0.3474259756,"data-quality":0.15917229,"ml-security":0.0920049787}}
{"text":"Writing OCaml user-defined primitives in C is a necessity, but is unsafe and error-prone.","meta":{"url":"http://arxiv.org/abs/2307.14909v1"},"cats":{"new-dataset":0.0765684983,"dev-research":0.4808671508,"prompt-eng":0.3808470203,"data-quality":0.1403197949,"ml-security":0.1539698587}}
{"text":"It does not benefit from either OCaml's or C's type checking, and existing C static analysers are not aware of the OCaml GC safety rules, and cannot infer them from existing macros alone.","meta":{"url":"http://arxiv.org/abs/2307.14909v1"},"cats":{"new-dataset":0.0148045222,"dev-research":0.4276104874,"prompt-eng":0.3504233562,"data-quality":0.2323387502,"ml-security":0.1929620338}}
{"text":"The alternative is automatically generating C stubs, which requires correctly managing value lifetimes.","meta":{"url":"http://arxiv.org/abs/2307.14909v1"},"cats":{"new-dataset":0.0401719386,"dev-research":0.4222022881,"prompt-eng":0.4798406384,"data-quality":0.2139951956,"ml-security":0.1662555601}}
{"text":"Having a static analyser for OCaml to C interfaces is useful outside the OCaml 5 porting effort too.   ","meta":{"url":"http://arxiv.org/abs/2307.14909v1"},"cats":{"new-dataset":0.0984009497,"dev-research":0.4520116782,"prompt-eng":0.3826147724,"data-quality":0.1258795611,"ml-security":0.0896652633}}
{"text":"After some motivating examples of real bugs in C bindings a static analyser is presented that finds these known classes of bugs.","meta":{"url":"http://arxiv.org/abs/2307.14909v1"},"cats":{"new-dataset":0.2295387339,"dev-research":0.4836811031,"prompt-eng":0.4522487718,"data-quality":0.3915469574,"ml-security":0.2520254803}}
{"text":"The tool works on the OCaml abstract parse and typed trees, and generates a header file and a caller model.","meta":{"url":"http://arxiv.org/abs/2307.14909v1"},"cats":{"new-dataset":0.3032926504,"dev-research":0.3569429765,"prompt-eng":0.4574477032,"data-quality":0.1770433777,"ml-security":0.0807782739}}
{"text":"Together with a simplified model of the OCaml runtime this is used as input to a static analysis framework, Goblint.","meta":{"url":"http://arxiv.org/abs/2307.14909v1"},"cats":{"new-dataset":0.2363733868,"dev-research":0.377424188,"prompt-eng":0.4141945124,"data-quality":0.1457358793,"ml-security":0.116607416}}
{"text":"An analysis is developed that tracks dereferences of OCaml values, and together with the existing framework reports incorrect dereferences.","meta":{"url":"http://arxiv.org/abs/2307.14909v1"},"cats":{"new-dataset":0.1650973633,"dev-research":0.4494301527,"prompt-eng":0.4003856484,"data-quality":0.3591191489,"ml-security":0.1825871569}}
{"text":"An example is shown how to extend the analysis to cover more safety properties.   ","meta":{"url":"http://arxiv.org/abs/2307.14909v1"},"cats":{"new-dataset":0.1292142825,"dev-research":0.3816397007,"prompt-eng":0.4022955072,"data-quality":0.216003852,"ml-security":0.3915006926}}
{"text":"The tools and runtime models are generic and could be reused with other static analysis tools.","meta":{"url":"http://arxiv.org/abs/2307.14909v1"},"cats":{"new-dataset":0.1034169777,"dev-research":0.380849841,"prompt-eng":0.3908651967,"data-quality":0.1239476725,"ml-security":0.1450815969}}
{"text":"This work introduces TRON, a scalable session-based Transformer Recommender using Optimized Negative-sampling.","meta":{"url":"http://arxiv.org/abs/2307.14906v1"},"cats":{"new-dataset":0.0762210194,"dev-research":0.2562043135,"prompt-eng":0.4583889973,"data-quality":0.1226342134,"ml-security":0.1189671681}}
{"text":"Motivated by the scalability and performance limitations of prevailing models such as SASRec and GRU4Rec+, TRON integrates top-k negative sampling and listwise loss functions to enhance its recommendation accuracy.","meta":{"url":"http://arxiv.org/abs/2307.14906v1"},"cats":{"new-dataset":0.0419689897,"dev-research":0.2125011402,"prompt-eng":0.3938534782,"data-quality":0.2037916549,"ml-security":0.1308863555}}
{"text":"Evaluations on relevant large-scale e-commerce datasets show that TRON improves upon the recommendation quality of current methods while maintaining training speeds similar to SASRec.","meta":{"url":"http://arxiv.org/abs/2307.14906v1"},"cats":{"new-dataset":0.108574028,"dev-research":0.2921551915,"prompt-eng":0.3916370668,"data-quality":0.1862361507,"ml-security":0.1259536822}}
{"text":"A live A/B test yielded an 18.14% increase in click-through rate over SASRec, highlighting the potential of TRON in practical settings.","meta":{"url":"http://arxiv.org/abs/2307.14906v1"},"cats":{"new-dataset":0.0365917717,"dev-research":0.3001869618,"prompt-eng":0.4718385614,"data-quality":0.1154629187,"ml-security":0.1286138924}}
{"text":"For further research, we provide access to our source code at https://github.com/otto-de/TRON and an anonymized dataset at https://github.com/otto-de/recsys-dataset.","meta":{"url":"http://arxiv.org/abs/2307.14906v1"},"cats":{"new-dataset":0.8056754629,"dev-research":0.2457638411,"prompt-eng":0.395514026,"data-quality":0.1781811435,"ml-security":0.1251638199}}
{"text":"Representing source code in a generic input format is crucial to automate software engineering tasks, e.g., applying machine learning algorithms to extract information.","meta":{"url":"http://arxiv.org/abs/2307.14902v1"},"cats":{"new-dataset":0.0593837313,"dev-research":0.4823182633,"prompt-eng":0.4191841381,"data-quality":0.3327562856,"ml-security":0.242387376}}
{"text":"Visualizing code representations can further enable human experts to gain an intuitive insight into the code.","meta":{"url":"http://arxiv.org/abs/2307.14902v1"},"cats":{"new-dataset":0.0332970093,"dev-research":0.6033567724,"prompt-eng":0.417083536,"data-quality":0.1910917138,"ml-security":0.1623583191}}
{"text":"Unfortunately, as of today, there is no universal tool that can simultaneously visualise different types of code representations.","meta":{"url":"http://arxiv.org/abs/2307.14902v1"},"cats":{"new-dataset":0.1338531626,"dev-research":0.4268429388,"prompt-eng":0.4297409433,"data-quality":0.1745096914,"ml-security":0.0790775274}}
{"text":"In this paper, we introduce a tool, CodeLens, which provides a visual interaction environment that supports various representation methods and helps developers understand and explore them.","meta":{"url":"http://arxiv.org/abs/2307.14902v1"},"cats":{"new-dataset":0.1842291001,"dev-research":0.6024514226,"prompt-eng":0.4386291426,"data-quality":0.2063520981,"ml-security":0.1229042023}}
{"text":"CodeLens is designed to support multiple programming languages, such as Java, Python, and JavaScript, and four types of code representations, including sequence of tokens, abstract syntax tree (AST), data flow graph (DFG), and control flow graph (CFG).","meta":{"url":"http://arxiv.org/abs/2307.14902v1"},"cats":{"new-dataset":0.2236267564,"dev-research":0.4749351533,"prompt-eng":0.3849121039,"data-quality":0.1856204361,"ml-security":0.1117098276}}
{"text":"By using CodeLens, developers can quickly visualize the specific code representation and also obtain the represented inputs for models of code.","meta":{"url":"http://arxiv.org/abs/2307.14902v1"},"cats":{"new-dataset":0.0906302509,"dev-research":0.5812368177,"prompt-eng":0.4412243707,"data-quality":0.1725438395,"ml-security":0.1422799147}}
{"text":"The Web-based interface of CodeLens is available at http://www.codelens.org.","meta":{"url":"http://arxiv.org/abs/2307.14902v1"},"cats":{"new-dataset":0.3135818162,"dev-research":0.4022497462,"prompt-eng":0.4244837921,"data-quality":0.1885959902,"ml-security":0.0795926041}}
{"text":"The demonstration video can be found at http://www.codelens.org/demo.","meta":{"url":"http://arxiv.org/abs/2307.14902v1"},"cats":{"new-dataset":0.1932536595,"dev-research":0.4122886313,"prompt-eng":0.4495977655,"data-quality":0.2149949336,"ml-security":0.1364141177}}
{"text":"The recent surge of foundation models in computer vision and natural language processing opens up perspectives in utilizing multi-modal clinical data to train large models with strong generalizability.","meta":{"url":"http://arxiv.org/abs/2307.14901v1"},"cats":{"new-dataset":0.1354955741,"dev-research":0.1993168788,"prompt-eng":0.4151512095,"data-quality":0.1932717927,"ml-security":0.1386548295}}
{"text":"Yet pathological image datasets often lack biomedical text annotation and enrichment.","meta":{"url":"http://arxiv.org/abs/2307.14901v1"},"cats":{"new-dataset":0.3520380928,"dev-research":0.2223210816,"prompt-eng":0.33368191,"data-quality":0.4200811013,"ml-security":0.1602022821}}
{"text":"Guiding data-efficient image diagnosis from the use of biomedical text knowledge becomes a substantial interest.","meta":{"url":"http://arxiv.org/abs/2307.14901v1"},"cats":{"new-dataset":0.0884041059,"dev-research":0.2496610061,"prompt-eng":0.3890091126,"data-quality":0.286550543,"ml-security":0.0876976555}}
{"text":"In this paper, we propose to Connect Image and Text Embeddings (CITE) to enhance pathological image classification.","meta":{"url":"http://arxiv.org/abs/2307.14901v1"},"cats":{"new-dataset":0.2395862648,"dev-research":0.2173348408,"prompt-eng":0.3647534581,"data-quality":0.3626523317,"ml-security":0.1452626283}}
{"text":"CITE injects text insights gained from language models pre-trained with a broad range of biomedical texts, leading to adapt foundation models towards pathological image understanding.","meta":{"url":"http://arxiv.org/abs/2307.14901v1"},"cats":{"new-dataset":0.165445153,"dev-research":0.2429963244,"prompt-eng":0.3393380846,"data-quality":0.2974010751,"ml-security":0.1087000283}}
{"text":"Through extensive experiments on the PatchGastric stomach tumor pathological image dataset, we demonstrate that CITE achieves leading performance compared with various baselines especially when training data is scarce.","meta":{"url":"http://arxiv.org/abs/2307.14901v1"},"cats":{"new-dataset":0.5489594906,"dev-research":0.2188971855,"prompt-eng":0.3413270953,"data-quality":0.2173235533,"ml-security":0.1398450379}}
{"text":"CITE offers insights into leveraging in-domain text knowledge to reinforce data-efficient pathological image classification.","meta":{"url":"http://arxiv.org/abs/2307.14901v1"},"cats":{"new-dataset":0.1853738798,"dev-research":0.2581717568,"prompt-eng":0.3519563945,"data-quality":0.296553086,"ml-security":0.1236444558}}
{"text":"Code is available at https://github.com/Yunkun-Zhang/CITE.","meta":{"url":"http://arxiv.org/abs/2307.14901v1"},"cats":{"new-dataset":0.3371151591,"dev-research":0.2669076108,"prompt-eng":0.4305579542,"data-quality":0.1493589122,"ml-security":0.0670928029}}
{"text":"This paper addresses the problem of selecting of a set of texts for annotation in text classification using retrieval methods when there are limits on the number of annotations due to constraints on human resources.","meta":{"url":"http://arxiv.org/abs/2307.14899v1"},"cats":{"new-dataset":0.1894180774,"dev-research":0.2789704749,"prompt-eng":0.3867718797,"data-quality":0.493197611,"ml-security":0.1249205093}}
{"text":"An additional challenge addressed is dealing with binary categories that have a small number of positive instances, reflecting severe class imbalance.","meta":{"url":"http://arxiv.org/abs/2307.14899v1"},"cats":{"new-dataset":0.0537738361,"dev-research":0.268014088,"prompt-eng":0.4488815371,"data-quality":0.4119860175,"ml-security":0.2729142746}}
{"text":"In our situation, where annotation occurs over a long time period, the selection of texts to be annotated can be made in batches, with previous annotations guiding the choice of the next set.","meta":{"url":"http://arxiv.org/abs/2307.14899v1"},"cats":{"new-dataset":0.3914788152,"dev-research":0.2943911126,"prompt-eng":0.4299294154,"data-quality":0.4038300621,"ml-security":0.0779275753}}
{"text":"To address these challenges, the paper proposes leveraging SHAP to construct a quality set of queries for Elasticsearch and semantic search, to try to identify optimal sets of texts for annotation that will help with class imbalance.","meta":{"url":"http://arxiv.org/abs/2307.14899v1"},"cats":{"new-dataset":0.1051042508,"dev-research":0.2348384509,"prompt-eng":0.4113261493,"data-quality":0.3699142903,"ml-security":0.1766776701}}
{"text":"The approach is tested on sets of cue texts describing possible future events, constructed by participants involved in studies aimed to help with the management of obesity and diabetes.","meta":{"url":"http://arxiv.org/abs/2307.14899v1"},"cats":{"new-dataset":0.067572936,"dev-research":0.3292083416,"prompt-eng":0.378355189,"data-quality":0.1698997383,"ml-security":0.0796071464}}
{"text":"We introduce an effective method for selecting a small set of texts for annotation and building high-quality classifiers.","meta":{"url":"http://arxiv.org/abs/2307.14899v1"},"cats":{"new-dataset":0.2712951265,"dev-research":0.3072207326,"prompt-eng":0.4278918933,"data-quality":0.5222436694,"ml-security":0.1093437652}}
{"text":"We integrate vector search, semantic search, and machine learning classifiers to yield a good solution.","meta":{"url":"http://arxiv.org/abs/2307.14899v1"},"cats":{"new-dataset":0.0577678582,"dev-research":0.2826720043,"prompt-eng":0.4144267903,"data-quality":0.2833451577,"ml-security":0.1575406646}}
{"text":"Our experiments demonstrate improved F1 scores for the minority classes in binary classification.","meta":{"url":"http://arxiv.org/abs/2307.14899v1"},"cats":{"new-dataset":0.1041328738,"dev-research":0.2175037757,"prompt-eng":0.43295122,"data-quality":0.39114766,"ml-security":0.2214552466}}
{"text":"Self-supervised learning is popular method because of its ability to learn features in images without using its labels and is able to overcome limited labeled datasets used in supervised learning.","meta":{"url":"http://arxiv.org/abs/2307.14897v1"},"cats":{"new-dataset":0.0231667247,"dev-research":0.2572795478,"prompt-eng":0.3643798781,"data-quality":0.3642889068,"ml-security":0.2284153891}}
{"text":"Self-supervised learning works by using a pretext task which will be trained on the model before being applied to a specific task.","meta":{"url":"http://arxiv.org/abs/2307.14897v1"},"cats":{"new-dataset":0.0067474045,"dev-research":0.2431939603,"prompt-eng":0.4347709998,"data-quality":0.2990683614,"ml-security":0.1764362079}}
{"text":"There are some examples of pretext tasks used in self-supervised learning in the field of image recognition, namely rotation prediction, solving jigsaw puzzles, and predicting relative positions on image.","meta":{"url":"http://arxiv.org/abs/2307.14897v1"},"cats":{"new-dataset":0.0411421885,"dev-research":0.256756266,"prompt-eng":0.4033681562,"data-quality":0.2012197129,"ml-security":0.1199222755}}
{"text":"Previous studies have only used one type of transformation as a pretext task.","meta":{"url":"http://arxiv.org/abs/2307.14897v1"},"cats":{"new-dataset":0.0067608644,"dev-research":0.1853085517,"prompt-eng":0.3008654344,"data-quality":0.0734261774,"ml-security":0.0674673238}}
{"text":"This raises the question of how it affects if more than one pretext task is used and to use a gating network to combine all pretext tasks.","meta":{"url":"http://arxiv.org/abs/2307.14897v1"},"cats":{"new-dataset":0.0047075821,"dev-research":0.2904635376,"prompt-eng":0.4096398226,"data-quality":0.182039085,"ml-security":0.112859544}}
{"text":"Therefore, we propose the Gated Self-Supervised Learning method to improve image classification which use more than one transformation as pretext task and uses the Mixture of Expert architecture as a gating network in combining each pretext task so that the model automatically can study and focus more on the most useful augmentations for classification.","meta":{"url":"http://arxiv.org/abs/2307.14897v1"},"cats":{"new-dataset":0.0977244062,"dev-research":0.2169854617,"prompt-eng":0.4274086967,"data-quality":0.2912354227,"ml-security":0.1481221627}}
{"text":"We test performance of the proposed method in several scenarios, namely CIFAR imbalance dataset classification, adversarial perturbations, Tiny-Imagenet dataset classification, and semi-supervised learning.","meta":{"url":"http://arxiv.org/abs/2307.14897v1"},"cats":{"new-dataset":0.0881304581,"dev-research":0.1874980328,"prompt-eng":0.3705081526,"data-quality":0.4142404992,"ml-security":0.4797206166}}
{"text":"Moreover, there are Grad-CAM and T-SNE analysis that are used to see the proposed method for identifying important features that influence image classification and representing data for each class and separating different classes properly.","meta":{"url":"http://arxiv.org/abs/2307.14897v1"},"cats":{"new-dataset":0.119774411,"dev-research":0.2974599734,"prompt-eng":0.3805867029,"data-quality":0.2652228125,"ml-security":0.1529558037}}
{"text":"Our code is in https://github.com/aristorenaldo/G-SSL","meta":{"url":"http://arxiv.org/abs/2307.14897v1"},"cats":{"new-dataset":0.3347653553,"dev-research":0.268828514,"prompt-eng":0.4106744917,"data-quality":0.1559062238,"ml-security":0.1227270965}}
{"text":"We present a novel semantics for the language of multi-agent only believing exploiting belief bases, and show how to use it for automatically checking formulas of this language and of its dynamic extension with private belief expansion operators.","meta":{"url":"http://arxiv.org/abs/2307.14893v1"},"cats":{"new-dataset":0.0468146547,"dev-research":0.2928469658,"prompt-eng":0.4515659741,"data-quality":0.1633498854,"ml-security":0.222263276}}
{"text":"We provide a PSPACE algorithm for model checking relying on a reduction to QBF and alternative dedicated algorithm relying on the exploration of the state space.","meta":{"url":"http://arxiv.org/abs/2307.14893v1"},"cats":{"new-dataset":0.0624187317,"dev-research":0.2486152351,"prompt-eng":0.4953065715,"data-quality":0.13056662,"ml-security":0.1454239155}}
{"text":"We present an implementation of the QBF-based algorithm and some experimental results on computation time in a concrete example.","meta":{"url":"http://arxiv.org/abs/2307.14893v1"},"cats":{"new-dataset":0.1161686699,"dev-research":0.3238581773,"prompt-eng":0.4170809663,"data-quality":0.1003519199,"ml-security":0.0522194173}}
{"text":"Accurate 3D human pose estimation (3D HPE) is crucial for enabling autonomous vehicles (AVs) to make informed decisions and respond proactively in critical road scenarios.","meta":{"url":"http://arxiv.org/abs/2307.14889v1"},"cats":{"new-dataset":0.0813297448,"dev-research":0.2774110751,"prompt-eng":0.4018594839,"data-quality":0.1088129086,"ml-security":0.1313856602}}
{"text":"Promising results of 3D HPE have been gained in several domains such as human-computer interaction, robotics, sports and medical analytics, often based on data collected in well-controlled laboratory environments.","meta":{"url":"http://arxiv.org/abs/2307.14889v1"},"cats":{"new-dataset":0.2342661953,"dev-research":0.2730751782,"prompt-eng":0.3926274095,"data-quality":0.0685555645,"ml-security":0.0913776644}}
{"text":"Nevertheless, the transfer of 3D HPE methods to AVs has received limited research attention, due to the challenges posed by obtaining accurate 3D pose annotations and the limited suitability of data from other domains.   ","meta":{"url":"http://arxiv.org/abs/2307.14889v1"},"cats":{"new-dataset":0.1181160579,"dev-research":0.2416464124,"prompt-eng":0.3793022688,"data-quality":0.1347788445,"ml-security":0.0831682562}}
{"text":"We present a simple yet efficient weakly supervised approach for 3D HPE in the AV context by employing a high-level sensor fusion between camera and LiDAR data.","meta":{"url":"http://arxiv.org/abs/2307.14889v1"},"cats":{"new-dataset":0.1500871523,"dev-research":0.2291501901,"prompt-eng":0.424261771,"data-quality":0.2166635705,"ml-security":0.1149314396}}
{"text":"The weakly supervised setting enables training on the target datasets without any 2D/3D keypoint labels by using an off-the-shelf 2D joint extractor and pseudo labels generated from LiDAR to image projections.","meta":{"url":"http://arxiv.org/abs/2307.14889v1"},"cats":{"new-dataset":0.154987484,"dev-research":0.2141439684,"prompt-eng":0.3868887361,"data-quality":0.3145733916,"ml-security":0.1591513688}}
{"text":"Our approach outperforms state-of-the-art results by up to $\\sim$ 13% on the Waymo Open Dataset in the weakly supervised setting and achieves state-of-the-art results in the supervised setting.","meta":{"url":"http://arxiv.org/abs/2307.14889v1"},"cats":{"new-dataset":0.4762895186,"dev-research":0.2214161096,"prompt-eng":0.4359994401,"data-quality":0.4055682572,"ml-security":0.1416470421}}
{"text":"This paper builds a novel bridge between algebraic coding theory and mathematical knot theory, with applications in both directions.","meta":{"url":"http://arxiv.org/abs/2307.14882v1"},"cats":{"new-dataset":0.0786087039,"dev-research":0.293277742,"prompt-eng":0.3655381785,"data-quality":0.1680297516,"ml-security":0.0862709047}}
{"text":"We give methods to construct error-correcting codes starting from the colorings of a knot, describing through a series of results how the properties of the knot translate into code parameters.","meta":{"url":"http://arxiv.org/abs/2307.14882v1"},"cats":{"new-dataset":0.1208843249,"dev-research":0.448001104,"prompt-eng":0.4576159365,"data-quality":0.4611433292,"ml-security":0.1088989074}}
{"text":"We show that knots can be used to obtain error-correcting codes with prescribed parameters and an efficient decoding algorithm.","meta":{"url":"http://arxiv.org/abs/2307.14882v1"},"cats":{"new-dataset":0.1100755976,"dev-research":0.3691110148,"prompt-eng":0.436263406,"data-quality":0.3824491108,"ml-security":0.0925215445}}
{"text":"Satellite Internet plays an increasingly important role in geopolitical conflicts.","meta":{"url":"http://arxiv.org/abs/2307.14879v1"},"cats":{"new-dataset":0.0585382677,"dev-research":0.2502860207,"prompt-eng":0.2743560733,"data-quality":0.0816202001,"ml-security":0.2046178125}}
{"text":"This notion was affirmed in the Ukrainian conflict escalating at the beginning of 2022, with the large-scale deployment of the Starlink satellite Internet service which consequently demonstrated the strategic importance of a free flow of information.","meta":{"url":"http://arxiv.org/abs/2307.14879v1"},"cats":{"new-dataset":0.103828698,"dev-research":0.2476735152,"prompt-eng":0.3341504442,"data-quality":0.115170968,"ml-security":0.1814861571}}
{"text":"Aside from military use, many citizens publish sensitive information on social media platforms to influence the public narrative.","meta":{"url":"http://arxiv.org/abs/2307.14879v1"},"cats":{"new-dataset":0.0552089451,"dev-research":0.2294240232,"prompt-eng":0.3269478163,"data-quality":0.157277271,"ml-security":0.3402706204}}
{"text":"However, the use of satellite communication has proven to be dangerous, as the signals can be monitored by other satellites and used to triangulate the source on the ground.","meta":{"url":"http://arxiv.org/abs/2307.14879v1"},"cats":{"new-dataset":0.0484437468,"dev-research":0.3259162176,"prompt-eng":0.3001780803,"data-quality":0.1418783476,"ml-security":0.3522062165}}
{"text":"Unfortunately, the targeted killings of journalists have shown this threat to be effective.","meta":{"url":"http://arxiv.org/abs/2307.14879v1"},"cats":{"new-dataset":0.0302183746,"dev-research":0.2146369741,"prompt-eng":0.3152835824,"data-quality":0.1897154953,"ml-security":0.4154425465}}
{"text":"While the increasing deployment of satellite Internet systems gives citizens an unprecedented mouthpiece in conflicts, protecting them against localization is an unaddressed problem.   ","meta":{"url":"http://arxiv.org/abs/2307.14879v1"},"cats":{"new-dataset":0.0996586441,"dev-research":0.3300506288,"prompt-eng":0.3314137171,"data-quality":0.2132654707,"ml-security":0.4391033632}}
{"text":"To address this threat, we present AnonSat, a novel scheme to protect satellite Internet users from triangulation.","meta":{"url":"http://arxiv.org/abs/2307.14879v1"},"cats":{"new-dataset":0.2696285468,"dev-research":0.3214987572,"prompt-eng":0.4056092042,"data-quality":0.1290160944,"ml-security":0.4489967594}}
{"text":"AnonSat works with cheap off-the-shelf devices, leveraging long-range wireless communication to span a local network among satellite base stations.","meta":{"url":"http://arxiv.org/abs/2307.14879v1"},"cats":{"new-dataset":0.2433517956,"dev-research":0.2519647163,"prompt-eng":0.3735151743,"data-quality":0.1188794517,"ml-security":0.0701132943}}
{"text":"This allows rerouting users' communication to other satellite base stations, some distance away from each user, thus, preventing their localization.","meta":{"url":"http://arxiv.org/abs/2307.14879v1"},"cats":{"new-dataset":0.015638682,"dev-research":0.3542096005,"prompt-eng":0.3683111512,"data-quality":0.1266160597,"ml-security":0.1501644284}}
{"text":"AnonSat is designed for easy deployment and usability, which we demonstrate with a prototype implementation.","meta":{"url":"http://arxiv.org/abs/2307.14879v1"},"cats":{"new-dataset":0.2430689778,"dev-research":0.4289050978,"prompt-eng":0.5098928327,"data-quality":0.1389220447,"ml-security":0.0953476754}}
{"text":"Our large-scale network simulations using real-world data sets show the effectiveness of AnonSat in various practical settings.","meta":{"url":"http://arxiv.org/abs/2307.14879v1"},"cats":{"new-dataset":0.2724218366,"dev-research":0.2403819355,"prompt-eng":0.3609229976,"data-quality":0.1531498251,"ml-security":0.1952742891}}
{"text":"The Entity Set Expansion (ESE) task aims to expand a handful of seed entities with new entities belonging to the same semantic class.","meta":{"url":"http://arxiv.org/abs/2307.14878v1"},"cats":{"new-dataset":0.0868440027,"dev-research":0.2933832413,"prompt-eng":0.4167116735,"data-quality":0.2356616075,"ml-security":0.0823594369}}
{"text":"Conventional ESE methods are based on mono-modality (i.e., literal modality), which struggle to deal with complex entities in the real world such as: (1) Negative entities with fine-grained semantic differences.","meta":{"url":"http://arxiv.org/abs/2307.14878v1"},"cats":{"new-dataset":0.0080841136,"dev-research":0.3380350874,"prompt-eng":0.3643896508,"data-quality":0.2678122963,"ml-security":0.1280462776}}
{"text":"(2) Synonymous entities.","meta":{"url":"http://arxiv.org/abs/2307.14878v1"},"cats":{"new-dataset":0.0872600169,"dev-research":0.2678586169,"prompt-eng":0.3545605599,"data-quality":0.2001008449,"ml-security":0.0473651459}}
{"text":"(3) Polysemous entities.","meta":{"url":"http://arxiv.org/abs/2307.14878v1"},"cats":{"new-dataset":0.1345127256,"dev-research":0.2471297869,"prompt-eng":0.4081299682,"data-quality":0.1944393178,"ml-security":0.1046326597}}
{"text":"(4) Long-tailed entities.","meta":{"url":"http://arxiv.org/abs/2307.14878v1"},"cats":{"new-dataset":0.2607448914,"dev-research":0.1451581969,"prompt-eng":0.3967877068,"data-quality":0.1270804881,"ml-security":0.0892387789}}
{"text":"These challenges prompt us to propose Multi-modal Entity Set Expansion (MESE), where models integrate information from multiple modalities to represent entities.","meta":{"url":"http://arxiv.org/abs/2307.14878v1"},"cats":{"new-dataset":0.116462989,"dev-research":0.2160867837,"prompt-eng":0.4599821954,"data-quality":0.1874171419,"ml-security":0.0638446246}}
{"text":"Intuitively, the benefits of multi-modal information for ESE are threefold: (1) Different modalities can provide complementary information.","meta":{"url":"http://arxiv.org/abs/2307.14878v1"},"cats":{"new-dataset":0.0055458559,"dev-research":0.2697107598,"prompt-eng":0.3733223927,"data-quality":0.1146035265,"ml-security":0.0951443626}}
{"text":"(2) Multi-modal information provides a unified signal via common visual properties for the same semantic class or entity.","meta":{"url":"http://arxiv.org/abs/2307.14878v1"},"cats":{"new-dataset":0.0491046673,"dev-research":0.2802400546,"prompt-eng":0.4452140406,"data-quality":0.2099609287,"ml-security":0.06619991}}
{"text":"(3) Multi-modal information offers robust alignment signal for synonymous entities.","meta":{"url":"http://arxiv.org/abs/2307.14878v1"},"cats":{"new-dataset":0.0645127625,"dev-research":0.2681707398,"prompt-eng":0.4426268432,"data-quality":0.3034956024,"ml-security":0.0542591795}}
{"text":"To assess the performance of model in MESE and facilitate further research, we constructed the MESED dataset which is the first multi-modal dataset for ESE with large-scale and elaborate manual calibration.","meta":{"url":"http://arxiv.org/abs/2307.14878v1"},"cats":{"new-dataset":0.5010040213,"dev-research":0.2172443275,"prompt-eng":0.438624168,"data-quality":0.1458607251,"ml-security":0.0659513101}}
{"text":"A powerful multi-modal model MultiExpan is proposed which is pre-trained on four multimodal pre-training tasks.","meta":{"url":"http://arxiv.org/abs/2307.14878v1"},"cats":{"new-dataset":0.1078192051,"dev-research":0.2083648713,"prompt-eng":0.46490631,"data-quality":0.1598299132,"ml-security":0.0698061698}}
{"text":"The extensive experiments and analyses on MESED demonstrate the high quality of the dataset and the effectiveness of our MultiExpan, as well as pointing the direction for future research.","meta":{"url":"http://arxiv.org/abs/2307.14878v1"},"cats":{"new-dataset":0.557435527,"dev-research":0.2488737025,"prompt-eng":0.3854313265,"data-quality":0.2403689189,"ml-security":0.0645258669}}
{"text":"Rescheduling problems arise in a variety of situations where a previously planned schedule needs to be adjusted to deal with unforeseen events.","meta":{"url":"http://arxiv.org/abs/2307.14876v1"},"cats":{"new-dataset":0.062138898,"dev-research":0.3489934497,"prompt-eng":0.3673358787,"data-quality":0.1963490849,"ml-security":0.1560633421}}
{"text":"A common problem is the arrival of new orders, i.e. jobs, which have to be integrated into the schedule of the so-called old jobs.","meta":{"url":"http://arxiv.org/abs/2307.14876v1"},"cats":{"new-dataset":0.0517092998,"dev-research":0.2873049611,"prompt-eng":0.3662771286,"data-quality":0.1942439919,"ml-security":0.1333617613}}
{"text":"The maximum and total absolute time deviations of the completion times of these jobs are modeled as a disruption constraint to limit the change in the original schedule.","meta":{"url":"http://arxiv.org/abs/2307.14876v1"},"cats":{"new-dataset":0.0513022158,"dev-research":0.22195914,"prompt-eng":0.3988152956,"data-quality":0.117293929,"ml-security":0.1508435812}}
{"text":"Disruption constraints affect the shape of an optimal schedule, particularly with respect to the sequencing of old jobs and the insertion of idle time.","meta":{"url":"http://arxiv.org/abs/2307.14876v1"},"cats":{"new-dataset":0.0193640312,"dev-research":0.2623861386,"prompt-eng":0.4006199454,"data-quality":0.1046626833,"ml-security":0.1482320404}}
{"text":"We therefore give a classification into idle and no-idle problems for a set of single-machine rescheduling problems with different objective functions.","meta":{"url":"http://arxiv.org/abs/2307.14876v1"},"cats":{"new-dataset":0.1308619033,"dev-research":0.294720337,"prompt-eng":0.4262428305,"data-quality":0.229615334,"ml-security":0.1935907617}}
{"text":"We then prove the complexity of five rescheduling problems that have been left open in the literature.","meta":{"url":"http://arxiv.org/abs/2307.14876v1"},"cats":{"new-dataset":0.1845199947,"dev-research":0.2653032471,"prompt-eng":0.3510782458,"data-quality":0.1617037764,"ml-security":0.1064026615}}
{"text":"We study the decay of correlation between locally constrained independent random variables in the local lemma regimes.","meta":{"url":"http://arxiv.org/abs/2307.14872v1"},"cats":{"new-dataset":0.1118535962,"dev-research":0.1729328535,"prompt-eng":0.3417004541,"data-quality":0.1998707829,"ml-security":0.1700889538}}
{"text":"the distribution defined by constraint satisfaction problems (CSPs) in the local lemma regime.","meta":{"url":"http://arxiv.org/abs/2307.14872v1"},"cats":{"new-dataset":0.0644767558,"dev-research":0.1837120806,"prompt-eng":0.3884407505,"data-quality":0.2242110695,"ml-security":0.1187824569}}
{"text":"For atomically constrained independent random variables of sufficiently large domains, we show that a decay of correlation property holds up to the local lemma condition $pD^{2+o(1)}\\lesssim 1$, asymptotically matching the sampling threshold for constraint satisfaction solutions","meta":{"url":"http://arxiv.org/abs/2307.14872v1"},"cats":{"new-dataset":0.0545374611,"dev-research":0.1695840242,"prompt-eng":0.3722543256,"data-quality":0.1848622062,"ml-security":0.1478555798}}
{"text":"[BGG+19,GGW22].","meta":{"url":"http://arxiv.org/abs/2307.14872v1"},"cats":{"new-dataset":0.1302384793,"dev-research":0.1870953904,"prompt-eng":0.4296734926,"data-quality":0.0899172983,"ml-security":0.0725922638}}
{"text":"This provides evidence for the conjectured $pD^2\\lesssim 1$ threshold for the \"sampling Lov\\'{a}sz local lemma\".   ","meta":{"url":"http://arxiv.org/abs/2307.14872v1"},"cats":{"new-dataset":0.1533659321,"dev-research":0.1419990969,"prompt-eng":0.3775005558,"data-quality":0.2481879462,"ml-security":0.1290518173}}
{"text":"We use a recursively-constructed coupling to bound the correlation decay.","meta":{"url":"http://arxiv.org/abs/2307.14872v1"},"cats":{"new-dataset":0.0732542901,"dev-research":0.2061967809,"prompt-eng":0.425840155,"data-quality":0.1428765336,"ml-security":0.1077713078}}
{"text":"Our approach completely dispenses with the \"freezing\" paradigm originated from Beck [Bec91], which was commonly used to deal with the non-self-reducibility of the local lemma regimes, and hence can bypass the current technical barriers due to the use of $\\{2,3\\}$-trees.","meta":{"url":"http://arxiv.org/abs/2307.14872v1"},"cats":{"new-dataset":0.1597162529,"dev-research":0.2195010983,"prompt-eng":0.3837936305,"data-quality":0.1940439415,"ml-security":0.1816084441}}
{"text":"This paper elaborates on Conditional Handover (CHO) modelling, aimed at maximizing the use of contention free random access (CFRA) during mobility.","meta":{"url":"http://arxiv.org/abs/2307.14870v1"},"cats":{"new-dataset":0.0450912868,"dev-research":0.2459983542,"prompt-eng":0.407747177,"data-quality":0.0583588761,"ml-security":0.2029002189}}
{"text":"This is a desirable behavior as CFRA increases the chance of fast and successful handover.","meta":{"url":"http://arxiv.org/abs/2307.14870v1"},"cats":{"new-dataset":0.0109134485,"dev-research":0.2685939671,"prompt-eng":0.4031763367,"data-quality":0.0745442775,"ml-security":0.1015315108}}
{"text":"In CHO this may be especially challenging as the time between the preparation and the actual cell change can be significantly longer in comparison to non-conditional handover.","meta":{"url":"http://arxiv.org/abs/2307.14870v1"},"cats":{"new-dataset":0.0274369379,"dev-research":0.3077757758,"prompt-eng":0.4338103047,"data-quality":0.0757228941,"ml-security":0.0563046817}}
{"text":"Thus, new means to mitigate this issue need to be defined.","meta":{"url":"http://arxiv.org/abs/2307.14870v1"},"cats":{"new-dataset":0.0595080094,"dev-research":0.3365684023,"prompt-eng":0.3705581158,"data-quality":0.2976411436,"ml-security":0.1461204575}}
{"text":"We present the scheme where beam-specific measurement reporting can lead to CFRA resource updating prior to CHO execution.","meta":{"url":"http://arxiv.org/abs/2307.14870v1"},"cats":{"new-dataset":0.1815743164,"dev-research":0.321748381,"prompt-eng":0.5028183766,"data-quality":0.1655066745,"ml-security":0.0889805333}}
{"text":"We have run system level simulations to confirm that the proposed solution increases the ratio of CFRA attempts during CHO.","meta":{"url":"http://arxiv.org/abs/2307.14870v1"},"cats":{"new-dataset":0.0282780178,"dev-research":0.2681463559,"prompt-eng":0.4626534177,"data-quality":0.0811974801,"ml-security":0.1389735239}}
{"text":"In the best-case scenario, we observe a gain exceeding 13%.","meta":{"url":"http://arxiv.org/abs/2307.14870v1"},"cats":{"new-dataset":0.0278518622,"dev-research":0.2218235121,"prompt-eng":0.3832340936,"data-quality":0.1970064798,"ml-security":0.2031647481}}
{"text":"We also show how the average delay of completing the handover is reduced.","meta":{"url":"http://arxiv.org/abs/2307.14870v1"},"cats":{"new-dataset":0.0352691685,"dev-research":0.2161209944,"prompt-eng":0.4246664538,"data-quality":0.1130108482,"ml-security":0.0699662707}}
{"text":"To provide the entire perspective, we present at what expense these gains can be achieved by analyzing the increased signaling overhead for updating the random access resources.","meta":{"url":"http://arxiv.org/abs/2307.14870v1"},"cats":{"new-dataset":0.0594219191,"dev-research":0.2463676259,"prompt-eng":0.4388135698,"data-quality":0.1308758646,"ml-security":0.3203561857}}
{"text":"The study has been conducted for various network settings and considering higher frequency ranges at which the user communicates with the network.","meta":{"url":"http://arxiv.org/abs/2307.14870v1"},"cats":{"new-dataset":0.0515281622,"dev-research":0.2708784472,"prompt-eng":0.3703020477,"data-quality":0.0827679112,"ml-security":0.0747893451}}
{"text":"Finally, we provide an outlook on future extensions of the investigated solution.","meta":{"url":"http://arxiv.org/abs/2307.14870v1"},"cats":{"new-dataset":0.1301812138,"dev-research":0.2370114513,"prompt-eng":0.4239242839,"data-quality":0.1452487476,"ml-security":0.0819561296}}
{"text":"Training an effective video action recognition model poses significant computational challenges, particularly under limited resource budgets.","meta":{"url":"http://arxiv.org/abs/2307.14866v1"},"cats":{"new-dataset":0.127717793,"dev-research":0.2059400757,"prompt-eng":0.343523624,"data-quality":0.1838288537,"ml-security":0.1524520898}}
{"text":"Current methods primarily aim to either reduce model size or utilize pre-trained models, limiting their adaptability to various backbone architectures.","meta":{"url":"http://arxiv.org/abs/2307.14866v1"},"cats":{"new-dataset":0.0046712111,"dev-research":0.289159438,"prompt-eng":0.368178773,"data-quality":0.1001063323,"ml-security":0.1940367482}}
{"text":"This paper investigates the issue of over-sampled frames, a prevalent problem in many approaches yet it has received relatively little attention.","meta":{"url":"http://arxiv.org/abs/2307.14866v1"},"cats":{"new-dataset":0.1257726369,"dev-research":0.1848426121,"prompt-eng":0.3694397789,"data-quality":0.2558726631,"ml-security":0.1147614735}}
{"text":"Despite the use of fewer frames being a potential solution, this approach often results in a substantial decline in performance.","meta":{"url":"http://arxiv.org/abs/2307.14866v1"},"cats":{"new-dataset":0.0046366068,"dev-research":0.3519214696,"prompt-eng":0.36878295,"data-quality":0.1815330962,"ml-security":0.0865615963}}
{"text":"To address this issue, we propose a novel method to restore the intermediate features for two sparsely sampled and adjacent video frames.","meta":{"url":"http://arxiv.org/abs/2307.14866v1"},"cats":{"new-dataset":0.1342227979,"dev-research":0.2222147703,"prompt-eng":0.3820488976,"data-quality":0.2920999099,"ml-security":0.0599743105}}
{"text":"This feature restoration technique brings a negligible increase in computational requirements compared to resource-intensive image encoders, such as ViT. To evaluate the effectiveness of our method, we conduct extensive experiments on four public datasets, including Kinetics-400, ActivityNet, UCF-101, and HMDB-51.","meta":{"url":"http://arxiv.org/abs/2307.14866v1"},"cats":{"new-dataset":0.1987285332,"dev-research":0.2752182525,"prompt-eng":0.3981365285,"data-quality":0.2521148858,"ml-security":0.1224245572}}
{"text":"With the integration of our method, the efficiency of three commonly used baselines has been improved by over 50%, with a mere 0.5% reduction in recognition accuracy.","meta":{"url":"http://arxiv.org/abs/2307.14866v1"},"cats":{"new-dataset":0.0605379603,"dev-research":0.3090077207,"prompt-eng":0.4476764189,"data-quality":0.3066882934,"ml-security":0.0482447482}}
{"text":"In addition, our method also surprisingly helps improve the generalization ability of the models under zero-shot settings.","meta":{"url":"http://arxiv.org/abs/2307.14866v1"},"cats":{"new-dataset":0.045076788,"dev-research":0.2020936589,"prompt-eng":0.4153218531,"data-quality":0.2460123101,"ml-security":0.1419586552}}
{"text":"Advanced image tampering techniques are increasingly challenging the trustworthiness of multimedia, leading to the development of Image Manipulation Localization (IML).","meta":{"url":"http://arxiv.org/abs/2307.14863v1"},"cats":{"new-dataset":0.0531284184,"dev-research":0.3016583755,"prompt-eng":0.4636068091,"data-quality":0.3652045855,"ml-security":0.2523423374}}
{"text":"But what makes a good IML model?","meta":{"url":"http://arxiv.org/abs/2307.14863v1"},"cats":{"new-dataset":0.0349733967,"dev-research":0.268291768,"prompt-eng":0.3746290139,"data-quality":0.0896041167,"ml-security":0.0804204683}}
{"text":"The answer lies in the way to capture artifacts.","meta":{"url":"http://arxiv.org/abs/2307.14863v1"},"cats":{"new-dataset":0.1349226796,"dev-research":0.2673639273,"prompt-eng":0.3591656333,"data-quality":0.2228625998,"ml-security":0.0931589949}}
{"text":"Exploiting artifacts requires the model to extract non-semantic discrepancies between the manipulated and authentic regions, which needs to compare differences between these two areas explicitly.","meta":{"url":"http://arxiv.org/abs/2307.14863v1"},"cats":{"new-dataset":0.0234684249,"dev-research":0.3396271829,"prompt-eng":0.4111434162,"data-quality":0.3760826406,"ml-security":0.1718243186}}
{"text":"With the self-attention mechanism, naturally, the Transformer is the best candidate.","meta":{"url":"http://arxiv.org/abs/2307.14863v1"},"cats":{"new-dataset":0.0146175437,"dev-research":0.1652743099,"prompt-eng":0.477625029,"data-quality":0.0924056394,"ml-security":0.0964768033}}
{"text":"Besides, artifacts are sensitive to image resolution, amplified under multi-scale features, and massive at the manipulation border.","meta":{"url":"http://arxiv.org/abs/2307.14863v1"},"cats":{"new-dataset":0.023505412,"dev-research":0.3178712746,"prompt-eng":0.3869195962,"data-quality":0.2971285777,"ml-security":0.106984192}}
{"text":"Therefore, we formulate the answer to the former question as building a ViT with high-resolution capacity, multi-scale feature extraction capability, and manipulation edge supervision.","meta":{"url":"http://arxiv.org/abs/2307.14863v1"},"cats":{"new-dataset":0.1629263733,"dev-research":0.2793073198,"prompt-eng":0.4320337089,"data-quality":0.172179948,"ml-security":0.0644948882}}
{"text":"We term this simple but effective ViT paradigm as the IML-ViT, which has great potential to become a new benchmark for IML.","meta":{"url":"http://arxiv.org/abs/2307.14863v1"},"cats":{"new-dataset":0.0449232026,"dev-research":0.3007517213,"prompt-eng":0.4305783192,"data-quality":0.1456170878,"ml-security":0.0896281558}}
{"text":"Extensive experiments on five benchmark datasets verified our model outperforms the state-of-the-art manipulation localization methods.","meta":{"url":"http://arxiv.org/abs/2307.14863v1"},"cats":{"new-dataset":0.228816601,"dev-research":0.296043381,"prompt-eng":0.4557463949,"data-quality":0.2475154811,"ml-security":0.0968138582}}
{"text":"Code and models are available at \\url{https://github.com/SunnyHaze/IML-ViT}","meta":{"url":"http://arxiv.org/abs/2307.14863v1"},"cats":{"new-dataset":0.4335842981,"dev-research":0.247098976,"prompt-eng":0.4206251871,"data-quality":0.1243641006,"ml-security":0.0873122127}}
{"text":"Quantum computer simulators are crucial for the development of quantum computing.","meta":{"url":"http://arxiv.org/abs/2307.14860v1"},"cats":{"new-dataset":0.0171015301,"dev-research":0.2588251513,"prompt-eng":0.3512160769,"data-quality":0.0623611173,"ml-security":0.1111733747}}
{"text":"In this work, we investigate the suitability and performance impact of GPU and multi-GPU systems on a widely used simulation tool - the state vector simulator Qiskit Aer.","meta":{"url":"http://arxiv.org/abs/2307.14860v1"},"cats":{"new-dataset":0.0609781555,"dev-research":0.2348711285,"prompt-eng":0.385666753,"data-quality":0.0732883419,"ml-security":0.133576784}}
{"text":"In particular, we evaluate the performance of both Qiskit's default Nvidia Thrust backend and the recent Nvidia cuQuantum backend on Nvidia A100 GPUs.","meta":{"url":"http://arxiv.org/abs/2307.14860v1"},"cats":{"new-dataset":0.0990766651,"dev-research":0.1976864936,"prompt-eng":0.390154507,"data-quality":0.097942836,"ml-security":0.0950941749}}
{"text":"We provide a benchmark suite of representative quantum applications for characterization.","meta":{"url":"http://arxiv.org/abs/2307.14860v1"},"cats":{"new-dataset":0.0530496326,"dev-research":0.1916051378,"prompt-eng":0.4214079053,"data-quality":0.1393771559,"ml-security":0.1066028411}}
{"text":"For simulations with a large number of qubits, the two GPU backends can provide up to 14x speedup over the CPU backend, with Nvidia cuQuantum providing further 1.5-3x speedup over the default Thrust backend.","meta":{"url":"http://arxiv.org/abs/2307.14860v1"},"cats":{"new-dataset":0.0487969185,"dev-research":0.2251971977,"prompt-eng":0.3297648871,"data-quality":0.0460429921,"ml-security":0.0929709191}}
{"text":"Our evaluation on a single GPU identifies the most important functions in Nvidia Thrust and cuQuantum for different quantum applications and their compute and memory bottlenecks.","meta":{"url":"http://arxiv.org/abs/2307.14860v1"},"cats":{"new-dataset":0.076994734,"dev-research":0.226147611,"prompt-eng":0.3518552525,"data-quality":0.0660406235,"ml-security":0.0864819623}}
{"text":"We also evaluate the gate fusion and cache-blocking optimizations on different quantum applications.","meta":{"url":"http://arxiv.org/abs/2307.14860v1"},"cats":{"new-dataset":0.0085842272,"dev-research":0.230141436,"prompt-eng":0.3770190551,"data-quality":0.0845454967,"ml-security":0.1566350582}}
{"text":"Finally, we evaluate large-number qubit quantum applications on multi-GPU and identify data movement between host and GPU as the limiting factor for the performance.","meta":{"url":"http://arxiv.org/abs/2307.14860v1"},"cats":{"new-dataset":0.0631958656,"dev-research":0.1969699318,"prompt-eng":0.3518341756,"data-quality":0.051653806,"ml-security":0.0890034905}}
{"text":"Purpose:Chest X-ray (CXR) is an essential tool and one of the most prescribed imaging to detect pulmonary abnormalities, with a yearly estimate of over 2 billion imaging performed worldwide.","meta":{"url":"http://arxiv.org/abs/2307.14859v1"},"cats":{"new-dataset":0.02374215,"dev-research":0.2762581279,"prompt-eng":0.3290907105,"data-quality":0.0929365673,"ml-security":0.0778668152}}
{"text":"However, the accurate and timely diagnosis of TB remains an unmet goal.","meta":{"url":"http://arxiv.org/abs/2307.14859v1"},"cats":{"new-dataset":0.0161831526,"dev-research":0.2495296437,"prompt-eng":0.3783612238,"data-quality":0.2701027477,"ml-security":0.1210616058}}
{"text":"The prevalence of TB is highest in low-middle-income countries, and the requirement of a portable, automated, and reliable solution is required.","meta":{"url":"http://arxiv.org/abs/2307.14859v1"},"cats":{"new-dataset":0.0706047857,"dev-research":0.2613277723,"prompt-eng":0.382837558,"data-quality":0.0911039389,"ml-security":0.08442226}}
{"text":"In this study, we compared the performance of DL-based devices on digital and analog CXR.","meta":{"url":"http://arxiv.org/abs/2307.14859v1"},"cats":{"new-dataset":0.0236503778,"dev-research":0.3944364043,"prompt-eng":0.4002766896,"data-quality":0.1260051059,"ml-security":0.088879876}}
{"text":"The evaluated DL-based device can be used in resource-constraint settings.","meta":{"url":"http://arxiv.org/abs/2307.14859v1"},"cats":{"new-dataset":0.0514096633,"dev-research":0.3614721476,"prompt-eng":0.4686100821,"data-quality":0.1268606395,"ml-security":0.0965989789}}
{"text":"Methods: A total of 10,000 CXR DICOMs(.dcm) and printed photos of the films acquired with three different cellular phones - Samsung S8, iPhone 8, and iPhone XS along with their radiological report were retrospectively collected from various sites across India from April 2020 to March 2021.","meta":{"url":"http://arxiv.org/abs/2307.14859v1"},"cats":{"new-dataset":0.5896023398,"dev-research":0.2239021562,"prompt-eng":0.3651788051,"data-quality":0.1784321097,"ml-security":0.0686684589}}
{"text":"Results: 10,000 chest X-rays were utilized to evaluate the DL-based device in identifying radiological signs of TB.","meta":{"url":"http://arxiv.org/abs/2307.14859v1"},"cats":{"new-dataset":0.0587398405,"dev-research":0.2474142605,"prompt-eng":0.403328981,"data-quality":0.141085199,"ml-security":0.0871767423}}
{"text":"The AUC of qXR for detecting signs of tuberculosis on the original DICOMs dataset was 0.928 with a sensitivity of 0.841 at a specificity of 0.806.","meta":{"url":"http://arxiv.org/abs/2307.14859v1"},"cats":{"new-dataset":0.218297617,"dev-research":0.2168732091,"prompt-eng":0.4609249226,"data-quality":0.1839481722,"ml-security":0.1037190593}}
{"text":"At an optimal threshold, the difference in the AUC of three cellular smartphones with the original DICOMs is 0.024 (2.55%), 0.048 (5.10%), and 0.038 (1.91%).","meta":{"url":"http://arxiv.org/abs/2307.14859v1"},"cats":{"new-dataset":0.0557939332,"dev-research":0.2182221146,"prompt-eng":0.4335746847,"data-quality":0.1533970984,"ml-security":0.0721279788}}
{"text":"The minimum difference demonstrates the robustness of the DL-based device in identifying radiological signs of TB in both digital and analog CXR.","meta":{"url":"http://arxiv.org/abs/2307.14859v1"},"cats":{"new-dataset":0.0500129628,"dev-research":0.3029107453,"prompt-eng":0.4638498232,"data-quality":0.2115561459,"ml-security":0.0927823782}}
{"text":"In-context learning, which offers substantial advantages over fine-tuning, is predominantly observed in decoder-only models, while encoder-decoder (i.e., seq2seq) models excel in methods that rely on weight updates.","meta":{"url":"http://arxiv.org/abs/2307.14856v1"},"cats":{"new-dataset":0.0319041301,"dev-research":0.256650893,"prompt-eng":0.3709072621,"data-quality":0.1975848377,"ml-security":0.1548321958}}
{"text":"Recently, a few studies have demonstrated the feasibility of few-shot learning with seq2seq models; however, this has been limited to tasks that align well with the seq2seq architecture, such as summarization and translation.","meta":{"url":"http://arxiv.org/abs/2307.14856v1"},"cats":{"new-dataset":0.0987292694,"dev-research":0.1766758979,"prompt-eng":0.3438697974,"data-quality":0.1520486197,"ml-security":0.0893918107}}
{"text":"Inspired by these initial studies, we provide a first-ever extensive experiment comparing the in-context few-shot learning capabilities of decoder-only and encoder-decoder models on a broad range of tasks.","meta":{"url":"http://arxiv.org/abs/2307.14856v1"},"cats":{"new-dataset":0.097171114,"dev-research":0.179732019,"prompt-eng":0.3632242638,"data-quality":0.1391347518,"ml-security":0.1198857481}}
{"text":"Furthermore, we propose two methods to more effectively elicit in-context learning ability in seq2seq models: objective-aligned prompting and a fusion-based approach.","meta":{"url":"http://arxiv.org/abs/2307.14856v1"},"cats":{"new-dataset":0.0540231414,"dev-research":0.2525039344,"prompt-eng":0.4682271072,"data-quality":0.1790004918,"ml-security":0.1428183211}}
{"text":"Remarkably, our approach outperforms a decoder-only model that is six times larger and exhibits significant performance improvements compared to conventional seq2seq models across a variety of settings.","meta":{"url":"http://arxiv.org/abs/2307.14856v1"},"cats":{"new-dataset":0.1037552576,"dev-research":0.2101006366,"prompt-eng":0.4277039188,"data-quality":0.1635673498,"ml-security":0.0959961124}}
{"text":"We posit that, with the right configuration and prompt design, seq2seq models can be highly effective few-shot learners for a wide spectrum of applications.","meta":{"url":"http://arxiv.org/abs/2307.14856v1"},"cats":{"new-dataset":0.1313517249,"dev-research":0.2285199327,"prompt-eng":0.4776916292,"data-quality":0.1725386254,"ml-security":0.1519053474}}
{"text":"Non-line-of-sight (NLOS) imaging methods are capable of reconstructing complex scenes that are not visible to an observer using indirect illumination.","meta":{"url":"http://arxiv.org/abs/2307.14341v1"},"cats":{"new-dataset":0.0949193898,"dev-research":0.2223425209,"prompt-eng":0.3471186057,"data-quality":0.1457717773,"ml-security":0.0568331101}}
{"text":"However, they assume only third-bounce illumination, so they are currently limited to single-corner configurations, and present limited visibility when imaging surfaces at certain orientations.","meta":{"url":"http://arxiv.org/abs/2307.14341v1"},"cats":{"new-dataset":0.0523604878,"dev-research":0.1852005085,"prompt-eng":0.3676139071,"data-quality":0.0767353831,"ml-security":0.0865498773}}
{"text":"To reason about and tackle these limitations, we make the key observation that planar diffuse surfaces behave specularly at wavelengths used in the computational wave-based NLOS imaging domain.","meta":{"url":"http://arxiv.org/abs/2307.14341v1"},"cats":{"new-dataset":0.1049973192,"dev-research":0.2000528908,"prompt-eng":0.3183655625,"data-quality":0.0701886105,"ml-security":0.0639080484}}
{"text":"We call such surfaces virtual mirrors.","meta":{"url":"http://arxiv.org/abs/2307.14341v1"},"cats":{"new-dataset":0.0460535042,"dev-research":0.1995172697,"prompt-eng":0.4148845,"data-quality":0.077676799,"ml-security":0.087186571}}
{"text":"We leverage this observation to expand the capabilities of NLOS imaging using illumination beyond the third bounce, addressing two problems: imaging single-corner objects at limited visibility angles, and imaging objects hidden behind two corners.","meta":{"url":"http://arxiv.org/abs/2307.14341v1"},"cats":{"new-dataset":0.1859408939,"dev-research":0.204929996,"prompt-eng":0.3826052167,"data-quality":0.1491534482,"ml-security":0.095059896}}
{"text":"To image objects at limited visibility angles, we first analyze the reflections of the known illuminated point on surfaces of the scene as an estimator of the position and orientation of objects with limited visibility.","meta":{"url":"http://arxiv.org/abs/2307.14341v1"},"cats":{"new-dataset":0.165426253,"dev-research":0.1689185113,"prompt-eng":0.4004257511,"data-quality":0.104026628,"ml-security":0.0712591551}}
{"text":"We then image those limited visibility objects by computationally building secondary apertures at other surfaces that observe the target object from a direct visibility perspective.","meta":{"url":"http://arxiv.org/abs/2307.14341v1"},"cats":{"new-dataset":0.1028163136,"dev-research":0.1743766124,"prompt-eng":0.3779396216,"data-quality":0.0625559439,"ml-security":0.1171105418}}
{"text":"Beyond single-corner NLOS imaging, we exploit the specular behavior of virtual mirrors to image objects hidden behind a second corner by imaging the space behind such virtual mirrors, where the mirror image of objects hidden around two corners is formed.","meta":{"url":"http://arxiv.org/abs/2307.14341v1"},"cats":{"new-dataset":0.1571077368,"dev-research":0.2511443042,"prompt-eng":0.4031092792,"data-quality":0.1422671029,"ml-security":0.1304544325}}
{"text":"No specular surfaces were involved in the making of this paper.","meta":{"url":"http://arxiv.org/abs/2307.14341v1"},"cats":{"new-dataset":0.0475763916,"dev-research":0.1727048265,"prompt-eng":0.2577298624,"data-quality":0.1483871445,"ml-security":0.0560072491}}
{"text":"Deep learning (DL) models for tabular data problems are receiving increasingly more attention, while the algorithms based on gradient-boosted decision trees (GBDT) remain a strong go-to solution.","meta":{"url":"http://arxiv.org/abs/2307.14338v1"},"cats":{"new-dataset":0.1014243863,"dev-research":0.2870853037,"prompt-eng":0.3410119713,"data-quality":0.2024457525,"ml-security":0.2104582243}}
{"text":"Following the recent trends in other domains, such as natural language processing and computer vision, several retrieval-augmented tabular DL models have been recently proposed.","meta":{"url":"http://arxiv.org/abs/2307.14338v1"},"cats":{"new-dataset":0.0428676265,"dev-research":0.223044597,"prompt-eng":0.4304188818,"data-quality":0.1826630073,"ml-security":0.0712512631}}
{"text":"For a given target object, a retrieval-based model retrieves other relevant objects, such as the nearest neighbors, from the available (training) data and uses their features or even labels to make a better prediction.","meta":{"url":"http://arxiv.org/abs/2307.14338v1"},"cats":{"new-dataset":0.0160690008,"dev-research":0.2305601773,"prompt-eng":0.3741020609,"data-quality":0.1486712796,"ml-security":0.1363910097}}
{"text":"However, we show that the existing retrieval-based tabular DL solutions provide only minor, if any, benefits over the properly tuned simple retrieval-free baselines.","meta":{"url":"http://arxiv.org/abs/2307.14338v1"},"cats":{"new-dataset":0.1467204166,"dev-research":0.3011148937,"prompt-eng":0.4174509021,"data-quality":0.1466535996,"ml-security":0.0689040962}}
{"text":"Thus, it remains unclear whether the retrieval-based approach is a worthy direction for tabular DL.   ","meta":{"url":"http://arxiv.org/abs/2307.14338v1"},"cats":{"new-dataset":0.0315167559,"dev-research":0.3029564404,"prompt-eng":0.4014549987,"data-quality":0.2086288407,"ml-security":0.0606623455}}
{"text":"In this work, we give a strong positive answer to this question.","meta":{"url":"http://arxiv.org/abs/2307.14338v1"},"cats":{"new-dataset":0.0638083075,"dev-research":0.2199065037,"prompt-eng":0.4271514368,"data-quality":0.1898434744,"ml-security":0.0805714896}}
{"text":"We start by incrementally augmenting a simple feed-forward architecture with an attention-like retrieval component similar to those of many (tabular) retrieval-based models.","meta":{"url":"http://arxiv.org/abs/2307.14338v1"},"cats":{"new-dataset":0.1422950176,"dev-research":0.2074957592,"prompt-eng":0.4376115845,"data-quality":0.1177441178,"ml-security":0.0582629232}}
{"text":"Then, we highlight several details of the attention mechanism that turn out to have a massive impact on the performance on tabular data problems, but that were not explored in prior work.","meta":{"url":"http://arxiv.org/abs/2307.14338v1"},"cats":{"new-dataset":0.0531999296,"dev-research":0.3175965864,"prompt-eng":0.4321325692,"data-quality":0.1849139185,"ml-security":0.1128791188}}
{"text":"As a result, we design TabR -- a simple retrieval-based tabular DL model which, on a set of public benchmarks, demonstrates the best average performance among tabular DL models, becomes the new state-of-the-art on several datasets, and even outperforms GBDT models on the recently proposed ``GBDT-friendly'' benchmark (see the first figure).","meta":{"url":"http://arxiv.org/abs/2307.14338v1"},"cats":{"new-dataset":0.2977700606,"dev-research":0.2659768938,"prompt-eng":0.4330102968,"data-quality":0.1338724421,"ml-security":0.0776149906}}
{"text":"We propose MAMo, a novel memory and attention frame-work for monocular video depth estimation.","meta":{"url":"http://arxiv.org/abs/2307.14336v1"},"cats":{"new-dataset":0.2643467676,"dev-research":0.1777402848,"prompt-eng":0.3639188422,"data-quality":0.1369113927,"ml-security":0.0581930317}}
{"text":"MAMo can augment and improve any single-image depth estimation networks into video depth estimation models, enabling them to take advantage of the temporal information to predict more accurate depth.","meta":{"url":"http://arxiv.org/abs/2307.14336v1"},"cats":{"new-dataset":0.1200387008,"dev-research":0.1909854016,"prompt-eng":0.3422762535,"data-quality":0.1167775401,"ml-security":0.0873758518}}
{"text":"In MAMo, we augment model with memory which aids the depth prediction as the model streams through the video.","meta":{"url":"http://arxiv.org/abs/2307.14336v1"},"cats":{"new-dataset":0.1426312,"dev-research":0.1936460071,"prompt-eng":0.3890524433,"data-quality":0.1245391571,"ml-security":0.1178953303}}
{"text":"Specifically, the memory stores learned visual and displacement tokens of the previous time instances.","meta":{"url":"http://arxiv.org/abs/2307.14336v1"},"cats":{"new-dataset":0.0581088135,"dev-research":0.3031113599,"prompt-eng":0.4033251632,"data-quality":0.209250984,"ml-security":0.1538117278}}
{"text":"This allows the depth network to cross-reference relevant features from the past when predicting depth on the current frame.","meta":{"url":"http://arxiv.org/abs/2307.14336v1"},"cats":{"new-dataset":0.0862293457,"dev-research":0.326296396,"prompt-eng":0.3658413958,"data-quality":0.1289560291,"ml-security":0.0953600201}}
{"text":"We introduce a novel scheme to continuously update the memory, optimizing it to keep tokens that correspond with both the past and the present visual information.","meta":{"url":"http://arxiv.org/abs/2307.14336v1"},"cats":{"new-dataset":0.1801213592,"dev-research":0.3644695279,"prompt-eng":0.4729182282,"data-quality":0.1940182636,"ml-security":0.162584594}}
{"text":"We adopt attention-based approach to process memory features where we first learn the spatio-temporal relation among the resultant visual and displacement memory tokens using self-attention module.","meta":{"url":"http://arxiv.org/abs/2307.14336v1"},"cats":{"new-dataset":0.1099751029,"dev-research":0.3307890529,"prompt-eng":0.436481203,"data-quality":0.1850423366,"ml-security":0.0913974174}}
{"text":"Further, the output features of self-attention are aggregated with the current visual features through cross-attention.","meta":{"url":"http://arxiv.org/abs/2307.14336v1"},"cats":{"new-dataset":0.071880616,"dev-research":0.267743372,"prompt-eng":0.4425547191,"data-quality":0.2033364927,"ml-security":0.1012487794}}
{"text":"The cross-attended features are finally given to a decoder to predict depth on the current frame.","meta":{"url":"http://arxiv.org/abs/2307.14336v1"},"cats":{"new-dataset":0.2524681496,"dev-research":0.2630324861,"prompt-eng":0.4528099101,"data-quality":0.1615898929,"ml-security":0.0823215032}}
{"text":"Through extensive experiments on several benchmarks, including KITTI, NYU-Depth V2, and DDAD, we show that MAMo consistently improves monocular depth estimation networks and sets new state-of-the-art (SOTA) accuracy.","meta":{"url":"http://arxiv.org/abs/2307.14336v1"},"cats":{"new-dataset":0.1809647029,"dev-research":0.1917255068,"prompt-eng":0.350552665,"data-quality":0.1388489352,"ml-security":0.0647829471}}
{"text":"Notably, our MAMo video depth estimation provides higher accuracy with lower latency, when omparing to SOTA cost-volume-based video depth models.","meta":{"url":"http://arxiv.org/abs/2307.14336v1"},"cats":{"new-dataset":0.1036763047,"dev-research":0.1933103396,"prompt-eng":0.3824587301,"data-quality":0.1550811806,"ml-security":0.0607341517}}
{"text":"Large Language Models (LLMs) have shown great promise in integrating diverse expert models to tackle intricate language and vision tasks.","meta":{"url":"http://arxiv.org/abs/2307.14335v1"},"cats":{"new-dataset":0.1103684703,"dev-research":0.2037624143,"prompt-eng":0.4200634937,"data-quality":0.1653423138,"ml-security":0.0883152791}}
{"text":"Despite their significance in advancing the field of Artificial Intelligence Generated Content (AIGC), their potential in intelligent audio content creation remains unexplored.","meta":{"url":"http://arxiv.org/abs/2307.14335v1"},"cats":{"new-dataset":0.1354820341,"dev-research":0.2766545017,"prompt-eng":0.3337641565,"data-quality":0.3113398533,"ml-security":0.0855181576}}
{"text":"In this work, we tackle the problem of creating audio content with storylines encompassing speech, music, and sound effects, guided by text instructions.","meta":{"url":"http://arxiv.org/abs/2307.14335v1"},"cats":{"new-dataset":0.310242851,"dev-research":0.31043696,"prompt-eng":0.3997664077,"data-quality":0.2031679491,"ml-security":0.0529764662}}
{"text":"We present WavJourney, a system that leverages LLMs to connect various audio models for audio content generation.","meta":{"url":"http://arxiv.org/abs/2307.14335v1"},"cats":{"new-dataset":0.3242260729,"dev-research":0.2409726513,"prompt-eng":0.4325833493,"data-quality":0.2117713491,"ml-security":0.0595690354}}
{"text":"Given a text description of an auditory scene, WavJourney first prompts LLMs to generate a structured script dedicated to audio storytelling.","meta":{"url":"http://arxiv.org/abs/2307.14335v1"},"cats":{"new-dataset":0.3051201332,"dev-research":0.2662342366,"prompt-eng":0.4592972086,"data-quality":0.1931478685,"ml-security":0.0577113002}}
{"text":"The audio script incorporates diverse audio elements, organized based on their spatio-temporal relationships.","meta":{"url":"http://arxiv.org/abs/2307.14335v1"},"cats":{"new-dataset":0.4050404258,"dev-research":0.2400237123,"prompt-eng":0.3590233768,"data-quality":0.1389185715,"ml-security":0.0483747714}}
{"text":"As a conceptual representation of audio, the audio script provides an interactive and interpretable rationale for human engagement.","meta":{"url":"http://arxiv.org/abs/2307.14335v1"},"cats":{"new-dataset":0.1169224105,"dev-research":0.4285481124,"prompt-eng":0.3806493954,"data-quality":0.1659676589,"ml-security":0.0728913428}}
{"text":"Afterward, the audio script is fed into a script compiler, converting it into a computer program.","meta":{"url":"http://arxiv.org/abs/2307.14335v1"},"cats":{"new-dataset":0.069399766,"dev-research":0.4504609822,"prompt-eng":0.4034599419,"data-quality":0.2074844084,"ml-security":0.0904013055}}
{"text":"Each line of the program calls a task-specific audio generation model or computational operation function (e.g., concatenate, mix).","meta":{"url":"http://arxiv.org/abs/2307.14335v1"},"cats":{"new-dataset":0.1395279599,"dev-research":0.3959803214,"prompt-eng":0.4217312616,"data-quality":0.2180168234,"ml-security":0.0613518084}}
{"text":"The computer program is then executed to obtain an explainable solution for audio generation.","meta":{"url":"http://arxiv.org/abs/2307.14335v1"},"cats":{"new-dataset":0.0430459556,"dev-research":0.428399916,"prompt-eng":0.4212607535,"data-quality":0.1984928869,"ml-security":0.0848234542}}
{"text":"We demonstrate the practicality of WavJourney across diverse real-world scenarios, including science fiction, education, and radio play.","meta":{"url":"http://arxiv.org/abs/2307.14335v1"},"cats":{"new-dataset":0.3288569289,"dev-research":0.2598768694,"prompt-eng":0.3627554579,"data-quality":0.1340378978,"ml-security":0.0847940372}}
{"text":"The explainable and interactive design of WavJourney fosters human-machine co-creation in multi-round dialogues, enhancing creative control and adaptability in audio production.","meta":{"url":"http://arxiv.org/abs/2307.14335v1"},"cats":{"new-dataset":0.1582384629,"dev-research":0.374698735,"prompt-eng":0.3902135533,"data-quality":0.1542425126,"ml-security":0.0589291309}}
{"text":"WavJourney audiolizes the human imagination, opening up new avenues for creativity in multimedia content creation.","meta":{"url":"http://arxiv.org/abs/2307.14335v1"},"cats":{"new-dataset":0.2270009068,"dev-research":0.3168434832,"prompt-eng":0.32513488,"data-quality":0.1502965411,"ml-security":0.0433310517}}
{"text":"Medicine is inherently multimodal, with rich data modalities spanning text, imaging, genomics, and more.","meta":{"url":"http://arxiv.org/abs/2307.14334v1"},"cats":{"new-dataset":0.2057005417,"dev-research":0.2321144259,"prompt-eng":0.3305843151,"data-quality":0.1153482196,"ml-security":0.0785459579}}
{"text":"Generalist biomedical artificial intelligence (AI) systems that flexibly encode, integrate, and interpret this data at scale can potentially enable impactful applications ranging from scientific discovery to care delivery.","meta":{"url":"http://arxiv.org/abs/2307.14334v1"},"cats":{"new-dataset":0.0673235967,"dev-research":0.2498273625,"prompt-eng":0.3450868381,"data-quality":0.1016121833,"ml-security":0.1592988567}}
{"text":"To enable the development of these models, we first curate MultiMedBench, a new multimodal biomedical benchmark.","meta":{"url":"http://arxiv.org/abs/2307.14334v1"},"cats":{"new-dataset":0.3629871405,"dev-research":0.1877849696,"prompt-eng":0.4071929389,"data-quality":0.1136879413,"ml-security":0.0600261437}}
{"text":"MultiMedBench encompasses 14 diverse tasks such as medical question answering, mammography and dermatology image interpretation, radiology report generation and summarization, and genomic variant calling.","meta":{"url":"http://arxiv.org/abs/2307.14334v1"},"cats":{"new-dataset":0.2624340747,"dev-research":0.2339490291,"prompt-eng":0.3865129087,"data-quality":0.1206349368,"ml-security":0.0592236387}}
{"text":"We then introduce Med-PaLM Multimodal (Med-PaLM M), our proof of concept for a generalist biomedical AI system.","meta":{"url":"http://arxiv.org/abs/2307.14334v1"},"cats":{"new-dataset":0.1801117757,"dev-research":0.199833925,"prompt-eng":0.4078427948,"data-quality":0.1332749197,"ml-security":0.1368467722}}
{"text":"Med-PaLM M is a large multimodal generative model that flexibly encodes and interprets biomedical data including clinical language, imaging, and genomics with the same set of model weights.","meta":{"url":"http://arxiv.org/abs/2307.14334v1"},"cats":{"new-dataset":0.1798982966,"dev-research":0.1827350908,"prompt-eng":0.3940630963,"data-quality":0.1181521479,"ml-security":0.0659400919}}
{"text":"Med-PaLM M reaches performance competitive with or exceeding the state of the art on all MultiMedBench tasks, often surpassing specialist models by a wide margin.","meta":{"url":"http://arxiv.org/abs/2307.14334v1"},"cats":{"new-dataset":0.0309972141,"dev-research":0.1769863848,"prompt-eng":0.3795690743,"data-quality":0.0681170824,"ml-security":0.0605308181}}
{"text":"We also report examples of zero-shot generalization to novel medical concepts and tasks, positive transfer learning across tasks, and emergent zero-shot medical reasoning.","meta":{"url":"http://arxiv.org/abs/2307.14334v1"},"cats":{"new-dataset":0.103168953,"dev-research":0.2367480833,"prompt-eng":0.3677390405,"data-quality":0.1642649804,"ml-security":0.1641446708}}
{"text":"To further probe the capabilities and limitations of Med-PaLM M, we conduct a radiologist evaluation of model-generated (and human) chest X-ray reports and observe encouraging performance across model scales.","meta":{"url":"http://arxiv.org/abs/2307.14334v1"},"cats":{"new-dataset":0.0635366486,"dev-research":0.2040967753,"prompt-eng":0.4001846109,"data-quality":0.0924634203,"ml-security":0.0817190723}}
{"text":"In a side-by-side ranking on 246 retrospective chest X-rays, clinicians express a pairwise preference for Med-PaLM M reports over those produced by radiologists in up to 40.50% of cases, suggesting potential clinical utility.","meta":{"url":"http://arxiv.org/abs/2307.14334v1"},"cats":{"new-dataset":0.0322461569,"dev-research":0.2088776362,"prompt-eng":0.3598804619,"data-quality":0.1295495719,"ml-security":0.0815925729}}
{"text":"While considerable work is needed to validate these models in real-world use cases, our results represent a milestone towards the development of generalist biomedical AI systems.","meta":{"url":"http://arxiv.org/abs/2307.14334v1"},"cats":{"new-dataset":0.0541239142,"dev-research":0.2021812093,"prompt-eng":0.4028994721,"data-quality":0.137140645,"ml-security":0.2225453583}}
{"text":"Neuromorphic visual sensors are artificial retinas that output sequences of asynchronous events when brightness changes occur in the scene.","meta":{"url":"http://arxiv.org/abs/2307.14332v1"},"cats":{"new-dataset":0.0650185438,"dev-research":0.2798725329,"prompt-eng":0.40914367,"data-quality":0.134314223,"ml-security":0.1438776689}}
{"text":"These sensors offer many advantages including very high temporal resolution, no motion blur and smart data compression ideal for real-time processing.","meta":{"url":"http://arxiv.org/abs/2307.14332v1"},"cats":{"new-dataset":0.1021144814,"dev-research":0.2695535266,"prompt-eng":0.3431926274,"data-quality":0.0686630582,"ml-security":0.0654624985}}
{"text":"In this study, we introduce an event-based dataset on fine-grained manipulation actions and perform an experimental study on the use of transformers for action prediction with events.","meta":{"url":"http://arxiv.org/abs/2307.14332v1"},"cats":{"new-dataset":0.3082356242,"dev-research":0.3133485049,"prompt-eng":0.4698728414,"data-quality":0.1475721033,"ml-security":0.1340641794}}
{"text":"There is enormous interest in the fields of cognitive robotics and human-robot interaction on understanding and predicting human actions as early as possible.","meta":{"url":"http://arxiv.org/abs/2307.14332v1"},"cats":{"new-dataset":0.0910560061,"dev-research":0.2577612244,"prompt-eng":0.445171239,"data-quality":0.0817248953,"ml-security":0.0989156528}}
{"text":"Early prediction allows anticipating complex stages for planning, enabling effective and real-time interaction.","meta":{"url":"http://arxiv.org/abs/2307.14332v1"},"cats":{"new-dataset":0.0662596934,"dev-research":0.3437303421,"prompt-eng":0.4564731815,"data-quality":0.0566405974,"ml-security":0.1036659485}}
{"text":"Our Transformer network uses events to predict manipulation actions as they occur, using online inference.","meta":{"url":"http://arxiv.org/abs/2307.14332v1"},"cats":{"new-dataset":0.0476038262,"dev-research":0.2876192818,"prompt-eng":0.4550079385,"data-quality":0.1133103121,"ml-security":0.229556194}}
{"text":"The model succeeds at predicting actions early on, building up confidence over time and achieving state-of-the-art classification.","meta":{"url":"http://arxiv.org/abs/2307.14332v1"},"cats":{"new-dataset":0.0564984086,"dev-research":0.2352259248,"prompt-eng":0.43806656,"data-quality":0.1616226421,"ml-security":0.1927287875}}
{"text":"Moreover, the attention-based transformer architecture allows us to study the role of the spatio-temporal patterns selected by the model.","meta":{"url":"http://arxiv.org/abs/2307.14332v1"},"cats":{"new-dataset":0.0722743808,"dev-research":0.199216479,"prompt-eng":0.4136823082,"data-quality":0.0669873018,"ml-security":0.0789779906}}
{"text":"Our experiments show that the Transformer network captures action dynamic features outperforming video-based approaches and succeeding with scenarios where the differences between actions lie in very subtle cues.","meta":{"url":"http://arxiv.org/abs/2307.14332v1"},"cats":{"new-dataset":0.0446958585,"dev-research":0.3137958751,"prompt-eng":0.40625232,"data-quality":0.1907088116,"ml-security":0.1724447217}}
{"text":"Finally, we release the new event dataset, which is the first in the literature for manipulation action recognition.","meta":{"url":"http://arxiv.org/abs/2307.14332v1"},"cats":{"new-dataset":0.8094320849,"dev-research":0.2528865981,"prompt-eng":0.4004048387,"data-quality":0.185532954,"ml-security":0.1201778869}}
{"text":"Code will be available at https://github.com/DaniDeniz/EventVisionTransformer.","meta":{"url":"http://arxiv.org/abs/2307.14332v1"},"cats":{"new-dataset":0.4581892577,"dev-research":0.3149032248,"prompt-eng":0.4674778182,"data-quality":0.1806276168,"ml-security":0.0545788592}}
{"text":"Text-conditioned image editing has emerged as a powerful tool for editing images.","meta":{"url":"http://arxiv.org/abs/2307.14331v1"},"cats":{"new-dataset":0.0295480808,"dev-research":0.2840896071,"prompt-eng":0.4620438328,"data-quality":0.1991628921,"ml-security":0.0676308984}}
{"text":"However, in many situations, language can be ambiguous and ineffective in describing specific image edits.","meta":{"url":"http://arxiv.org/abs/2307.14331v1"},"cats":{"new-dataset":0.0085450819,"dev-research":0.3710269561,"prompt-eng":0.3776817454,"data-quality":0.5378027731,"ml-security":0.1306285751}}
{"text":"When faced with such challenges, visual prompts can be a more informative and intuitive way to convey ideas.","meta":{"url":"http://arxiv.org/abs/2307.14331v1"},"cats":{"new-dataset":0.035736226,"dev-research":0.4389137814,"prompt-eng":0.5007362299,"data-quality":0.1559820989,"ml-security":0.1705905135}}
{"text":"We present a method for image editing via visual prompting.","meta":{"url":"http://arxiv.org/abs/2307.14331v1"},"cats":{"new-dataset":0.0512541898,"dev-research":0.3979522238,"prompt-eng":0.5475846236,"data-quality":0.1846071037,"ml-security":0.0753397754}}
{"text":"Given pairs of example that represent the \"before\" and \"after\" images of an edit, our goal is to learn a text-based editing direction that can be used to perform the same edit on new images.","meta":{"url":"http://arxiv.org/abs/2307.14331v1"},"cats":{"new-dataset":0.0762107758,"dev-research":0.319538305,"prompt-eng":0.4199366069,"data-quality":0.2270972994,"ml-security":0.0547800514}}
{"text":"We leverage the rich, pretrained editing capabilities of text-to-image diffusion models by inverting visual prompts into editing instructions.","meta":{"url":"http://arxiv.org/abs/2307.14331v1"},"cats":{"new-dataset":0.103972123,"dev-research":0.2635958146,"prompt-eng":0.5198670581,"data-quality":0.2041031954,"ml-security":0.1490004108}}
{"text":"Our results show that with just one example pair, we can achieve competitive results compared to state-of-the-art text-conditioned image editing frameworks.","meta":{"url":"http://arxiv.org/abs/2307.14331v1"},"cats":{"new-dataset":0.0805230279,"dev-research":0.2680914527,"prompt-eng":0.4706159813,"data-quality":0.2470291637,"ml-security":0.079734841}}
{"text":"While imitation learning methods have seen a resurgent interest for robotic manipulation, the well-known problem of compounding errors continues to afflict behavioral cloning (BC).","meta":{"url":"http://arxiv.org/abs/2307.14326v1"},"cats":{"new-dataset":0.0148314626,"dev-research":0.2910946599,"prompt-eng":0.4260493628,"data-quality":0.2909108239,"ml-security":0.2019128424}}
{"text":"Waypoints can help address this problem by reducing the horizon of the learning problem for BC, and thus, the errors compounded over time.","meta":{"url":"http://arxiv.org/abs/2307.14326v1"},"cats":{"new-dataset":0.0793095935,"dev-research":0.3229264874,"prompt-eng":0.3982036249,"data-quality":0.38783722,"ml-security":0.2137725712}}
{"text":"However, waypoint labeling is underspecified, and requires additional human supervision.","meta":{"url":"http://arxiv.org/abs/2307.14326v1"},"cats":{"new-dataset":0.0265667344,"dev-research":0.3386392099,"prompt-eng":0.4580517138,"data-quality":0.431243257,"ml-security":0.114439058}}
{"text":"Can we generate waypoints automatically without any additional human supervision?","meta":{"url":"http://arxiv.org/abs/2307.14326v1"},"cats":{"new-dataset":0.0598875923,"dev-research":0.3635027087,"prompt-eng":0.4913344481,"data-quality":0.1499795914,"ml-security":0.0924478883}}
{"text":"Our key insight is that if a trajectory segment can be approximated by linear motion, the endpoints can be used as waypoints.","meta":{"url":"http://arxiv.org/abs/2307.14326v1"},"cats":{"new-dataset":0.009208429,"dev-research":0.2494781465,"prompt-eng":0.3584840678,"data-quality":0.1139057384,"ml-security":0.0776386962}}
{"text":"We propose Automatic Waypoint Extraction (AWE) for imitation learning, a preprocessing module to decompose a demonstration into a minimal set of waypoints which when interpolated linearly can approximate the trajectory up to a specified error threshold.","meta":{"url":"http://arxiv.org/abs/2307.14326v1"},"cats":{"new-dataset":0.1219819523,"dev-research":0.2772063135,"prompt-eng":0.4343126252,"data-quality":0.1630668985,"ml-security":0.1020005756}}
{"text":"AWE can be combined with any BC algorithm, and we find that AWE can increase the success rate of state-of-the-art algorithms by up to 25% in simulation and by 4-28% on real-world bimanual manipulation tasks, reducing the decision making horizon by up to a factor of 10.","meta":{"url":"http://arxiv.org/abs/2307.14326v1"},"cats":{"new-dataset":0.0222785183,"dev-research":0.2859094141,"prompt-eng":0.4383814802,"data-quality":0.096440574,"ml-security":0.078666875}}
{"text":"Videos and code are available at https://lucys0.github.io/awe/","meta":{"url":"http://arxiv.org/abs/2307.14326v1"},"cats":{"new-dataset":0.5837868067,"dev-research":0.2804133594,"prompt-eng":0.4160757615,"data-quality":0.1410708419,"ml-security":0.0685108048}}
{"text":"This paper presents a case study on the design, administration, post-processing, and evaluation of surveys on large language models (LLMs).","meta":{"url":"http://arxiv.org/abs/2307.14324v1"},"cats":{"new-dataset":0.1124571055,"dev-research":0.2019029554,"prompt-eng":0.4479905948,"data-quality":0.2454030483,"ml-security":0.0755379812}}
{"text":"It comprises two components: (1) A statistical method for eliciting beliefs encoded in LLMs.","meta":{"url":"http://arxiv.org/abs/2307.14324v1"},"cats":{"new-dataset":0.036911597,"dev-research":0.2482302248,"prompt-eng":0.4715194738,"data-quality":0.1642167962,"ml-security":0.0873660539}}
{"text":"We introduce statistical measures and evaluation metrics that quantify the probability of an LLM \"making a choice\", the associated uncertainty, and the consistency of that choice.","meta":{"url":"http://arxiv.org/abs/2307.14324v1"},"cats":{"new-dataset":0.0178758026,"dev-research":0.2107044089,"prompt-eng":0.4823000356,"data-quality":0.2219324881,"ml-security":0.1037762117}}
{"text":"(2) We apply this method to study what moral beliefs are encoded in different LLMs, especially in ambiguous cases where the right choice is not obvious.","meta":{"url":"http://arxiv.org/abs/2307.14324v1"},"cats":{"new-dataset":0.0180091846,"dev-research":0.2426641964,"prompt-eng":0.4047088173,"data-quality":0.2358405502,"ml-security":0.1218646661}}
{"text":"We design a large-scale survey comprising 680 high-ambiguity moral scenarios (e.g., \"Should I tell a white lie?\") and 687 low-ambiguity moral scenarios (e.g., \"Should I stop for a pedestrian on the road?\").","meta":{"url":"http://arxiv.org/abs/2307.14324v1"},"cats":{"new-dataset":0.0896684158,"dev-research":0.2532290271,"prompt-eng":0.4011675354,"data-quality":0.1868164136,"ml-security":0.2008872552}}
{"text":"Each scenario includes a description, two possible actions, and auxiliary labels indicating violated rules (e.g., \"do not kill\").","meta":{"url":"http://arxiv.org/abs/2307.14324v1"},"cats":{"new-dataset":0.097752952,"dev-research":0.3211639851,"prompt-eng":0.4668988157,"data-quality":0.3406501442,"ml-security":0.2521402394}}
{"text":"We administer the survey to 28 open- and closed-source LLMs.","meta":{"url":"http://arxiv.org/abs/2307.14324v1"},"cats":{"new-dataset":0.5555508139,"dev-research":0.2055873072,"prompt-eng":0.4261913484,"data-quality":0.1571948853,"ml-security":0.1238803854}}
{"text":"We find that (a) in unambiguous scenarios, most models \"choose\" actions that align with commonsense.","meta":{"url":"http://arxiv.org/abs/2307.14324v1"},"cats":{"new-dataset":0.0241755306,"dev-research":0.2222540133,"prompt-eng":0.4003518661,"data-quality":0.1956002304,"ml-security":0.1453327831}}
{"text":"In ambiguous cases, most models express uncertainty.","meta":{"url":"http://arxiv.org/abs/2307.14324v1"},"cats":{"new-dataset":0.0050389526,"dev-research":0.237737392,"prompt-eng":0.3777002975,"data-quality":0.2253709952,"ml-security":0.141301746}}
{"text":"(b) Some models are uncertain about choosing the commonsense action because their responses are sensitive to the question-wording.","meta":{"url":"http://arxiv.org/abs/2307.14324v1"},"cats":{"new-dataset":0.0165450425,"dev-research":0.1847234043,"prompt-eng":0.4202368833,"data-quality":0.3143892311,"ml-security":0.1812821744}}
{"text":"(c) Some models reflect clear preferences in ambiguous scenarios.","meta":{"url":"http://arxiv.org/abs/2307.14324v1"},"cats":{"new-dataset":0.0099278865,"dev-research":0.2705876954,"prompt-eng":0.459684022,"data-quality":0.2285142532,"ml-security":0.1873784097}}
{"text":"Specifically, closed-source models tend to agree with each other.","meta":{"url":"http://arxiv.org/abs/2307.14324v1"},"cats":{"new-dataset":0.0275670512,"dev-research":0.2849215243,"prompt-eng":0.3306780566,"data-quality":0.196938008,"ml-security":0.170464106}}
{"text":"Safety is critical to broadening the application of reinforcement learning (RL).","meta":{"url":"http://arxiv.org/abs/2307.14316v1"},"cats":{"new-dataset":0.0210888147,"dev-research":0.2844988035,"prompt-eng":0.3397665068,"data-quality":0.1569610486,"ml-security":0.4663791669}}
{"text":"Often, we train RL agents in a controlled environment, such as a laboratory, before deploying them in the real world.","meta":{"url":"http://arxiv.org/abs/2307.14316v1"},"cats":{"new-dataset":0.1188860904,"dev-research":0.292389133,"prompt-eng":0.4098879485,"data-quality":0.1043727482,"ml-security":0.2532281776}}
{"text":"However, the real-world target task might be unknown prior to deployment.","meta":{"url":"http://arxiv.org/abs/2307.14316v1"},"cats":{"new-dataset":0.018370088,"dev-research":0.3105004997,"prompt-eng":0.4591843163,"data-quality":0.1809293689,"ml-security":0.2228072146}}
{"text":"Reward-free RL trains an agent without the reward to adapt quickly once the reward is revealed.","meta":{"url":"http://arxiv.org/abs/2307.14316v1"},"cats":{"new-dataset":0.0590280388,"dev-research":0.1942962991,"prompt-eng":0.4039685892,"data-quality":0.0839896136,"ml-security":0.1623391544}}
{"text":"We consider the constrained reward-free setting, where an agent (the guide) learns to explore safely without the reward signal.","meta":{"url":"http://arxiv.org/abs/2307.14316v1"},"cats":{"new-dataset":0.0800611007,"dev-research":0.1895000808,"prompt-eng":0.4142266004,"data-quality":0.0885298717,"ml-security":0.270859186}}
{"text":"This agent is trained in a controlled environment, which allows unsafe interactions and still provides the safety signal.","meta":{"url":"http://arxiv.org/abs/2307.14316v1"},"cats":{"new-dataset":0.0642651399,"dev-research":0.2847419153,"prompt-eng":0.3833673391,"data-quality":0.1048986944,"ml-security":0.4556086064}}
{"text":"After the target task is revealed, safety violations are not allowed anymore.","meta":{"url":"http://arxiv.org/abs/2307.14316v1"},"cats":{"new-dataset":0.1085398603,"dev-research":0.2922318315,"prompt-eng":0.3445307479,"data-quality":0.2807777812,"ml-security":0.4531229636}}
{"text":"Thus, the guide is leveraged to compose a safe behaviour policy.","meta":{"url":"http://arxiv.org/abs/2307.14316v1"},"cats":{"new-dataset":0.087346885,"dev-research":0.3159208282,"prompt-eng":0.4021792232,"data-quality":0.1687874648,"ml-security":0.2835313655}}
{"text":"Drawing from transfer learning, we also regularize a target policy (the student) towards the guide while the student is unreliable and gradually eliminate the influence of the guide as training progresses.","meta":{"url":"http://arxiv.org/abs/2307.14316v1"},"cats":{"new-dataset":0.0191362556,"dev-research":0.3222610114,"prompt-eng":0.4752347791,"data-quality":0.2436298482,"ml-security":0.2109258875}}
{"text":"The empirical analysis shows that this method can achieve safe transfer learning and helps the student solve the target task faster.","meta":{"url":"http://arxiv.org/abs/2307.14316v1"},"cats":{"new-dataset":0.0191042645,"dev-research":0.375999981,"prompt-eng":0.377346914,"data-quality":0.1765895913,"ml-security":0.255507437}}
{"text":"Reinforcement learning is of increasing importance in the field of robot control and simulation plays a~key role in this process.","meta":{"url":"http://arxiv.org/abs/2307.14313v1"},"cats":{"new-dataset":0.0284761954,"dev-research":0.2292975982,"prompt-eng":0.3624710885,"data-quality":0.0629102036,"ml-security":0.1395359501}}
{"text":"In the unmanned aerial vehicles (UAVs, drones), there is also an increase in the number of published scientific papers involving this approach.","meta":{"url":"http://arxiv.org/abs/2307.14313v1"},"cats":{"new-dataset":0.0282922273,"dev-research":0.2228702496,"prompt-eng":0.3242712691,"data-quality":0.0954502515,"ml-security":0.0999971512}}
{"text":"In this work, an autonomous drone control system was prepared to fly forward (according to its coordinates system) and pass the trees encountered in the forest based on the data from a rotating LiDAR sensor.","meta":{"url":"http://arxiv.org/abs/2307.14313v1"},"cats":{"new-dataset":0.2192380267,"dev-research":0.2190510609,"prompt-eng":0.4071355731,"data-quality":0.1045911714,"ml-security":0.1207974003}}
{"text":"The Proximal Policy Optimization (PPO) algorithm, an example of reinforcement learning (RL), was used to prepare it.","meta":{"url":"http://arxiv.org/abs/2307.14313v1"},"cats":{"new-dataset":0.0489986812,"dev-research":0.2290165229,"prompt-eng":0.3813563139,"data-quality":0.0870870227,"ml-security":0.1635792487}}
{"text":"A custom simulator in the Python language was developed for this purpose.","meta":{"url":"http://arxiv.org/abs/2307.14313v1"},"cats":{"new-dataset":0.2648637128,"dev-research":0.2502595472,"prompt-eng":0.4977702405,"data-quality":0.1129443167,"ml-security":0.0722349635}}
{"text":"The Gazebo environment, integrated with the Robot Operating System (ROS), was also used to test the resulting control algorithm.","meta":{"url":"http://arxiv.org/abs/2307.14313v1"},"cats":{"new-dataset":0.0502217058,"dev-research":0.3035861861,"prompt-eng":0.4477918523,"data-quality":0.0731597305,"ml-security":0.0928824782}}
{"text":"Finally, the prepared solution was implemented in the Nvidia Jetson Nano eGPU and verified in the real tests scenarios.","meta":{"url":"http://arxiv.org/abs/2307.14313v1"},"cats":{"new-dataset":0.1353918685,"dev-research":0.2076718568,"prompt-eng":0.4383431701,"data-quality":0.129619892,"ml-security":0.0836461709}}
{"text":"During them, the drone successfully completed the set task and was able to repeatably avoid trees and fly through the forest.","meta":{"url":"http://arxiv.org/abs/2307.14313v1"},"cats":{"new-dataset":0.0661079875,"dev-research":0.260370767,"prompt-eng":0.3836565353,"data-quality":0.1586432235,"ml-security":0.0709582511}}
{"text":"Experimental data can aid in gaining insights about a system operation, as well as determining critical aspects of a modelling or simulation process.","meta":{"url":"http://arxiv.org/abs/2307.14312v1"},"cats":{"new-dataset":0.1331517503,"dev-research":0.3741653642,"prompt-eng":0.4279958822,"data-quality":0.1492699698,"ml-security":0.1919310474}}
{"text":"In this paper, we analyze the data acquired from an extensive experimentation process in a serverless Function as a Service system (based on the open source Apache Openwhisk) that has been deployed across 3 available cloud/edge locations with different system setups.","meta":{"url":"http://arxiv.org/abs/2307.14312v1"},"cats":{"new-dataset":0.4105938967,"dev-research":0.2309692782,"prompt-eng":0.3815639616,"data-quality":0.1487459722,"ml-security":0.177605355}}
{"text":"Thus, they can be used to model distribution of functions through multi-location aware scheduling mechanisms.","meta":{"url":"http://arxiv.org/abs/2307.14312v1"},"cats":{"new-dataset":0.0215549966,"dev-research":0.3026454129,"prompt-eng":0.4133448202,"data-quality":0.0826062211,"ml-security":0.1043398122}}
{"text":"The experiments include different traffic arrival rates, different setups for the FaaS system, as well as different configurations for the hardware and platform used.","meta":{"url":"http://arxiv.org/abs/2307.14312v1"},"cats":{"new-dataset":0.0337972154,"dev-research":0.2020749119,"prompt-eng":0.3695251721,"data-quality":0.1280389075,"ml-security":0.0776255825}}
{"text":"We analyse the acquired data for the three FaaS system setups and discuss their differences presenting interesting conclusions with relation to transient effects of the system, such as the effect on wait and execution time.","meta":{"url":"http://arxiv.org/abs/2307.14312v1"},"cats":{"new-dataset":0.0576649246,"dev-research":0.2420042902,"prompt-eng":0.4064186875,"data-quality":0.1256056407,"ml-security":0.1766988173}}
{"text":"We also demonstrate interesting trade-offs with relation to system setup and indicate a number of factors that can affect system performance and should be taken under consideration in modelling attempts of such systems.","meta":{"url":"http://arxiv.org/abs/2307.14312v1"},"cats":{"new-dataset":0.0198050676,"dev-research":0.3536396902,"prompt-eng":0.5046747493,"data-quality":0.0990028891,"ml-security":0.1252441848}}
{"text":"This study is main goal is to provide a comparative comparison of libraries using machine learning methods.","meta":{"url":"http://arxiv.org/abs/2307.14311v1"},"cats":{"new-dataset":0.0708552481,"dev-research":0.3208218595,"prompt-eng":0.3512935384,"data-quality":0.1831923559,"ml-security":0.1258985982}}
{"text":"Experts in natural language processing (NLP) are becoming more and more interested in sentiment analysis (SA) of text changes.","meta":{"url":"http://arxiv.org/abs/2307.14311v1"},"cats":{"new-dataset":0.0617632043,"dev-research":0.3272427169,"prompt-eng":0.3305192264,"data-quality":0.2596202209,"ml-security":0.083581269}}
{"text":"The objective of employing NLP text analysis techniques is to recognize and categorize feelings related to twitter users utterances.","meta":{"url":"http://arxiv.org/abs/2307.14311v1"},"cats":{"new-dataset":0.0261644842,"dev-research":0.3420993668,"prompt-eng":0.3266953742,"data-quality":0.2649590738,"ml-security":0.0809221903}}
{"text":"In this examination, issues with SA and the libraries utilized are also looked at.","meta":{"url":"http://arxiv.org/abs/2307.14311v1"},"cats":{"new-dataset":0.1643574569,"dev-research":0.3029808307,"prompt-eng":0.3676881855,"data-quality":0.1717501942,"ml-security":0.0744348647}}
{"text":"provides a number of cooperative methods to classify emotional polarity.","meta":{"url":"http://arxiv.org/abs/2307.14311v1"},"cats":{"new-dataset":0.0203976251,"dev-research":0.3212277644,"prompt-eng":0.3934122727,"data-quality":0.1689152571,"ml-security":0.1119796859}}
{"text":"The Naive Bayes Classifier, Decision Tree Classifier, Maxent Classifier, Sklearn Classifier, Sklearn Classifier MultinomialNB, and other conjoint learning algorithms, according to recent research, are very effective.","meta":{"url":"http://arxiv.org/abs/2307.14311v1"},"cats":{"new-dataset":0.1121025466,"dev-research":0.2611009395,"prompt-eng":0.3808506681,"data-quality":0.2388899809,"ml-security":0.213531493}}
{"text":"In the project will use Five Python and R libraries NLTK, TextBlob, Vader, Transformers (GPT and BERT pretrained), and Tidytext will be used in the study to apply sentiment analysis techniques.","meta":{"url":"http://arxiv.org/abs/2307.14311v1"},"cats":{"new-dataset":0.4091505839,"dev-research":0.3179214075,"prompt-eng":0.3592325969,"data-quality":0.2441545483,"ml-security":0.0573738648}}
{"text":"Four machine learning models Tree of Decisions (DT), Support Vector Machine (SVM), Naive Bayes (NB), and K-Nearest Neighbor (KNN) will also be used.","meta":{"url":"http://arxiv.org/abs/2307.14311v1"},"cats":{"new-dataset":0.1710828331,"dev-research":0.2482881287,"prompt-eng":0.3920637146,"data-quality":0.1877567367,"ml-security":0.1619106473}}
{"text":"To evaluate how well libraries for SA operate in the social network environment, comparative study was also carried out.","meta":{"url":"http://arxiv.org/abs/2307.14311v1"},"cats":{"new-dataset":0.0657195113,"dev-research":0.3199929403,"prompt-eng":0.3301978279,"data-quality":0.1191194639,"ml-security":0.0620379122}}
{"text":"The measures to assess the best algorithms in this experiment, which used a single data set for each method, were precision, recall, and F1 score.","meta":{"url":"http://arxiv.org/abs/2307.14311v1"},"cats":{"new-dataset":0.0634260883,"dev-research":0.2714677683,"prompt-eng":0.3988448886,"data-quality":0.2085664583,"ml-security":0.0684432394}}
{"text":"We conclude that the BERT transformer method with an Accuracy: 0.973 is recommended for sentiment analysis.","meta":{"url":"http://arxiv.org/abs/2307.14311v1"},"cats":{"new-dataset":0.0324962967,"dev-research":0.2823993836,"prompt-eng":0.3726781333,"data-quality":0.365180603,"ml-security":0.0658543474}}
{"text":"When faced with a large number of product reviews, it is not clear that a human can remember all of them and weight opinions representatively to write a good reference summary.","meta":{"url":"http://arxiv.org/abs/2307.14305v1"},"cats":{"new-dataset":0.0736614834,"dev-research":0.3172492968,"prompt-eng":0.3725674033,"data-quality":0.2348264532,"ml-security":0.0619753892}}
{"text":"We propose an automatic metric to test the prevalence of the opinions that a summary expresses, based on counting the number of reviews that are consistent with each statement in the summary, while discrediting trivial or redundant statements.","meta":{"url":"http://arxiv.org/abs/2307.14305v1"},"cats":{"new-dataset":0.0453710432,"dev-research":0.3440558037,"prompt-eng":0.4281557136,"data-quality":0.4443096215,"ml-security":0.0825833182}}
{"text":"To formulate this opinion prevalence metric, we consider several existing methods to score the factual consistency of a summary statement with respect to each individual source review.","meta":{"url":"http://arxiv.org/abs/2307.14305v1"},"cats":{"new-dataset":0.0612105696,"dev-research":0.329141456,"prompt-eng":0.3812707701,"data-quality":0.4137582063,"ml-security":0.0582618984}}
{"text":"On a corpus of Amazon product reviews, we gather multiple human judgments of the opinion consistency, to determine which automatic metric best expresses consistency in product reviews.","meta":{"url":"http://arxiv.org/abs/2307.14305v1"},"cats":{"new-dataset":0.101127767,"dev-research":0.3100034091,"prompt-eng":0.4476189549,"data-quality":0.4981818642,"ml-security":0.0427701299}}
{"text":"Using the resulting opinion prevalence metric, we show that a human authored summary has only slightly better opinion prevalence than randomly selected extracts from the source reviews, and previous extractive and abstractive unsupervised opinion summarization methods perform worse than humans.","meta":{"url":"http://arxiv.org/abs/2307.14305v1"},"cats":{"new-dataset":0.085456982,"dev-research":0.3164762704,"prompt-eng":0.3514384377,"data-quality":0.3472473425,"ml-security":0.0984663206}}
{"text":"We demonstrate room for improvement with a greedy construction of extractive summaries with twice the opinion prevalence achieved by humans.","meta":{"url":"http://arxiv.org/abs/2307.14305v1"},"cats":{"new-dataset":0.1045177661,"dev-research":0.3022827013,"prompt-eng":0.3716356536,"data-quality":0.2817627153,"ml-security":0.0911576264}}
{"text":"Finally, we show that preprocessing source reviews by simplification can raise the opinion prevalence achieved by existing abstractive opinion summarization systems to the level of human performance.","meta":{"url":"http://arxiv.org/abs/2307.14305v1"},"cats":{"new-dataset":0.0614183232,"dev-research":0.4460240468,"prompt-eng":0.3927653804,"data-quality":0.3463711432,"ml-security":0.0979375935}}
{"text":"Recent tropical cyclones, e.g., Hurricane Harvey (2017), have lead to significant rainfall and resulting runoff with accompanying flooding.","meta":{"url":"http://arxiv.org/abs/2307.14302v1"},"cats":{"new-dataset":0.0716423424,"dev-research":0.2618978786,"prompt-eng":0.3618957655,"data-quality":0.1141548265,"ml-security":0.1762712381}}
{"text":"When the runoff interacts with storm surge, the resulting floods can be greatly amplified and lead to effects that cannot be modeled by simple superposition of its distinctive sources.","meta":{"url":"http://arxiv.org/abs/2307.14302v1"},"cats":{"new-dataset":0.0183415524,"dev-research":0.256341184,"prompt-eng":0.4061336683,"data-quality":0.1873702587,"ml-security":0.2836634374}}
{"text":"In an effort to develop accurate numerical simulations of runoff, surge, and compounding floods, we develop a local discontinuous Galerkin method for modified shallow water equations.","meta":{"url":"http://arxiv.org/abs/2307.14302v1"},"cats":{"new-dataset":0.0475502583,"dev-research":0.2683586948,"prompt-eng":0.3084517393,"data-quality":0.1324320303,"ml-security":0.1073360492}}
{"text":"In this modification, nonzero sources to the continuity equation are included to incorporate rainfall into the model using parametric rainfall models from literature as well as hindcast data.","meta":{"url":"http://arxiv.org/abs/2307.14302v1"},"cats":{"new-dataset":0.1732533573,"dev-research":0.1929662618,"prompt-eng":0.3845766607,"data-quality":0.0969717655,"ml-security":0.0646823565}}
{"text":"The discontinuous Galerkin spatial discretization is accompanied with a strong stability preserving explicit Runge Kutta time integrator.","meta":{"url":"http://arxiv.org/abs/2307.14302v1"},"cats":{"new-dataset":0.0273297634,"dev-research":0.1978915777,"prompt-eng":0.3128247765,"data-quality":0.082808503,"ml-security":0.0938475962}}
{"text":"Hence, temporal stability is ensured through the CFL condition and we exploit the embarrassingly parallel nature of the developed method using MPI parallelization.","meta":{"url":"http://arxiv.org/abs/2307.14302v1"},"cats":{"new-dataset":0.0280470634,"dev-research":0.2747903965,"prompt-eng":0.3974300522,"data-quality":0.1242939428,"ml-security":0.0925811142}}
{"text":"We demonstrate the capabilities of the developed method though a sequence of physically relevant numerical tests, including small scale test cases based on laboratory measurements and large scale experiments with Hurricane Harvey in the Gulf of Mexico.","meta":{"url":"http://arxiv.org/abs/2307.14302v1"},"cats":{"new-dataset":0.0717775651,"dev-research":0.2270159319,"prompt-eng":0.3770569938,"data-quality":0.1092069556,"ml-security":0.1303463768}}
{"text":"The results highlight the conservation properties and robustness of the developed method and show the potential of compound flood modeling using our approach.","meta":{"url":"http://arxiv.org/abs/2307.14302v1"},"cats":{"new-dataset":0.060450492,"dev-research":0.2550191029,"prompt-eng":0.3550070727,"data-quality":0.1427483216,"ml-security":0.1545671789}}
{"text":"We contribute to the knowledge of linear codes from special polynomials and functions, which have been studied intensively in the past few years.","meta":{"url":"http://arxiv.org/abs/2307.14300v1"},"cats":{"new-dataset":0.1300511618,"dev-research":0.2312321788,"prompt-eng":0.3436217013,"data-quality":0.1592601329,"ml-security":0.1840968798}}
{"text":"Such codes have several applications in secret sharing, authentication codes, association schemes and strongly regular graphs.   ","meta":{"url":"http://arxiv.org/abs/2307.14300v1"},"cats":{"new-dataset":0.0598379718,"dev-research":0.2611367553,"prompt-eng":0.4042863049,"data-quality":0.1647942114,"ml-security":0.2832676489}}
{"text":"This is the first work in which we study the dual codes in the framework of the two generic constructions; in particular, we propose a Gram-Schmidt (complexity of $\\mathcal{O}(n^3)$) process to compute them explicitly.","meta":{"url":"http://arxiv.org/abs/2307.14300v1"},"cats":{"new-dataset":0.1191687621,"dev-research":0.2192445842,"prompt-eng":0.3411643712,"data-quality":0.1674808198,"ml-security":0.0888053169}}
{"text":"The originality of this contribution is in the study of the existence or not of defining sets $D'$, which can be used as ingredients to construct the dual code $\\mathcal{C}'$ for a given code $\\mathcal{C}$ in the context of the second generic construction.","meta":{"url":"http://arxiv.org/abs/2307.14300v1"},"cats":{"new-dataset":0.060150098,"dev-research":0.3667774455,"prompt-eng":0.3274747495,"data-quality":0.1778240279,"ml-security":0.1320417365}}
{"text":"We also determine a necessary condition expressed by employing the Walsh transform for a codeword of $\\mathcal{C}$ to belong in the dual.","meta":{"url":"http://arxiv.org/abs/2307.14300v1"},"cats":{"new-dataset":0.0414946175,"dev-research":0.2205704683,"prompt-eng":0.3994893765,"data-quality":0.2554900316,"ml-security":0.1342953175}}
{"text":"This achievement was done in general and when the involved functions are weakly regularly bent.","meta":{"url":"http://arxiv.org/abs/2307.14300v1"},"cats":{"new-dataset":0.0223671907,"dev-research":0.2048725402,"prompt-eng":0.4004085218,"data-quality":0.1666816718,"ml-security":0.0730330174}}
{"text":"We shall give a novel description of the Hull code in the framework of the two generic constructions.","meta":{"url":"http://arxiv.org/abs/2307.14300v1"},"cats":{"new-dataset":0.1963011522,"dev-research":0.2905227562,"prompt-eng":0.3863655577,"data-quality":0.1476825695,"ml-security":0.0832258627}}
{"text":"Our primary interest is constructing linear codes of fixed Hull dimension and determining the (Hamming) weight of the codewords in their duals.","meta":{"url":"http://arxiv.org/abs/2307.14300v1"},"cats":{"new-dataset":0.1441882099,"dev-research":0.2994500371,"prompt-eng":0.3779370674,"data-quality":0.2222293187,"ml-security":0.114639492}}
{"text":"Recommender systems have become indispensable tools in the hotel hospitality industry, enabling personalized and tailored experiences for guests.","meta":{"url":"http://arxiv.org/abs/2307.14298v1"},"cats":{"new-dataset":0.0123393636,"dev-research":0.3035802941,"prompt-eng":0.3996281906,"data-quality":0.1290764572,"ml-security":0.1258209439}}
{"text":"Recent advancements in large language models (LLMs), such as ChatGPT, and persuasive technologies, have opened new avenues for enhancing the effectiveness of those systems.","meta":{"url":"http://arxiv.org/abs/2307.14298v1"},"cats":{"new-dataset":0.0461020875,"dev-research":0.2371785345,"prompt-eng":0.3918873357,"data-quality":0.1679601399,"ml-security":0.1336425242}}
{"text":"This paper explores the potential of integrating ChatGPT and persuasive technologies for automating and improving hotel hospitality recommender systems.","meta":{"url":"http://arxiv.org/abs/2307.14298v1"},"cats":{"new-dataset":0.053055784,"dev-research":0.3609770896,"prompt-eng":0.4299962242,"data-quality":0.1550575704,"ml-security":0.0990687865}}
{"text":"First, we delve into the capabilities of ChatGPT, which can understand and generate human-like text, enabling more accurate and context-aware recommendations.","meta":{"url":"http://arxiv.org/abs/2307.14298v1"},"cats":{"new-dataset":0.3496250922,"dev-research":0.3185073802,"prompt-eng":0.4184606523,"data-quality":0.1952194079,"ml-security":0.0648891374}}
{"text":"We discuss the integration of ChatGPT into recommender systems, highlighting the ability to analyze user preferences, extract valuable insights from online reviews, and generate personalized recommendations based on guest profiles.","meta":{"url":"http://arxiv.org/abs/2307.14298v1"},"cats":{"new-dataset":0.1569753143,"dev-research":0.3521522168,"prompt-eng":0.4293924426,"data-quality":0.1548961022,"ml-security":0.0778902698}}
{"text":"Second, we investigate the role of persuasive technology in influencing user behavior and enhancing the persuasive impact of hotel recommendations.","meta":{"url":"http://arxiv.org/abs/2307.14298v1"},"cats":{"new-dataset":0.0100988392,"dev-research":0.3669159968,"prompt-eng":0.4111131261,"data-quality":0.1316365169,"ml-security":0.167506588}}
{"text":"By incorporating persuasive techniques, such as social proof, scarcity and personalization, recommender systems can effectively influence user decision-making and encourage desired actions, such as booking a specific hotel or upgrading their room.","meta":{"url":"http://arxiv.org/abs/2307.14298v1"},"cats":{"new-dataset":0.006492651,"dev-research":0.3976232621,"prompt-eng":0.4189849001,"data-quality":0.1190144985,"ml-security":0.2562671684}}
{"text":"To investigate the efficacy of ChatGPT and persuasive technologies, we present a pilot experi-ment with a case study involving a hotel recommender system.","meta":{"url":"http://arxiv.org/abs/2307.14298v1"},"cats":{"new-dataset":0.0587501371,"dev-research":0.3657057224,"prompt-eng":0.4157306993,"data-quality":0.1426096197,"ml-security":0.129186077}}
{"text":"We aim to study the impact of integrating ChatGPT and persua-sive techniques on user engagement, satisfaction, and conversion rates.","meta":{"url":"http://arxiv.org/abs/2307.14298v1"},"cats":{"new-dataset":0.0624992212,"dev-research":0.3305909065,"prompt-eng":0.4212477954,"data-quality":0.1669275646,"ml-security":0.0528016975}}
{"text":"The preliminary results demonstrate the potential of these technologies in enhancing the overall guest experience and business performance.","meta":{"url":"http://arxiv.org/abs/2307.14298v1"},"cats":{"new-dataset":0.0189845221,"dev-research":0.3180011974,"prompt-eng":0.403185253,"data-quality":0.0968104965,"ml-security":0.056911588}}
{"text":"Overall, this paper contributes to the field of hotel hospitality by exploring the synergistic relationship between LLMs and persuasive technology in recommender systems, ultimately influencing guest satisfaction and hotel revenue.","meta":{"url":"http://arxiv.org/abs/2307.14298v1"},"cats":{"new-dataset":0.0119953926,"dev-research":0.2749201705,"prompt-eng":0.3975288982,"data-quality":0.1446136071,"ml-security":0.1161556856}}
{"text":"Splitting of sequential data, such as videos and time series, is an essential step in various data analysis tasks, including object tracking and anomaly detection.","meta":{"url":"http://arxiv.org/abs/2307.14294v1"},"cats":{"new-dataset":0.1575328776,"dev-research":0.2860979201,"prompt-eng":0.3586297615,"data-quality":0.1563799196,"ml-security":0.1382534771}}
{"text":"However, splitting sequential data presents a variety of challenges that can impact the accuracy and reliability of subsequent analyses.","meta":{"url":"http://arxiv.org/abs/2307.14294v1"},"cats":{"new-dataset":0.1292387523,"dev-research":0.286207255,"prompt-eng":0.3686857907,"data-quality":0.2011756089,"ml-security":0.1106617244}}
{"text":"This concept article examines the challenges associated with splitting sequential data, including data acquisition, data representation, split ratio selection, setting up quality criteria, and choosing suitable selection strategies.","meta":{"url":"http://arxiv.org/abs/2307.14294v1"},"cats":{"new-dataset":0.1970831414,"dev-research":0.2675056337,"prompt-eng":0.3983718421,"data-quality":0.1564049842,"ml-security":0.0863421394}}
{"text":"We explore these challenges through two real-world examples: motor test benches and particle tracking in liquids.","meta":{"url":"http://arxiv.org/abs/2307.14294v1"},"cats":{"new-dataset":0.0428071917,"dev-research":0.2449274847,"prompt-eng":0.3910770128,"data-quality":0.1364261271,"ml-security":0.1211587859}}
{"text":"A proof-labeling scheme (PLS) for a boolean predicate $\\Pi$ on labeled graphs is a mechanism used for certifying the legality with respect to $\\Pi$ of global network states in a distributed manner.","meta":{"url":"http://arxiv.org/abs/2307.14292v1"},"cats":{"new-dataset":0.0364107997,"dev-research":0.3035304251,"prompt-eng":0.4112954374,"data-quality":0.3178051752,"ml-security":0.1982752185}}
{"text":"In a PLS, a certificate is assigned to each processing node of the network, and the nodes are in charge of checking that the collection of certificates forms a global proof that the system is in a correct state, by exchanging the certificates once, between neighbors only.","meta":{"url":"http://arxiv.org/abs/2307.14292v1"},"cats":{"new-dataset":0.0232559404,"dev-research":0.2610655609,"prompt-eng":0.3882786089,"data-quality":0.1661574588,"ml-security":0.1865185558}}
{"text":"The main measure of complexity is the size of the certificates.","meta":{"url":"http://arxiv.org/abs/2307.14292v1"},"cats":{"new-dataset":0.0185524074,"dev-research":0.289326809,"prompt-eng":0.3029200048,"data-quality":0.087483746,"ml-security":0.0756999391}}
{"text":"Many PLSs have been designed for certifying specific predicates, including cycle-freeness, minimum-weight spanning tree, planarity, etc.   ","meta":{"url":"http://arxiv.org/abs/2307.14292v1"},"cats":{"new-dataset":0.0296051593,"dev-research":0.2752223334,"prompt-eng":0.4223004828,"data-quality":0.1420679524,"ml-security":0.1041212396}}
{"text":"In 2021, a breakthrough has been obtained, as a meta-theorem stating that a large set of properties have compact PLSs in a large class of networks.","meta":{"url":"http://arxiv.org/abs/2307.14292v1"},"cats":{"new-dataset":0.1325677648,"dev-research":0.1767698056,"prompt-eng":0.325054939,"data-quality":0.1401122907,"ml-security":0.142401375}}
{"text":"Namely, for every $\\mathrm{MSO}_2$ property $\\Pi$ on labeled graphs, there exists a PLS for $\\Pi$ with $O(\\log n)$-bit certificates for all graphs of bounded tree-depth.","meta":{"url":"http://arxiv.org/abs/2307.14292v1"},"cats":{"new-dataset":0.0708162385,"dev-research":0.227948221,"prompt-eng":0.3583033737,"data-quality":0.2335285714,"ml-security":0.1779302687}}
{"text":"This result has been extended to the larger class of graphs with bounded {tree-width}, using certificates on $O(\\log^2 n)$ bits.   ","meta":{"url":"http://arxiv.org/abs/2307.14292v1"},"cats":{"new-dataset":0.2248501291,"dev-research":0.1950210131,"prompt-eng":0.3324724144,"data-quality":0.2242439094,"ml-security":0.1576205279}}
{"text":"We extend this result even further, to the larger class of graphs with bounded clique-width, which, as opposed to the other two aforementioned classes, includes dense graphs.","meta":{"url":"http://arxiv.org/abs/2307.14292v1"},"cats":{"new-dataset":0.1332339878,"dev-research":0.2012286258,"prompt-eng":0.30673628,"data-quality":0.2113400087,"ml-security":0.1337465987}}
{"text":"We show that, for every $\\mathrm{MSO}_1$ property $\\Pi$ on labeled graphs, there exists a PLS for $\\Pi$ with $O(\\log^2 n)$ bit certificates for all graphs of bounded clique-width.","meta":{"url":"http://arxiv.org/abs/2307.14292v1"},"cats":{"new-dataset":0.1404261617,"dev-research":0.2256670458,"prompt-eng":0.3510476135,"data-quality":0.1968216318,"ml-security":0.1758408977}}
{"text":"We take as a case study the spread of Germanic syntactic features into Romance dialects of North-Eastern Italy, which occurred after the immigration of German people in the Tyrol during the High Middle Ages.   ","meta":{"url":"http://arxiv.org/abs/2307.14291v1"},"cats":{"new-dataset":0.168687888,"dev-research":0.2423752234,"prompt-eng":0.3648741346,"data-quality":0.2202277267,"ml-security":0.1383270882}}
{"text":"An interactive map is produced using tools of what is called Geographic Data Science.","meta":{"url":"http://arxiv.org/abs/2307.14291v1"},"cats":{"new-dataset":0.3056039701,"dev-research":0.4077171259,"prompt-eng":0.4092217963,"data-quality":0.1295917385,"ml-security":0.0522603071}}
{"text":"A smooth two-dimensional surface $\\mathcal{G}$ expresses locally which fraction of territory uses a given German language feature: it is obtained by interpolating a discrete function that says if at any surveyed locality that feature is used or not.\\newline   This surface $\\mathcal{G}$ is thought of as the value at the present time of a function describing a diffusion-convection phenomenon in two dimensions (here said \\emph{tidal} mode), which is subjected in a very natural way to the same equation, suitably contextualized, used in physics for a number of phenomenological facts like the heat diffusion.","meta":{"url":"http://arxiv.org/abs/2307.14291v1"},"cats":{"new-dataset":0.0830432678,"dev-research":0.2171293626,"prompt-eng":0.3144584444,"data-quality":0.1232530091,"ml-security":0.1038331315}}
{"text":"It is shown that solutions of this equation, evaluated at the present time, fit well with the data as interpolated by $\\mathcal{G}$, thus providing convincing pictures of diffusion-convection of the linguistic features of the case study, albeit simplifications and approximations.\\newline   Very importantly, it is shown that Schmidt's 'waves' can be counted among the solutions of the diffusion equation: superimposing Schmidt 'waves' to a 'tidal flooding' can reproduce complexities of real linguistic diffusion events.","meta":{"url":"http://arxiv.org/abs/2307.14291v1"},"cats":{"new-dataset":0.1583436319,"dev-research":0.2097939718,"prompt-eng":0.3198346025,"data-quality":0.1917672755,"ml-security":0.1274199248}}
{"text":"We introduce and study the cumulative information generating function, which provides a unifying mathematical tool suitable to deal with classical and fractional entropies based on the cumulative distribution function and on the survival function.","meta":{"url":"http://arxiv.org/abs/2307.14290v1"},"cats":{"new-dataset":0.0966284484,"dev-research":0.1783149326,"prompt-eng":0.4410082373,"data-quality":0.1442142095,"ml-security":0.1169197827}}
{"text":"Specifically, after establishing its main properties and some bounds, we show that it is a variability measure itself that extends the Gini mean semi-difference.","meta":{"url":"http://arxiv.org/abs/2307.14290v1"},"cats":{"new-dataset":0.0759556705,"dev-research":0.200453029,"prompt-eng":0.4017247537,"data-quality":0.1528356083,"ml-security":0.0674930041}}
{"text":"We also provide (i) an extension of such a measure, based on distortion functions, and (ii) a weighted version based on a mixture distribution.","meta":{"url":"http://arxiv.org/abs/2307.14290v1"},"cats":{"new-dataset":0.0838460442,"dev-research":0.1397424761,"prompt-eng":0.4175500026,"data-quality":0.2090945944,"ml-security":0.0745883527}}
{"text":"Furthermore, we explore some connections with the reliability of $k$-out-of-$n$ systems and with stress-strength models for multi-component systems.","meta":{"url":"http://arxiv.org/abs/2307.14290v1"},"cats":{"new-dataset":0.0372795189,"dev-research":0.2076611404,"prompt-eng":0.3942457653,"data-quality":0.1841393822,"ml-security":0.1842896276}}
{"text":"Also, we address the problem of extending the cumulative information generating function to higher dimensions.","meta":{"url":"http://arxiv.org/abs/2307.14290v1"},"cats":{"new-dataset":0.1013539213,"dev-research":0.2253025927,"prompt-eng":0.4250586864,"data-quality":0.1695455591,"ml-security":0.1222776917}}
{"text":"The study and development of innovative solutions for the advanced visualisation, representation and analysis of medical images offer different research directions.","meta":{"url":"http://arxiv.org/abs/2307.14288v1"},"cats":{"new-dataset":0.1200396985,"dev-research":0.3072485051,"prompt-eng":0.3306338675,"data-quality":0.0635774245,"ml-security":0.0609362506}}
{"text":"Current practice in medical imaging consists in combining real-time US with imaging modalities that allow internal anatomy acquisitions, such as CT, MRI, PET or similar.","meta":{"url":"http://arxiv.org/abs/2307.14288v1"},"cats":{"new-dataset":0.0337561263,"dev-research":0.2694877231,"prompt-eng":0.3462853919,"data-quality":0.0730918192,"ml-security":0.0608234898}}
{"text":"Application of image-fusion approaches can be found in tracking surgical tools and/or needles, in real-time during interventions.","meta":{"url":"http://arxiv.org/abs/2307.14288v1"},"cats":{"new-dataset":0.0265377247,"dev-research":0.2723480013,"prompt-eng":0.3832971138,"data-quality":0.084025869,"ml-security":0.0555070298}}
{"text":"Thus, this work proposes a fusion imaging system for the registration of CT and MRI images with real-time US acquisition leveraging a 3D camera sensor.","meta":{"url":"http://arxiv.org/abs/2307.14288v1"},"cats":{"new-dataset":0.1801148772,"dev-research":0.2438897958,"prompt-eng":0.4223171374,"data-quality":0.0939842683,"ml-security":0.0488995605}}
{"text":"The main focus of the work is the portability of the system and its applicability to different anatomical districts.","meta":{"url":"http://arxiv.org/abs/2307.14288v1"},"cats":{"new-dataset":0.030946829,"dev-research":0.2587657838,"prompt-eng":0.3413362996,"data-quality":0.0690541687,"ml-security":0.060335996}}
{"text":"Stream processing has become a critical component in the architecture of modern applications.","meta":{"url":"http://arxiv.org/abs/2307.14287v1"},"cats":{"new-dataset":0.032422013,"dev-research":0.3405916806,"prompt-eng":0.3366387349,"data-quality":0.1692293239,"ml-security":0.1051974899}}
{"text":"With the exponential growth of data generation from sources such as the Internet of Things, business intelligence, and telecommunications, real-time processing of unbounded data streams has become a necessity.","meta":{"url":"http://arxiv.org/abs/2307.14287v1"},"cats":{"new-dataset":0.1243079489,"dev-research":0.2637247714,"prompt-eng":0.3042289145,"data-quality":0.0851375918,"ml-security":0.1277062457}}
{"text":"DSP systems provide a solution to this challenge, offering high horizontal scalability, fault-tolerant execution, and the ability to process data streams from multiple sources in a single DSP job.","meta":{"url":"http://arxiv.org/abs/2307.14287v1"},"cats":{"new-dataset":0.1356345927,"dev-research":0.3399371118,"prompt-eng":0.4307924643,"data-quality":0.1838940059,"ml-security":0.1561396007}}
{"text":"Often enough though, data streams need to be enriched with extra information for correct processing, which introduces additional dependencies and potential bottlenecks.   ","meta":{"url":"http://arxiv.org/abs/2307.14287v1"},"cats":{"new-dataset":0.0313101579,"dev-research":0.394872532,"prompt-eng":0.3653028605,"data-quality":0.2405959754,"ml-security":0.1398254636}}
{"text":"In this paper, we present an in-depth evaluation of data enrichment methods for DSP systems and identify the different use cases for stream processing in modern systems.","meta":{"url":"http://arxiv.org/abs/2307.14287v1"},"cats":{"new-dataset":0.0949099678,"dev-research":0.3771404297,"prompt-eng":0.4082548742,"data-quality":0.220926544,"ml-security":0.1305090927}}
{"text":"Using a representative DSP system and conducting the evaluation in a realistic cloud environment, we found that outsourcing enrichment data to the DSP system can improve performance for specific use cases.","meta":{"url":"http://arxiv.org/abs/2307.14287v1"},"cats":{"new-dataset":0.2614622931,"dev-research":0.3212445402,"prompt-eng":0.3972540854,"data-quality":0.1730941493,"ml-security":0.1891295853}}
{"text":"However, this increased resource consumption highlights the need for stream processing solutions specifically designed for the performance-intensive workloads of cloud-based applications.","meta":{"url":"http://arxiv.org/abs/2307.14287v1"},"cats":{"new-dataset":0.0604251865,"dev-research":0.3344448072,"prompt-eng":0.34606783,"data-quality":0.0816535922,"ml-security":0.098989017}}
{"text":"Most applications of Artificial Intelligence (AI) are designed for a confined and specific task.","meta":{"url":"http://arxiv.org/abs/2307.14283v1"},"cats":{"new-dataset":0.0111688557,"dev-research":0.3041664835,"prompt-eng":0.3288345025,"data-quality":0.0822411984,"ml-security":0.2749693615}}
{"text":"However, there are many scenarios that call for a more general AI, capable of solving a wide array of tasks without being specifically designed for them.","meta":{"url":"http://arxiv.org/abs/2307.14283v1"},"cats":{"new-dataset":0.0089924756,"dev-research":0.2761489295,"prompt-eng":0.3296174774,"data-quality":0.0583539743,"ml-security":0.1852367684}}
{"text":"The term General-Purpose Artificial Intelligence Systems (GPAIS) has been defined to refer to these AI systems.","meta":{"url":"http://arxiv.org/abs/2307.14283v1"},"cats":{"new-dataset":0.0351894716,"dev-research":0.2852178636,"prompt-eng":0.3159802098,"data-quality":0.1064861475,"ml-security":0.1645695686}}
{"text":"To date, the possibility of an Artificial General Intelligence, powerful enough to perform any intellectual task as if it were human, or even improve it, has remained an aspiration, fiction, and considered a risk for our society.","meta":{"url":"http://arxiv.org/abs/2307.14283v1"},"cats":{"new-dataset":0.0154735531,"dev-research":0.2435506226,"prompt-eng":0.3358267896,"data-quality":0.0780938665,"ml-security":0.2698907623}}
{"text":"Whilst we might still be far from achieving that, GPAIS is a reality and sitting at the forefront of AI research.   ","meta":{"url":"http://arxiv.org/abs/2307.14283v1"},"cats":{"new-dataset":0.0213600899,"dev-research":0.2394219527,"prompt-eng":0.3017314244,"data-quality":0.0920167227,"ml-security":0.1453026504}}
{"text":"This work discusses existing definitions for GPAIS and proposes a new definition that allows for a gradual differentiation among types of GPAIS according to their properties and limitations.","meta":{"url":"http://arxiv.org/abs/2307.14283v1"},"cats":{"new-dataset":0.0775820593,"dev-research":0.2148459236,"prompt-eng":0.3833313793,"data-quality":0.1439899033,"ml-security":0.0909494604}}
{"text":"We distinguish between closed-world and open-world GPAIS, characterising their degree of autonomy and ability based on several factors such as adaptation to new tasks, competence in domains not intentionally trained for, ability to learn from few data, or proactive acknowledgment of their own limitations.","meta":{"url":"http://arxiv.org/abs/2307.14283v1"},"cats":{"new-dataset":0.0904469683,"dev-research":0.2231693483,"prompt-eng":0.3626736406,"data-quality":0.1223743257,"ml-security":0.1638742287}}
{"text":"We then propose a taxonomy of approaches to realise GPAIS, describing research trends such as the use of AI techniques to improve another AI or foundation models.","meta":{"url":"http://arxiv.org/abs/2307.14283v1"},"cats":{"new-dataset":0.0500564056,"dev-research":0.2969737561,"prompt-eng":0.3455142046,"data-quality":0.1203298499,"ml-security":0.1144971114}}
{"text":"As a prime example, we delve into generative AI, aligning them with the terms and concepts presented in the taxonomy.","meta":{"url":"http://arxiv.org/abs/2307.14283v1"},"cats":{"new-dataset":0.0292379965,"dev-research":0.3078565062,"prompt-eng":0.3884171618,"data-quality":0.2394217339,"ml-security":0.1204174082}}
{"text":"Through the proposed definition and taxonomy, our aim is to facilitate research collaboration across different areas that are tackling general-purpose tasks, as they share many common aspects.","meta":{"url":"http://arxiv.org/abs/2307.14283v1"},"cats":{"new-dataset":0.1624223462,"dev-research":0.3728037525,"prompt-eng":0.4472217603,"data-quality":0.1187501687,"ml-security":0.0486364496}}
{"text":"Finally, we discuss the current state of GPAIS, its challenges and prospects, implications for our society, and the need for responsible and trustworthy AI systems and regulation, with the goal of providing a holistic view of GPAIS.","meta":{"url":"http://arxiv.org/abs/2307.14283v1"},"cats":{"new-dataset":0.200487813,"dev-research":0.2320372161,"prompt-eng":0.368595384,"data-quality":0.1476577245,"ml-security":0.2278823309}}
{"text":"Sequences with low aperiodic autocorrelation are used in communications and remote sensing for synchronization and ranging.","meta":{"url":"http://arxiv.org/abs/2307.14281v1"},"cats":{"new-dataset":0.1930956461,"dev-research":0.1955828814,"prompt-eng":0.4079518719,"data-quality":0.1354806596,"ml-security":0.0569596193}}
{"text":"The autocorrelation demerit factor of a sequence is the sum of the squared magnitudes of its autocorrelation values at every nonzero shift when we normalize the sequence to have unit Euclidean length.","meta":{"url":"http://arxiv.org/abs/2307.14281v1"},"cats":{"new-dataset":0.0746094864,"dev-research":0.2547886579,"prompt-eng":0.3105767857,"data-quality":0.150855613,"ml-security":0.0935294814}}
{"text":"The merit factor, introduced by Golay, is the reciprocal of the demerit factor.","meta":{"url":"http://arxiv.org/abs/2307.14281v1"},"cats":{"new-dataset":0.0111930007,"dev-research":0.2230326502,"prompt-eng":0.379642892,"data-quality":0.1073721646,"ml-security":0.0969998639}}
{"text":"We consider the uniform probability measure on the $2^\\ell$ binary sequences of length $\\ell$ and investigate the distribution of the demerit factors of these sequences.","meta":{"url":"http://arxiv.org/abs/2307.14281v1"},"cats":{"new-dataset":0.4460053302,"dev-research":0.174615309,"prompt-eng":0.3938398574,"data-quality":0.1609042854,"ml-security":0.1443023763}}
{"text":"Previous researchers have calculated the mean and variance of this distribution.","meta":{"url":"http://arxiv.org/abs/2307.14281v1"},"cats":{"new-dataset":0.0771497593,"dev-research":0.1709980182,"prompt-eng":0.3643731188,"data-quality":0.0941466729,"ml-security":0.0437172939}}
{"text":"We develop new combinatorial techniques to calculate the $p$th central moment of the demerit factor for binary sequences of length $\\ell$. These techniques prove that for $p\\geq 2$ and $\\ell \\geq 4$, all the central moments are strictly positive.","meta":{"url":"http://arxiv.org/abs/2307.14281v1"},"cats":{"new-dataset":0.1811531878,"dev-research":0.2038619442,"prompt-eng":0.3660778594,"data-quality":0.1208961462,"ml-security":0.0917380956}}
{"text":"For any given $p$, one may use the technique to obtain an exact formula for the $p$th central moment of the demerit factor as a function of the length $\\ell$. The previously obtained formula for variance is confirmed by our technique with a short calculation, and we demonstrate that our techniques go beyond this by also deriving an exact formula for the skewness.","meta":{"url":"http://arxiv.org/abs/2307.14281v1"},"cats":{"new-dataset":0.0563420881,"dev-research":0.1816907057,"prompt-eng":0.3902446396,"data-quality":0.1261470819,"ml-security":0.0699625699}}
{"text":"With the advent of standards for deterministic network behavior, synthesizing network designs under delay constraints becomes the natural next task to tackle.","meta":{"url":"http://arxiv.org/abs/2307.14280v1"},"cats":{"new-dataset":0.0371531135,"dev-research":0.3391257637,"prompt-eng":0.4489359927,"data-quality":0.1061171776,"ml-security":0.2376255154}}
{"text":"Network Calculus (NC) has become a key method for validating industrial networks, as it computes formally verified end-to-end delay bounds.","meta":{"url":"http://arxiv.org/abs/2307.14280v1"},"cats":{"new-dataset":0.0157910695,"dev-research":0.3675328741,"prompt-eng":0.351477116,"data-quality":0.1668458057,"ml-security":0.1466415354}}
{"text":"However, analyses from the NC framework have been designed to bound the delay of one flow at a time.","meta":{"url":"http://arxiv.org/abs/2307.14280v1"},"cats":{"new-dataset":0.0262901725,"dev-research":0.2777947784,"prompt-eng":0.3562279527,"data-quality":0.077449413,"ml-security":0.0827312817}}
{"text":"Attempts to use classical analyses to derive a network configuration have shown that this approach is poorly suited to practical use cases.","meta":{"url":"http://arxiv.org/abs/2307.14280v1"},"cats":{"new-dataset":0.0071285148,"dev-research":0.319092324,"prompt-eng":0.3328302746,"data-quality":0.1965340035,"ml-security":0.1522461246}}
{"text":"Consider finding a delay-optimal routing configuration: one model had to be created for each routing alternative, then each flow delay had to be bounded, and then the bounds had to be compared to the given constraints.","meta":{"url":"http://arxiv.org/abs/2307.14280v1"},"cats":{"new-dataset":0.0086920074,"dev-research":0.1737796243,"prompt-eng":0.3536320067,"data-quality":0.0675769809,"ml-security":0.1149314319}}
{"text":"To overcome this three-step process, we introduce Differential Network Calculus.","meta":{"url":"http://arxiv.org/abs/2307.14280v1"},"cats":{"new-dataset":0.0233905195,"dev-research":0.2660674702,"prompt-eng":0.339181789,"data-quality":0.126066858,"ml-security":0.181006574}}
{"text":"We extend NC to allow the differentiation of delay bounds w.r.t.","meta":{"url":"http://arxiv.org/abs/2307.14280v1"},"cats":{"new-dataset":0.0611086649,"dev-research":0.2490896281,"prompt-eng":0.3979068949,"data-quality":0.1353599071,"ml-security":0.1331847278}}
{"text":"to a wide range of network parameters - such as flow paths or priority.","meta":{"url":"http://arxiv.org/abs/2307.14280v1"},"cats":{"new-dataset":0.0166339745,"dev-research":0.3414261465,"prompt-eng":0.4296788827,"data-quality":0.0723321386,"ml-security":0.1359847992}}
{"text":"This opens up NC to a class of efficient nonlinear optimization techniques that exploit the gradient of the delay bound.","meta":{"url":"http://arxiv.org/abs/2307.14280v1"},"cats":{"new-dataset":0.0285951459,"dev-research":0.2280644152,"prompt-eng":0.3695345984,"data-quality":0.1124263404,"ml-security":0.1101106607}}
{"text":"Our numerical evaluation on the routing and priority assignment problem shows that our novel method can synthesize flow paths and priorities in a matter of seconds, outperforming existing methods by several orders of magnitude.","meta":{"url":"http://arxiv.org/abs/2307.14280v1"},"cats":{"new-dataset":0.0323141929,"dev-research":0.30527463,"prompt-eng":0.4432112896,"data-quality":0.0984180041,"ml-security":0.0975951362}}
{"text":"Fully-unsupervised Person and Vehicle Re-Identification have received increasing attention due to their broad applicability in surveillance, forensics, event understanding, and smart cities, without requiring any manual annotation.","meta":{"url":"http://arxiv.org/abs/2307.14278v1"},"cats":{"new-dataset":0.217783068,"dev-research":0.2140322532,"prompt-eng":0.4299418446,"data-quality":0.1933714382,"ml-security":0.2015893117}}
{"text":"However, most of the prior art has been evaluated in datasets that have just a couple thousand samples.","meta":{"url":"http://arxiv.org/abs/2307.14278v1"},"cats":{"new-dataset":0.2711697315,"dev-research":0.2215651503,"prompt-eng":0.3082580884,"data-quality":0.2141829196,"ml-security":0.0908798518}}
{"text":"Such small-data setups often allow the use of costly techniques in time and memory footprints, such as Re-Ranking, to improve clustering results.","meta":{"url":"http://arxiv.org/abs/2307.14278v1"},"cats":{"new-dataset":0.0273351659,"dev-research":0.3761277264,"prompt-eng":0.3559721725,"data-quality":0.1558111808,"ml-security":0.0852409934}}
{"text":"Moreover, some previous work even pre-selects the best clustering hyper-parameters for each dataset, which is unrealistic in a large-scale fully-unsupervised scenario.","meta":{"url":"http://arxiv.org/abs/2307.14278v1"},"cats":{"new-dataset":0.1470546896,"dev-research":0.2035469793,"prompt-eng":0.3984370354,"data-quality":0.2044234094,"ml-security":0.134475281}}
{"text":"In this context, this work tackles a more realistic scenario and proposes two strategies to learn from large-scale unlabeled data.","meta":{"url":"http://arxiv.org/abs/2307.14278v1"},"cats":{"new-dataset":0.3951749433,"dev-research":0.2522962235,"prompt-eng":0.4150752184,"data-quality":0.3483430386,"ml-security":0.1375588771}}
{"text":"The first strategy performs a local neighborhood sampling to reduce the dataset size in each iteration without violating neighborhood relationships.","meta":{"url":"http://arxiv.org/abs/2307.14278v1"},"cats":{"new-dataset":0.1679101256,"dev-research":0.2562581631,"prompt-eng":0.3384432931,"data-quality":0.1629513695,"ml-security":0.1282435978}}
{"text":"A second strategy leverages a novel Re-Ranking technique, which has a lower time upper bound complexity and reduces the memory complexity from O(n^2) to O(kn) with k << n. To avoid the pre-selection of specific hyper-parameter values for the clustering algorithm, we also present a novel scheduling algorithm that adjusts the density parameter during training, to leverage the diversity of samples and keep the learning robust to noisy labeling.","meta":{"url":"http://arxiv.org/abs/2307.14278v1"},"cats":{"new-dataset":0.1066607285,"dev-research":0.2682261818,"prompt-eng":0.3877143077,"data-quality":0.3377989726,"ml-security":0.1036683969}}
{"text":"Finally, due to the complementary knowledge learned by different models, we also introduce a co-training strategy that relies upon the permutation of predicted pseudo-labels, among the backbones, with no need for any hyper-parameters or weighting optimization.","meta":{"url":"http://arxiv.org/abs/2307.14278v1"},"cats":{"new-dataset":0.0826384303,"dev-research":0.1749464048,"prompt-eng":0.4450811522,"data-quality":0.2902415476,"ml-security":0.2167212551}}
{"text":"The proposed methodology outperforms the state-of-the-art methods in well-known benchmarks and in the challenging large-scale Veri-Wild dataset, with a faster and memory-efficient Re-Ranking strategy, and a large-scale, noisy-robust, and ensemble-based learning approach.","meta":{"url":"http://arxiv.org/abs/2307.14278v1"},"cats":{"new-dataset":0.5363062403,"dev-research":0.2075688067,"prompt-eng":0.3697584789,"data-quality":0.2696846324,"ml-security":0.1175670657}}
{"text":"The recent video grounding works attempt to introduce vanilla contrastive learning into video grounding.","meta":{"url":"http://arxiv.org/abs/2307.14277v1"},"cats":{"new-dataset":0.1702851816,"dev-research":0.2750990642,"prompt-eng":0.3762653047,"data-quality":0.2167507362,"ml-security":0.1199479768}}
{"text":"However, we claim that this naive solution is suboptimal.","meta":{"url":"http://arxiv.org/abs/2307.14277v1"},"cats":{"new-dataset":0.0159482494,"dev-research":0.2671769137,"prompt-eng":0.2975153118,"data-quality":0.2168622056,"ml-security":0.1691612165}}
{"text":"Contrastive learning requires two key properties: (1) \\emph{alignment} of features of similar samples, and (2) \\emph{uniformity} of the induced distribution of the normalized features on the hypersphere.","meta":{"url":"http://arxiv.org/abs/2307.14277v1"},"cats":{"new-dataset":0.0214156544,"dev-research":0.1694346469,"prompt-eng":0.312028212,"data-quality":0.2413885594,"ml-security":0.1225944397}}
{"text":"Due to two annoying issues in video grounding: (1) the co-existence of some visual entities in both ground truth and other moments, \\ie semantic overlapping; (2) only a few moments in the video are annotated, \\ie sparse annotation dilemma, vanilla contrastive learning is unable to model the correlations between temporally distant moments and learned inconsistent video representations.","meta":{"url":"http://arxiv.org/abs/2307.14277v1"},"cats":{"new-dataset":0.1321915948,"dev-research":0.2456179713,"prompt-eng":0.3416150613,"data-quality":0.3882687568,"ml-security":0.1025880046}}
{"text":"Both characteristics lead to vanilla contrastive learning being unsuitable for video grounding.","meta":{"url":"http://arxiv.org/abs/2307.14277v1"},"cats":{"new-dataset":0.0132782709,"dev-research":0.2942523047,"prompt-eng":0.307405019,"data-quality":0.2299809168,"ml-security":0.1998435208}}
{"text":"In this paper, we introduce Geodesic and Game Localization (G2L), a semantically aligned and uniform video grounding framework via geodesic and game theory.","meta":{"url":"http://arxiv.org/abs/2307.14277v1"},"cats":{"new-dataset":0.1905338942,"dev-research":0.282057768,"prompt-eng":0.4010810293,"data-quality":0.1980105272,"ml-security":0.0907916898}}
{"text":"We quantify the correlations among moments leveraging the geodesic distance that guides the model to learn the correct cross-modal representations.","meta":{"url":"http://arxiv.org/abs/2307.14277v1"},"cats":{"new-dataset":0.0611551932,"dev-research":0.1600984768,"prompt-eng":0.4063138691,"data-quality":0.1815494778,"ml-security":0.0649008713}}
{"text":"Furthermore, from the novel perspective of game theory, we propose semantic Shapley interaction based on geodesic distance sampling to learn fine-grained semantic alignment in similar moments.","meta":{"url":"http://arxiv.org/abs/2307.14277v1"},"cats":{"new-dataset":0.2067264978,"dev-research":0.2144026879,"prompt-eng":0.4040322353,"data-quality":0.210578724,"ml-security":0.0843019696}}
{"text":"Experiments on three benchmarks demonstrate the effectiveness of our method.","meta":{"url":"http://arxiv.org/abs/2307.14277v1"},"cats":{"new-dataset":0.0115978418,"dev-research":0.2996918525,"prompt-eng":0.4414610921,"data-quality":0.2025637762,"ml-security":0.0627860098}}
{"text":"Object pushing presents a key non-prehensile manipulation problem that is illustrative of more complex robotic manipulation tasks.","meta":{"url":"http://arxiv.org/abs/2307.14272v1"},"cats":{"new-dataset":0.0233602764,"dev-research":0.259320688,"prompt-eng":0.4128000671,"data-quality":0.0848357778,"ml-security":0.1007817495}}
{"text":"While deep reinforcement learning (RL) methods have demonstrated impressive learning capabilities using visual input, a lack of tactile sensing limits their capability for fine and reliable control during manipulation.","meta":{"url":"http://arxiv.org/abs/2307.14272v1"},"cats":{"new-dataset":0.0449391224,"dev-research":0.2246484666,"prompt-eng":0.3780949794,"data-quality":0.1035033145,"ml-security":0.2121937767}}
{"text":"Here we propose a deep RL approach to object pushing using tactile sensing without visual input, namely tactile pushing.","meta":{"url":"http://arxiv.org/abs/2307.14272v1"},"cats":{"new-dataset":0.1102953902,"dev-research":0.2428425927,"prompt-eng":0.4426344186,"data-quality":0.1010847722,"ml-security":0.1181224665}}
{"text":"We present a goal-conditioned formulation that allows both model-free and model-based RL to obtain accurate policies for pushing an object to a goal.","meta":{"url":"http://arxiv.org/abs/2307.14272v1"},"cats":{"new-dataset":0.0301692845,"dev-research":0.2518546207,"prompt-eng":0.5063730366,"data-quality":0.0862505011,"ml-security":0.120991236}}
{"text":"To achieve real-world performance, we adopt a sim-to-real approach.","meta":{"url":"http://arxiv.org/abs/2307.14272v1"},"cats":{"new-dataset":0.0555234632,"dev-research":0.3395152647,"prompt-eng":0.3743721795,"data-quality":0.0786250356,"ml-security":0.0726756733}}
{"text":"Our results demonstrate that it is possible to train on a single object and a limited sample of goals to produce precise and reliable policies that can generalize to a variety of unseen objects and pushing scenarios without domain randomization.","meta":{"url":"http://arxiv.org/abs/2307.14272v1"},"cats":{"new-dataset":0.144658684,"dev-research":0.2345673027,"prompt-eng":0.456489715,"data-quality":0.170519232,"ml-security":0.3556406454}}
{"text":"We experiment with the trained agents in harsh pushing conditions, and show that with significantly more training samples, a model-free policy can outperform a model-based planner, generating shorter and more reliable pushing trajectories despite large disturbances.","meta":{"url":"http://arxiv.org/abs/2307.14272v1"},"cats":{"new-dataset":0.0856882223,"dev-research":0.2207236101,"prompt-eng":0.3997001793,"data-quality":0.0654684823,"ml-security":0.2266674983}}
{"text":"The simplicity of our training environment and effective real-world performance highlights the value of rich tactile information for fine manipulation.","meta":{"url":"http://arxiv.org/abs/2307.14272v1"},"cats":{"new-dataset":0.0846456589,"dev-research":0.3472234327,"prompt-eng":0.4479563332,"data-quality":0.1370926045,"ml-security":0.2140730246}}
{"text":"Code and videos are available at https://sites.google.com/view/tactile-rl-pushing/.","meta":{"url":"http://arxiv.org/abs/2307.14272v1"},"cats":{"new-dataset":0.5041885696,"dev-research":0.3400773793,"prompt-eng":0.409829443,"data-quality":0.1296753695,"ml-security":0.1015221927}}
{"text":"The Internet of Things (IoT) is growing rapidly and so the need of ensuring protection against cybersecurity attacks to IoT devices.","meta":{"url":"http://arxiv.org/abs/2307.14268v1"},"cats":{"new-dataset":0.0576918111,"dev-research":0.300866979,"prompt-eng":0.3582338778,"data-quality":0.0628531042,"ml-security":0.5057431311}}
{"text":"In this scenario, Intrusion Detection Systems (IDSs) play a crucial role and data-driven IDSs based on machine learning (ML) have recently attracted more and more interest by the research community.","meta":{"url":"http://arxiv.org/abs/2307.14268v1"},"cats":{"new-dataset":0.1219254748,"dev-research":0.330863196,"prompt-eng":0.352919366,"data-quality":0.1734228127,"ml-security":0.6117135761}}
{"text":"While conventional ML-based IDSs are based on a centralized architecture where IoT devices share their data with a central server for model training, we propose a novel approach that is based on federated learning (FL).","meta":{"url":"http://arxiv.org/abs/2307.14268v1"},"cats":{"new-dataset":0.1400996726,"dev-research":0.251969789,"prompt-eng":0.3815415306,"data-quality":0.1541878674,"ml-security":0.3207877501}}
{"text":"However, conventional FL is ineffective in the considered scenario, due to the high statistical heterogeneity of data collected by IoT devices.","meta":{"url":"http://arxiv.org/abs/2307.14268v1"},"cats":{"new-dataset":0.0299974091,"dev-research":0.2003400912,"prompt-eng":0.3814786746,"data-quality":0.1861453403,"ml-security":0.160120261}}
{"text":"To overcome this limitation, we propose a three-tier FL-based architecture where IoT devices are clustered together based on their statistical properties.","meta":{"url":"http://arxiv.org/abs/2307.14268v1"},"cats":{"new-dataset":0.138090488,"dev-research":0.2163528883,"prompt-eng":0.4282595343,"data-quality":0.0756179859,"ml-security":0.095413544}}
{"text":"Clustering decisions are taken by means of a novel entropy-based strategy, which helps improve model training performance.","meta":{"url":"http://arxiv.org/abs/2307.14268v1"},"cats":{"new-dataset":0.0111472453,"dev-research":0.2898704904,"prompt-eng":0.4157665473,"data-quality":0.1653662745,"ml-security":0.1565171886}}
{"text":"We tested our solution on the CIC-ToN-IoT dataset: our clustering strategy increases intrusion detection performance with respect to a conventional FL approach up to +17% in terms of F1-score, along with a significant reduction of the number of training rounds.","meta":{"url":"http://arxiv.org/abs/2307.14268v1"},"cats":{"new-dataset":0.3506836506,"dev-research":0.2429397069,"prompt-eng":0.3709588415,"data-quality":0.1888355488,"ml-security":0.3847118588}}
{"text":"The Paris Agreement, considered a significant milestone in climate negotiations, has faced challenges in effectively addressing climate change due to the unconditional nature of most Nationally Determined Contributions (NDCs).","meta":{"url":"http://arxiv.org/abs/2307.14267v1"},"cats":{"new-dataset":0.0564929628,"dev-research":0.2625882237,"prompt-eng":0.3405408267,"data-quality":0.1663557094,"ml-security":0.0866421673}}
{"text":"This has resulted in a prevalence of free-riding behavior among major polluters and a lack of concrete conditionality in NDCs.","meta":{"url":"http://arxiv.org/abs/2307.14267v1"},"cats":{"new-dataset":0.0404175739,"dev-research":0.2810028519,"prompt-eng":0.3845292228,"data-quality":0.1247494966,"ml-security":0.3031777381}}
{"text":"To address this issue, we propose the implementation of a decentralized, bottom-up approach called the Conditional Commitment Mechanism.","meta":{"url":"http://arxiv.org/abs/2307.14267v1"},"cats":{"new-dataset":0.0309030267,"dev-research":0.2404017324,"prompt-eng":0.4618047497,"data-quality":0.0920980095,"ml-security":0.11467931}}
{"text":"This mechanism, inspired by the National Popular Vote Interstate Compact, offers flexibility and incentives for early adopters, aiming to formalize conditional cooperation in international climate policy.","meta":{"url":"http://arxiv.org/abs/2307.14267v1"},"cats":{"new-dataset":0.0259433903,"dev-research":0.2399077028,"prompt-eng":0.4233657143,"data-quality":0.0861628721,"ml-security":0.0610649658}}
{"text":"In this paper, we provide an overview of the mechanism, its performance in the AI4ClimateCooperation challenge, and discuss potential real-world implementation aspects.","meta":{"url":"http://arxiv.org/abs/2307.14267v1"},"cats":{"new-dataset":0.0864375074,"dev-research":0.3149988958,"prompt-eng":0.4408781196,"data-quality":0.1682521216,"ml-security":0.085859315}}
{"text":"Prior knowledge of the climate mitigation collective action problem, basic economic principles, and game theory concepts are assumed.","meta":{"url":"http://arxiv.org/abs/2307.14267v1"},"cats":{"new-dataset":0.0992514058,"dev-research":0.2355768584,"prompt-eng":0.3373453198,"data-quality":0.1206673149,"ml-security":0.1847372348}}
{"text":"This paper proposes enhancements to the RICE-N simulation and multi-agent reinforcement learning framework to improve the realism of international climate policy negotiations.","meta":{"url":"http://arxiv.org/abs/2307.14266v1"},"cats":{"new-dataset":0.1484604545,"dev-research":0.1687142456,"prompt-eng":0.3484843966,"data-quality":0.0828999783,"ml-security":0.1201676749}}
{"text":"Acknowledging the framework's value, we highlight the necessity of significant enhancements to address the diverse array of factors in modeling climate negotiations.","meta":{"url":"http://arxiv.org/abs/2307.14266v1"},"cats":{"new-dataset":0.1303863639,"dev-research":0.2638931761,"prompt-eng":0.391930253,"data-quality":0.1011011649,"ml-security":0.0889132483}}
{"text":"Building upon our previous work on the \"Conditional Commitments Mechanism\" (CCF mechanism) we discuss ways to bridge the gap between simulation and reality.","meta":{"url":"http://arxiv.org/abs/2307.14266v1"},"cats":{"new-dataset":0.0693523835,"dev-research":0.2634314492,"prompt-eng":0.4668881896,"data-quality":0.1092757835,"ml-security":0.1158499359}}
{"text":"We suggest the inclusion of a recommender or planner agent to enhance coordination, address the Real2Sim gap by incorporating social factors and non-party stakeholder sub-agents, and propose enhancements to the underlying Reinforcement Learning solution algorithm.","meta":{"url":"http://arxiv.org/abs/2307.14266v1"},"cats":{"new-dataset":0.0862516553,"dev-research":0.2745112844,"prompt-eng":0.3968195301,"data-quality":0.0771708907,"ml-security":0.089483292}}
{"text":"These proposed improvements aim to advance the evaluation and formulation of negotiation protocols for more effective international climate policy decision-making in Rice-N.","meta":{"url":"http://arxiv.org/abs/2307.14266v1"},"cats":{"new-dataset":0.1737534995,"dev-research":0.2362710517,"prompt-eng":0.3800987795,"data-quality":0.1051685698,"ml-security":0.0597601876}}
{"text":"However, further experimentation and testing are required to determine the implications and effectiveness of these suggestions.","meta":{"url":"http://arxiv.org/abs/2307.14266v1"},"cats":{"new-dataset":0.0046209136,"dev-research":0.3031182733,"prompt-eng":0.4674539592,"data-quality":0.1707342055,"ml-security":0.1044536072}}
{"text":"Recently, Hegerfeld and Kratsch [ESA 2023] obtained the first tight algorithmic results for hard connectivity problems parameterized by clique-width.","meta":{"url":"http://arxiv.org/abs/2307.14264v1"},"cats":{"new-dataset":0.065526113,"dev-research":0.2243859331,"prompt-eng":0.3662286347,"data-quality":0.1941628993,"ml-security":0.1246615503}}
{"text":"Concretely, they gave one-sided error Monte-Carlo algorithms that given a $k$-clique-expression solve Connected Vertex Cover in time $6^kn^{O(1)}$ and Connected Dominating Set in time $5^kn^{O(1)}$. Moreover, under the Strong Exponential-Time Hypothesis (SETH) these results were showed to be tight.","meta":{"url":"http://arxiv.org/abs/2307.14264v1"},"cats":{"new-dataset":0.0773504253,"dev-research":0.2361176,"prompt-eng":0.3194924625,"data-quality":0.1472925932,"ml-security":0.1468177157}}
{"text":"However, they leave open several important benchmark problems, whose complexity relative to treewidth had been settled by Cygan et al.","meta":{"url":"http://arxiv.org/abs/2307.14264v1"},"cats":{"new-dataset":0.1261637454,"dev-research":0.2999871794,"prompt-eng":0.2930888439,"data-quality":0.1891383326,"ml-security":0.0805638799}}
{"text":"[SODA 2011 & TALG 2018].","meta":{"url":"http://arxiv.org/abs/2307.14264v1"},"cats":{"new-dataset":0.3501702789,"dev-research":0.232395088,"prompt-eng":0.3477945612,"data-quality":0.1837647181,"ml-security":0.1171748326}}
{"text":"Among which is the Steiner Tree problem.","meta":{"url":"http://arxiv.org/abs/2307.14264v1"},"cats":{"new-dataset":0.0565406747,"dev-research":0.2322684704,"prompt-eng":0.3316391168,"data-quality":0.1575590033,"ml-security":0.0654007536}}
{"text":"As a key obstruction they point out the exponential gap between the rank of certain compatibility matrices, which is often used for algorithms, and the largest triangular submatrix therein, which is essential for current lower bound methods.","meta":{"url":"http://arxiv.org/abs/2307.14264v1"},"cats":{"new-dataset":0.0096751738,"dev-research":0.3149195843,"prompt-eng":0.2926963752,"data-quality":0.1255382147,"ml-security":0.1488729631}}
{"text":"Concretely, for Steiner Tree the $GF(2)$-rank is $4^k$, while no triangular submatrix larger than $3^k$ was known.","meta":{"url":"http://arxiv.org/abs/2307.14264v1"},"cats":{"new-dataset":0.0881718689,"dev-research":0.1797219303,"prompt-eng":0.2972009531,"data-quality":0.1178087213,"ml-security":0.0525080764}}
{"text":"This yields time $4^kn^{O(1)}$, while the obtainable impossibility of time $(3-\\varepsilon)^kn^{O(1)}$ under SETH was already known relative to pathwidth.   ","meta":{"url":"http://arxiv.org/abs/2307.14264v1"},"cats":{"new-dataset":0.0557004138,"dev-research":0.2112394687,"prompt-eng":0.2909761199,"data-quality":0.1201859915,"ml-security":0.1173054467}}
{"text":"We close this gap by showing that Steiner Tree can be solved in time $3^kn^{O(1)}$ given a $k$-clique-expression.","meta":{"url":"http://arxiv.org/abs/2307.14264v1"},"cats":{"new-dataset":0.1781914913,"dev-research":0.2079034732,"prompt-eng":0.3294172654,"data-quality":0.1375067123,"ml-security":0.0712866108}}
{"text":"Hence, for all parameters between cutwidth and clique-width it has the same tight complexity.","meta":{"url":"http://arxiv.org/abs/2307.14264v1"},"cats":{"new-dataset":0.0058519171,"dev-research":0.2785998466,"prompt-eng":0.3001538803,"data-quality":0.0951452126,"ml-security":0.0607217166}}
{"text":"We first show that there is a ``representative submatrix'' of GF(2)-rank $3^k$ (ruling out larger triangular submatrices).","meta":{"url":"http://arxiv.org/abs/2307.14264v1"},"cats":{"new-dataset":0.4266884465,"dev-research":0.1583507337,"prompt-eng":0.3533125874,"data-quality":0.1419049556,"ml-security":0.0975661044}}
{"text":"At first glance, this only allows to count (modulo 2) the number of representations of valid solutions, but not the number of solutions (even if a unique solution exists).","meta":{"url":"http://arxiv.org/abs/2307.14264v1"},"cats":{"new-dataset":0.0209899757,"dev-research":0.2795140683,"prompt-eng":0.3305947223,"data-quality":0.1527744894,"ml-security":0.1112161636}}
{"text":"We show how to overcome this problem by isolating a unique representative of a unique solution, if one exists.","meta":{"url":"http://arxiv.org/abs/2307.14264v1"},"cats":{"new-dataset":0.0343925182,"dev-research":0.2537119122,"prompt-eng":0.4063762954,"data-quality":0.2126301056,"ml-security":0.1740045606}}
{"text":"We believe that our approach will be instrumental for settling further open problems in this research program.","meta":{"url":"http://arxiv.org/abs/2307.14264v1"},"cats":{"new-dataset":0.0793183881,"dev-research":0.3248049238,"prompt-eng":0.357399614,"data-quality":0.1452309008,"ml-security":0.1059369202}}
{"text":"Vision transformers (ViT) have been of broad interest in recent theoretical and empirical works.","meta":{"url":"http://arxiv.org/abs/2307.14253v1"},"cats":{"new-dataset":0.0543839835,"dev-research":0.2028896083,"prompt-eng":0.363909072,"data-quality":0.078608813,"ml-security":0.0592871989}}
{"text":"They are state-of-the-art thanks to their attention-based approach, which boosts the identification of key features and patterns within images thanks to the capability of avoiding inductive bias, resulting in highly accurate image analysis.","meta":{"url":"http://arxiv.org/abs/2307.14253v1"},"cats":{"new-dataset":0.1607035942,"dev-research":0.2837000625,"prompt-eng":0.4043943224,"data-quality":0.216629642,"ml-security":0.0928108379}}
{"text":"Meanwhile, neoteric studies have reported a ``sparse double descent'' phenomenon that can occur in modern deep-learning models, where extremely over-parametrized models can generalize well.","meta":{"url":"http://arxiv.org/abs/2307.14253v1"},"cats":{"new-dataset":0.0197879017,"dev-research":0.1652325378,"prompt-eng":0.344178456,"data-quality":0.1876743656,"ml-security":0.4257503265}}
{"text":"This raises practical questions about the optimal size of the model and the quest over finding the best trade-off between sparsity and performance is launched: are Vision Transformers also prone to sparse double descent?","meta":{"url":"http://arxiv.org/abs/2307.14253v1"},"cats":{"new-dataset":0.0054557698,"dev-research":0.1731786823,"prompt-eng":0.3656092641,"data-quality":0.1026233127,"ml-security":0.1288494235}}
{"text":"Can we find a way to avoid such a phenomenon?","meta":{"url":"http://arxiv.org/abs/2307.14253v1"},"cats":{"new-dataset":0.0078420005,"dev-research":0.255553931,"prompt-eng":0.4144565933,"data-quality":0.2339921407,"ml-security":0.3395511968}}
{"text":"Our work tackles the occurrence of sparse double descent on ViTs.","meta":{"url":"http://arxiv.org/abs/2307.14253v1"},"cats":{"new-dataset":0.0730540273,"dev-research":0.2141911463,"prompt-eng":0.4247201868,"data-quality":0.2103215866,"ml-security":0.1611277137}}
{"text":"Despite some works that have shown that traditional architectures, like Resnet, are condemned to the sparse double descent phenomenon, for ViTs we observe that an optimally-tuned $\\ell_2$ regularization relieves such a phenomenon.","meta":{"url":"http://arxiv.org/abs/2307.14253v1"},"cats":{"new-dataset":0.0147440393,"dev-research":0.1955007579,"prompt-eng":0.3609324643,"data-quality":0.1764135058,"ml-security":0.2825506373}}
{"text":"However, everything comes at a cost: optimal lambda will sacrifice the potential compression of the ViT.","meta":{"url":"http://arxiv.org/abs/2307.14253v1"},"cats":{"new-dataset":0.0051836642,"dev-research":0.2309489878,"prompt-eng":0.3931068451,"data-quality":0.1074144554,"ml-security":0.1428838349}}
{"text":"Navigation of a mobile robot is conditioned on the knowledge of its pose.","meta":{"url":"http://arxiv.org/abs/2307.14247v1"},"cats":{"new-dataset":0.0422057475,"dev-research":0.2296607626,"prompt-eng":0.429042618,"data-quality":0.1041916689,"ml-security":0.1036227953}}
{"text":"In observer-based localisation configurations its initial pose may not be knowable in advance, leading to the need of its estimation.","meta":{"url":"http://arxiv.org/abs/2307.14247v1"},"cats":{"new-dataset":0.0317701968,"dev-research":0.2592623168,"prompt-eng":0.3930764626,"data-quality":0.2158176622,"ml-security":0.1263693486}}
{"text":"Solutions to the problem of global localisation are either robust against noise and environment arbitrariness but require motion and time, which may (need to) be economised on, or require minimal estimation time but assume environmental structure, may be sensitive to noise, and demand preprocessing and tuning.","meta":{"url":"http://arxiv.org/abs/2307.14247v1"},"cats":{"new-dataset":0.0472731447,"dev-research":0.2694543107,"prompt-eng":0.3999281799,"data-quality":0.180248573,"ml-security":0.1417145209}}
{"text":"This article proposes a method that retains the strengths and avoids the weaknesses of the two approaches.","meta":{"url":"http://arxiv.org/abs/2307.14247v1"},"cats":{"new-dataset":0.0100970537,"dev-research":0.3218974975,"prompt-eng":0.4174550076,"data-quality":0.2314771237,"ml-security":0.0931921911}}
{"text":"The method leverages properties of the Cumulative Absolute Error per Ray metric with respect to the errors of pose estimates of a 2D LIDAR sensor, and utilises scan--to--map-scan matching for fine(r) pose approximations.","meta":{"url":"http://arxiv.org/abs/2307.14247v1"},"cats":{"new-dataset":0.0743813228,"dev-research":0.2630991999,"prompt-eng":0.3812801991,"data-quality":0.2141236794,"ml-security":0.0614258452}}
{"text":"A large number of tests, in real and simulated conditions, involving disparate environments and sensor properties, illustrate that the proposed method outperforms state-of-the-art methods of both classes of solutions in terms of pose discovery rate and execution time.","meta":{"url":"http://arxiv.org/abs/2307.14247v1"},"cats":{"new-dataset":0.136236423,"dev-research":0.2483144756,"prompt-eng":0.4144617843,"data-quality":0.0997502888,"ml-security":0.0904117115}}
{"text":"The source code is available for download.","meta":{"url":"http://arxiv.org/abs/2307.14247v1"},"cats":{"new-dataset":0.446404553,"dev-research":0.3795193039,"prompt-eng":0.337206508,"data-quality":0.1721184899,"ml-security":0.0973392314}}
{"text":"Within the field of Requirements Engineering (RE), the increasing significance of Explainable Artificial Intelligence (XAI) in aligning AI-supported systems with user needs, societal expectations, and regulatory standards has garnered recognition.","meta":{"url":"http://arxiv.org/abs/2307.14239v1"},"cats":{"new-dataset":0.0471476509,"dev-research":0.4481595817,"prompt-eng":0.4313922859,"data-quality":0.1856447467,"ml-security":0.2356929838}}
{"text":"In general, explainability has emerged as an important non-functional requirement that impacts system quality.","meta":{"url":"http://arxiv.org/abs/2307.14239v1"},"cats":{"new-dataset":0.0051749048,"dev-research":0.4750143706,"prompt-eng":0.372560614,"data-quality":0.2194309822,"ml-security":0.1892645064}}
{"text":"However, the supposed trade-off between explainability and performance challenges the presumed positive influence of explainability.","meta":{"url":"http://arxiv.org/abs/2307.14239v1"},"cats":{"new-dataset":0.0025083374,"dev-research":0.4558881018,"prompt-eng":0.3796856359,"data-quality":0.2689956303,"ml-security":0.2138907964}}
{"text":"If meeting the requirement of explainability entails a reduction in system performance, then careful consideration must be given to which of these quality aspects takes precedence and how to compromise between them.","meta":{"url":"http://arxiv.org/abs/2307.14239v1"},"cats":{"new-dataset":0.0049280094,"dev-research":0.5271279015,"prompt-eng":0.4585244834,"data-quality":0.2306175007,"ml-security":0.2273722205}}
{"text":"In this paper, we critically examine the alleged trade-off.","meta":{"url":"http://arxiv.org/abs/2307.14239v1"},"cats":{"new-dataset":0.0385218835,"dev-research":0.2327315983,"prompt-eng":0.3266728435,"data-quality":0.1539505902,"ml-security":0.1575776735}}
{"text":"We argue that it is best approached in a nuanced way that incorporates resource availability, domain characteristics, and considerations of risk.","meta":{"url":"http://arxiv.org/abs/2307.14239v1"},"cats":{"new-dataset":0.0197768645,"dev-research":0.2652753985,"prompt-eng":0.3995885685,"data-quality":0.1474612052,"ml-security":0.242095309}}
{"text":"By providing a foundation for future research and best practices, this work aims to advance the field of RE for AI.","meta":{"url":"http://arxiv.org/abs/2307.14239v1"},"cats":{"new-dataset":0.0694859464,"dev-research":0.2955488063,"prompt-eng":0.3819494807,"data-quality":0.1119064773,"ml-security":0.1041753533}}
{"text":"Creating an intelligent search and retrieval system for artwork images, particularly paintings, is crucial for documenting cultural heritage, fostering wider public engagement, and advancing artistic analysis and interpretation.","meta":{"url":"http://arxiv.org/abs/2307.14244v1"},"cats":{"new-dataset":0.1395014372,"dev-research":0.2748486192,"prompt-eng":0.3810866826,"data-quality":0.197906861,"ml-security":0.0458085826}}
{"text":"Visual-Semantic Embedding (VSE) networks are deep learning models used for information retrieval, which learn joint representations of textual and visual data, enabling 1) cross-modal search and retrieval tasks, such as image-to-text and text-to-image retrieval; and 2) relation-focused retrieval to capture entity relationships and provide more contextually relevant search results.","meta":{"url":"http://arxiv.org/abs/2307.14244v1"},"cats":{"new-dataset":0.0593507707,"dev-research":0.2933232285,"prompt-eng":0.3238226957,"data-quality":0.2112304969,"ml-security":0.077564378}}
{"text":"Although VSE networks have played a significant role in cross-modal information retrieval, their application to painting datasets, such as ArtUK, remains unexplored.","meta":{"url":"http://arxiv.org/abs/2307.14244v1"},"cats":{"new-dataset":0.2324060141,"dev-research":0.3388964916,"prompt-eng":0.3540332644,"data-quality":0.2333736385,"ml-security":0.1096389499}}
{"text":"This paper introduces BoonArt, a VSE-based cross-modal search engine that allows users to search for images using textual queries, and to obtain textual descriptions along with the corresponding images when using image queries.","meta":{"url":"http://arxiv.org/abs/2307.14244v1"},"cats":{"new-dataset":0.1536582824,"dev-research":0.2847837635,"prompt-eng":0.4282896731,"data-quality":0.1867652425,"ml-security":0.0717391426}}
{"text":"The performance of BoonArt was evaluated using the ArtUK dataset.","meta":{"url":"http://arxiv.org/abs/2307.14244v1"},"cats":{"new-dataset":0.2301285557,"dev-research":0.3348520197,"prompt-eng":0.3667077676,"data-quality":0.1321884092,"ml-security":0.0762624676}}
{"text":"Experimental evaluations revealed that BoonArt achieved 97% Recall@10 for image-to-text retrieval, and 97.4% Recall@10 for text-to-image Retrieval.","meta":{"url":"http://arxiv.org/abs/2307.14244v1"},"cats":{"new-dataset":0.1575054956,"dev-research":0.2369506739,"prompt-eng":0.4185011004,"data-quality":0.2823531055,"ml-security":0.0812266687}}
{"text":"By bridging the gap between textual and visual modalities, BoonArt provides a much-improved search performance compared to traditional search engines, such as the one provided by the ArtUK website.","meta":{"url":"http://arxiv.org/abs/2307.14244v1"},"cats":{"new-dataset":0.0775107237,"dev-research":0.3216043734,"prompt-eng":0.3811982301,"data-quality":0.1563671038,"ml-security":0.0735999036}}
{"text":"BoonArt can be utilised to work with other artwork datasets.","meta":{"url":"http://arxiv.org/abs/2307.14244v1"},"cats":{"new-dataset":0.2816148038,"dev-research":0.2911759407,"prompt-eng":0.3637485765,"data-quality":0.1506489518,"ml-security":0.1048668049}}
{"text":"Fluorescent Neuronal Cells v2 is a collection of fluorescence microscopy images and the corresponding ground-truth annotations, designed to foster innovative research in the domains of Life Sciences and Deep Learning.","meta":{"url":"http://arxiv.org/abs/2307.14243v1"},"cats":{"new-dataset":0.2700959661,"dev-research":0.2541134774,"prompt-eng":0.3341735129,"data-quality":0.2930843865,"ml-security":0.1297114559}}
{"text":"This dataset encompasses three image collections in which rodent neuronal cells' nuclei and cytoplasm are stained with diverse markers to highlight their anatomical or functional characteristics.","meta":{"url":"http://arxiv.org/abs/2307.14243v1"},"cats":{"new-dataset":0.7584689858,"dev-research":0.2081643842,"prompt-eng":0.3812897167,"data-quality":0.2397761869,"ml-security":0.0518837429}}
{"text":"Alongside the images, we provide ground-truth annotations for several learning tasks, including semantic segmentation, object detection, and counting.","meta":{"url":"http://arxiv.org/abs/2307.14243v1"},"cats":{"new-dataset":0.5157310888,"dev-research":0.2356799272,"prompt-eng":0.4280930926,"data-quality":0.3987658364,"ml-security":0.1385558532}}
{"text":"The contribution is two-fold.","meta":{"url":"http://arxiv.org/abs/2307.14243v1"},"cats":{"new-dataset":0.0442360003,"dev-research":0.1925289199,"prompt-eng":0.3349173265,"data-quality":0.1011777148,"ml-security":0.075915139}}
{"text":"First, given the variety of annotations and their accessible formats, we envision our work facilitating methodological advancements in computer vision approaches for segmentation, detection, feature learning, unsupervised and self-supervised learning, transfer learning, and related areas.","meta":{"url":"http://arxiv.org/abs/2307.14243v1"},"cats":{"new-dataset":0.3331176975,"dev-research":0.2558855679,"prompt-eng":0.430751641,"data-quality":0.3256973463,"ml-security":0.1241883032}}
{"text":"Second, by enabling extensive exploration and benchmarking, we hope Fluorescent Neuronal Cells v2 will catalyze breakthroughs in fluorescence microscopy analysis and promote cutting-edge discoveries in life sciences.","meta":{"url":"http://arxiv.org/abs/2307.14243v1"},"cats":{"new-dataset":0.1780104755,"dev-research":0.2951675661,"prompt-eng":0.3630036052,"data-quality":0.2284377089,"ml-security":0.111085156}}
{"text":"The data are available at: https://amsacta.unibo.it/id/eprint/7347","meta":{"url":"http://arxiv.org/abs/2307.14243v1"},"cats":{"new-dataset":0.9018028283,"dev-research":0.1310301026,"prompt-eng":0.3703514101,"data-quality":0.1211399856,"ml-security":0.0603500982}}
{"text":"Deep neural networks are successfully used in various applications, but show their vulnerability to adversarial examples.","meta":{"url":"http://arxiv.org/abs/2307.14242v1"},"cats":{"new-dataset":0.0492114617,"dev-research":0.2504181019,"prompt-eng":0.3670269285,"data-quality":0.4250210437,"ml-security":0.8480013618}}
{"text":"With the development of adversarial patches, the feasibility of attacks in physical scenes increases, and the defenses against patch attacks are urgently needed.","meta":{"url":"http://arxiv.org/abs/2307.14242v1"},"cats":{"new-dataset":0.1560088884,"dev-research":0.301972316,"prompt-eng":0.4067810667,"data-quality":0.2541874139,"ml-security":0.7230397748}}
{"text":"However, defending such adversarial patch attacks is still an unsolved problem.","meta":{"url":"http://arxiv.org/abs/2307.14242v1"},"cats":{"new-dataset":0.0364888717,"dev-research":0.3478369425,"prompt-eng":0.3394146103,"data-quality":0.3317689152,"ml-security":0.8071596403}}
{"text":"In this paper, we analyse the properties of adversarial patches, and find that: on the one hand, adversarial patches will lead to the appearance or contextual inconsistency in the target objects; on the other hand, the patch region will show abnormal changes on the high-level feature maps of the objects extracted by a backbone network.","meta":{"url":"http://arxiv.org/abs/2307.14242v1"},"cats":{"new-dataset":0.1075858695,"dev-research":0.271232679,"prompt-eng":0.3631604674,"data-quality":0.4273717228,"ml-security":0.7061514638}}
{"text":"Considering the above two points, we propose a novel defense method based on a ``localizing and inpainting\" mechanism to pre-process the input examples.","meta":{"url":"http://arxiv.org/abs/2307.14242v1"},"cats":{"new-dataset":0.0510140873,"dev-research":0.3932390307,"prompt-eng":0.484340622,"data-quality":0.3659940624,"ml-security":0.6413777307}}
{"text":"Specifically, we design an unified framework, where the ``localizing\" sub-network utilizes a two-branch structure to represent the above two aspects to accurately detect the adversarial patch region in the image.","meta":{"url":"http://arxiv.org/abs/2307.14242v1"},"cats":{"new-dataset":0.1203142086,"dev-research":0.2819037504,"prompt-eng":0.4263837948,"data-quality":0.3462492781,"ml-security":0.4059259301}}
{"text":"For the ``inpainting\" sub-network, it utilizes the surrounding contextual cues to recover the original content covered by the adversarial patch.","meta":{"url":"http://arxiv.org/abs/2307.14242v1"},"cats":{"new-dataset":0.0717037041,"dev-research":0.2832046982,"prompt-eng":0.3484938638,"data-quality":0.3048674683,"ml-security":0.2975866648}}
{"text":"The quality of inpainted images is also evaluated by measuring the appearance consistency and the effects of adversarial attacks.","meta":{"url":"http://arxiv.org/abs/2307.14242v1"},"cats":{"new-dataset":0.0389343304,"dev-research":0.2324067022,"prompt-eng":0.3670042574,"data-quality":0.3897989987,"ml-security":0.4602128422}}
{"text":"These two sub-networks are then jointly trained via an iterative optimization manner.","meta":{"url":"http://arxiv.org/abs/2307.14242v1"},"cats":{"new-dataset":0.0250958438,"dev-research":0.2110808103,"prompt-eng":0.3799018666,"data-quality":0.2062943302,"ml-security":0.1040825377}}
{"text":"In this way, the ``localizing\" and ``inpainting\" modules can interact closely with each other, and thus learn a better solution.","meta":{"url":"http://arxiv.org/abs/2307.14242v1"},"cats":{"new-dataset":0.023792049,"dev-research":0.3451535125,"prompt-eng":0.4040293733,"data-quality":0.2551625356,"ml-security":0.0863653227}}
{"text":"A series of experiments versus traffic sign classification and detection tasks are conducted to defend against various adversarial patch attacks.","meta":{"url":"http://arxiv.org/abs/2307.14242v1"},"cats":{"new-dataset":0.0713653792,"dev-research":0.3122486107,"prompt-eng":0.4048137792,"data-quality":0.3452388795,"ml-security":0.7638500915}}
{"text":"Purpose:","meta":{"url":"http://arxiv.org/abs/2307.14241v1"},"cats":{"new-dataset":0.0376553999,"dev-research":0.3180857092,"prompt-eng":0.3906855113,"data-quality":0.1029667623,"ml-security":0.0945636305}}
{"text":"Recent advances in Surgical Data Science (SDS) have contributed to an increase in video recordings from hospital environments.","meta":{"url":"http://arxiv.org/abs/2307.14241v1"},"cats":{"new-dataset":0.2110725363,"dev-research":0.2737646257,"prompt-eng":0.3479092136,"data-quality":0.1596550084,"ml-security":0.0849055482}}
{"text":"While methods such as surgical workflow recognition show potential in increasing the quality of patient care, the quantity of video data has surpassed the scale at which images can be manually anonymized.","meta":{"url":"http://arxiv.org/abs/2307.14241v1"},"cats":{"new-dataset":0.1216419879,"dev-research":0.2462181988,"prompt-eng":0.3420499323,"data-quality":0.1917179748,"ml-security":0.1743816703}}
{"text":"Existing automated 2D anonymization methods under-perform in Operating Rooms (OR), due to occlusions and obstructions.","meta":{"url":"http://arxiv.org/abs/2307.14241v1"},"cats":{"new-dataset":0.1671324905,"dev-research":0.3175188477,"prompt-eng":0.4046865145,"data-quality":0.2022712436,"ml-security":0.4460362648}}
{"text":"We propose to anonymize multi-view OR recordings using 3D data from multiple camera streams.","meta":{"url":"http://arxiv.org/abs/2307.14241v1"},"cats":{"new-dataset":0.4437493307,"dev-research":0.2395451493,"prompt-eng":0.3189600877,"data-quality":0.2172262696,"ml-security":0.3036000826}}
{"text":"Methods: RGB and depth images from multiple cameras are fused into a 3D point cloud representation of the scene.","meta":{"url":"http://arxiv.org/abs/2307.14241v1"},"cats":{"new-dataset":0.1800908538,"dev-research":0.218340287,"prompt-eng":0.3595919561,"data-quality":0.0999427453,"ml-security":0.0475248791}}
{"text":"We then detect each individual's face in 3D by regressing a parametric human mesh model onto detected 3D human keypoints and aligning the face mesh with the fused 3D point cloud.","meta":{"url":"http://arxiv.org/abs/2307.14241v1"},"cats":{"new-dataset":0.069567333,"dev-research":0.2451408244,"prompt-eng":0.4019619448,"data-quality":0.0746822347,"ml-security":0.1198481039}}
{"text":"The mesh model is rendered into every acquired camera view, replacing each individual's face.","meta":{"url":"http://arxiv.org/abs/2307.14241v1"},"cats":{"new-dataset":0.0501067815,"dev-research":0.2193255321,"prompt-eng":0.3642337157,"data-quality":0.1145333256,"ml-security":0.1126348388}}
{"text":"Results:","meta":{"url":"http://arxiv.org/abs/2307.14241v1"},"cats":{"new-dataset":0.1752677123,"dev-research":0.2146244897,"prompt-eng":0.3757808351,"data-quality":0.2238351842,"ml-security":0.0642365002}}
{"text":"Our method shows promise in locating faces at a higher rate than existing approaches.","meta":{"url":"http://arxiv.org/abs/2307.14241v1"},"cats":{"new-dataset":0.0369155683,"dev-research":0.2552275587,"prompt-eng":0.3869370459,"data-quality":0.1454681272,"ml-security":0.1097080085}}
{"text":"DisguisOR produces geometrically consistent anonymizations for each camera view, enabling more realistic anonymization that is less detrimental to downstream tasks.","meta":{"url":"http://arxiv.org/abs/2307.14241v1"},"cats":{"new-dataset":0.085519356,"dev-research":0.3346203794,"prompt-eng":0.3525005839,"data-quality":0.2419456396,"ml-security":0.3473947475}}
{"text":"Conclusion: Frequent obstructions and crowding in operating rooms leaves significant room for improvement for off-the-shelf anonymization methods.","meta":{"url":"http://arxiv.org/abs/2307.14241v1"},"cats":{"new-dataset":0.0866135779,"dev-research":0.3296474199,"prompt-eng":0.3651532169,"data-quality":0.2493616208,"ml-security":0.5490073185}}
{"text":"DisguisOR","meta":{"url":"http://arxiv.org/abs/2307.14241v1"},"cats":{"new-dataset":0.2408426846,"dev-research":0.2805856282,"prompt-eng":0.3486467244,"data-quality":0.1629549396,"ml-security":0.2278911}}
{"text":"addresses privacy on a scene level and has the potential to facilitate further research in SDS.","meta":{"url":"http://arxiv.org/abs/2307.14241v1"},"cats":{"new-dataset":0.1433845724,"dev-research":0.2612031726,"prompt-eng":0.3842341451,"data-quality":0.1336128104,"ml-security":0.2736691701}}
{"text":"Visual-Semantic Embedding (VSE) networks can help search engines better understand the meaning behind visual content and associate it with relevant textual information, leading to more accurate search results.","meta":{"url":"http://arxiv.org/abs/2307.14240v1"},"cats":{"new-dataset":0.0340525928,"dev-research":0.3748376779,"prompt-eng":0.3489086434,"data-quality":0.3303420506,"ml-security":0.089804911}}
{"text":"VSE networks can be used in cross-modal search engines to embed image and textual descriptions in a shared space, enabling image-to-text and text-to-image retrieval tasks.","meta":{"url":"http://arxiv.org/abs/2307.14240v1"},"cats":{"new-dataset":0.0843469939,"dev-research":0.2534738947,"prompt-eng":0.3912016015,"data-quality":0.1986999762,"ml-security":0.0715203561}}
{"text":"However, the full potential of VSE networks for search engines has yet to be fully explored.","meta":{"url":"http://arxiv.org/abs/2307.14240v1"},"cats":{"new-dataset":0.0304365253,"dev-research":0.2632147059,"prompt-eng":0.3291618294,"data-quality":0.1580778228,"ml-security":0.1347498677}}
{"text":"This paper presents Boon, a novel cross-modal search engine that combines two state-of-the-art networks: the GPT-3.5-turbo large language model, and the VSE network VITR (VIsion Transformers with Relation-focused learning) to enhance the engine's capabilities in extracting and reasoning with regional relationships in images.","meta":{"url":"http://arxiv.org/abs/2307.14240v1"},"cats":{"new-dataset":0.309394731,"dev-research":0.2785359145,"prompt-eng":0.3971929702,"data-quality":0.1881187623,"ml-security":0.0647576993}}
{"text":"VITR employs encoders from CLIP that were trained with 400 million image-description pairs and it was fine-turned on the RefCOCOg dataset.","meta":{"url":"http://arxiv.org/abs/2307.14240v1"},"cats":{"new-dataset":0.3796447784,"dev-research":0.244936819,"prompt-eng":0.3864748917,"data-quality":0.2314175932,"ml-security":0.0932548639}}
{"text":"Boon's neural-based components serve as its main functionalities: 1) a 'cross-modal search engine' that enables end-users to perform image-to-text and text-to-image retrieval.","meta":{"url":"http://arxiv.org/abs/2307.14240v1"},"cats":{"new-dataset":0.1199543126,"dev-research":0.2563815425,"prompt-eng":0.3880616189,"data-quality":0.1418707786,"ml-security":0.1350172081}}
{"text":"2) a 'multi-lingual conversational AI' component that enables the end-user to converse about one or more images selected by the end-user.","meta":{"url":"http://arxiv.org/abs/2307.14240v1"},"cats":{"new-dataset":0.1278353778,"dev-research":0.2951562661,"prompt-eng":0.3840248802,"data-quality":0.1951856217,"ml-security":0.080860169}}
{"text":"Such a feature makes the search engine accessible to a wide audience, including those with visual impairments.","meta":{"url":"http://arxiv.org/abs/2307.14240v1"},"cats":{"new-dataset":0.014111634,"dev-research":0.3229276147,"prompt-eng":0.383448788,"data-quality":0.1818547114,"ml-security":0.129936999}}
{"text":"3) Boon is multi-lingual and can take queries and handle conversations about images in multiple languages.","meta":{"url":"http://arxiv.org/abs/2307.14240v1"},"cats":{"new-dataset":0.209259719,"dev-research":0.3372938043,"prompt-eng":0.3697573898,"data-quality":0.1853194501,"ml-security":0.0923929342}}
{"text":"Boon was implemented using the Django and PyTorch frameworks.","meta":{"url":"http://arxiv.org/abs/2307.14240v1"},"cats":{"new-dataset":0.0625469074,"dev-research":0.2902300906,"prompt-eng":0.4241128181,"data-quality":0.1555737666,"ml-security":0.1688990665}}
{"text":"The interface and capabilities of the Boon search engine are demonstrated using the RefCOCOg dataset, and the engine's ability to search for multimedia through the web is facilitated by Google's API.","meta":{"url":"http://arxiv.org/abs/2307.14240v1"},"cats":{"new-dataset":0.3455783454,"dev-research":0.248968035,"prompt-eng":0.3822281713,"data-quality":0.192343409,"ml-security":0.1289068713}}
{"text":"Many swarm robotics tasks consist of multiple conflicting objectives.","meta":{"url":"http://arxiv.org/abs/2307.14237v1"},"cats":{"new-dataset":0.0134544736,"dev-research":0.2796481145,"prompt-eng":0.4590260718,"data-quality":0.1408291138,"ml-security":0.0972084299}}
{"text":"This research proposes a multi-objective evolutionary neural network approach to developing controllers for swarms of robots.","meta":{"url":"http://arxiv.org/abs/2307.14237v1"},"cats":{"new-dataset":0.0424495461,"dev-research":0.2463083315,"prompt-eng":0.3718454365,"data-quality":0.0656733788,"ml-security":0.1515795373}}
{"text":"The swarm robot controllers are trained in a low-fidelity Python simulator and then tested in a high-fidelity simulated environment using Webots.","meta":{"url":"http://arxiv.org/abs/2307.14237v1"},"cats":{"new-dataset":0.0951746626,"dev-research":0.2703024722,"prompt-eng":0.4312797084,"data-quality":0.0927255903,"ml-security":0.1449439085}}
{"text":"Simulations are then conducted to test the scalability of the evolved multi-objective robot controllers to environments with a larger number of robots.","meta":{"url":"http://arxiv.org/abs/2307.14237v1"},"cats":{"new-dataset":0.0783885988,"dev-research":0.2302945163,"prompt-eng":0.404224578,"data-quality":0.0556710023,"ml-security":0.1113194976}}
{"text":"The results presented demonstrate that the proposed approach can effectively control each of the robots.","meta":{"url":"http://arxiv.org/abs/2307.14237v1"},"cats":{"new-dataset":0.0211813487,"dev-research":0.2572708905,"prompt-eng":0.4519895266,"data-quality":0.0837385149,"ml-security":0.0920121948}}
{"text":"The robot swarm exhibits different behaviours as the weighting for each objective is adjusted.","meta":{"url":"http://arxiv.org/abs/2307.14237v1"},"cats":{"new-dataset":0.0286177999,"dev-research":0.2538199406,"prompt-eng":0.4446669585,"data-quality":0.0968710822,"ml-security":0.1299703516}}
{"text":"The results also confirm that multi-objective neural network controllers evolved in a low-fidelity simulator can be transferred to high-fidelity simulated environments and that the controllers can scale to environments with a larger number of robots without further retraining needed.","meta":{"url":"http://arxiv.org/abs/2307.14237v1"},"cats":{"new-dataset":0.0540896473,"dev-research":0.2284927856,"prompt-eng":0.3657422333,"data-quality":0.1141243829,"ml-security":0.1952444485}}
{"text":"This demo paper presents UnScientify, an interactive system designed to detect scientific uncertainty in scholarly full text.","meta":{"url":"http://arxiv.org/abs/2307.14236v1"},"cats":{"new-dataset":0.2560913398,"dev-research":0.2934377831,"prompt-eng":0.4466034021,"data-quality":0.4063902317,"ml-security":0.0786222804}}
{"text":"The system utilizes a weakly supervised technique that employs a fine-grained annotation scheme to identify verbally formulated uncertainty at the sentence level in scientific texts.","meta":{"url":"http://arxiv.org/abs/2307.14236v1"},"cats":{"new-dataset":0.0956711624,"dev-research":0.3080998403,"prompt-eng":0.4487645993,"data-quality":0.5882981174,"ml-security":0.0827330832}}
{"text":"The pipeline for the system includes a combination of pattern matching, complex sentence checking, and authorial reference checking.","meta":{"url":"http://arxiv.org/abs/2307.14236v1"},"cats":{"new-dataset":0.0764380742,"dev-research":0.3145249145,"prompt-eng":0.4633677141,"data-quality":0.22501166,"ml-security":0.0586603691}}
{"text":"Our approach automates labeling and annotation tasks for scientific uncertainty identification, taking into account different types of scientific uncertainty, that can serve various applications such as information retrieval, text mining, and scholarly document processing.","meta":{"url":"http://arxiv.org/abs/2307.14236v1"},"cats":{"new-dataset":0.1652440156,"dev-research":0.271711348,"prompt-eng":0.4703234374,"data-quality":0.6239173072,"ml-security":0.0609487267}}
{"text":"Additionally, UnScientify provides interpretable results, aiding in the comprehension of identified instances of scientific uncertainty in text.","meta":{"url":"http://arxiv.org/abs/2307.14236v1"},"cats":{"new-dataset":0.0671826196,"dev-research":0.3546322444,"prompt-eng":0.4182226224,"data-quality":0.401702542,"ml-security":0.1008247047}}
{"text":"Modern computer systems are ubiquitous in contemporary life yet many of them remain opaque.","meta":{"url":"http://arxiv.org/abs/2307.14232v1"},"cats":{"new-dataset":0.048807086,"dev-research":0.3243058365,"prompt-eng":0.3510457698,"data-quality":0.1188655182,"ml-security":0.2473544316}}
{"text":"This poses significant challenges in domains where desiderata such as fairness or accountability are crucial.","meta":{"url":"http://arxiv.org/abs/2307.14232v1"},"cats":{"new-dataset":0.0586007696,"dev-research":0.3292928864,"prompt-eng":0.3595255733,"data-quality":0.2314969729,"ml-security":0.3264478099}}
{"text":"We suggest that the best strategy for achieving system transparency varies depending on the specific source of opacity prevalent in a given context.","meta":{"url":"http://arxiv.org/abs/2307.14232v1"},"cats":{"new-dataset":0.032068268,"dev-research":0.2763381795,"prompt-eng":0.4286420195,"data-quality":0.1652455238,"ml-security":0.2652911486}}
{"text":"Synthesizing and extending existing discussions, we propose a taxonomy consisting of eight sources of opacity that fall into three main categories: architectural, analytical, and socio-technical.","meta":{"url":"http://arxiv.org/abs/2307.14232v1"},"cats":{"new-dataset":0.272300643,"dev-research":0.3623202691,"prompt-eng":0.3751646568,"data-quality":0.1254251835,"ml-security":0.1067718789}}
{"text":"For each source, we provide initial suggestions as to how to address the resulting opacity in practice.","meta":{"url":"http://arxiv.org/abs/2307.14232v1"},"cats":{"new-dataset":0.1134363854,"dev-research":0.2778127111,"prompt-eng":0.4354056342,"data-quality":0.179818702,"ml-security":0.0656740418}}
{"text":"The taxonomy provides a starting point for requirements engineers and other practitioners to understand contextually prevalent sources of opacity, and to select or develop appropriate strategies for overcoming them.","meta":{"url":"http://arxiv.org/abs/2307.14232v1"},"cats":{"new-dataset":0.0843903289,"dev-research":0.4323215668,"prompt-eng":0.4311012229,"data-quality":0.155873073,"ml-security":0.0927971477}}
{"text":"Traditional Chinese Painting (TCP) is an invaluable cultural heritage resource and a unique visual art style.","meta":{"url":"http://arxiv.org/abs/2307.14227v1"},"cats":{"new-dataset":0.1365707752,"dev-research":0.3124711647,"prompt-eng":0.3384227586,"data-quality":0.0963433783,"ml-security":0.0667804273}}
{"text":"In recent years, increasing interest has been placed on digitalizing TCPs to preserve and revive the culture.","meta":{"url":"http://arxiv.org/abs/2307.14227v1"},"cats":{"new-dataset":0.1284530434,"dev-research":0.3326189246,"prompt-eng":0.3548527266,"data-quality":0.1105717445,"ml-security":0.1000571778}}
{"text":"The resulting digital copies have enabled the advancement of computational methods for structured and systematic understanding of TCPs.","meta":{"url":"http://arxiv.org/abs/2307.14227v1"},"cats":{"new-dataset":0.0732929615,"dev-research":0.3893598049,"prompt-eng":0.4267849361,"data-quality":0.1321733267,"ml-security":0.1024911333}}
{"text":"To explore this topic, we conducted an in-depth analysis of 92 pieces of literature.","meta":{"url":"http://arxiv.org/abs/2307.14227v1"},"cats":{"new-dataset":0.1252356246,"dev-research":0.2470279234,"prompt-eng":0.3303474248,"data-quality":0.1301409846,"ml-security":0.0873538216}}
{"text":"We examined the current use of computer technologies on TCPs from three perspectives, based on numerous conversations with specialists.","meta":{"url":"http://arxiv.org/abs/2307.14227v1"},"cats":{"new-dataset":0.0645445761,"dev-research":0.4077604798,"prompt-eng":0.3601106946,"data-quality":0.0869101825,"ml-security":0.1356002188}}
{"text":"First, in light of the \"Six Principles of Painting\" theory, we categorized the articles according to their research focus on artistic elements.","meta":{"url":"http://arxiv.org/abs/2307.14227v1"},"cats":{"new-dataset":0.0573577473,"dev-research":0.1959877711,"prompt-eng":0.3212884963,"data-quality":0.1225157526,"ml-security":0.0483731188}}
{"text":"Second, we created a four-stage framework to illustrate the purposes of TCP applications.","meta":{"url":"http://arxiv.org/abs/2307.14227v1"},"cats":{"new-dataset":0.1048110519,"dev-research":0.3192942354,"prompt-eng":0.4248569355,"data-quality":0.0913521191,"ml-security":0.1287263958}}
{"text":"Third, we summarized the popular computational techniques applied to TCPs.","meta":{"url":"http://arxiv.org/abs/2307.14227v1"},"cats":{"new-dataset":0.0514790746,"dev-research":0.2565636455,"prompt-eng":0.4148579314,"data-quality":0.098499071,"ml-security":0.100443167}}
{"text":"The framework also provides insights into potential applications and future prospects, with professional opinion.","meta":{"url":"http://arxiv.org/abs/2307.14227v1"},"cats":{"new-dataset":0.0337585648,"dev-research":0.3651515172,"prompt-eng":0.3313158125,"data-quality":0.0929688325,"ml-security":0.0909925299}}
{"text":"The list of surveyed publications and related information is available online at https://ca4tcp.com.","meta":{"url":"http://arxiv.org/abs/2307.14227v1"},"cats":{"new-dataset":0.5664839087,"dev-research":0.1720975723,"prompt-eng":0.3378796864,"data-quality":0.1002922055,"ml-security":0.0389098173}}
{"text":"Climate issues have become more and more important now.","meta":{"url":"http://arxiv.org/abs/2307.14226v1"},"cats":{"new-dataset":0.096987976,"dev-research":0.3216463683,"prompt-eng":0.2911236366,"data-quality":0.1078688585,"ml-security":0.1023533173}}
{"text":"Although global governments have made some progress, we are still facing the truth that the prospect of international cooperation is not clear at present.","meta":{"url":"http://arxiv.org/abs/2307.14226v1"},"cats":{"new-dataset":0.0258717603,"dev-research":0.2317512722,"prompt-eng":0.2942790736,"data-quality":0.1372548628,"ml-security":0.0938329234}}
{"text":"Due to the limitations of the Integrated assessment models (IAMs) model, it is difficult to simulate the dynamic negotiation process.","meta":{"url":"http://arxiv.org/abs/2307.14226v1"},"cats":{"new-dataset":0.0234511885,"dev-research":0.2340354393,"prompt-eng":0.451415761,"data-quality":0.0870488967,"ml-security":0.1090697938}}
{"text":"Therefore, using deep learning to build a new agents based model (ABM) might can provide new theoretical support for climate negotiations.","meta":{"url":"http://arxiv.org/abs/2307.14226v1"},"cats":{"new-dataset":0.1427653717,"dev-research":0.2228190444,"prompt-eng":0.3320953108,"data-quality":0.0863559779,"ml-security":0.1635269793}}
{"text":"Building on the RICE-N model, this work proposed an approach to climate negotiations based on existing trade groups.","meta":{"url":"http://arxiv.org/abs/2307.14226v1"},"cats":{"new-dataset":0.3629937455,"dev-research":0.1739273929,"prompt-eng":0.3396487151,"data-quality":0.0713363345,"ml-security":0.0691933394}}
{"text":"Simulation results show that the scheme has a good prospect.","meta":{"url":"http://arxiv.org/abs/2307.14226v1"},"cats":{"new-dataset":0.0351898771,"dev-research":0.1881876912,"prompt-eng":0.4127820944,"data-quality":0.0742728899,"ml-security":0.1020619301}}
{"text":"Traditional recommender systems leverage users' item preference history to recommend novel content that users may like.","meta":{"url":"http://arxiv.org/abs/2307.14225v1"},"cats":{"new-dataset":0.0093735531,"dev-research":0.291663754,"prompt-eng":0.4032798542,"data-quality":0.0899462827,"ml-security":0.095872357}}
{"text":"However, modern dialog interfaces that allow users to express language-based preferences offer a fundamentally different modality for preference input.","meta":{"url":"http://arxiv.org/abs/2307.14225v1"},"cats":{"new-dataset":0.0187677869,"dev-research":0.2711288011,"prompt-eng":0.4342594375,"data-quality":0.1913684936,"ml-security":0.0903851601}}
{"text":"Inspired by recent successes of prompting paradigms for large language models (LLMs), we study their use for making recommendations from both item-based and language-based preferences in comparison to state-of-the-art item-based collaborative filtering (CF) methods.","meta":{"url":"http://arxiv.org/abs/2307.14225v1"},"cats":{"new-dataset":0.0397081139,"dev-research":0.2460548339,"prompt-eng":0.4591011174,"data-quality":0.2101437036,"ml-security":0.1070734147}}
{"text":"To support this investigation, we collect a new dataset consisting of both item-based and language-based preferences elicited from users along with their ratings on a variety of (biased) recommended items and (unbiased) random items.","meta":{"url":"http://arxiv.org/abs/2307.14225v1"},"cats":{"new-dataset":0.2740039835,"dev-research":0.2559768146,"prompt-eng":0.4784062061,"data-quality":0.2758181185,"ml-security":0.1509134867}}
{"text":"Among numerous experimental results, we find that LLMs provide competitive recommendation performance for pure language-based preferences (no item preferences) in the near cold-start case in comparison to item-based CF methods, despite having no supervised training for this specific task (zero-shot) or only a few labels (few-shot).","meta":{"url":"http://arxiv.org/abs/2307.14225v1"},"cats":{"new-dataset":0.0519309338,"dev-research":0.2319697115,"prompt-eng":0.4397235038,"data-quality":0.2211861211,"ml-security":0.1123120088}}
{"text":"This is particularly promising as language-based preference representations are more explainable and scrutable than item-based or vector-based representations.","meta":{"url":"http://arxiv.org/abs/2307.14225v1"},"cats":{"new-dataset":0.0184597322,"dev-research":0.3440828054,"prompt-eng":0.4623904897,"data-quality":0.2285753621,"ml-security":0.1331273614}}
{"text":"The \"Sum-Over-Paths\" formalism is a way to symbolically manipulate linear maps that describe quantum systems, and is a tool that is used in formal verification of such systems.","meta":{"url":"http://arxiv.org/abs/2307.14223v1"},"cats":{"new-dataset":0.0121377443,"dev-research":0.3348382019,"prompt-eng":0.4184081911,"data-quality":0.1256261374,"ml-security":0.1271275914}}
{"text":"We give here a new set of rewrite rules for the formalism, and show that it is complete for \"Toffoli-Hadamard\", the simplest approximately universal fragment of quantum mechanics.","meta":{"url":"http://arxiv.org/abs/2307.14223v1"},"cats":{"new-dataset":0.0474404604,"dev-research":0.185337644,"prompt-eng":0.3806618429,"data-quality":0.1226663809,"ml-security":0.0922800881}}
{"text":"We show that the rewriting is terminating, but not confluent (which is expected from the universality of the fragment).","meta":{"url":"http://arxiv.org/abs/2307.14223v1"},"cats":{"new-dataset":0.0324869606,"dev-research":0.2035373606,"prompt-eng":0.3897865316,"data-quality":0.2167439304,"ml-security":0.0889578634}}
{"text":"We do so using the connection between Sum-over-Paths and graphical language ZH-calculus, and also show how the axiomatisation translates into the latter.","meta":{"url":"http://arxiv.org/abs/2307.14223v1"},"cats":{"new-dataset":0.0480453031,"dev-research":0.3218506896,"prompt-eng":0.3775723323,"data-quality":0.1184681952,"ml-security":0.0922645337}}
{"text":"We provide generalisations of the presented rewrite rules, that can prove useful when trying to reduce terms in practice, and we show how to graphically make sense of these new rules.","meta":{"url":"http://arxiv.org/abs/2307.14223v1"},"cats":{"new-dataset":0.0269074503,"dev-research":0.3709301858,"prompt-eng":0.4308324714,"data-quality":0.2629141606,"ml-security":0.1169270182}}
{"text":"We show how to enrich the rewrite system to reach completeness for the dyadic fragments of quantum computation, used in particular in the Quantum Fourier Transform, and obtained by adding phase gates with dyadic multiples of $\\pi$ to the Toffoli-Hadamard gate-set.","meta":{"url":"http://arxiv.org/abs/2307.14223v1"},"cats":{"new-dataset":0.0446871996,"dev-research":0.2229049655,"prompt-eng":0.3745609904,"data-quality":0.0861825103,"ml-security":0.0966451518}}
{"text":"Finally, we show how to perform sums and concatenation of arbitrary terms, something which is not native in a system designed for analysing gate-based quantum computation, but necessary when considering Hamiltonian-based quantum computation.","meta":{"url":"http://arxiv.org/abs/2307.14223v1"},"cats":{"new-dataset":0.0157775387,"dev-research":0.2341911839,"prompt-eng":0.3774444669,"data-quality":0.0940633633,"ml-security":0.1512716101}}
{"text":"Flexible robots have advantages over rigid robots in their ability to conform physically to their environment and to form a wide variety of shapes.","meta":{"url":"http://arxiv.org/abs/2307.14213v1"},"cats":{"new-dataset":0.0055755281,"dev-research":0.2102245304,"prompt-eng":0.3424254824,"data-quality":0.0538689347,"ml-security":0.1161670024}}
{"text":"Sensing the force applied by or to flexible robots is useful for both navigation and manipulation tasks, but it is challenging due to the need for the sensors to withstand the robots' shape change without encumbering their functionality.","meta":{"url":"http://arxiv.org/abs/2307.14213v1"},"cats":{"new-dataset":0.0126838974,"dev-research":0.2134418064,"prompt-eng":0.415316172,"data-quality":0.0759201396,"ml-security":0.1435351592}}
{"text":"Also, for robots with long or large bodies, the number of sensors required to cover the entire surface area of the robot body can be prohibitive due to high cost and complexity.","meta":{"url":"http://arxiv.org/abs/2307.14213v1"},"cats":{"new-dataset":0.0258444289,"dev-research":0.2386231318,"prompt-eng":0.3461600018,"data-quality":0.0465756681,"ml-security":0.147867939}}
{"text":"We present a novel soft air pocket force sensor that is highly flexible, lightweight, relatively inexpensive, and easily scalable to various sizes.","meta":{"url":"http://arxiv.org/abs/2307.14213v1"},"cats":{"new-dataset":0.1065727665,"dev-research":0.1883424549,"prompt-eng":0.4083254777,"data-quality":0.0830780842,"ml-security":0.080168039}}
{"text":"Our sensor produces a change in internal pressure that is linear with the applied force.","meta":{"url":"http://arxiv.org/abs/2307.14213v1"},"cats":{"new-dataset":0.0299672172,"dev-research":0.242162859,"prompt-eng":0.3611854788,"data-quality":0.1723132446,"ml-security":0.1732340789}}
{"text":"We present results of experimental testing of how uncontrollable factors (contact location and contact area) and controllable factors (initial internal pressure, thickness, size, and number of interior seals) affect the sensitivity.","meta":{"url":"http://arxiv.org/abs/2307.14213v1"},"cats":{"new-dataset":0.0298591424,"dev-research":0.2457126509,"prompt-eng":0.4885951874,"data-quality":0.136481015,"ml-security":0.1836399652}}
{"text":"We demonstrate our sensor applied to a vine robot-a soft inflatable robot that \"grows\" from the tip via eversion-and we show that the robot can successfully grow and steer towards an object with which it senses contact.","meta":{"url":"http://arxiv.org/abs/2307.14213v1"},"cats":{"new-dataset":0.1069841183,"dev-research":0.2090764038,"prompt-eng":0.4184278076,"data-quality":0.0845325477,"ml-security":0.0759325591}}
{"text":"Data-driven requirements engineering leverages the abundance of openly accessible and crowdsourced information on the web.","meta":{"url":"http://arxiv.org/abs/2307.14212v1"},"cats":{"new-dataset":0.1680904683,"dev-research":0.440199308,"prompt-eng":0.4351992145,"data-quality":0.1746510017,"ml-security":0.1582796551}}
{"text":"By incorporating user feedback provided about a software product, such as reviews in mobile app stores, these approaches facilitate the identification of issues, bug fixes, and implementation of change requests.","meta":{"url":"http://arxiv.org/abs/2307.14212v1"},"cats":{"new-dataset":0.0662179467,"dev-research":0.5217390075,"prompt-eng":0.465890631,"data-quality":0.3449362998,"ml-security":0.1352182856}}
{"text":"However, relying solely on user feedback about a software product limits the possibility of eliciting all requirements, as users may not always have a clear understanding of their exact needs from the software, despite their wealth of experience with the problem, event, or challenges they encounter and use the software to assist them.","meta":{"url":"http://arxiv.org/abs/2307.14212v1"},"cats":{"new-dataset":0.017177804,"dev-research":0.5025687305,"prompt-eng":0.4687345463,"data-quality":0.1762532997,"ml-security":0.1895938092}}
{"text":"In this study, we propose a shift in requirements elicitation, focusing on gathering feedback related to the problem itself rather than relying solely on feedback about the software product.","meta":{"url":"http://arxiv.org/abs/2307.14212v1"},"cats":{"new-dataset":0.0513003925,"dev-research":0.5169351357,"prompt-eng":0.5186428999,"data-quality":0.2112621664,"ml-security":0.088844601}}
{"text":"We conducted a case study on student requirements during the COVID-19 pandemic in a higher education institution.","meta":{"url":"http://arxiv.org/abs/2307.14212v1"},"cats":{"new-dataset":0.1372885864,"dev-research":0.2772340731,"prompt-eng":0.4107979514,"data-quality":0.0718659146,"ml-security":0.1737709275}}
{"text":"We gathered their communications from Reddit during the pandemic and employed multiple machine-learning and natural language processing techniques to identify requirement sentences.","meta":{"url":"http://arxiv.org/abs/2307.14212v1"},"cats":{"new-dataset":0.3379610735,"dev-research":0.2582856675,"prompt-eng":0.4334993025,"data-quality":0.2129504976,"ml-security":0.1330149877}}
{"text":"We achieved the F-score of 0.79 using Naive Bayes with TF-IDF when benchmarking multiple techniques.","meta":{"url":"http://arxiv.org/abs/2307.14212v1"},"cats":{"new-dataset":0.1486286761,"dev-research":0.2596587931,"prompt-eng":0.4338416298,"data-quality":0.3678881408,"ml-security":0.0813273828}}
{"text":"The results lead us to believe that mining requirements from communication about a problem are feasible.","meta":{"url":"http://arxiv.org/abs/2307.14212v1"},"cats":{"new-dataset":0.0278050117,"dev-research":0.3006079162,"prompt-eng":0.3856638654,"data-quality":0.1773457442,"ml-security":0.1280389212}}
{"text":"While we present the preliminary results, we envision a future where these requirements complement conventionally elicited requirements and help to close the requirements gap.","meta":{"url":"http://arxiv.org/abs/2307.14212v1"},"cats":{"new-dataset":0.1381667384,"dev-research":0.3338919872,"prompt-eng":0.5358008427,"data-quality":0.1898883667,"ml-security":0.0755856969}}
{"text":"Monitoring a population of dependent processes under limited resources is critical for abnormal events detection.","meta":{"url":"http://arxiv.org/abs/2307.14208v1"},"cats":{"new-dataset":0.066753042,"dev-research":0.2622172568,"prompt-eng":0.4487918382,"data-quality":0.2128987591,"ml-security":0.2075661664}}
{"text":"A novel online collaborative learning method is proposed to adaptively allocate the resources for exploitation of high-risk processes and exploration of dependent dynamics.","meta":{"url":"http://arxiv.org/abs/2307.14208v1"},"cats":{"new-dataset":0.0404358782,"dev-research":0.3492723722,"prompt-eng":0.3669285471,"data-quality":0.0824991451,"ml-security":0.2719632479}}
{"text":"Efficiency of the proposed method is proved through theoretical analysis and experiments.","meta":{"url":"http://arxiv.org/abs/2307.14208v1"},"cats":{"new-dataset":0.0018582288,"dev-research":0.2523871111,"prompt-eng":0.3744466422,"data-quality":0.10509518,"ml-security":0.0457288411}}
{"text":"This exploratory study investigates the potential of the artificial intelligence tool, ChatGPT, to support systems thinking (ST) in various subjects.","meta":{"url":"http://arxiv.org/abs/2307.14206v1"},"cats":{"new-dataset":0.0988190297,"dev-research":0.4411344515,"prompt-eng":0.3653161312,"data-quality":0.0958948077,"ml-security":0.1061070998}}
{"text":"Using both general and subject specific prompts, the study assesses the accuracy, helpfulness, and reliability of ChatGPT's responses across different versions of the tool.","meta":{"url":"http://arxiv.org/abs/2307.14206v1"},"cats":{"new-dataset":0.0567204936,"dev-research":0.3890922809,"prompt-eng":0.4792408053,"data-quality":0.2328577137,"ml-security":0.0628058327}}
{"text":"The results indicate that ChatGPT can provide largely correct and very helpful responses in various subjects, demonstrating its potential as a tool for enhancing ST skills.","meta":{"url":"http://arxiv.org/abs/2307.14206v1"},"cats":{"new-dataset":0.0482251438,"dev-research":0.3714823258,"prompt-eng":0.3887743941,"data-quality":0.1652033004,"ml-security":0.0711864696}}
{"text":"However, occasional inaccuracies highlight the need for users to remain critical of ChatGPT's responses.","meta":{"url":"http://arxiv.org/abs/2307.14206v1"},"cats":{"new-dataset":0.024171328,"dev-research":0.421388827,"prompt-eng":0.4363444913,"data-quality":0.3348000346,"ml-security":0.1488532172}}
{"text":"Despite some limitations, this study suggests that with careful use and attention to its idiosyncrasies, ChatGPT can be a valuable tool for teaching and learning ST.","meta":{"url":"http://arxiv.org/abs/2307.14206v1"},"cats":{"new-dataset":0.208339345,"dev-research":0.4138548162,"prompt-eng":0.3373763422,"data-quality":0.1479224176,"ml-security":0.110935855}}
{"text":"This paper investigates a spherical transmitter (TX) with a membrane covered by heterogeneous receptors of varying sizes and arbitrary locations for molecular communication (MC), where molecules are encapsulated within vesicles and released from the TX through membrane fusion.","meta":{"url":"http://arxiv.org/abs/2307.14202v1"},"cats":{"new-dataset":0.030078955,"dev-research":0.216423995,"prompt-eng":0.384823589,"data-quality":0.0810021584,"ml-security":0.1720040841}}
{"text":"Assuming continuous vesicle generation at the TX and a transparent receiver (RX), we calculate the molecule release rate, the fraction of absorbed molecules at the TX, and the received signal at the RX.","meta":{"url":"http://arxiv.org/abs/2307.14202v1"},"cats":{"new-dataset":0.0201447023,"dev-research":0.1818821796,"prompt-eng":0.4320351315,"data-quality":0.0975318539,"ml-security":0.0803509754}}
{"text":"All obtained analytical expressions are functions of all receptors locations and sizes, and are validated by particle-based simulations.","meta":{"url":"http://arxiv.org/abs/2307.14202v1"},"cats":{"new-dataset":0.022305484,"dev-research":0.1964063049,"prompt-eng":0.3889187951,"data-quality":0.0950831669,"ml-security":0.1057698166}}
{"text":"Our numerical results indicate that evenly distributed receptors on the TX membrane can absorb more molecules than randomly distributed receptors or a single receptor.","meta":{"url":"http://arxiv.org/abs/2307.14202v1"},"cats":{"new-dataset":0.0187927003,"dev-research":0.1334094459,"prompt-eng":0.4104931173,"data-quality":0.0896369558,"ml-security":0.1434259225}}
{"text":"Furthermore, inspired by the autoreceptor functionality in synaptic communication, we incorporate a negative feedback mechanism (NFM) at the TX, such that molecule release stops after a certain period.","meta":{"url":"http://arxiv.org/abs/2307.14202v1"},"cats":{"new-dataset":0.0229287904,"dev-research":0.249768667,"prompt-eng":0.4730401265,"data-quality":0.1583392699,"ml-security":0.1227576189}}
{"text":"We then derive the fraction of molecules that can be reused for the subsequent emissions when considering both NFM and molecule harvesting.","meta":{"url":"http://arxiv.org/abs/2307.14202v1"},"cats":{"new-dataset":0.0267003977,"dev-research":0.1952113322,"prompt-eng":0.3841096168,"data-quality":0.1527078011,"ml-security":0.075090365}}
{"text":"Our numerical results demonstrate that incorporating NFM can reduce inter-symbol interference (ISI) while maintaining the same peak received signal as the case without NFM.","meta":{"url":"http://arxiv.org/abs/2307.14202v1"},"cats":{"new-dataset":0.0130324828,"dev-research":0.2663430008,"prompt-eng":0.4396543101,"data-quality":0.1773458106,"ml-security":0.0899593832}}
{"text":"Additionally, our results show that TXs incorporating both molecule harvesting and NFM can achieve a higher energy efficiency and lower error probability than TXs employing only molecule harvesting or neither functionality.","meta":{"url":"http://arxiv.org/abs/2307.14202v1"},"cats":{"new-dataset":0.0144872681,"dev-research":0.2462416952,"prompt-eng":0.4072692314,"data-quality":0.1531344396,"ml-security":0.0551246951}}
{"text":"The hydrometallurgical method of zinc production involves leaching zinc from ore and then separating the solid residue from the liquid solution by pressure filtration.","meta":{"url":"http://arxiv.org/abs/2307.14199v1"},"cats":{"new-dataset":0.0096056188,"dev-research":0.3074839344,"prompt-eng":0.3034302836,"data-quality":0.1240306247,"ml-security":0.09025081}}
{"text":"This separation process is very important since the solid residue contains some moisture that can reduce the amount of zinc recovered.","meta":{"url":"http://arxiv.org/abs/2307.14199v1"},"cats":{"new-dataset":0.0100028132,"dev-research":0.2940433805,"prompt-eng":0.3152217317,"data-quality":0.1405174112,"ml-security":0.0814430427}}
{"text":"This study modeled the pressure filtration process through Random Forest (RF) and Support Vector Machine (SVM).","meta":{"url":"http://arxiv.org/abs/2307.14199v1"},"cats":{"new-dataset":0.0404898328,"dev-research":0.2342487483,"prompt-eng":0.3764263252,"data-quality":0.1694079232,"ml-security":0.2043922563}}
{"text":"The models take continuous variables (extracted features) from the lab samples as inputs.","meta":{"url":"http://arxiv.org/abs/2307.14199v1"},"cats":{"new-dataset":0.1348828412,"dev-research":0.2416022355,"prompt-eng":0.4212771223,"data-quality":0.1494676791,"ml-security":0.1117091326}}
{"text":"Thus, regression models namely Random Forest Regression (RFR) and Support Vector Regression (SVR) were chosen.","meta":{"url":"http://arxiv.org/abs/2307.14199v1"},"cats":{"new-dataset":0.0930223869,"dev-research":0.2444147383,"prompt-eng":0.370776233,"data-quality":0.1607594583,"ml-security":0.1639528941}}
{"text":"A total dataset was obtained during the pressure filtration process in two conditions: 1) Polypropylene (S1) and 2) Polyester fabrics (S2).","meta":{"url":"http://arxiv.org/abs/2307.14199v1"},"cats":{"new-dataset":0.2057589821,"dev-research":0.2198852685,"prompt-eng":0.3767101689,"data-quality":0.139839018,"ml-security":0.083719501}}
{"text":"To predict the cake moisture, solids concentration (0.2 and 0.38), temperature (35 and 65 centigrade), pH (2, 3.5, and 5), pressure, cake thickness (14, 20, 26, and 34 mm), air-blow time (2, 10 and 15 min) and filtration time were applied as input variables.","meta":{"url":"http://arxiv.org/abs/2307.14199v1"},"cats":{"new-dataset":0.1227247882,"dev-research":0.2197749982,"prompt-eng":0.4049174488,"data-quality":0.0683395248,"ml-security":0.1150076676}}
{"text":"The models' predictive accuracy was evaluated by the coefficient of determination (R2) parameter.","meta":{"url":"http://arxiv.org/abs/2307.14199v1"},"cats":{"new-dataset":0.0237112904,"dev-research":0.2040595484,"prompt-eng":0.4430379046,"data-quality":0.2224239571,"ml-security":0.0996758111}}
{"text":"The results revealed that the RFR model is superior to the SVR model for cake moisture prediction.","meta":{"url":"http://arxiv.org/abs/2307.14199v1"},"cats":{"new-dataset":0.0505200725,"dev-research":0.2118804651,"prompt-eng":0.4355281337,"data-quality":0.0940681278,"ml-security":0.0789682213}}
{"text":"Numerous models for supervised and reinforcement learning benefit from combinations of discrete and continuous model components.","meta":{"url":"http://arxiv.org/abs/2307.14193v1"},"cats":{"new-dataset":0.0159830153,"dev-research":0.1631644253,"prompt-eng":0.3798168819,"data-quality":0.0860033944,"ml-security":0.1215195447}}
{"text":"End-to-end learnable discrete-continuous models are compositional, tend to generalize better, and are more interpretable.","meta":{"url":"http://arxiv.org/abs/2307.14193v1"},"cats":{"new-dataset":0.0160114991,"dev-research":0.215079181,"prompt-eng":0.3475055368,"data-quality":0.1697676645,"ml-security":0.1395101353}}
{"text":"A popular approach to building discrete-continuous computation graphs is that of integrating discrete probability distributions into neural networks using stochastic softmax tricks.","meta":{"url":"http://arxiv.org/abs/2307.14193v1"},"cats":{"new-dataset":0.0624118645,"dev-research":0.2500349204,"prompt-eng":0.3412904182,"data-quality":0.1541550074,"ml-security":0.1920306628}}
{"text":"Prior work has mainly focused on computation graphs with a single discrete component on each of the graph's execution paths.","meta":{"url":"http://arxiv.org/abs/2307.14193v1"},"cats":{"new-dataset":0.0772691918,"dev-research":0.3592457577,"prompt-eng":0.3923843841,"data-quality":0.1043810332,"ml-security":0.0775332878}}
{"text":"We analyze the behavior of more complex stochastic computations graphs with multiple sequential discrete components.","meta":{"url":"http://arxiv.org/abs/2307.14193v1"},"cats":{"new-dataset":0.0596689408,"dev-research":0.2493629327,"prompt-eng":0.3631842918,"data-quality":0.1244926169,"ml-security":0.091798406}}
{"text":"We show that it is challenging to optimize the parameters of these models, mainly due to small gradients and local minima.","meta":{"url":"http://arxiv.org/abs/2307.14193v1"},"cats":{"new-dataset":0.0447755038,"dev-research":0.1641424847,"prompt-eng":0.4444281302,"data-quality":0.1847246798,"ml-security":0.1583920279}}
{"text":"We then propose two new strategies to overcome these challenges.","meta":{"url":"http://arxiv.org/abs/2307.14193v1"},"cats":{"new-dataset":0.0244161131,"dev-research":0.295711948,"prompt-eng":0.3802789686,"data-quality":0.0961965874,"ml-security":0.137288007}}
{"text":"First, we show that increasing the scale parameter of the Gumbel noise perturbations during training improves the learning behavior.","meta":{"url":"http://arxiv.org/abs/2307.14193v1"},"cats":{"new-dataset":0.0453996157,"dev-research":0.2348873305,"prompt-eng":0.3845869546,"data-quality":0.3082215729,"ml-security":0.2923519562}}
{"text":"Second, we propose dropout residual connections specifically tailored to stochastic, discrete-continuous computation graphs.","meta":{"url":"http://arxiv.org/abs/2307.14193v1"},"cats":{"new-dataset":0.067979364,"dev-research":0.268020004,"prompt-eng":0.3590655536,"data-quality":0.2166624778,"ml-security":0.1335411097}}
{"text":"With an extensive set of experiments, we show that we can train complex discrete-continuous models which one cannot train with standard stochastic softmax tricks.","meta":{"url":"http://arxiv.org/abs/2307.14193v1"},"cats":{"new-dataset":0.0608539363,"dev-research":0.1499941755,"prompt-eng":0.3503804202,"data-quality":0.1846909372,"ml-security":0.2428641997}}
{"text":"We also show that complex discrete-stochastic models generalize better than their continuous counterparts on several benchmark datasets.","meta":{"url":"http://arxiv.org/abs/2307.14193v1"},"cats":{"new-dataset":0.1078278921,"dev-research":0.1871153309,"prompt-eng":0.3521545998,"data-quality":0.1468565615,"ml-security":0.1085715678}}
{"text":"This paper delves into the realm of ChatGPT, an AI-powered chatbot that utilizes topic modeling and reinforcement learning to generate natural responses.","meta":{"url":"http://arxiv.org/abs/2307.14192v1"},"cats":{"new-dataset":0.1516941735,"dev-research":0.2643572022,"prompt-eng":0.427985818,"data-quality":0.163158853,"ml-security":0.117850904}}
{"text":"Although ChatGPT holds immense promise across various industries, such as customer service, education, mental health treatment, personal productivity, and content creation, it is essential to address its security, privacy, and ethical implications.","meta":{"url":"http://arxiv.org/abs/2307.14192v1"},"cats":{"new-dataset":0.1475464217,"dev-research":0.3359250815,"prompt-eng":0.2903415918,"data-quality":0.1153145797,"ml-security":0.2530655636}}
{"text":"By exploring the upgrade path from GPT-1 to GPT-4, discussing the model's features, limitations, and potential applications, this study aims to shed light on the potential risks of integrating ChatGPT into our daily lives.","meta":{"url":"http://arxiv.org/abs/2307.14192v1"},"cats":{"new-dataset":0.2177429044,"dev-research":0.3317628899,"prompt-eng":0.377157146,"data-quality":0.1517801669,"ml-security":0.2421670963}}
{"text":"Focusing on security, privacy, and ethics issues, we highlight the challenges these concerns pose for widespread adoption.","meta":{"url":"http://arxiv.org/abs/2307.14192v1"},"cats":{"new-dataset":0.0756004969,"dev-research":0.2913924919,"prompt-eng":0.3526231445,"data-quality":0.1704770224,"ml-security":0.4482119949}}
{"text":"Finally, we analyze the open problems in these areas, calling for concerted efforts to ensure the development of secure and ethically sound large language models.","meta":{"url":"http://arxiv.org/abs/2307.14192v1"},"cats":{"new-dataset":0.2102634523,"dev-research":0.2238228944,"prompt-eng":0.389929755,"data-quality":0.2335677381,"ml-security":0.3409805522}}
{"text":"Forecasting future trajectories of agents in complex traffic scenes requires reliable and efficient predictions for all agents in the scene.","meta":{"url":"http://arxiv.org/abs/2307.14187v1"},"cats":{"new-dataset":0.1237901154,"dev-research":0.1885465362,"prompt-eng":0.3973945198,"data-quality":0.0864232432,"ml-security":0.1304056915}}
{"text":"However, existing methods for trajectory prediction are either inefficient or sacrifice accuracy.","meta":{"url":"http://arxiv.org/abs/2307.14187v1"},"cats":{"new-dataset":0.0105544666,"dev-research":0.2423079911,"prompt-eng":0.3528997126,"data-quality":0.1199983082,"ml-security":0.1353842222}}
{"text":"To address this challenge, we propose ADAPT, a novel approach for jointly predicting the trajectories of all agents in the scene with dynamic weight learning.","meta":{"url":"http://arxiv.org/abs/2307.14187v1"},"cats":{"new-dataset":0.2087777541,"dev-research":0.1444476226,"prompt-eng":0.4062524903,"data-quality":0.11217339,"ml-security":0.1608663543}}
{"text":"Our approach outperforms state-of-the-art methods in both single-agent and multi-agent settings on the Argoverse and Interaction datasets, with a fraction of their computational overhead.","meta":{"url":"http://arxiv.org/abs/2307.14187v1"},"cats":{"new-dataset":0.4246233038,"dev-research":0.2193894395,"prompt-eng":0.3989372322,"data-quality":0.1102170154,"ml-security":0.1044229011}}
{"text":"We attribute the improvement in our performance: first, to the adaptive head augmenting the model capacity without increasing the model size; second, to our design choices in the endpoint-conditioned prediction, reinforced by gradient stopping.","meta":{"url":"http://arxiv.org/abs/2307.14187v1"},"cats":{"new-dataset":0.0336949212,"dev-research":0.2068357542,"prompt-eng":0.4933538387,"data-quality":0.1446197439,"ml-security":0.1477877833}}
{"text":"Our analyses show that ADAPT can focus on each agent with adaptive prediction, allowing for accurate predictions efficiently.","meta":{"url":"http://arxiv.org/abs/2307.14187v1"},"cats":{"new-dataset":0.0402230877,"dev-research":0.2406261524,"prompt-eng":0.4588848098,"data-quality":0.098544741,"ml-security":0.1805021174}}
{"text":"https://KUIS-AI.github.io/adapt","meta":{"url":"http://arxiv.org/abs/2307.14187v1"},"cats":{"new-dataset":0.4701750363,"dev-research":0.2679643368,"prompt-eng":0.4334702976,"data-quality":0.1323540829,"ml-security":0.0711735628}}
{"text":"Wireless communication is enabling billions of people to connect to each other and the internet, transforming every sector of the economy, and building the foundations for powerful new technologies that hold great promise to improve lives at an unprecedented rate and scale.","meta":{"url":"http://arxiv.org/abs/2307.14186v1"},"cats":{"new-dataset":0.035935375,"dev-research":0.2876175642,"prompt-eng":0.2962973135,"data-quality":0.0593657684,"ml-security":0.0981653497}}
{"text":"The rapid increase in the number of devices and the associated demands for higher data rates and broader network coverage fuels the need for more robust wireless technologies.","meta":{"url":"http://arxiv.org/abs/2307.14186v1"},"cats":{"new-dataset":0.0419118838,"dev-research":0.2827824869,"prompt-eng":0.3980636327,"data-quality":0.1136717808,"ml-security":0.1733082824}}
{"text":"The key technology identified to address this problem is referred to as Cell-Free Massive MIMO (CF-mMIMO).","meta":{"url":"http://arxiv.org/abs/2307.14186v1"},"cats":{"new-dataset":0.0590980639,"dev-research":0.2052644536,"prompt-eng":0.3564331549,"data-quality":0.0797020533,"ml-security":0.0844365602}}
{"text":"CF-mMIMO is accompanied by many challenges, one of which is efficiently allocating limited resources.","meta":{"url":"http://arxiv.org/abs/2307.14186v1"},"cats":{"new-dataset":0.0546660661,"dev-research":0.3162845368,"prompt-eng":0.3687952073,"data-quality":0.1053548292,"ml-security":0.1241971196}}
{"text":"In this paper, we focus on a major resource allocation problem in wireless networks, namely the Pilot Assignment problem (PA).","meta":{"url":"http://arxiv.org/abs/2307.14186v1"},"cats":{"new-dataset":0.0236891139,"dev-research":0.2440344173,"prompt-eng":0.3910143646,"data-quality":0.1341818562,"ml-security":0.1476814751}}
{"text":"We show that PA is strongly NP-hard and that it does not admit a polynomial-time constant-factor approximation algorithm.","meta":{"url":"http://arxiv.org/abs/2307.14186v1"},"cats":{"new-dataset":0.0491725986,"dev-research":0.1951634008,"prompt-eng":0.3656891206,"data-quality":0.1344871716,"ml-security":0.1098772229}}
{"text":"Further, we show that PA cannot be approximated in polynomial time within $\\mathcal{O}(K^2)$ (where $K$ is the number of users) when the system consists of at least three pilots.","meta":{"url":"http://arxiv.org/abs/2307.14186v1"},"cats":{"new-dataset":0.044689703,"dev-research":0.2204411827,"prompt-eng":0.3737485993,"data-quality":0.1147214782,"ml-security":0.1671856777}}
{"text":"Finally, we present an approximation lower bound of $1.058$ (resp.","meta":{"url":"http://arxiv.org/abs/2307.14186v1"},"cats":{"new-dataset":0.1627193615,"dev-research":0.1847665804,"prompt-eng":0.3980351447,"data-quality":0.2194592792,"ml-security":0.0848623224}}
{"text":"$\\epsilon|K|^2$, for $\\epsilon >0$) in special cases where the system consists of exactly two (resp.","meta":{"url":"http://arxiv.org/abs/2307.14186v1"},"cats":{"new-dataset":0.0580450181,"dev-research":0.2006783912,"prompt-eng":0.3104619074,"data-quality":0.1801320441,"ml-security":0.2242275832}}
{"text":"three) pilots.","meta":{"url":"http://arxiv.org/abs/2307.14186v1"},"cats":{"new-dataset":0.1902458174,"dev-research":0.2070556781,"prompt-eng":0.366897456,"data-quality":0.1612028158,"ml-security":0.1015213481}}
{"text":"Low-lying coastal cities, exemplified by Norfolk, Virginia, face the challenge of street flooding caused by rainfall and tides, which strain transportation and sewer systems and can lead to property damage.","meta":{"url":"http://arxiv.org/abs/2307.14185v1"},"cats":{"new-dataset":0.0909825141,"dev-research":0.3057792529,"prompt-eng":0.3286637865,"data-quality":0.0996794544,"ml-security":0.2551654728}}
{"text":"While high-fidelity, physics-based simulations provide accurate predictions of urban pluvial flooding, their computational complexity renders them unsuitable for real-time applications.","meta":{"url":"http://arxiv.org/abs/2307.14185v1"},"cats":{"new-dataset":0.0917702536,"dev-research":0.2971084103,"prompt-eng":0.3549947056,"data-quality":0.0923277606,"ml-security":0.1413676694}}
{"text":"Using data from Norfolk rainfall events between 2016 and 2018, this study compares the performance of a previous surrogate model based on a random forest algorithm with two deep learning models: Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU).","meta":{"url":"http://arxiv.org/abs/2307.14185v1"},"cats":{"new-dataset":0.3260484549,"dev-research":0.1910785038,"prompt-eng":0.3506962958,"data-quality":0.1056194558,"ml-security":0.1392825496}}
{"text":"This investigation underscores the importance of using a model architecture that supports the communication of prediction uncertainty and the effective integration of relevant, multi-modal features.","meta":{"url":"http://arxiv.org/abs/2307.14185v1"},"cats":{"new-dataset":0.0335238619,"dev-research":0.2698615555,"prompt-eng":0.4562992912,"data-quality":0.2117505408,"ml-security":0.1350164388}}
{"text":"DeepLab is a widely used deep neural network for semantic segmentation, whose success is attributed to its parallel architecture called atrous spatial pyramid pooling (ASPP).","meta":{"url":"http://arxiv.org/abs/2307.14179v1"},"cats":{"new-dataset":0.2770398908,"dev-research":0.2626378782,"prompt-eng":0.3544228183,"data-quality":0.2358413479,"ml-security":0.1088375914}}
{"text":"ASPP uses multiple atrous convolutions with different atrous rates to extract both local and global information.","meta":{"url":"http://arxiv.org/abs/2307.14179v1"},"cats":{"new-dataset":0.0901309642,"dev-research":0.2011499151,"prompt-eng":0.3871386728,"data-quality":0.1389539047,"ml-security":0.0735292533}}
{"text":"However, fixed values of atrous rates are used for the ASPP module, which restricts the size of its field of view.","meta":{"url":"http://arxiv.org/abs/2307.14179v1"},"cats":{"new-dataset":0.0603523226,"dev-research":0.199592098,"prompt-eng":0.3658534348,"data-quality":0.1197049826,"ml-security":0.0727492564}}
{"text":"In principle, atrous rate should be a hyperparameter to change the field of view size according to the target task or dataset.","meta":{"url":"http://arxiv.org/abs/2307.14179v1"},"cats":{"new-dataset":0.0160658245,"dev-research":0.2117355901,"prompt-eng":0.4314989639,"data-quality":0.1170648966,"ml-security":0.0772473423}}
{"text":"However, the manipulation of atrous rate is not governed by any guidelines.","meta":{"url":"http://arxiv.org/abs/2307.14179v1"},"cats":{"new-dataset":0.005482538,"dev-research":0.1847976591,"prompt-eng":0.3241215524,"data-quality":0.1258108234,"ml-security":0.1124905539}}
{"text":"This study proposes practical guidelines for obtaining an optimal atrous rate.","meta":{"url":"http://arxiv.org/abs/2307.14179v1"},"cats":{"new-dataset":0.0053003902,"dev-research":0.2323044749,"prompt-eng":0.4022090067,"data-quality":0.0702233252,"ml-security":0.0672392348}}
{"text":"First, an effective receptive field for semantic segmentation is introduced to analyze the inner behavior of segmentation networks.","meta":{"url":"http://arxiv.org/abs/2307.14179v1"},"cats":{"new-dataset":0.0434706353,"dev-research":0.2618296904,"prompt-eng":0.4171816841,"data-quality":0.2602274835,"ml-security":0.1378089174}}
{"text":"We observed that the use of ASPP module yielded a specific pattern in the effective receptive field, which was traced to reveal the module's underlying mechanism.","meta":{"url":"http://arxiv.org/abs/2307.14179v1"},"cats":{"new-dataset":0.0321183688,"dev-research":0.2794630907,"prompt-eng":0.4434164307,"data-quality":0.171963196,"ml-security":0.1641688625}}
{"text":"Accordingly, we derive practical guidelines for obtaining the optimal atrous rate, which should be controlled based on the size of input image.","meta":{"url":"http://arxiv.org/abs/2307.14179v1"},"cats":{"new-dataset":0.0263562595,"dev-research":0.2054210866,"prompt-eng":0.4643895239,"data-quality":0.1113632757,"ml-security":0.064690882}}
{"text":"Compared to other values, using the optimal atrous rate consistently improved the segmentation results across multiple datasets, including the STARE, CHASE_DB1, HRF, Cityscapes, and iSAID datasets.","meta":{"url":"http://arxiv.org/abs/2307.14179v1"},"cats":{"new-dataset":0.3866770357,"dev-research":0.2106223686,"prompt-eng":0.4101205673,"data-quality":0.2243227847,"ml-security":0.0598138316}}
{"text":"In this paper we have addressed the implementation of the accumulation and projection of high-resolution event data stream (HD -1280 x 720 pixels) onto the image plane in FPGA devices.","meta":{"url":"http://arxiv.org/abs/2307.14177v1"},"cats":{"new-dataset":0.4794778706,"dev-research":0.2917123243,"prompt-eng":0.3951557292,"data-quality":0.1071118954,"ml-security":0.0818062897}}
{"text":"The results confirm the feasibility of this approach, but there are a number of challenges, limitations and trade-offs to be considered.","meta":{"url":"http://arxiv.org/abs/2307.14177v1"},"cats":{"new-dataset":0.0216125126,"dev-research":0.2101165498,"prompt-eng":0.4149212183,"data-quality":0.1423972541,"ml-security":0.0841902409}}
{"text":"The required hardware resources of selected data representations, such as binary frame, event frame, exponentially decaying time surface and event frequency, were compared with those available on several popular platforms from AMD Xilinx.","meta":{"url":"http://arxiv.org/abs/2307.14177v1"},"cats":{"new-dataset":0.2360193584,"dev-research":0.3122303226,"prompt-eng":0.3997166795,"data-quality":0.0986485998,"ml-security":0.0764459186}}
{"text":"The resulting event frames can be used for typical vision algorithms, such as object classification and detection, using both classical and deep neural network methods.","meta":{"url":"http://arxiv.org/abs/2307.14177v1"},"cats":{"new-dataset":0.1038902348,"dev-research":0.2372856989,"prompt-eng":0.3830917247,"data-quality":0.1992164888,"ml-security":0.1092029802}}
{"text":"$\\text{TT}^{\\Box}_{{\\mathcal C}}$ is a generic family of effectful, extensional type theories with a forcing interpretation parameterized by modalities.","meta":{"url":"http://arxiv.org/abs/2307.14168v1"},"cats":{"new-dataset":0.0392207457,"dev-research":0.2208765475,"prompt-eng":0.393941987,"data-quality":0.1220281396,"ml-security":0.1456122162}}
{"text":"This paper identifies a subclass of $\\text{TT}^{\\Box}_{{\\mathcal C}}$ theories that internally realizes continuity principles through stateful computations, such as reference cells.","meta":{"url":"http://arxiv.org/abs/2307.14168v1"},"cats":{"new-dataset":0.0819388031,"dev-research":0.2479516753,"prompt-eng":0.3684740626,"data-quality":0.1087655847,"ml-security":0.1249641134}}
{"text":"The principle of continuity is a seminal property that holds for a number of intuitionistic theories such as System T. Roughly speaking, it states that functions on real numbers only need approximations of these numbers to compute.","meta":{"url":"http://arxiv.org/abs/2307.14168v1"},"cats":{"new-dataset":0.0074968773,"dev-research":0.320349455,"prompt-eng":0.2839121707,"data-quality":0.0760205578,"ml-security":0.1171974761}}
{"text":"Generally, continuity principles have been justified using semantical arguments, but it is known that the modulus of continuity of functions can be computed using effectful computations such as exceptions or reference cells.","meta":{"url":"http://arxiv.org/abs/2307.14168v1"},"cats":{"new-dataset":0.0111358565,"dev-research":0.4112472095,"prompt-eng":0.3312689649,"data-quality":0.172051256,"ml-security":0.0913385108}}
{"text":"In this paper, the modulus of continuity of the functionals on the Baire space is directly computed using the stateful computations enabled internally in the theory.","meta":{"url":"http://arxiv.org/abs/2307.14168v1"},"cats":{"new-dataset":0.0528921582,"dev-research":0.2268156787,"prompt-eng":0.3595467246,"data-quality":0.1631610146,"ml-security":0.088310611}}
{"text":"The control of free-floating robots requires dealing with several challenges.","meta":{"url":"http://arxiv.org/abs/2307.14164v1"},"cats":{"new-dataset":0.0404109914,"dev-research":0.2932655339,"prompt-eng":0.3787614171,"data-quality":0.0750903342,"ml-security":0.1319147156}}
{"text":"The motion of such robots evolves on a continuous manifold described by the Special Euclidean Group of dimension 3, known as SE(3).","meta":{"url":"http://arxiv.org/abs/2307.14164v1"},"cats":{"new-dataset":0.108181632,"dev-research":0.1465851238,"prompt-eng":0.3231529819,"data-quality":0.0553409388,"ml-security":0.0928553757}}
{"text":"Methods from finite horizon Linear Quadratic Regulators (LQR) control have gained recent traction in the robotics community.","meta":{"url":"http://arxiv.org/abs/2307.14164v1"},"cats":{"new-dataset":0.0149680382,"dev-research":0.2265533012,"prompt-eng":0.3594904634,"data-quality":0.0683014358,"ml-security":0.0943178717}}
{"text":"However, such approaches are inherently solving an unconstrained optimization problem and hence are unable to respect the manifold constraints imposed by the group structure of SE(3).","meta":{"url":"http://arxiv.org/abs/2307.14164v1"},"cats":{"new-dataset":0.0182415928,"dev-research":0.1931558897,"prompt-eng":0.3195292077,"data-quality":0.1381085468,"ml-security":0.1067351489}}
{"text":"This may lead to small errors, singularity problems and double cover issues depending on the choice of coordinates to model the floating base motion.","meta":{"url":"http://arxiv.org/abs/2307.14164v1"},"cats":{"new-dataset":0.0087620266,"dev-research":0.2403071906,"prompt-eng":0.4002470444,"data-quality":0.2362669558,"ml-security":0.1286235267}}
{"text":"In this paper, we propose the use of canonical exponential coordinates of SE(3) and the associated Exponential map along with its differentials to embed this structure in the theory of finite horizon LQR controllers.","meta":{"url":"http://arxiv.org/abs/2307.14164v1"},"cats":{"new-dataset":0.0356665266,"dev-research":0.1796653793,"prompt-eng":0.3675595721,"data-quality":0.0608650323,"ml-security":0.1467186072}}
{"text":"We propose a new method to quantify the impact of cyber attacks in Cyber Physical Systems (CPSs).","meta":{"url":"http://arxiv.org/abs/2307.14161v1"},"cats":{"new-dataset":0.0459491284,"dev-research":0.3728409946,"prompt-eng":0.3815009359,"data-quality":0.1371387591,"ml-security":0.4938687313}}
{"text":"In particular, our method allows to identify the Design Parameter (DPs) affected due to a cyber attack launched on a different set of DPs in the same CPS.","meta":{"url":"http://arxiv.org/abs/2307.14161v1"},"cats":{"new-dataset":0.0795407044,"dev-research":0.4344792461,"prompt-eng":0.5309378182,"data-quality":0.2278756504,"ml-security":0.3581415689}}
{"text":"To achieve this, we adopt causal graphs to causally link DPs with each other and quantify the impact of one DP on another.","meta":{"url":"http://arxiv.org/abs/2307.14161v1"},"cats":{"new-dataset":0.0858324918,"dev-research":0.3497175261,"prompt-eng":0.4473897321,"data-quality":0.1984983609,"ml-security":0.1325559679}}
{"text":"Using SWaT, a real world testbed of a water treatment system, we demonstrate that causal graphs can be build in two ways: i) using domain knowledge of the control logic and the physical connectivity structure of the DPs, we call these causal domain graphs and ii) learning from operational data logs, we call these causal learnt graphs.","meta":{"url":"http://arxiv.org/abs/2307.14161v1"},"cats":{"new-dataset":0.1793723638,"dev-research":0.3684178004,"prompt-eng":0.4097709796,"data-quality":0.1651573791,"ml-security":0.1700394664}}
{"text":"We then compare these graphs when a same set of DPs is used.","meta":{"url":"http://arxiv.org/abs/2307.14161v1"},"cats":{"new-dataset":0.0781836083,"dev-research":0.3419298584,"prompt-eng":0.3972798318,"data-quality":0.2282967125,"ml-security":0.0909339502}}
{"text":"Our analysis shows a common set of edges between the causal domain graphs and the causal learnt graphs exists, which helps validate the causal learnt graphs.","meta":{"url":"http://arxiv.org/abs/2307.14161v1"},"cats":{"new-dataset":0.1050503192,"dev-research":0.3304984738,"prompt-eng":0.368183404,"data-quality":0.2653275976,"ml-security":0.1611727011}}
{"text":"Additionally, we show that the learnt graphs can discover new causal relations, not initially considered in the domain graphs, that help significantly characterising the impact of the attack.","meta":{"url":"http://arxiv.org/abs/2307.14161v1"},"cats":{"new-dataset":0.1146643575,"dev-research":0.3489817264,"prompt-eng":0.370425659,"data-quality":0.270991182,"ml-security":0.5103930352}}
{"text":"We use causal domain graphs to estimate the parameters of the graphs, and the causal learnt graphs for causal inference.","meta":{"url":"http://arxiv.org/abs/2307.14161v1"},"cats":{"new-dataset":0.0871418187,"dev-research":0.2930534321,"prompt-eng":0.3875002338,"data-quality":0.2077764648,"ml-security":0.1419970887}}
{"text":"To learn the structure of the causal learnt graphs in all the six-stages of SWaT, we experiment with three learning algorithms: Peter Clarke (PC), Hill Climb (HC) search and Chow-Lie (CH).","meta":{"url":"http://arxiv.org/abs/2307.14161v1"},"cats":{"new-dataset":0.1244041527,"dev-research":0.2498629114,"prompt-eng":0.3533571383,"data-quality":0.2011730192,"ml-security":0.1755352434}}
{"text":"Finally, we demonstrate how causal graphs can be used to analyse the impact of cyber attacks by analysing nine well known cyber attacks on the SWaT test bed.","meta":{"url":"http://arxiv.org/abs/2307.14161v1"},"cats":{"new-dataset":0.1257186352,"dev-research":0.3690787636,"prompt-eng":0.3992996071,"data-quality":0.1875140586,"ml-security":0.5731302462}}
{"text":"We find that by using causal learnt graphs the DPs impacted by the attacks are correctly discovered with a probability greater than 0.9.","meta":{"url":"http://arxiv.org/abs/2307.14161v1"},"cats":{"new-dataset":0.0992804132,"dev-research":0.3636847693,"prompt-eng":0.4026347641,"data-quality":0.375512195,"ml-security":0.636089502}}
{"text":"The advent of 5G New Radio (NR) technology has revolutionized the landscape of wireless communication, offering various enhancements such as elevated system capacity, improved spectrum efficiency, and higher data transmission rates.","meta":{"url":"http://arxiv.org/abs/2307.14152v1"},"cats":{"new-dataset":0.0475789371,"dev-research":0.2469080803,"prompt-eng":0.3193480766,"data-quality":0.0844988223,"ml-security":0.0556214875}}
{"text":"To achieve these benefits, 5G has implemented the Ultra-Dense Network (UDN) architecture, characterized by the deployment of numerous small general Node B (gNB) units.","meta":{"url":"http://arxiv.org/abs/2307.14152v1"},"cats":{"new-dataset":0.0773310853,"dev-research":0.2130926416,"prompt-eng":0.2950681062,"data-quality":0.0949724252,"ml-security":0.1054530692}}
{"text":"While this approach boosts system capacity and frequency reuse, it also raises concerns such as increased signal interference, longer handover times, and higher handover failure rates.","meta":{"url":"http://arxiv.org/abs/2307.14152v1"},"cats":{"new-dataset":0.0034206353,"dev-research":0.3695494085,"prompt-eng":0.3842566931,"data-quality":0.1573279573,"ml-security":0.1810244137}}
{"text":"To address these challenges, the critical factor of Time to Trigger (TTT) in handover management must be accurately determined.","meta":{"url":"http://arxiv.org/abs/2307.14152v1"},"cats":{"new-dataset":0.0278037536,"dev-research":0.3024116211,"prompt-eng":0.4866610274,"data-quality":0.1156764825,"ml-security":0.1291200802}}
{"text":"Furthermore, the density of gNBs has a significant impact on handover performance.","meta":{"url":"http://arxiv.org/abs/2307.14152v1"},"cats":{"new-dataset":0.0272381932,"dev-research":0.1952059982,"prompt-eng":0.322244635,"data-quality":0.0850001958,"ml-security":0.0881675572}}
{"text":"This study provides a comprehensive analysis of 5G handover management.","meta":{"url":"http://arxiv.org/abs/2307.14152v1"},"cats":{"new-dataset":0.0700502191,"dev-research":0.2242616744,"prompt-eng":0.3378031126,"data-quality":0.0729403487,"ml-security":0.0646477771}}
{"text":"Through the development and utilization of a downlink system-level simulator, the effects of various TTT values and gNB densities on 5G handover were evaluated, taking into consideration the movement of Traffic Users (TUs) with varying velocities.","meta":{"url":"http://arxiv.org/abs/2307.14152v1"},"cats":{"new-dataset":0.0408840304,"dev-research":0.2303441843,"prompt-eng":0.3744557197,"data-quality":0.0714694803,"ml-security":0.1084906951}}
{"text":"Simulation results showed that the handover performance can be optimized by adjusting the TTT under different gNB densities, providing valuable insights into the proper selection of TTT, UDN, and TU velocity to enhance 5G handover performance.","meta":{"url":"http://arxiv.org/abs/2307.14152v1"},"cats":{"new-dataset":0.0310766222,"dev-research":0.2213275821,"prompt-eng":0.3661154576,"data-quality":0.068232317,"ml-security":0.0694213057}}
{"text":"Recent successes in image generation, model-based reinforcement learning, and text-to-image generation have demonstrated the empirical advantages of discrete latent representations, although the reasons behind their benefits remain unclear.","meta":{"url":"http://arxiv.org/abs/2307.14151v1"},"cats":{"new-dataset":0.0270251322,"dev-research":0.1965598235,"prompt-eng":0.4011408904,"data-quality":0.1417304779,"ml-security":0.0949884962}}
{"text":"We explore the relationship between discrete latent spaces and disentangled representations by replacing the standard Gaussian variational autoencoder (VAE) with a tailored categorical variational autoencoder.","meta":{"url":"http://arxiv.org/abs/2307.14151v1"},"cats":{"new-dataset":0.0340668274,"dev-research":0.1689984256,"prompt-eng":0.3693519897,"data-quality":0.1303124922,"ml-security":0.0922312541}}
{"text":"We show that the underlying grid structure of categorical distributions mitigates the problem of rotational invariance associated with multivariate Gaussian distributions, acting as an efficient inductive prior for disentangled representations.","meta":{"url":"http://arxiv.org/abs/2307.14151v1"},"cats":{"new-dataset":0.0542208079,"dev-research":0.1871565344,"prompt-eng":0.3868342071,"data-quality":0.1576897104,"ml-security":0.154766584}}
{"text":"We provide both analytical and empirical findings that demonstrate the advantages of discrete VAEs for learning disentangled representations.","meta":{"url":"http://arxiv.org/abs/2307.14151v1"},"cats":{"new-dataset":0.0128176089,"dev-research":0.2135674524,"prompt-eng":0.3262197818,"data-quality":0.1409248513,"ml-security":0.138566896}}
{"text":"Furthermore, we introduce the first unsupervised model selection strategy that favors disentangled representations.","meta":{"url":"http://arxiv.org/abs/2307.14151v1"},"cats":{"new-dataset":0.0122646352,"dev-research":0.1713610364,"prompt-eng":0.4061946194,"data-quality":0.1347281737,"ml-security":0.1563976108}}
{"text":"We provide a critical assessment of the current set of benchmarks for relative SRS termination in the Termination Problems Database (TPDB): most of the benchmarks in Waldmann_19 and ICFP_10_relative are, in fact, strictly terminating (i. e., terminating when non-strict rules are considered strict), so these benchmarks should be removed, or relabelled.   ","meta":{"url":"http://arxiv.org/abs/2307.14149v1"},"cats":{"new-dataset":0.1116263905,"dev-research":0.3018255031,"prompt-eng":0.4371246929,"data-quality":0.3155510392,"ml-security":0.138065718}}
{"text":"To fill this gap, we enumerate small relative string rewrite systems.","meta":{"url":"http://arxiv.org/abs/2307.14149v1"},"cats":{"new-dataset":0.1814403399,"dev-research":0.2630982053,"prompt-eng":0.4519865404,"data-quality":0.311806229,"ml-security":0.1055693939}}
{"text":"At present, we have complete enumerations for a 2-letter alphabet up to size 11, and for a 3-letter alphabet up to size 8.   ","meta":{"url":"http://arxiv.org/abs/2307.14149v1"},"cats":{"new-dataset":0.2560891628,"dev-research":0.1951350677,"prompt-eng":0.4299607321,"data-quality":0.1019804042,"ml-security":0.0481979355}}
{"text":"For some selected benchmarks, old and new, we discuss how to prove termination, automated or not.","meta":{"url":"http://arxiv.org/abs/2307.14149v1"},"cats":{"new-dataset":0.076736045,"dev-research":0.38905991,"prompt-eng":0.5033725125,"data-quality":0.2971891085,"ml-security":0.1634636807}}
{"text":"This paper focuses on a novel robotic system MorphoLander representing heterogeneous swarm of drones for exploring rough terrain environments.","meta":{"url":"http://arxiv.org/abs/2307.14147v1"},"cats":{"new-dataset":0.0775578955,"dev-research":0.2183160629,"prompt-eng":0.3586903652,"data-quality":0.0664690067,"ml-security":0.1020330809}}
{"text":"The morphogenetic leader drone is capable of landing on uneven terrain, traversing it, and maintaining horizontal position to deploy smaller drones for extensive area exploration.","meta":{"url":"http://arxiv.org/abs/2307.14147v1"},"cats":{"new-dataset":0.0566886961,"dev-research":0.2151706597,"prompt-eng":0.3904157214,"data-quality":0.0564805908,"ml-security":0.0668692766}}
{"text":"After completing their tasks, these drones return and land back on the landing pads of MorphoGear.","meta":{"url":"http://arxiv.org/abs/2307.14147v1"},"cats":{"new-dataset":0.1413854208,"dev-research":0.2186518373,"prompt-eng":0.4153897318,"data-quality":0.1077637264,"ml-security":0.0918312457}}
{"text":"The reinforcement learning algorithm was developed for a precise landing of drones on the leader robot that either remains static during their mission or relocates to the new position.","meta":{"url":"http://arxiv.org/abs/2307.14147v1"},"cats":{"new-dataset":0.0703695136,"dev-research":0.2256165801,"prompt-eng":0.3854554457,"data-quality":0.1094768859,"ml-security":0.1820549828}}
{"text":"Several experiments were conducted to evaluate the performance of the developed landing algorithm under both even and uneven terrain conditions.","meta":{"url":"http://arxiv.org/abs/2307.14147v1"},"cats":{"new-dataset":0.0234322009,"dev-research":0.2672823714,"prompt-eng":0.3922860023,"data-quality":0.0798132405,"ml-security":0.065075732}}
{"text":"The experiments revealed that the proposed system results in high landing accuracy of 0.5 cm when landing on the leader drone under even terrain conditions and 2.35 cm under uneven terrain conditions.","meta":{"url":"http://arxiv.org/abs/2307.14147v1"},"cats":{"new-dataset":0.0790036492,"dev-research":0.2211074125,"prompt-eng":0.4125878196,"data-quality":0.1427453964,"ml-security":0.1003157791}}
{"text":"MorphoLander has the potential to significantly enhance the efficiency of the industrial inspections, seismic surveys, and rescue missions in highly cluttered and unstructured environments.","meta":{"url":"http://arxiv.org/abs/2307.14147v1"},"cats":{"new-dataset":0.1703629381,"dev-research":0.2784990957,"prompt-eng":0.4059891098,"data-quality":0.0848309001,"ml-security":0.0997405872}}
{"text":"Visual question answering (VQA) has been intensively studied as a multimodal task that requires effort in bridging vision and language to infer answers correctly.","meta":{"url":"http://arxiv.org/abs/2307.14142v1"},"cats":{"new-dataset":0.0884062459,"dev-research":0.3198518295,"prompt-eng":0.4059003283,"data-quality":0.1780578965,"ml-security":0.0680619965}}
{"text":"Recent attempts have developed various attention-based modules for solving VQA tasks.","meta":{"url":"http://arxiv.org/abs/2307.14142v1"},"cats":{"new-dataset":0.0393725429,"dev-research":0.2903870154,"prompt-eng":0.4411458353,"data-quality":0.1354927902,"ml-security":0.0549407194}}
{"text":"However, the performance of model inference is largely bottlenecked by visual processing for semantics understanding.","meta":{"url":"http://arxiv.org/abs/2307.14142v1"},"cats":{"new-dataset":0.018569752,"dev-research":0.3407090684,"prompt-eng":0.3867356588,"data-quality":0.1802371682,"ml-security":0.0987357736}}
{"text":"Most existing detection methods rely on bounding boxes, remaining a serious challenge for VQA models to understand the causal nexus of object semantics in images and correctly infer contextual information.","meta":{"url":"http://arxiv.org/abs/2307.14142v1"},"cats":{"new-dataset":0.0484194758,"dev-research":0.2316081457,"prompt-eng":0.3920859786,"data-quality":0.3503950389,"ml-security":0.171847218}}
{"text":"To this end, we propose a finer model framework without bounding boxes in this work, termed Looking Out of Instance Semantics (LOIS) to tackle this important issue.","meta":{"url":"http://arxiv.org/abs/2307.14142v1"},"cats":{"new-dataset":0.0948796634,"dev-research":0.2395935684,"prompt-eng":0.4270701885,"data-quality":0.2432722675,"ml-security":0.1438588731}}
{"text":"LOIS enables more fine-grained feature descriptions to produce visual facts.","meta":{"url":"http://arxiv.org/abs/2307.14142v1"},"cats":{"new-dataset":0.1906096864,"dev-research":0.4158207354,"prompt-eng":0.4023445727,"data-quality":0.251858612,"ml-security":0.0695183288}}
{"text":"Furthermore, to overcome the label ambiguity caused by instance masks, two types of relation attention modules: 1) intra-modality and 2) inter-modality, are devised to infer the correct answers from the different multi-view features.","meta":{"url":"http://arxiv.org/abs/2307.14142v1"},"cats":{"new-dataset":0.0408714452,"dev-research":0.2690514554,"prompt-eng":0.4095015305,"data-quality":0.2995155821,"ml-security":0.0818250421}}
{"text":"Specifically, we implement a mutual relation attention module to model sophisticated and deeper visual semantic relations between instance objects and background information.","meta":{"url":"http://arxiv.org/abs/2307.14142v1"},"cats":{"new-dataset":0.2163726747,"dev-research":0.2784046463,"prompt-eng":0.4294184667,"data-quality":0.1720611758,"ml-security":0.0747596194}}
{"text":"In addition, our proposed attention model can further analyze salient image regions by focusing on important word-related questions.","meta":{"url":"http://arxiv.org/abs/2307.14142v1"},"cats":{"new-dataset":0.1214556888,"dev-research":0.2223932158,"prompt-eng":0.445803525,"data-quality":0.2310911584,"ml-security":0.0435051785}}
{"text":"Experimental results on four benchmark VQA datasets prove that our proposed method has favorable performance in improving visual reasoning capability.","meta":{"url":"http://arxiv.org/abs/2307.14142v1"},"cats":{"new-dataset":0.2706293988,"dev-research":0.3526166828,"prompt-eng":0.3810248934,"data-quality":0.1978107108,"ml-security":0.0812565664}}
{"text":"We study the piecewise stationary combinatorial semi-bandit problem with causally related rewards.","meta":{"url":"http://arxiv.org/abs/2307.14138v1"},"cats":{"new-dataset":0.0619252046,"dev-research":0.1760701338,"prompt-eng":0.3749032639,"data-quality":0.1326843482,"ml-security":0.1633133159}}
{"text":"In our nonstationary environment, variations in the base arms' distributions, causal relationships between rewards, or both, change the reward generation process.","meta":{"url":"http://arxiv.org/abs/2307.14138v1"},"cats":{"new-dataset":0.0241618062,"dev-research":0.2076397529,"prompt-eng":0.4077073221,"data-quality":0.1180023872,"ml-security":0.16039506}}
{"text":"In such an environment, an optimal decision-maker must follow both sources of change and adapt accordingly.","meta":{"url":"http://arxiv.org/abs/2307.14138v1"},"cats":{"new-dataset":0.0101769084,"dev-research":0.3044401817,"prompt-eng":0.3971504811,"data-quality":0.1112776915,"ml-security":0.1070668438}}
{"text":"The problem becomes aggravated in the combinatorial semi-bandit setting, where the decision-maker only observes the outcome of the selected bundle of arms.","meta":{"url":"http://arxiv.org/abs/2307.14138v1"},"cats":{"new-dataset":0.046905406,"dev-research":0.2549182317,"prompt-eng":0.3647892337,"data-quality":0.2143014515,"ml-security":0.1846383615}}
{"text":"The core of our proposed policy is the Upper Confidence Bound (UCB) algorithm.","meta":{"url":"http://arxiv.org/abs/2307.14138v1"},"cats":{"new-dataset":0.043626716,"dev-research":0.2007496447,"prompt-eng":0.4519017775,"data-quality":0.1381528973,"ml-security":0.168711024}}
{"text":"We assume the agent relies on an adaptive approach to overcome the challenge.","meta":{"url":"http://arxiv.org/abs/2307.14138v1"},"cats":{"new-dataset":0.007397297,"dev-research":0.2484353013,"prompt-eng":0.4155159361,"data-quality":0.1124630479,"ml-security":0.2599777547}}
{"text":"More specifically, it employs a change-point detector based on the Generalized Likelihood Ratio (GLR) test.","meta":{"url":"http://arxiv.org/abs/2307.14138v1"},"cats":{"new-dataset":0.0379231137,"dev-research":0.2111441805,"prompt-eng":0.4772136139,"data-quality":0.2293797496,"ml-security":0.1286308627}}
{"text":"Besides, we introduce the notion of group restart as a new alternative restarting strategy in the decision making process in structured environments.","meta":{"url":"http://arxiv.org/abs/2307.14138v1"},"cats":{"new-dataset":0.1014705395,"dev-research":0.3942478669,"prompt-eng":0.4710099543,"data-quality":0.1237884961,"ml-security":0.0799997538}}
{"text":"Finally, our algorithm integrates a mechanism to trace the variations of the underlying graph structure, which captures the causal relationships between the rewards in the bandit setting.","meta":{"url":"http://arxiv.org/abs/2307.14138v1"},"cats":{"new-dataset":0.0722756071,"dev-research":0.2372715437,"prompt-eng":0.427269504,"data-quality":0.1871825599,"ml-security":0.1728359848}}
{"text":"Theoretically, we establish a regret upper bound that reflects the effects of the number of structural- and distribution changes on the performance.","meta":{"url":"http://arxiv.org/abs/2307.14138v1"},"cats":{"new-dataset":0.0159875515,"dev-research":0.1844853892,"prompt-eng":0.4362366477,"data-quality":0.1339524639,"ml-security":0.1748860853}}
{"text":"The outcome of our numerical experiments in real-world scenarios exhibits applicability and superior performance of our proposal compared to the state-of-the-art benchmarks.","meta":{"url":"http://arxiv.org/abs/2307.14138v1"},"cats":{"new-dataset":0.0571565026,"dev-research":0.2323175516,"prompt-eng":0.4414197721,"data-quality":0.1172513527,"ml-security":0.1097181907}}
{"text":"This study introduces and evaluates tiny, mini, small, and medium-sized uncased Turkish BERT models, aiming to bridge the research gap in less-resourced languages.","meta":{"url":"http://arxiv.org/abs/2307.14134v1"},"cats":{"new-dataset":0.1184902851,"dev-research":0.2204703349,"prompt-eng":0.3788847096,"data-quality":0.2028136105,"ml-security":0.1176207946}}
{"text":"We trained these models on a diverse dataset encompassing over 75GB of text from multiple sources and tested them on several tasks, including mask prediction, sentiment analysis, news classification, and, zero-shot classification.","meta":{"url":"http://arxiv.org/abs/2307.14134v1"},"cats":{"new-dataset":0.5182958723,"dev-research":0.1748486173,"prompt-eng":0.3414793352,"data-quality":0.2781731792,"ml-security":0.2151287915}}
{"text":"Despite their smaller size, our models exhibited robust performance, including zero-shot task, while ensuring computational efficiency and faster execution times.","meta":{"url":"http://arxiv.org/abs/2307.14134v1"},"cats":{"new-dataset":0.0961309465,"dev-research":0.2623400632,"prompt-eng":0.4667682029,"data-quality":0.1666174311,"ml-security":0.1676998354}}
{"text":"Our findings provide valuable insights into the development and application of smaller language models, especially in the context of the Turkish language.","meta":{"url":"http://arxiv.org/abs/2307.14134v1"},"cats":{"new-dataset":0.0660046955,"dev-research":0.203271608,"prompt-eng":0.340701078,"data-quality":0.2418566546,"ml-security":0.0899016389}}
{"text":"RNN-T models are widely used in ASR, which rely on the RNN-T loss to achieve length alignment between input audio and target sequence.","meta":{"url":"http://arxiv.org/abs/2307.14132v1"},"cats":{"new-dataset":0.0662063935,"dev-research":0.1995275791,"prompt-eng":0.3846258477,"data-quality":0.1616174816,"ml-security":0.0891275885}}
{"text":"However, the implementation complexity and the alignment-based optimization target of RNN-T loss lead to computational redundancy and a reduced role for predictor network, respectively.","meta":{"url":"http://arxiv.org/abs/2307.14132v1"},"cats":{"new-dataset":0.022612148,"dev-research":0.2545401694,"prompt-eng":0.360171713,"data-quality":0.1350211503,"ml-security":0.1308188022}}
{"text":"In this paper, we propose a novel model named CIF-Transducer (CIF-T) which incorporates the Continuous Integrate-and-Fire (CIF) mechanism with the RNN-T model to achieve efficient alignment.","meta":{"url":"http://arxiv.org/abs/2307.14132v1"},"cats":{"new-dataset":0.1590735609,"dev-research":0.2316584257,"prompt-eng":0.4331923374,"data-quality":0.1531718562,"ml-security":0.0376730706}}
{"text":"In this way, the RNN-T loss is abandoned, thus bringing a computational reduction and allowing the predictor network a more significant role.","meta":{"url":"http://arxiv.org/abs/2307.14132v1"},"cats":{"new-dataset":0.0337933264,"dev-research":0.2199334748,"prompt-eng":0.3586712257,"data-quality":0.1388574684,"ml-security":0.1444126007}}
{"text":"We also introduce Funnel-CIF, Context Blocks, Unified Gating and Bilinear Pooling joint network, and auxiliary training strategy to further improve performance.","meta":{"url":"http://arxiv.org/abs/2307.14132v1"},"cats":{"new-dataset":0.1038558249,"dev-research":0.2250691241,"prompt-eng":0.402004819,"data-quality":0.2043811298,"ml-security":0.0801357189}}
{"text":"Experiments on the 178-hour AISHELL-1 and 10000-hour WenetSpeech datasets show that CIF-T achieves state-of-the-art results with lower computational overhead compared to RNN-T models.","meta":{"url":"http://arxiv.org/abs/2307.14132v1"},"cats":{"new-dataset":0.2983119992,"dev-research":0.1965771731,"prompt-eng":0.3661919105,"data-quality":0.0742337361,"ml-security":0.0762933527}}
{"text":"In this paper, we propose a novel method for single-view 3D style transfer that generates a unique 3D object with both shape and texture transfer.","meta":{"url":"http://arxiv.org/abs/2307.14127v1"},"cats":{"new-dataset":0.0505233219,"dev-research":0.2319341896,"prompt-eng":0.3769483694,"data-quality":0.0916363192,"ml-security":0.079010337}}
{"text":"Our focus lies primarily on birds, a popular subject in 3D reconstruction, for which no existing single-view 3D transfer methods have been developed.","meta":{"url":"http://arxiv.org/abs/2307.14127v1"},"cats":{"new-dataset":0.1584795217,"dev-research":0.1700578835,"prompt-eng":0.3305482219,"data-quality":0.0757174788,"ml-security":0.0664845057}}
{"text":"The method we propose seeks to generate a 3D mesh shape and texture of a bird from two single-view images.","meta":{"url":"http://arxiv.org/abs/2307.14127v1"},"cats":{"new-dataset":0.1253075064,"dev-research":0.194803358,"prompt-eng":0.3747314227,"data-quality":0.0819258466,"ml-security":0.0556048411}}
{"text":"To achieve this, we introduce a novel shape transfer generator that comprises a dual residual gated network (DRGNet), and a multi-layer perceptron (MLP).","meta":{"url":"http://arxiv.org/abs/2307.14127v1"},"cats":{"new-dataset":0.0613075734,"dev-research":0.2073771239,"prompt-eng":0.3999313493,"data-quality":0.1177608474,"ml-security":0.095788957}}
{"text":"DRGNet extracts the features of source and target images using a shared coordinate gate unit, while the MLP generates spatial coordinates for building a 3D mesh.","meta":{"url":"http://arxiv.org/abs/2307.14127v1"},"cats":{"new-dataset":0.1269835967,"dev-research":0.3034965342,"prompt-eng":0.3886811761,"data-quality":0.12234634,"ml-security":0.1144192043}}
{"text":"We also introduce a semantic UV texture transfer module that implements textural style transfer using semantic UV segmentation, which ensures consistency in the semantic meaning of the transferred regions.","meta":{"url":"http://arxiv.org/abs/2307.14127v1"},"cats":{"new-dataset":0.1621735506,"dev-research":0.3033914413,"prompt-eng":0.3999790442,"data-quality":0.2550482802,"ml-security":0.0644408008}}
{"text":"This module can be widely adapted to many existing approaches.","meta":{"url":"http://arxiv.org/abs/2307.14127v1"},"cats":{"new-dataset":0.0624327362,"dev-research":0.2782847392,"prompt-eng":0.4244649863,"data-quality":0.0902508546,"ml-security":0.0607158597}}
{"text":"Finally, our method constructs a novel 3D bird using a differentiable renderer.","meta":{"url":"http://arxiv.org/abs/2307.14127v1"},"cats":{"new-dataset":0.1405805037,"dev-research":0.2135872949,"prompt-eng":0.4213188299,"data-quality":0.1049264808,"ml-security":0.0665205009}}
{"text":"Experimental results on the CUB dataset verify that our method achieves state-of-the-art performance on the single-view 3D style transfer task.","meta":{"url":"http://arxiv.org/abs/2307.14127v1"},"cats":{"new-dataset":0.1553979267,"dev-research":0.2520904903,"prompt-eng":0.3857152535,"data-quality":0.1136343374,"ml-security":0.0583077265}}
{"text":"Code is available in https://github.com/wrk226/2D-to-3D-Evolution-Transfer.","meta":{"url":"http://arxiv.org/abs/2307.14127v1"},"cats":{"new-dataset":0.2295855671,"dev-research":0.2560348743,"prompt-eng":0.4059129783,"data-quality":0.1001414777,"ml-security":0.0658337494}}
{"text":"The missing modality issue is critical but non-trivial to be solved by multi-modal models.","meta":{"url":"http://arxiv.org/abs/2307.14126v1"},"cats":{"new-dataset":0.0363168794,"dev-research":0.1563326965,"prompt-eng":0.4005084552,"data-quality":0.3149822584,"ml-security":0.0711854267}}
{"text":"Current methods aiming to handle the missing modality problem in multi-modal tasks, either deal with missing modalities only during evaluation or train separate models to handle specific missing modality settings.","meta":{"url":"http://arxiv.org/abs/2307.14126v1"},"cats":{"new-dataset":0.0196981614,"dev-research":0.2088530484,"prompt-eng":0.4399323914,"data-quality":0.2933089455,"ml-security":0.0750022756}}
{"text":"In addition, these models are designed for specific tasks, so for example, classification models are not easily adapted to segmentation tasks and vice versa.","meta":{"url":"http://arxiv.org/abs/2307.14126v1"},"cats":{"new-dataset":0.0063273579,"dev-research":0.2477073238,"prompt-eng":0.3615252246,"data-quality":0.1620470536,"ml-security":0.1669182444}}
{"text":"In this paper, we propose the Shared-Specific Feature Modelling (ShaSpec) method that is considerably simpler and more effective than competing approaches that address the issues above.","meta":{"url":"http://arxiv.org/abs/2307.14126v1"},"cats":{"new-dataset":0.0452980497,"dev-research":0.3891802071,"prompt-eng":0.4691496005,"data-quality":0.1890733062,"ml-security":0.1552008067}}
{"text":"ShaSpec is designed to take advantage of all available input modalities during training and evaluation by learning shared and specific features to better represent the input data.","meta":{"url":"http://arxiv.org/abs/2307.14126v1"},"cats":{"new-dataset":0.1262986376,"dev-research":0.339331302,"prompt-eng":0.4475072485,"data-quality":0.1398999634,"ml-security":0.1965944299}}
{"text":"This is achieved from a strategy that relies on auxiliary tasks based on distribution alignment and domain classification, in addition to a residual feature fusion procedure.","meta":{"url":"http://arxiv.org/abs/2307.14126v1"},"cats":{"new-dataset":0.1203114072,"dev-research":0.2379776238,"prompt-eng":0.4911126281,"data-quality":0.3057962135,"ml-security":0.1104943338}}
{"text":"Also, the design simplicity of ShaSpec enables its easy adaptation to multiple tasks, such as classification and segmentation.","meta":{"url":"http://arxiv.org/abs/2307.14126v1"},"cats":{"new-dataset":0.0322158418,"dev-research":0.3324622121,"prompt-eng":0.4265403673,"data-quality":0.0996640556,"ml-security":0.1194878008}}
{"text":"Experiments are conducted on both medical image segmentation and computer vision classification, with results indicating that ShaSpec outperforms competing methods by a large margin.","meta":{"url":"http://arxiv.org/abs/2307.14126v1"},"cats":{"new-dataset":0.0696955812,"dev-research":0.2189412472,"prompt-eng":0.3824232254,"data-quality":0.1914693146,"ml-security":0.1436739582}}
{"text":"For instance, on BraTS2018, ShaSpec improves the SOTA by more than 3% for enhancing tumour, 5% for tumour core and 3% for whole tumour.","meta":{"url":"http://arxiv.org/abs/2307.14126v1"},"cats":{"new-dataset":0.0582593453,"dev-research":0.2420087579,"prompt-eng":0.3740600877,"data-quality":0.1141206874,"ml-security":0.1302672996}}
{"text":"Algorithms for state estimation of humanoid robots usually assume that the feet remain flat and in a constant position while in contact with the ground.","meta":{"url":"http://arxiv.org/abs/2307.14125v1"},"cats":{"new-dataset":0.1452021338,"dev-research":0.2069074429,"prompt-eng":0.4033509163,"data-quality":0.0994050923,"ml-security":0.0954324191}}
{"text":"However, this hypothesis is easily violated while walking, especially for human-like gaits with heel-toe motion.","meta":{"url":"http://arxiv.org/abs/2307.14125v1"},"cats":{"new-dataset":0.0082602436,"dev-research":0.2530169216,"prompt-eng":0.3648089069,"data-quality":0.1658338087,"ml-security":0.2194857259}}
{"text":"This reduces the time during which the contact assumption can be used, or requires higher variances to account for errors.","meta":{"url":"http://arxiv.org/abs/2307.14125v1"},"cats":{"new-dataset":0.0009086339,"dev-research":0.3600254472,"prompt-eng":0.4012120143,"data-quality":0.1453523117,"ml-security":0.181428254}}
{"text":"In this paper, we present a novel state estimator based on the extended Kalman filter that can properly handle any contact configuration.","meta":{"url":"http://arxiv.org/abs/2307.14125v1"},"cats":{"new-dataset":0.099299845,"dev-research":0.1680343803,"prompt-eng":0.4449891803,"data-quality":0.0878245427,"ml-security":0.0735332503}}
{"text":"We consider multiple inertial measurement units (IMUs) distributed throughout the robot's structure, including on both feet, which are used to track multiple bodies of the robot.","meta":{"url":"http://arxiv.org/abs/2307.14125v1"},"cats":{"new-dataset":0.2093894204,"dev-research":0.2235913193,"prompt-eng":0.3868105897,"data-quality":0.0848615907,"ml-security":0.0459412424}}
{"text":"This multi-IMU instrumentation setup also has the advantage of allowing the deformations in the robot's structure to be estimated, improving the kinematic model used in the filter.","meta":{"url":"http://arxiv.org/abs/2307.14125v1"},"cats":{"new-dataset":0.058915371,"dev-research":0.2296292449,"prompt-eng":0.4072191175,"data-quality":0.0884822648,"ml-security":0.0594099636}}
{"text":"The proposed approach is validated experimentally on the exoskeleton Atalante and is shown to present low drift, performing better than similar single-IMU filters.","meta":{"url":"http://arxiv.org/abs/2307.14125v1"},"cats":{"new-dataset":0.0101647243,"dev-research":0.1914618746,"prompt-eng":0.351958089,"data-quality":0.1194320602,"ml-security":0.1339363061}}
{"text":"The obtained trajectory estimates are accurate enough to construct elevation maps that have little distortion with respect to the ground truth.","meta":{"url":"http://arxiv.org/abs/2307.14125v1"},"cats":{"new-dataset":0.0350838348,"dev-research":0.2016565505,"prompt-eng":0.365211393,"data-quality":0.1596920745,"ml-security":0.089319156}}
{"text":"Recent advances in event camera research emphasize processing data in its original sparse form, which allows the use of its unique features such as high temporal resolution, high dynamic range, low latency, and resistance to image blur.","meta":{"url":"http://arxiv.org/abs/2307.14124v1"},"cats":{"new-dataset":0.1319453302,"dev-research":0.2680404305,"prompt-eng":0.3655594932,"data-quality":0.131017575,"ml-security":0.0523027334}}
{"text":"One promising approach for analyzing event data is through graph convolutional networks (GCNs).","meta":{"url":"http://arxiv.org/abs/2307.14124v1"},"cats":{"new-dataset":0.237330721,"dev-research":0.2863483035,"prompt-eng":0.3328514053,"data-quality":0.2370811439,"ml-security":0.1028134949}}
{"text":"However, current research in this domain primarily focuses on optimizing computational costs, neglecting the associated memory costs.","meta":{"url":"http://arxiv.org/abs/2307.14124v1"},"cats":{"new-dataset":0.008990747,"dev-research":0.3148539164,"prompt-eng":0.3392518763,"data-quality":0.0952441281,"ml-security":0.0759831314}}
{"text":"In this paper, we consider both factors together in order to achieve satisfying results and relatively low model complexity.","meta":{"url":"http://arxiv.org/abs/2307.14124v1"},"cats":{"new-dataset":0.0204075812,"dev-research":0.1297438976,"prompt-eng":0.3970780086,"data-quality":0.1095954751,"ml-security":0.0629345616}}
{"text":"For this purpose, we performed a comparative analysis of different graph convolution operations, considering factors such as execution time, the number of trainable model parameters, data format requirements, and training outcomes.","meta":{"url":"http://arxiv.org/abs/2307.14124v1"},"cats":{"new-dataset":0.1373450477,"dev-research":0.2373995321,"prompt-eng":0.3679211633,"data-quality":0.1657573131,"ml-security":0.0816192439}}
{"text":"Our results show a 450-fold reduction in the number of parameters for the feature extraction module and a 4.5-fold reduction in the size of the data representation while maintaining a classification accuracy of 52.3%, which is 6.3% higher compared to the operation used in state-of-the-art approaches.","meta":{"url":"http://arxiv.org/abs/2307.14124v1"},"cats":{"new-dataset":0.3111680534,"dev-research":0.3331724993,"prompt-eng":0.4445732394,"data-quality":0.3135737966,"ml-security":0.1352217805}}
{"text":"To further evaluate performance, we implemented the object detection architecture and evaluated its performance on the N-Caltech101 dataset.","meta":{"url":"http://arxiv.org/abs/2307.14124v1"},"cats":{"new-dataset":0.4418504059,"dev-research":0.2305464428,"prompt-eng":0.3945915146,"data-quality":0.246781937,"ml-security":0.1526093202}}
{"text":"The results showed an accuracy of 53.7 % mAP@0.5 and reached an execution rate of 82 graphs per second.","meta":{"url":"http://arxiv.org/abs/2307.14124v1"},"cats":{"new-dataset":0.1649625647,"dev-research":0.2788711149,"prompt-eng":0.3989981884,"data-quality":0.2471634893,"ml-security":0.0608946843}}
{"text":"Recent work in Machine Learning and Computer Vision has highlighted the presence of various types of systematic flaws inside ground truth object recognition benchmark datasets.","meta":{"url":"http://arxiv.org/abs/2307.14119v1"},"cats":{"new-dataset":0.3166427692,"dev-research":0.248338339,"prompt-eng":0.3462821747,"data-quality":0.4643102771,"ml-security":0.320873922}}
{"text":"Our basic tenet is that these flaws are rooted in the many-to-many mappings which exist between the visual information encoded in images and the intended semantics of the labels annotating them.","meta":{"url":"http://arxiv.org/abs/2307.14119v1"},"cats":{"new-dataset":0.1700911206,"dev-research":0.3682352387,"prompt-eng":0.4037333623,"data-quality":0.6280557859,"ml-security":0.1750570616}}
{"text":"The net consequence is that the current annotation process is largely under-specified, thus leaving too much freedom to the subjective judgment of annotators.","meta":{"url":"http://arxiv.org/abs/2307.14119v1"},"cats":{"new-dataset":0.0095302463,"dev-research":0.4101399032,"prompt-eng":0.3369251838,"data-quality":0.5617354751,"ml-security":0.2510928232}}
{"text":"In this paper, we propose vTelos, an integrated Natural Language Processing, Knowledge Representation, and Computer Vision methodology whose main goal is to make explicit the (otherwise implicit) intended annotation semantics, thus minimizing the number and role of subjective choices.","meta":{"url":"http://arxiv.org/abs/2307.14119v1"},"cats":{"new-dataset":0.1045130503,"dev-research":0.44239264,"prompt-eng":0.4229625673,"data-quality":0.4447825603,"ml-security":0.0819784715}}
{"text":"A key element of vTelos is the exploitation of the WordNet lexico-semantic hierarchy as the main means for providing the meaning of natural language labels and, as a consequence, for driving the annotation of images based on the objects and the visual properties they depict.","meta":{"url":"http://arxiv.org/abs/2307.14119v1"},"cats":{"new-dataset":0.1110904069,"dev-research":0.3346160117,"prompt-eng":0.3912275927,"data-quality":0.3570560584,"ml-security":0.1069780618}}
{"text":"The methodology is validated on images populating a subset of the ImageNet hierarchy.","meta":{"url":"http://arxiv.org/abs/2307.14119v1"},"cats":{"new-dataset":0.1361475693,"dev-research":0.2421449027,"prompt-eng":0.422632853,"data-quality":0.2301760356,"ml-security":0.1301226174}}
{"text":"We study improving social conversational agents by learning from natural dialogue between users and a deployed model, without extra annotations.","meta":{"url":"http://arxiv.org/abs/2307.14117v1"},"cats":{"new-dataset":0.1682250693,"dev-research":0.2535310243,"prompt-eng":0.4139039108,"data-quality":0.2856379601,"ml-security":0.151958935}}
{"text":"To implicitly measure the quality of a machine-generated utterance, we leverage signals like user response length, sentiment and reaction of the future human utterances in the collected dialogue episodes.","meta":{"url":"http://arxiv.org/abs/2307.14117v1"},"cats":{"new-dataset":0.1645999464,"dev-research":0.3242409847,"prompt-eng":0.4680304085,"data-quality":0.2811106982,"ml-security":0.1196531545}}
{"text":"Our experiments use the publicly released deployment data from BlenderBot (Xu et al., 2023).","meta":{"url":"http://arxiv.org/abs/2307.14117v1"},"cats":{"new-dataset":0.5013414461,"dev-research":0.2575330511,"prompt-eng":0.4377149485,"data-quality":0.1960030426,"ml-security":0.1356240861}}
{"text":"Human evaluation indicates improvements in our new models over baseline responses; however, we find that some proxy signals can lead to more generations with undesirable properties as well.","meta":{"url":"http://arxiv.org/abs/2307.14117v1"},"cats":{"new-dataset":0.0360771601,"dev-research":0.2301618691,"prompt-eng":0.4907713884,"data-quality":0.1631746852,"ml-security":0.1988819684}}
{"text":"For example, optimizing for conversation length can lead to more controversial or unfriendly generations compared to the baseline, whereas optimizing for positive sentiment or reaction can decrease these behaviors.","meta":{"url":"http://arxiv.org/abs/2307.14117v1"},"cats":{"new-dataset":0.0012634454,"dev-research":0.4461684153,"prompt-eng":0.3067340659,"data-quality":0.1325277313,"ml-security":0.088832546}}
{"text":"Risk assessment plays a crucial role in ensuring the security and resilience of modern computer systems.","meta":{"url":"http://arxiv.org/abs/2307.14114v1"},"cats":{"new-dataset":0.0384911264,"dev-research":0.4374327052,"prompt-eng":0.4447685135,"data-quality":0.1569231129,"ml-security":0.6228096105}}
{"text":"Existing methods for conducting risk assessments often suffer from tedious and time-consuming processes, making it challenging to maintain a comprehensive overview of potential security issues.","meta":{"url":"http://arxiv.org/abs/2307.14114v1"},"cats":{"new-dataset":0.0403672587,"dev-research":0.423033251,"prompt-eng":0.4628202496,"data-quality":0.1597990276,"ml-security":0.5120994806}}
{"text":"In this paper, we propose a novel approach that leverages attack graphs to enhance the efficiency and effectiveness of risk assessment.","meta":{"url":"http://arxiv.org/abs/2307.14114v1"},"cats":{"new-dataset":0.0762609771,"dev-research":0.3524200204,"prompt-eng":0.4268368858,"data-quality":0.1793453432,"ml-security":0.7292513973}}
{"text":"Attack graphs visually represent the various attack paths that adversaries can exploit within a system, enabling a systematic exploration of potential vulnerabilities.","meta":{"url":"http://arxiv.org/abs/2307.14114v1"},"cats":{"new-dataset":0.143374396,"dev-research":0.419907963,"prompt-eng":0.4192654515,"data-quality":0.1852058707,"ml-security":0.6634972557}}
{"text":"By extending attack graphs with capabilities to include countermeasures and consequences, they can be leveraged to constitute the complete risk assessment process.","meta":{"url":"http://arxiv.org/abs/2307.14114v1"},"cats":{"new-dataset":0.0536979011,"dev-research":0.319552934,"prompt-eng":0.4134981283,"data-quality":0.1250374168,"ml-security":0.4929890786}}
{"text":"Our method offers a more streamlined and comprehensive analysis of system vulnerabilities, where system changes, or environment changes can easily be adapted and the issues exposing the highest risk can easily be identified.","meta":{"url":"http://arxiv.org/abs/2307.14114v1"},"cats":{"new-dataset":0.1670080492,"dev-research":0.4712960421,"prompt-eng":0.4525996626,"data-quality":0.2201209797,"ml-security":0.468017237}}
{"text":"We demonstrate the effectiveness of our approach through a case study, as well as the applicability by combining existing risk assessment standards with our method.","meta":{"url":"http://arxiv.org/abs/2307.14114v1"},"cats":{"new-dataset":0.0829330056,"dev-research":0.33457678,"prompt-eng":0.4378848343,"data-quality":0.216155067,"ml-security":0.2853895685}}
{"text":"Our work aims to bridge the gap between risk assessment practices and evolving threat landscapes, offering an improved methodology for managing and mitigating risks in modern computer systems.","meta":{"url":"http://arxiv.org/abs/2307.14114v1"},"cats":{"new-dataset":0.0914294087,"dev-research":0.5046171498,"prompt-eng":0.4448560445,"data-quality":0.1693075814,"ml-security":0.5915761956}}
{"text":"Periocular biometrics has been established as an independent modality due to concerns on the performance of iris or face systems in uncontrolled conditions.","meta":{"url":"http://arxiv.org/abs/2307.14111v1"},"cats":{"new-dataset":0.015064699,"dev-research":0.2070649449,"prompt-eng":0.4013287375,"data-quality":0.1033030031,"ml-security":0.1195980558}}
{"text":"Periocular refers to the facial region in the eye vicinity, including eyelids, lashes and eyebrows.","meta":{"url":"http://arxiv.org/abs/2307.14111v1"},"cats":{"new-dataset":0.0267426498,"dev-research":0.2648774294,"prompt-eng":0.3120932615,"data-quality":0.1658928188,"ml-security":0.0750729393}}
{"text":"It is available over a wide range of acquisition distances, representing a trade-off between the whole face (which can be occluded at close distances) and the iris texture (which do not have enough resolution at long distances).","meta":{"url":"http://arxiv.org/abs/2307.14111v1"},"cats":{"new-dataset":0.167075007,"dev-research":0.1961557236,"prompt-eng":0.3442195549,"data-quality":0.0759758102,"ml-security":0.0640923971}}
{"text":"Since the periocular region appears in face or iris images, it can be used also in conjunction with these modalities.","meta":{"url":"http://arxiv.org/abs/2307.14111v1"},"cats":{"new-dataset":0.0140826714,"dev-research":0.207283554,"prompt-eng":0.3668122089,"data-quality":0.1091708289,"ml-security":0.0596935845}}
{"text":"Features extracted from the periocular region have been also used successfully for gender classification and ethnicity classification, and to study the impact of gender transformation or plastic surgery in the recognition performance.","meta":{"url":"http://arxiv.org/abs/2307.14111v1"},"cats":{"new-dataset":0.0726534816,"dev-research":0.2239394873,"prompt-eng":0.3625538722,"data-quality":0.21202802,"ml-security":0.0904245339}}
{"text":"This paper presents a review of the state of the art in periocular biometric research, providing an insight of the most relevant issues and giving a thorough coverage of the existing literature.","meta":{"url":"http://arxiv.org/abs/2307.14111v1"},"cats":{"new-dataset":0.0417830289,"dev-research":0.1962259705,"prompt-eng":0.429410002,"data-quality":0.166734385,"ml-security":0.1061541973}}
{"text":"Future research trends are also briefly discussed.","meta":{"url":"http://arxiv.org/abs/2307.14111v1"},"cats":{"new-dataset":0.0815736774,"dev-research":0.2237544587,"prompt-eng":0.3461011139,"data-quality":0.0832501959,"ml-security":0.0694969799}}
{"text":"Motion planning is challenging for multiple robots in cluttered environments without communication, especially in view of real-time efficiency, motion safety, distributed computation, and trajectory optimality, etc.","meta":{"url":"http://arxiv.org/abs/2307.14110v1"},"cats":{"new-dataset":0.0975102755,"dev-research":0.2645002362,"prompt-eng":0.3714593029,"data-quality":0.0424725474,"ml-security":0.0809576485}}
{"text":"In this paper, a reinforced potential field method is developed for distributed multi-robot motion planning, which is a synthesized design of reinforcement learning and artificial potential fields.","meta":{"url":"http://arxiv.org/abs/2307.14110v1"},"cats":{"new-dataset":0.038698228,"dev-research":0.2029515945,"prompt-eng":0.3531701,"data-quality":0.0394470474,"ml-security":0.1014059925}}
{"text":"An observation embedding with a self-attention mechanism is presented to model the robot-robot and robot-environment interactions.","meta":{"url":"http://arxiv.org/abs/2307.14110v1"},"cats":{"new-dataset":0.1007207764,"dev-research":0.2391457144,"prompt-eng":0.4848919893,"data-quality":0.116626231,"ml-security":0.0943385915}}
{"text":"A soft wall-following rule is developed to improve the trajectory smoothness.","meta":{"url":"http://arxiv.org/abs/2307.14110v1"},"cats":{"new-dataset":0.01040005,"dev-research":0.2739625347,"prompt-eng":0.389389153,"data-quality":0.0981723596,"ml-security":0.0808123799}}
{"text":"Our method belongs to reactive planning, but environment properties are implicitly encoded.","meta":{"url":"http://arxiv.org/abs/2307.14110v1"},"cats":{"new-dataset":0.0898471016,"dev-research":0.3029326013,"prompt-eng":0.4650969231,"data-quality":0.1067641708,"ml-security":0.1041734414}}
{"text":"The total amount of robots in our method can be scaled up to any number.","meta":{"url":"http://arxiv.org/abs/2307.14110v1"},"cats":{"new-dataset":0.0542304131,"dev-research":0.2211801304,"prompt-eng":0.3843953353,"data-quality":0.0925062961,"ml-security":0.0934842705}}
{"text":"The performance improvement over a vanilla APF and RL method has been demonstrated via numerical simulations.","meta":{"url":"http://arxiv.org/abs/2307.14110v1"},"cats":{"new-dataset":0.0170858428,"dev-research":0.2522783863,"prompt-eng":0.3947118652,"data-quality":0.0972629086,"ml-security":0.0456195648}}
{"text":"Experiments are also performed using quadrotors to further illustrate the competence of our method.","meta":{"url":"http://arxiv.org/abs/2307.14110v1"},"cats":{"new-dataset":0.0163951764,"dev-research":0.244291098,"prompt-eng":0.3925905143,"data-quality":0.0863064027,"ml-security":0.0621147723}}
{"text":"GraphRNN is a deep learning-based architecture proposed by You et al. for learning generative models for graphs.","meta":{"url":"http://arxiv.org/abs/2307.14109v1"},"cats":{"new-dataset":0.202504264,"dev-research":0.2475700081,"prompt-eng":0.322812194,"data-quality":0.1645624284,"ml-security":0.0936551571}}
{"text":"We replicate the results of You et al.","meta":{"url":"http://arxiv.org/abs/2307.14109v1"},"cats":{"new-dataset":0.149219334,"dev-research":0.2039653173,"prompt-eng":0.3697887545,"data-quality":0.1727381934,"ml-security":0.0849673429}}
{"text":"using a reproduced implementation of the GraphRNN architecture and evaluate this against baseline models using new metrics.","meta":{"url":"http://arxiv.org/abs/2307.14109v1"},"cats":{"new-dataset":0.2077678337,"dev-research":0.2439354964,"prompt-eng":0.4058880149,"data-quality":0.2235909254,"ml-security":0.0600920778}}
{"text":"Through an ablation study, we find that the BFS traversal suggested by You et al. to collapse representations of isomorphic graphs contributes significantly to model performance.","meta":{"url":"http://arxiv.org/abs/2307.14109v1"},"cats":{"new-dataset":0.036348462,"dev-research":0.2177743347,"prompt-eng":0.3886446222,"data-quality":0.1505987841,"ml-security":0.1207702269}}
{"text":"Additionally, we extend GraphRNN to generate directed acyclic graphs by replacing the BFS traversal with a topological sort.","meta":{"url":"http://arxiv.org/abs/2307.14109v1"},"cats":{"new-dataset":0.308137473,"dev-research":0.2809886489,"prompt-eng":0.3870449875,"data-quality":0.1482810821,"ml-security":0.0797831111}}
{"text":"We demonstrate that this method improves significantly over a directed-multiclass variant of GraphRNN on a real-world dataset.","meta":{"url":"http://arxiv.org/abs/2307.14109v1"},"cats":{"new-dataset":0.3893289805,"dev-research":0.2685027251,"prompt-eng":0.3501159526,"data-quality":0.3136620571,"ml-security":0.1329564526}}
{"text":"Chat Generative Pre-trained Transformer (ChatGPT) has gained significant interest and attention since its launch in November 2022.","meta":{"url":"http://arxiv.org/abs/2307.14107v1"},"cats":{"new-dataset":0.2368667997,"dev-research":0.2656603025,"prompt-eng":0.4032074914,"data-quality":0.1224198099,"ml-security":0.098544289}}
{"text":"It has shown impressive performance in various domains, including passing exams and creative writing.","meta":{"url":"http://arxiv.org/abs/2307.14107v1"},"cats":{"new-dataset":0.0547066538,"dev-research":0.3082996797,"prompt-eng":0.3593899322,"data-quality":0.1112360252,"ml-security":0.0470377974}}
{"text":"However, challenges and concerns related to biases and trust persist.","meta":{"url":"http://arxiv.org/abs/2307.14107v1"},"cats":{"new-dataset":0.0168018345,"dev-research":0.2806496729,"prompt-eng":0.369259956,"data-quality":0.2450129956,"ml-security":0.3143552811}}
{"text":"In this work, we present a comprehensive review of over 100 Scopus-indexed publications on ChatGPT, aiming to provide a taxonomy of ChatGPT research and explore its applications.","meta":{"url":"http://arxiv.org/abs/2307.14107v1"},"cats":{"new-dataset":0.3448397508,"dev-research":0.2038619853,"prompt-eng":0.3091470303,"data-quality":0.1140175827,"ml-security":0.0534213255}}
{"text":"We critically analyze the existing literature, identifying common approaches employed in the studies.","meta":{"url":"http://arxiv.org/abs/2307.14107v1"},"cats":{"new-dataset":0.0901058872,"dev-research":0.246190351,"prompt-eng":0.3079729877,"data-quality":0.1178576571,"ml-security":0.0612486069}}
{"text":"Additionally, we investigate diverse application areas where ChatGPT has found utility, such as healthcare, marketing and financial services, software engineering, academic and scientific writing, research and education, environmental science, and natural language processing.","meta":{"url":"http://arxiv.org/abs/2307.14107v1"},"cats":{"new-dataset":0.2381583494,"dev-research":0.3351555147,"prompt-eng":0.3530531262,"data-quality":0.1522160044,"ml-security":0.0816806949}}
{"text":"Through examining these applications, we gain valuable insights into the potential of ChatGPT in addressing real-world challenges.","meta":{"url":"http://arxiv.org/abs/2307.14107v1"},"cats":{"new-dataset":0.2300148152,"dev-research":0.3857209555,"prompt-eng":0.3574582078,"data-quality":0.1612378724,"ml-security":0.1286678946}}
{"text":"We also discuss crucial issues related to ChatGPT, including biases and trustworthiness, emphasizing the need for further research and development in these areas.","meta":{"url":"http://arxiv.org/abs/2307.14107v1"},"cats":{"new-dataset":0.1602765595,"dev-research":0.3193368719,"prompt-eng":0.3875856293,"data-quality":0.2422748997,"ml-security":0.1502218916}}
{"text":"Furthermore, we identify potential future directions for ChatGPT research, proposing solutions to current challenges and speculating on expected advancements.","meta":{"url":"http://arxiv.org/abs/2307.14107v1"},"cats":{"new-dataset":0.1896185058,"dev-research":0.2752692311,"prompt-eng":0.3891086126,"data-quality":0.1303503519,"ml-security":0.0810476639}}
{"text":"By fully leveraging the capabilities of ChatGPT, we can unlock its potential across various domains, leading to advancements in conversational AI and transformative impacts in society.","meta":{"url":"http://arxiv.org/abs/2307.14107v1"},"cats":{"new-dataset":0.24746212,"dev-research":0.2742620988,"prompt-eng":0.3414363809,"data-quality":0.1389317318,"ml-security":0.1755725081}}
{"text":"In ground-view object change detection, the recently emerging map-less navigation has great potential as a means of navigating a robot to distantly detected objects and identifying their changing states (appear/disappear/no-change) with high resolution imagery.","meta":{"url":"http://arxiv.org/abs/2307.14105v1"},"cats":{"new-dataset":0.1486540635,"dev-research":0.2134392796,"prompt-eng":0.4143308447,"data-quality":0.1591484034,"ml-security":0.1023783014}}
{"text":"However, the brute-force naive action strategy of navigating to every distant object requires huge sense/plan/action costs proportional to the number of objects.","meta":{"url":"http://arxiv.org/abs/2307.14105v1"},"cats":{"new-dataset":0.0241663749,"dev-research":0.2603118017,"prompt-eng":0.3594405622,"data-quality":0.0704751772,"ml-security":0.1671547868}}
{"text":"In this work, we study this new problem of ``Which distant objects should be prioritized for map-less navigation?\" and in order to speed up the R{\\&}D cycle, propose a highly-simplified approach that is easy to implement and easy to extend.","meta":{"url":"http://arxiv.org/abs/2307.14105v1"},"cats":{"new-dataset":0.1661515004,"dev-research":0.2444124849,"prompt-eng":0.4227248684,"data-quality":0.1355636622,"ml-security":0.0565784314}}
{"text":"In our approach, a new layer called map-based navigation is added on top of the map-less navigation, which constitutes a hierarchical planner.","meta":{"url":"http://arxiv.org/abs/2307.14105v1"},"cats":{"new-dataset":0.125951642,"dev-research":0.2847445497,"prompt-eng":0.4127903857,"data-quality":0.0595411809,"ml-security":0.0502311038}}
{"text":"First, a dataset consisting of $N$ view sequences is acquired by a real robot via map-less navigation.","meta":{"url":"http://arxiv.org/abs/2307.14105v1"},"cats":{"new-dataset":0.6648498034,"dev-research":0.2000159924,"prompt-eng":0.3311458917,"data-quality":0.0923555159,"ml-security":0.093117007}}
{"text":"Then, an environment simulator was built to simulate a simple action planning problem: ``Which view sequence should the robot select next?\".","meta":{"url":"http://arxiv.org/abs/2307.14105v1"},"cats":{"new-dataset":0.1366050064,"dev-research":0.3081519331,"prompt-eng":0.4414211101,"data-quality":0.0894208726,"ml-security":0.0879812088}}
{"text":"Then, a solver was built inspired by the analogy to the multi-armed bandit problem: ``Which arm should the player select next?\".","meta":{"url":"http://arxiv.org/abs/2307.14105v1"},"cats":{"new-dataset":0.0146053964,"dev-research":0.3673623917,"prompt-eng":0.4106240334,"data-quality":0.0998415979,"ml-security":0.155296669}}
{"text":"Finally, the effectiveness of the proposed framework was verified using the semantically non-trivial scenario ``sofa as bookshelf\".","meta":{"url":"http://arxiv.org/abs/2307.14105v1"},"cats":{"new-dataset":0.2236952084,"dev-research":0.3058771636,"prompt-eng":0.4308691775,"data-quality":0.1658901417,"ml-security":0.0916698964}}
{"text":"As for term rewrite systems, the dependency pair (DP, for short) framework with several kinds of DP processors is useful for proving termination of logically constrained term rewrite systems (LCTRSs, for short).","meta":{"url":"http://arxiv.org/abs/2307.14094v1"},"cats":{"new-dataset":0.0398487796,"dev-research":0.3936946718,"prompt-eng":0.3803306264,"data-quality":0.14831598,"ml-security":0.0968479373}}
{"text":"However, the polynomial interpretation processor is not so effective against LCTRSs with bit-vector arithmetic (BV-LCTRSs, for short).","meta":{"url":"http://arxiv.org/abs/2307.14094v1"},"cats":{"new-dataset":0.0223706049,"dev-research":0.3133968262,"prompt-eng":0.3997360153,"data-quality":0.1478469532,"ml-security":0.1252742162}}
{"text":"In this paper, we propose a novel DP processor for BV-LCTRSs to solve a singleton DP problem consisting of a dependency pair forming a self-loop.","meta":{"url":"http://arxiv.org/abs/2307.14094v1"},"cats":{"new-dataset":0.1313806531,"dev-research":0.3054665384,"prompt-eng":0.4421274627,"data-quality":0.134138872,"ml-security":0.1137678656}}
{"text":"The processor is based on an acyclic directed graph such that the nodes are bit-vectors and any dependency chain of the problem is projected to a path of the graph.","meta":{"url":"http://arxiv.org/abs/2307.14094v1"},"cats":{"new-dataset":0.0374989885,"dev-research":0.3841072351,"prompt-eng":0.3969097993,"data-quality":0.1337431368,"ml-security":0.1117111042}}
{"text":"We show a sufficient condition for the existence of such an acyclic graph, and simplify it for a specific case.","meta":{"url":"http://arxiv.org/abs/2307.14094v1"},"cats":{"new-dataset":0.0959626186,"dev-research":0.2209297365,"prompt-eng":0.3360492602,"data-quality":0.1551475338,"ml-security":0.0840407473}}
{"text":"This paper proposes a stochastic block model with dynamics where the population grows using preferential attachment.","meta":{"url":"http://arxiv.org/abs/2307.13713v1"},"cats":{"new-dataset":0.0252947162,"dev-research":0.1422321621,"prompt-eng":0.3983356782,"data-quality":0.0752239738,"ml-security":0.0874900392}}
{"text":"Nodes with higher weighted degree are more likely to recruit new nodes, and nodes always recruit nodes from their own community.","meta":{"url":"http://arxiv.org/abs/2307.13713v1"},"cats":{"new-dataset":0.0016108853,"dev-research":0.2460004029,"prompt-eng":0.3369127931,"data-quality":0.1073730185,"ml-security":0.1147271926}}
{"text":"This model can capture how communities grow or shrink based on their collaborations with other nodes in the network, where an edge represents collaboration on a project.   ","meta":{"url":"http://arxiv.org/abs/2307.13713v1"},"cats":{"new-dataset":0.0760901235,"dev-research":0.3298904494,"prompt-eng":0.3583734737,"data-quality":0.116645061,"ml-security":0.0735530389}}
{"text":"Focusing on the case of two communities, we derive a deterministic approximation to the dynamics and characterize the phase transitions for diversity, i.e. the parameter regimes in which either one of the communities dies out or the two communities reach parity over time.   ","meta":{"url":"http://arxiv.org/abs/2307.13713v1"},"cats":{"new-dataset":0.0794210077,"dev-research":0.136979724,"prompt-eng":0.3819376718,"data-quality":0.1213415814,"ml-security":0.1073575022}}
{"text":"In particular, we find that the minority may vanish when the probability of cross-community edges is low, even when cross-community projects are more valuable than projects with collaborators from the same community.","meta":{"url":"http://arxiv.org/abs/2307.13713v1"},"cats":{"new-dataset":0.0511538296,"dev-research":0.2642528507,"prompt-eng":0.3729194503,"data-quality":0.2473259717,"ml-security":0.1774037325}}
{"text":"We study reinforcement learning (RL) for learning a Quantal Stackelberg Equilibrium (QSE) in an episodic Markov game with a leader-follower structure.","meta":{"url":"http://arxiv.org/abs/2307.14085v1"},"cats":{"new-dataset":0.0415275849,"dev-research":0.1306377326,"prompt-eng":0.3946494805,"data-quality":0.0648029973,"ml-security":0.1760321812}}
{"text":"In specific, at the outset of the game, the leader announces her policy to the follower and commits to it.","meta":{"url":"http://arxiv.org/abs/2307.14085v1"},"cats":{"new-dataset":0.0595724589,"dev-research":0.2720753014,"prompt-eng":0.4392915405,"data-quality":0.0776162397,"ml-security":0.1219020313}}
{"text":"The follower observes the leader's policy and, in turn, adopts a quantal response policy by solving an entropy-regularized policy optimization problem induced by leader's policy.","meta":{"url":"http://arxiv.org/abs/2307.14085v1"},"cats":{"new-dataset":0.0237920117,"dev-research":0.1830891982,"prompt-eng":0.4513497229,"data-quality":0.0867276782,"ml-security":0.1253950258}}
{"text":"The goal of the leader is to find her optimal policy, which yields the optimal expected total return, by interacting with the follower and learning from data.","meta":{"url":"http://arxiv.org/abs/2307.14085v1"},"cats":{"new-dataset":0.0135335367,"dev-research":0.178252961,"prompt-eng":0.4106195707,"data-quality":0.0603971026,"ml-security":0.1522957692}}
{"text":"A key challenge of this problem is that the leader cannot observe the follower's reward, and needs to infer the follower's quantal response model from his actions against leader's policies.","meta":{"url":"http://arxiv.org/abs/2307.14085v1"},"cats":{"new-dataset":0.0377469731,"dev-research":0.1274489013,"prompt-eng":0.4804829255,"data-quality":0.1297386496,"ml-security":0.2375103594}}
{"text":"We propose sample-efficient algorithms for both the online and offline settings, in the context of function approximation.","meta":{"url":"http://arxiv.org/abs/2307.14085v1"},"cats":{"new-dataset":0.0706401413,"dev-research":0.2387034878,"prompt-eng":0.380541215,"data-quality":0.1295553996,"ml-security":0.0671822449}}
{"text":"Our algorithms are based on (i) learning the quantal response model via maximum likelihood estimation and (ii) model-free or model-based RL for solving the leader's decision making problem, and we show that they achieve sublinear regret upper bounds.","meta":{"url":"http://arxiv.org/abs/2307.14085v1"},"cats":{"new-dataset":0.0441192605,"dev-research":0.1755851143,"prompt-eng":0.4146948311,"data-quality":0.1348877935,"ml-security":0.2105190378}}
{"text":"Moreover, we quantify the uncertainty of these estimators and leverage the uncertainty to implement optimistic and pessimistic algorithms for online and offline settings.","meta":{"url":"http://arxiv.org/abs/2307.14085v1"},"cats":{"new-dataset":0.1247948599,"dev-research":0.2098459835,"prompt-eng":0.4095849781,"data-quality":0.1713998948,"ml-security":0.1161954691}}
{"text":"Besides, when specialized to the linear and myopic setting, our algorithms are also computationally efficient.","meta":{"url":"http://arxiv.org/abs/2307.14085v1"},"cats":{"new-dataset":0.0072204665,"dev-research":0.3151066259,"prompt-eng":0.3277307838,"data-quality":0.0835846677,"ml-security":0.0855163364}}
{"text":"Our theoretical analysis features a novel performance-difference lemma which incorporates the error of quantal response model, which might be of independent interest.","meta":{"url":"http://arxiv.org/abs/2307.14085v1"},"cats":{"new-dataset":0.010355892,"dev-research":0.1660543731,"prompt-eng":0.4198951392,"data-quality":0.1589636691,"ml-security":0.1079279721}}
{"text":"RDMA has been widely adopted for high-speed datacenter networks.","meta":{"url":"http://arxiv.org/abs/2307.14074v1"},"cats":{"new-dataset":0.0876693277,"dev-research":0.3257316264,"prompt-eng":0.3246904808,"data-quality":0.0899231931,"ml-security":0.0935114285}}
{"text":"However, native RDMA merely supports one-to-one reliable connection, which mismatches various applications with group communication patterns (e.g., one-to-many).","meta":{"url":"http://arxiv.org/abs/2307.14074v1"},"cats":{"new-dataset":0.0307724843,"dev-research":0.2746076802,"prompt-eng":0.3568853258,"data-quality":0.1733064475,"ml-security":0.1131005728}}
{"text":"While there are some multicast enhancements to address it, they all fail to simultaneously achieve optimal multicast forwarding and fully unleash the distinguished RDMA capabilities.   ","meta":{"url":"http://arxiv.org/abs/2307.14074v1"},"cats":{"new-dataset":0.0315524685,"dev-research":0.2272886646,"prompt-eng":0.3762032914,"data-quality":0.2066783164,"ml-security":0.1547454146}}
{"text":"In this paper, we present Gleam, an RDMA-accelerated multicast protocol that simultaneously supports optimal multicast forwarding, efficient utilization of the prominent RDMA capabilities, and compatibility with the commodity RNICs.","meta":{"url":"http://arxiv.org/abs/2307.14074v1"},"cats":{"new-dataset":0.2312386336,"dev-research":0.233797794,"prompt-eng":0.3708790686,"data-quality":0.1028888415,"ml-security":0.0992817062}}
{"text":"At its core, Gleam re-purposes the existing RDMA RC logic with careful switch coordination as an efficient multicast transport.","meta":{"url":"http://arxiv.org/abs/2307.14074v1"},"cats":{"new-dataset":0.0463609056,"dev-research":0.3336711987,"prompt-eng":0.4161859148,"data-quality":0.0823482793,"ml-security":0.084705711}}
{"text":"Gleam performs the one-to-many connection maintenance and many-to-one feedback aggregation, based on an extended multicast forwarding table structure, to achieve integration between standard RC logic and in-fabric multicast.","meta":{"url":"http://arxiv.org/abs/2307.14074v1"},"cats":{"new-dataset":0.0637306165,"dev-research":0.3050110729,"prompt-eng":0.463268612,"data-quality":0.1002157983,"ml-security":0.0722486464}}
{"text":"We implement a fully functional Gleam prototype.","meta":{"url":"http://arxiv.org/abs/2307.14074v1"},"cats":{"new-dataset":0.0536869623,"dev-research":0.3155178606,"prompt-eng":0.4932005637,"data-quality":0.1123708259,"ml-security":0.1103319394}}
{"text":"With extensive testbed experiments and simulations, we demonstrate Gleam's significant improvement in accelerating multicast communication of realistic applications.","meta":{"url":"http://arxiv.org/abs/2307.14074v1"},"cats":{"new-dataset":0.0939390005,"dev-research":0.2796782367,"prompt-eng":0.4006576795,"data-quality":0.1498034443,"ml-security":0.1217951568}}
{"text":"For instance, Gleam achieves 2.9X lower communication time of an HPC benchmark application and 2.7X higher data replication throughput.","meta":{"url":"http://arxiv.org/abs/2307.14074v1"},"cats":{"new-dataset":0.0256268858,"dev-research":0.3390851662,"prompt-eng":0.3686770607,"data-quality":0.1014402015,"ml-security":0.1126087347}}
{"text":"Recently, diffusion models like StableDiffusion have achieved impressive image generation results.","meta":{"url":"http://arxiv.org/abs/2307.14073v1"},"cats":{"new-dataset":0.085929215,"dev-research":0.167546115,"prompt-eng":0.3878087201,"data-quality":0.1020288047,"ml-security":0.0755100336}}
{"text":"However, the generation process of such diffusion models is uncontrollable, which makes it hard to generate videos with continuous and consistent content.","meta":{"url":"http://arxiv.org/abs/2307.14073v1"},"cats":{"new-dataset":0.0234088105,"dev-research":0.1817788959,"prompt-eng":0.358385444,"data-quality":0.1402312546,"ml-security":0.105247483}}
{"text":"In this work, by using the diffusion model with ControlNet, we proposed a new motion-guided video-to-video translation framework called VideoControlNet to generate various videos based on the given prompts and the condition from the input video.","meta":{"url":"http://arxiv.org/abs/2307.14073v1"},"cats":{"new-dataset":0.1539062509,"dev-research":0.2161061584,"prompt-eng":0.4415259136,"data-quality":0.1633821038,"ml-security":0.0751058881}}
{"text":"Inspired by the video codecs that use motion information for reducing temporal redundancy, our framework uses motion information to prevent the regeneration of the redundant areas for content consistency.","meta":{"url":"http://arxiv.org/abs/2307.14073v1"},"cats":{"new-dataset":0.1358766329,"dev-research":0.3274169121,"prompt-eng":0.4201712416,"data-quality":0.2520101684,"ml-security":0.0673881839}}
{"text":"Specifically, we generate the first frame (i.e., the I-frame) by using the diffusion model with ControlNet.","meta":{"url":"http://arxiv.org/abs/2307.14073v1"},"cats":{"new-dataset":0.1248955602,"dev-research":0.1677471839,"prompt-eng":0.4321789327,"data-quality":0.093376588,"ml-security":0.0701412567}}
{"text":"Then we generate other key frames (i.e., the P-frame) based on the previous I/P-frame by using our newly proposed motion-guided P-frame generation (MgPG) method, in which the P-frames are generated based on the motion information and the occlusion areas are inpainted by using the diffusion model.","meta":{"url":"http://arxiv.org/abs/2307.14073v1"},"cats":{"new-dataset":0.1667861825,"dev-research":0.1953375849,"prompt-eng":0.4192057111,"data-quality":0.0777541735,"ml-security":0.0335845887}}
{"text":"Finally, the rest frames (i.e., the B-frame) are generated by using our motion-guided B-frame interpolation (MgBI) module.","meta":{"url":"http://arxiv.org/abs/2307.14073v1"},"cats":{"new-dataset":0.3256694051,"dev-research":0.1752139901,"prompt-eng":0.4173104544,"data-quality":0.1144790235,"ml-security":0.0323482242}}
{"text":"Our experiments demonstrate that our proposed VideoControlNet inherits the generation capability of the pre-trained large diffusion model and extends the image diffusion model to the video diffusion model by using motion information.","meta":{"url":"http://arxiv.org/abs/2307.14073v1"},"cats":{"new-dataset":0.1195267183,"dev-research":0.1970626042,"prompt-eng":0.4057822752,"data-quality":0.130322993,"ml-security":0.1306731432}}
{"text":"More results are provided at our project page.","meta":{"url":"http://arxiv.org/abs/2307.14073v1"},"cats":{"new-dataset":0.2531940134,"dev-research":0.2237787541,"prompt-eng":0.3915274742,"data-quality":0.1264873069,"ml-security":0.0512342297}}
{"text":"Correlation based stereo matching has achieved outstanding performance, which pursues cost volume between two feature maps.","meta":{"url":"http://arxiv.org/abs/2307.14071v1"},"cats":{"new-dataset":0.0294298697,"dev-research":0.2896385139,"prompt-eng":0.3964217077,"data-quality":0.1339142111,"ml-security":0.0352200058}}
{"text":"Unfortunately, current methods with a fixed model do not work uniformly well across various datasets, greatly limiting their real-world applicability.","meta":{"url":"http://arxiv.org/abs/2307.14071v1"},"cats":{"new-dataset":0.1475233394,"dev-research":0.171726804,"prompt-eng":0.3825844861,"data-quality":0.2736295019,"ml-security":0.1193189249}}
{"text":"To tackle this issue, this paper proposes a new perspective to dynamically calculate correlation for robust stereo matching.","meta":{"url":"http://arxiv.org/abs/2307.14071v1"},"cats":{"new-dataset":0.0524127595,"dev-research":0.2249200873,"prompt-eng":0.409128292,"data-quality":0.2106962117,"ml-security":0.052627741}}
{"text":"A novel Uncertainty Guided Adaptive Correlation (UGAC) module is introduced to robustly adapt the same model for different scenarios.","meta":{"url":"http://arxiv.org/abs/2307.14071v1"},"cats":{"new-dataset":0.0433842804,"dev-research":0.2660515231,"prompt-eng":0.4564404923,"data-quality":0.2082185211,"ml-security":0.0913065993}}
{"text":"Specifically, a variance-based uncertainty estimation is employed to adaptively adjust the sampling area during warping operation.","meta":{"url":"http://arxiv.org/abs/2307.14071v1"},"cats":{"new-dataset":0.0162477088,"dev-research":0.2789296748,"prompt-eng":0.4435839476,"data-quality":0.2516082434,"ml-security":0.0764467282}}
{"text":"Additionally, we improve the traditional non-parametric warping with learnable parameters, such that the position-specific weights can be learned.","meta":{"url":"http://arxiv.org/abs/2307.14071v1"},"cats":{"new-dataset":0.0693367171,"dev-research":0.2039783111,"prompt-eng":0.4071744797,"data-quality":0.1723310034,"ml-security":0.1138183029}}
{"text":"We show that by empowering the recurrent network with the UGAC module, stereo matching can be exploited more robustly and effectively.","meta":{"url":"http://arxiv.org/abs/2307.14071v1"},"cats":{"new-dataset":0.1991845967,"dev-research":0.2149837113,"prompt-eng":0.3983997162,"data-quality":0.2252815817,"ml-security":0.0942395904}}
{"text":"Extensive experiments demonstrate that our method achieves state-of-the-art performance over the ETH3D, KITTI, and Middlebury datasets when employing the same fixed model over these datasets without any retraining procedure.","meta":{"url":"http://arxiv.org/abs/2307.14071v1"},"cats":{"new-dataset":0.3429814634,"dev-research":0.2217088032,"prompt-eng":0.3700258145,"data-quality":0.1364289457,"ml-security":0.0945467912}}
{"text":"To target real-time applications, we further design a lightweight model based on UGAC, which also outperforms other methods over KITTI benchmarks with only 0.6 M parameters.","meta":{"url":"http://arxiv.org/abs/2307.14071v1"},"cats":{"new-dataset":0.1338918343,"dev-research":0.3062548437,"prompt-eng":0.4561321014,"data-quality":0.1003812771,"ml-security":0.1035021951}}
{"text":"Relying on large-scale training data with pixel-level labels, previous edge detection methods have achieved high performance.","meta":{"url":"http://arxiv.org/abs/2307.14070v1"},"cats":{"new-dataset":0.0761899717,"dev-research":0.2471882073,"prompt-eng":0.3626485299,"data-quality":0.2492704132,"ml-security":0.1598885629}}
{"text":"However, it is hard to manually label edges accurately, especially for large datasets, and thus the datasets inevitably contain noisy labels.","meta":{"url":"http://arxiv.org/abs/2307.14070v1"},"cats":{"new-dataset":0.0724885491,"dev-research":0.3320386341,"prompt-eng":0.3777984889,"data-quality":0.6318849446,"ml-security":0.1168195636}}
{"text":"This label-noise issue has been studied extensively for classification, while still remaining under-explored for edge detection.","meta":{"url":"http://arxiv.org/abs/2307.14070v1"},"cats":{"new-dataset":0.0877557611,"dev-research":0.221257738,"prompt-eng":0.39319394,"data-quality":0.7016543584,"ml-security":0.1906746816}}
{"text":"To address the label-noise issue for edge detection, this paper proposes to learn Pixel-level NoiseTransitions to model the label-corruption process.","meta":{"url":"http://arxiv.org/abs/2307.14070v1"},"cats":{"new-dataset":0.1720970953,"dev-research":0.3175509959,"prompt-eng":0.4016651286,"data-quality":0.7309246463,"ml-security":0.2275922659}}
{"text":"To achieve it, we develop a novel Pixel-wise Shift Learning (PSL) module to estimate the transition from clean to noisy labels as a displacement field.","meta":{"url":"http://arxiv.org/abs/2307.14070v1"},"cats":{"new-dataset":0.1827026415,"dev-research":0.2250663289,"prompt-eng":0.4237414187,"data-quality":0.3717240732,"ml-security":0.0870693258}}
{"text":"Exploiting the estimated noise transitions, our model, named PNT-Edge, is able to fit the prediction to clean labels.","meta":{"url":"http://arxiv.org/abs/2307.14070v1"},"cats":{"new-dataset":0.1806809051,"dev-research":0.2020350387,"prompt-eng":0.461203304,"data-quality":0.5648572814,"ml-security":0.1312014208}}
{"text":"In addition, a local edge density regularization term is devised to exploit local structure information for better transition learning.","meta":{"url":"http://arxiv.org/abs/2307.14070v1"},"cats":{"new-dataset":0.0388915157,"dev-research":0.1842480956,"prompt-eng":0.3882803691,"data-quality":0.1836901987,"ml-security":0.1054966657}}
{"text":"This term encourages learning large shifts for the edges with complex local structures.","meta":{"url":"http://arxiv.org/abs/2307.14070v1"},"cats":{"new-dataset":0.0443654445,"dev-research":0.2629646899,"prompt-eng":0.3147466893,"data-quality":0.1586233483,"ml-security":0.1142652798}}
{"text":"Experiments on SBD and Cityscapes demonstrate the effectiveness of our method in relieving the impact of label noise.","meta":{"url":"http://arxiv.org/abs/2307.14070v1"},"cats":{"new-dataset":0.1045902772,"dev-research":0.3028821128,"prompt-eng":0.4775464521,"data-quality":0.7201240917,"ml-security":0.1409973946}}
{"text":"Codes will be available at github.","meta":{"url":"http://arxiv.org/abs/2307.14070v1"},"cats":{"new-dataset":0.4119022542,"dev-research":0.2872278385,"prompt-eng":0.4295347876,"data-quality":0.1854827394,"ml-security":0.0786686138}}
{"text":"Multi-source unsupervised domain adaptation (MUDA) aims to transfer knowledge from related source domains to an unlabeled target domain.","meta":{"url":"http://arxiv.org/abs/2307.14068v1"},"cats":{"new-dataset":0.14009442,"dev-research":0.2599189138,"prompt-eng":0.3898395789,"data-quality":0.2818550204,"ml-security":0.1298149236}}
{"text":"While recent MUDA methods have shown promising results, most focus on aligning the overall feature distributions across source domains, which can lead to negative effects due to redundant features within each domain.","meta":{"url":"http://arxiv.org/abs/2307.14068v1"},"cats":{"new-dataset":0.0560343447,"dev-research":0.3543345154,"prompt-eng":0.4118228576,"data-quality":0.3413019501,"ml-security":0.1783849065}}
{"text":"Moreover, there is a significant performance gap between MUDA and supervised methods.","meta":{"url":"http://arxiv.org/abs/2307.14068v1"},"cats":{"new-dataset":0.0594775941,"dev-research":0.2763538488,"prompt-eng":0.3594804855,"data-quality":0.2815776311,"ml-security":0.0988047812}}
{"text":"To address these challenges, we propose a novel approach called Dynamic Domain Discrepancy Adjustment for Active Multi-Domain Adaptation (D3AAMDA).","meta":{"url":"http://arxiv.org/abs/2307.14068v1"},"cats":{"new-dataset":0.0620544207,"dev-research":0.287081649,"prompt-eng":0.4372830357,"data-quality":0.3344737177,"ml-security":0.099098363}}
{"text":"Firstly, we establish a multi-source dynamic modulation mechanism during the training process based on the degree of distribution differences between source and target domains.","meta":{"url":"http://arxiv.org/abs/2307.14068v1"},"cats":{"new-dataset":0.0393288351,"dev-research":0.2269206068,"prompt-eng":0.4446403976,"data-quality":0.1802156233,"ml-security":0.2415270111}}
{"text":"This mechanism controls the alignment level of features between each source domain and the target domain, effectively leveraging the local advantageous feature information within the source domains.","meta":{"url":"http://arxiv.org/abs/2307.14068v1"},"cats":{"new-dataset":0.0331360398,"dev-research":0.389897334,"prompt-eng":0.4629707681,"data-quality":0.2060729351,"ml-security":0.1085754422}}
{"text":"Additionally, we propose a Multi-source Active Boundary Sample Selection (MABS) strategy, which utilizes a guided dynamic boundary loss to design an efficient query function for selecting important samples.","meta":{"url":"http://arxiv.org/abs/2307.14068v1"},"cats":{"new-dataset":0.1470294746,"dev-research":0.2337656978,"prompt-eng":0.4300686284,"data-quality":0.2269330395,"ml-security":0.1517617929}}
{"text":"This strategy achieves improved generalization to the target domain with minimal sampling costs.","meta":{"url":"http://arxiv.org/abs/2307.14068v1"},"cats":{"new-dataset":0.0287180308,"dev-research":0.2024589587,"prompt-eng":0.4555637733,"data-quality":0.2065747878,"ml-security":0.1529919497}}
{"text":"We extensively evaluate our proposed method on commonly used domain adaptation datasets, comparing it against existing UDA and ADA methods.","meta":{"url":"http://arxiv.org/abs/2307.14068v1"},"cats":{"new-dataset":0.2709763862,"dev-research":0.2785561691,"prompt-eng":0.4047349948,"data-quality":0.304624053,"ml-security":0.148524665}}
{"text":"The experimental results unequivocally demonstrate the superiority of our approach.","meta":{"url":"http://arxiv.org/abs/2307.14068v1"},"cats":{"new-dataset":0.0082703306,"dev-research":0.2567690345,"prompt-eng":0.4034493997,"data-quality":0.303351066,"ml-security":0.1627551591}}
{"text":"Detection of easily missed hidden patterns with fast processing power makes machine learning (ML) indispensable to today's healthcare system.","meta":{"url":"http://arxiv.org/abs/2307.14067v1"},"cats":{"new-dataset":0.0390822078,"dev-research":0.3229809026,"prompt-eng":0.3510294885,"data-quality":0.2620578206,"ml-security":0.489853674}}
{"text":"Though many ML applications have already been discovered and many are still under investigation, only a few have been adopted by current healthcare systems.","meta":{"url":"http://arxiv.org/abs/2307.14067v1"},"cats":{"new-dataset":0.0295728261,"dev-research":0.2087903969,"prompt-eng":0.2939261792,"data-quality":0.1364672193,"ml-security":0.2115067165}}
{"text":"As a result, there exists an enormous opportunity in healthcare system for ML but distributed information, scarcity of properly arranged and easily explainable documentation in related sector are major impede which are making ML applications difficult to healthcare professionals.","meta":{"url":"http://arxiv.org/abs/2307.14067v1"},"cats":{"new-dataset":0.0509022845,"dev-research":0.3188147869,"prompt-eng":0.3387690739,"data-quality":0.1378539916,"ml-security":0.2574357289}}
{"text":"This study aimed to gather ML applications in different areas of healthcare concisely and more effectively so that necessary information can be accessed immediately with relevant references.","meta":{"url":"http://arxiv.org/abs/2307.14067v1"},"cats":{"new-dataset":0.1358659505,"dev-research":0.2939315522,"prompt-eng":0.3585389386,"data-quality":0.1059844959,"ml-security":0.1492069722}}
{"text":"We divided our study into five major groups: community level work, risk management/ preventive care, healthcare operation management, remote care, and early detection.","meta":{"url":"http://arxiv.org/abs/2307.14067v1"},"cats":{"new-dataset":0.142747466,"dev-research":0.2439450299,"prompt-eng":0.3469103866,"data-quality":0.0596815462,"ml-security":0.1152346555}}
{"text":"Dividing these groups into subgroups, we provided relevant references with description in tabular form for quick access.","meta":{"url":"http://arxiv.org/abs/2307.14067v1"},"cats":{"new-dataset":0.4672608093,"dev-research":0.2053342275,"prompt-eng":0.4123705756,"data-quality":0.1230247222,"ml-security":0.0517617296}}
{"text":"Our objective is to inform people about ML applicability in healthcare industry, reduce the knowledge gap of clinicians about the ML applications and motivate healthcare professionals towards more machine learning based healthcare system.","meta":{"url":"http://arxiv.org/abs/2307.14067v1"},"cats":{"new-dataset":0.05312289,"dev-research":0.3577620325,"prompt-eng":0.3472266679,"data-quality":0.156038976,"ml-security":0.3465727877}}
{"text":"Medical radiography segmentation, and specifically dental radiography, is highly limited by the cost of labeling which requires specific expertise and labor-intensive annotations.","meta":{"url":"http://arxiv.org/abs/2307.14066v1"},"cats":{"new-dataset":0.0374733751,"dev-research":0.246531234,"prompt-eng":0.3423292287,"data-quality":0.1792160897,"ml-security":0.0950138653}}
{"text":"In this work, we propose a straightforward pre-training method for semantic segmentation leveraging Denoising Diffusion Probabilistic Models (DDPM), which have shown impressive results for generative modeling.","meta":{"url":"http://arxiv.org/abs/2307.14066v1"},"cats":{"new-dataset":0.269894724,"dev-research":0.1941754885,"prompt-eng":0.4319449862,"data-quality":0.2457226385,"ml-security":0.1045929848}}
{"text":"Our straightforward approach achieves remarkable performance in terms of label efficiency and does not require architectural modifications between pre-training and downstream tasks.","meta":{"url":"http://arxiv.org/abs/2307.14066v1"},"cats":{"new-dataset":0.1117744173,"dev-research":0.2667763482,"prompt-eng":0.4815215996,"data-quality":0.3811482534,"ml-security":0.0818174946}}
{"text":"We propose to first pre-train a Unet by exploiting the DDPM training objective, and then fine-tune the resulting model on a segmentation task.","meta":{"url":"http://arxiv.org/abs/2307.14066v1"},"cats":{"new-dataset":0.3950488492,"dev-research":0.2163596826,"prompt-eng":0.4780714411,"data-quality":0.2726444568,"ml-security":0.1243545056}}
{"text":"Our experimental results on the segmentation of dental radiographs demonstrate that the proposed method is competitive with state-of-the-art pre-training methods.","meta":{"url":"http://arxiv.org/abs/2307.14066v1"},"cats":{"new-dataset":0.0532659641,"dev-research":0.2239554101,"prompt-eng":0.4002256918,"data-quality":0.2311237883,"ml-security":0.0945748092}}
{"text":"Relay-enabled backscatter communication (BC) is an intriguing paradigm to alleviate energy shortage and improve throughput of Internet-of-Things (IoT) devices.","meta":{"url":"http://arxiv.org/abs/2307.14064v1"},"cats":{"new-dataset":0.0314338113,"dev-research":0.2713199411,"prompt-eng":0.4257444008,"data-quality":0.1034202031,"ml-security":0.1959156067}}
{"text":"Most of the existing works focus on the resource allocation that considered the unequal and continuous time allocation for both source-relay and relay-destination links.","meta":{"url":"http://arxiv.org/abs/2307.14064v1"},"cats":{"new-dataset":0.0554702532,"dev-research":0.2655002823,"prompt-eng":0.3657811189,"data-quality":0.1004306835,"ml-security":0.0836219125}}
{"text":"However, the continuous time allocation may be infeasible since in practice, the time allocation shall be carried out in integral multiple of the subframe duration unit.","meta":{"url":"http://arxiv.org/abs/2307.14064v1"},"cats":{"new-dataset":0.0370452715,"dev-research":0.2281863422,"prompt-eng":0.3581450224,"data-quality":0.0756483857,"ml-security":0.0872751228}}
{"text":"In this article, we study a discrete time scheme from the perspective of frame structure, where one transmission block is divided into two phases and the linear mapping is employed as a re-encoding method to determine the number of subframes for both phases and the power allocation for each subframe in a relay-enabled BC system.","meta":{"url":"http://arxiv.org/abs/2307.14064v1"},"cats":{"new-dataset":0.068715379,"dev-research":0.2328273467,"prompt-eng":0.3957690175,"data-quality":0.0835505655,"ml-security":0.107244764}}
{"text":"Based on this, we derive an accurate system-throughput expression and formulate a mixed-integral non-convex optimization problem to maximize the system throughput by jointly optimizing the power reflection coefficient (PRC) of the IoT node, the power allocation of the hybrid access point (HAP) and the linear mapping matrix, and solve it via a three-step approach.","meta":{"url":"http://arxiv.org/abs/2307.14064v1"},"cats":{"new-dataset":0.0148299138,"dev-research":0.2102540695,"prompt-eng":0.3772631848,"data-quality":0.0710770806,"ml-security":0.1232290883}}
{"text":"Accordingly, we propose a low complexity iterative algorithm to obtain the throughput maximization-based resource allocation solution.","meta":{"url":"http://arxiv.org/abs/2307.14064v1"},"cats":{"new-dataset":0.0345668862,"dev-research":0.29479103,"prompt-eng":0.3836161828,"data-quality":0.0804151929,"ml-security":0.0974025353}}
{"text":"Numerical results analyze the performance of our proposed algorithm, verify the superiority of our proposed scheme, and evaluate the impacts of network parameters on the system throughput.","meta":{"url":"http://arxiv.org/abs/2307.14064v1"},"cats":{"new-dataset":0.0238862021,"dev-research":0.2539387295,"prompt-eng":0.3949856033,"data-quality":0.1272219964,"ml-security":0.1250415647}}
{"text":"Image recognition has recently witnessed a paradigm shift, where vision-language models are now used to perform few-shot classification based on textual prompts.","meta":{"url":"http://arxiv.org/abs/2307.14063v1"},"cats":{"new-dataset":0.1219301507,"dev-research":0.2065425389,"prompt-eng":0.436866053,"data-quality":0.3259879694,"ml-security":0.1472397711}}
{"text":"Among these, the CLIP model has shown remarkable capabilities for zero-shot transfer by matching an image and a custom textual prompt in its latent space.","meta":{"url":"http://arxiv.org/abs/2307.14063v1"},"cats":{"new-dataset":0.2013568688,"dev-research":0.1598772647,"prompt-eng":0.4861686279,"data-quality":0.1501976763,"ml-security":0.1036330157}}
{"text":"This has paved the way for several works that focus on engineering or learning textual contexts for maximizing CLIP's classification capabilities.","meta":{"url":"http://arxiv.org/abs/2307.14063v1"},"cats":{"new-dataset":0.061391968,"dev-research":0.3293820007,"prompt-eng":0.3940830152,"data-quality":0.2871790448,"ml-security":0.1553451737}}
{"text":"In this paper, we follow this trend by learning an ensemble of prompts for image classification.","meta":{"url":"http://arxiv.org/abs/2307.14063v1"},"cats":{"new-dataset":0.1819249198,"dev-research":0.1893137172,"prompt-eng":0.5039825646,"data-quality":0.3481623046,"ml-security":0.2749256768}}
{"text":"We show that learning diverse and possibly shorter contexts improves considerably and consistently the results rather than relying on a single trainable prompt.","meta":{"url":"http://arxiv.org/abs/2307.14063v1"},"cats":{"new-dataset":0.072417409,"dev-research":0.2721439351,"prompt-eng":0.5063297801,"data-quality":0.3048210068,"ml-security":0.1652075676}}
{"text":"In particular, we report better few-shot capabilities with no additional cost at inference time.","meta":{"url":"http://arxiv.org/abs/2307.14063v1"},"cats":{"new-dataset":0.0366953515,"dev-research":0.2292716649,"prompt-eng":0.3854213392,"data-quality":0.1326511678,"ml-security":0.0853091048}}
{"text":"We demonstrate the capabilities of our approach on 11 different benchmarks.","meta":{"url":"http://arxiv.org/abs/2307.14063v1"},"cats":{"new-dataset":0.1949125977,"dev-research":0.2614767719,"prompt-eng":0.4473367937,"data-quality":0.1622075153,"ml-security":0.0721044769}}
{"text":"Vision-language pre-training (VLP) models have shown vulnerability to adversarial examples in multimodal tasks.","meta":{"url":"http://arxiv.org/abs/2307.14061v1"},"cats":{"new-dataset":0.1011736171,"dev-research":0.2720470922,"prompt-eng":0.421630514,"data-quality":0.3852369094,"ml-security":0.5928493143}}
{"text":"Furthermore, malicious adversaries can be deliberately transferred to attack other black-box models.","meta":{"url":"http://arxiv.org/abs/2307.14061v1"},"cats":{"new-dataset":0.0092243581,"dev-research":0.2930664416,"prompt-eng":0.3826806726,"data-quality":0.1863154433,"ml-security":0.8290269518}}
{"text":"However, existing work has mainly focused on investigating white-box attacks.","meta":{"url":"http://arxiv.org/abs/2307.14061v1"},"cats":{"new-dataset":0.0426433145,"dev-research":0.3155063921,"prompt-eng":0.3488086867,"data-quality":0.1119137086,"ml-security":0.4796873287}}
{"text":"In this paper, we present the first study to investigate the adversarial transferability of recent VLP models.","meta":{"url":"http://arxiv.org/abs/2307.14061v1"},"cats":{"new-dataset":0.0236047003,"dev-research":0.1879160171,"prompt-eng":0.3746022539,"data-quality":0.302125733,"ml-security":0.6483296311}}
{"text":"We observe that existing methods exhibit much lower transferability, compared to the strong attack performance in white-box settings.","meta":{"url":"http://arxiv.org/abs/2307.14061v1"},"cats":{"new-dataset":0.0142147192,"dev-research":0.2719250402,"prompt-eng":0.4147471852,"data-quality":0.1490374248,"ml-security":0.6517069289}}
{"text":"The transferability degradation is partly caused by the under-utilization of cross-modal interactions.","meta":{"url":"http://arxiv.org/abs/2307.14061v1"},"cats":{"new-dataset":0.0101242189,"dev-research":0.2645130299,"prompt-eng":0.4057140395,"data-quality":0.1567101664,"ml-security":0.1319607502}}
{"text":"Particularly, unlike unimodal learning, VLP models rely heavily on cross-modal interactions and the multimodal alignments are many-to-many, e.g., an image can be described in various natural languages.","meta":{"url":"http://arxiv.org/abs/2307.14061v1"},"cats":{"new-dataset":0.0731712679,"dev-research":0.2175001434,"prompt-eng":0.3706527479,"data-quality":0.1560898746,"ml-security":0.0966180879}}
{"text":"To this end, we propose a highly transferable Set-level Guidance Attack (SGA) that thoroughly leverages modality interactions and incorporates alignment-preserving augmentation with cross-modal guidance.","meta":{"url":"http://arxiv.org/abs/2307.14061v1"},"cats":{"new-dataset":0.128622701,"dev-research":0.2896225808,"prompt-eng":0.4731643926,"data-quality":0.2797113325,"ml-security":0.418581746}}
{"text":"Experimental results demonstrate that SGA could generate adversarial examples that can strongly transfer across different VLP models on multiple downstream vision-language tasks.","meta":{"url":"http://arxiv.org/abs/2307.14061v1"},"cats":{"new-dataset":0.0366882126,"dev-research":0.2282665114,"prompt-eng":0.3955018191,"data-quality":0.3242497853,"ml-security":0.4039436664}}
{"text":"On image-text retrieval, SGA significantly enhances the attack success rate for transfer attacks from ALBEF to TCL by a large margin (at least 9.78% and up to 30.21%), compared to the state-of-the-art.","meta":{"url":"http://arxiv.org/abs/2307.14061v1"},"cats":{"new-dataset":0.1019566453,"dev-research":0.2333981044,"prompt-eng":0.3875049984,"data-quality":0.2143506627,"ml-security":0.327458533}}
{"text":"Modern web-based platforms show ranked lists of recommendations to users, attempting to maximise user satisfaction or business metrics.","meta":{"url":"http://arxiv.org/abs/2307.14059v1"},"cats":{"new-dataset":0.0625163805,"dev-research":0.295543482,"prompt-eng":0.3972388173,"data-quality":0.1237061061,"ml-security":0.0859607048}}
{"text":"Typically, the goal of such systems boils down to maximising the exposure probability for items that are deemed \"reward-maximising\" according to a metric of interest.","meta":{"url":"http://arxiv.org/abs/2307.14059v1"},"cats":{"new-dataset":0.0073841366,"dev-research":0.2462886413,"prompt-eng":0.4194064693,"data-quality":0.1065173167,"ml-security":0.244357373}}
{"text":"This general framing comprises streaming applications, as well as e-commerce or job recommendations, and even web search.","meta":{"url":"http://arxiv.org/abs/2307.14059v1"},"cats":{"new-dataset":0.0915494266,"dev-research":0.2391532703,"prompt-eng":0.3677994349,"data-quality":0.1129273799,"ml-security":0.076733493}}
{"text":"Position bias or user models can be used to estimate exposure probabilities for each use-case, specifically tailored to how users interact with the presented rankings.","meta":{"url":"http://arxiv.org/abs/2307.14059v1"},"cats":{"new-dataset":0.0210786092,"dev-research":0.2778250008,"prompt-eng":0.4905431324,"data-quality":0.1241030524,"ml-security":0.2684397591}}
{"text":"A unifying factor in these diverse problem settings is that typically only one or several items will be engaged with (clicked, streamed,...) before a user leaves the ranked list.","meta":{"url":"http://arxiv.org/abs/2307.14059v1"},"cats":{"new-dataset":0.0243412164,"dev-research":0.2999948016,"prompt-eng":0.4038311658,"data-quality":0.1796430634,"ml-security":0.1603817185}}
{"text":"Short-video feeds on social media platforms diverge from this general framing in several ways, most notably that users do not tend to leave the feed after e.g. liking a post.","meta":{"url":"http://arxiv.org/abs/2307.14059v1"},"cats":{"new-dataset":0.0147038771,"dev-research":0.2203959697,"prompt-eng":0.3256535249,"data-quality":0.1788329151,"ml-security":0.1090081157}}
{"text":"Indeed, seemingly infinite feeds invite users to scroll further down the ranked list.","meta":{"url":"http://arxiv.org/abs/2307.14059v1"},"cats":{"new-dataset":0.07049788,"dev-research":0.1980622403,"prompt-eng":0.3647413614,"data-quality":0.0970926751,"ml-security":0.1212884317}}
{"text":"For this reason, existing position bias or user models tend to fall short in such settings, as they do not accurately capture users' interaction modalities.   ","meta":{"url":"http://arxiv.org/abs/2307.14059v1"},"cats":{"new-dataset":0.0103524791,"dev-research":0.2621721638,"prompt-eng":0.4421595121,"data-quality":0.1703552792,"ml-security":0.1307256724}}
{"text":"In this work, we propose a novel and probabilistically sound personalised position bias model for feed recommendations.","meta":{"url":"http://arxiv.org/abs/2307.14059v1"},"cats":{"new-dataset":0.0450151938,"dev-research":0.1776725001,"prompt-eng":0.4647382331,"data-quality":0.1927050818,"ml-security":0.1257264298}}
{"text":"We focus on a 1st-level feed in a hierarchical structure, where users may enter a 2nd-level feed via any given 1st-level item.","meta":{"url":"http://arxiv.org/abs/2307.14059v1"},"cats":{"new-dataset":0.0883485176,"dev-research":0.1839799521,"prompt-eng":0.4960999848,"data-quality":0.1289388132,"ml-security":0.0851836439}}
{"text":"We posit that users come to the platform with a scrolling budget drawn according to some distribution, and show how the survival function of said distribution can be used to obtain closed-form estimates for personalised exposure probabilities.","meta":{"url":"http://arxiv.org/abs/2307.14059v1"},"cats":{"new-dataset":0.4452996699,"dev-research":0.2240581802,"prompt-eng":0.4536585923,"data-quality":0.08909501,"ml-security":0.2562492609}}
{"text":"Empirical insights from a large-scale social media platform show how our probabilistic position bias model more accurately captures empirical exposure than existing models, and paves the way for unbiased evaluation and learning-to-rank.","meta":{"url":"http://arxiv.org/abs/2307.14059v1"},"cats":{"new-dataset":0.06604088,"dev-research":0.2390790683,"prompt-eng":0.3778312446,"data-quality":0.256287467,"ml-security":0.2163308395}}
{"text":"Despite the presence of the classification task in many different benchmark datasets for perception in the automotive domain, few efforts have been undertaken to define consistent classification requirements.","meta":{"url":"http://arxiv.org/abs/2307.14058v1"},"cats":{"new-dataset":0.0891579219,"dev-research":0.2742247237,"prompt-eng":0.4348985458,"data-quality":0.3846319484,"ml-security":0.0848484618}}
{"text":"This work addresses the topic by proposing a structured method to generate a classification structure.","meta":{"url":"http://arxiv.org/abs/2307.14058v1"},"cats":{"new-dataset":0.2012382291,"dev-research":0.2697907776,"prompt-eng":0.4979689713,"data-quality":0.2938858944,"ml-security":0.1340184119}}
{"text":"First, legal categories are identified based on behavioral requirements for the vehicle.","meta":{"url":"http://arxiv.org/abs/2307.14058v1"},"cats":{"new-dataset":0.0515390768,"dev-research":0.2558784432,"prompt-eng":0.4005988657,"data-quality":0.2007820888,"ml-security":0.1280132036}}
{"text":"This structure is further substantiated by considering the two aspects of collision safety for objects as well as perceptual categories.","meta":{"url":"http://arxiv.org/abs/2307.14058v1"},"cats":{"new-dataset":0.0751565148,"dev-research":0.241678725,"prompt-eng":0.3789605596,"data-quality":0.1549741237,"ml-security":0.1917481635}}
{"text":"A classification hierarchy is obtained by applying the method to an exemplary legal text.","meta":{"url":"http://arxiv.org/abs/2307.14058v1"},"cats":{"new-dataset":0.1129938239,"dev-research":0.2694294233,"prompt-eng":0.3649452842,"data-quality":0.2680601099,"ml-security":0.1194951276}}
{"text":"A comparison of the results with benchmark dataset categories shows limited agreement.","meta":{"url":"http://arxiv.org/abs/2307.14058v1"},"cats":{"new-dataset":0.7119974262,"dev-research":0.2478286993,"prompt-eng":0.3508515398,"data-quality":0.3403487144,"ml-security":0.0953335246}}
{"text":"This indicates the necessity for explicit consideration of legal requirements regarding perception.","meta":{"url":"http://arxiv.org/abs/2307.14058v1"},"cats":{"new-dataset":0.0087342497,"dev-research":0.353066119,"prompt-eng":0.3858406444,"data-quality":0.1801727214,"ml-security":0.0949330211}}
{"text":"With the advance in malware technology, attackers create new ways to hide their malicious code from antivirus services.","meta":{"url":"http://arxiv.org/abs/2307.14057v1"},"cats":{"new-dataset":0.0149986614,"dev-research":0.4071034192,"prompt-eng":0.3700171095,"data-quality":0.1552006415,"ml-security":0.6612577145}}
{"text":"One way to obfuscate an attack is to use common files as cover to hide the malicious scripts, so the malware will look like a legitimate file.","meta":{"url":"http://arxiv.org/abs/2307.14057v1"},"cats":{"new-dataset":0.0254860944,"dev-research":0.3490675772,"prompt-eng":0.342294982,"data-quality":0.2160147571,"ml-security":0.73775441}}
{"text":"Although cutting-edge Artificial Intelligence and content signature exist, evasive malware successfully bypasses next-generation malware detection using advanced methods like steganography.","meta":{"url":"http://arxiv.org/abs/2307.14057v1"},"cats":{"new-dataset":0.0233307275,"dev-research":0.2913905037,"prompt-eng":0.3799894968,"data-quality":0.2467057029,"ml-security":0.5930611854}}
{"text":"Some of the files commonly used to hide malware are image files (e.g., JPEG).","meta":{"url":"http://arxiv.org/abs/2307.14057v1"},"cats":{"new-dataset":0.1016207883,"dev-research":0.3079717279,"prompt-eng":0.3291921931,"data-quality":0.2095467855,"ml-security":0.5023783965}}
{"text":"In addition, some malware use steganography to hide malicious scripts or sensitive data in images.","meta":{"url":"http://arxiv.org/abs/2307.14057v1"},"cats":{"new-dataset":0.0200825149,"dev-research":0.3348696289,"prompt-eng":0.3187997865,"data-quality":0.2096379214,"ml-security":0.5373318569}}
{"text":"Steganography in images is difficult to detect even with specialized tools.","meta":{"url":"http://arxiv.org/abs/2307.14057v1"},"cats":{"new-dataset":0.0157451455,"dev-research":0.2348314069,"prompt-eng":0.3374033176,"data-quality":0.3178421666,"ml-security":0.3442986682}}
{"text":"Image-based attacks try to attack the user's device using malicious payloads or utilize image steganography to hide sensitive data inside legitimate images and leak it outside the user's device.","meta":{"url":"http://arxiv.org/abs/2307.14057v1"},"cats":{"new-dataset":0.0366033886,"dev-research":0.3251661638,"prompt-eng":0.3686829115,"data-quality":0.2256519185,"ml-security":0.6746296566}}
{"text":"Therefore in this paper, we present a novel Image Content Disarm and Reconstruction (ICDR).","meta":{"url":"http://arxiv.org/abs/2307.14057v1"},"cats":{"new-dataset":0.3652690301,"dev-research":0.1802275187,"prompt-eng":0.3677285625,"data-quality":0.1483026422,"ml-security":0.0810886683}}
{"text":"Our ICDR system removes potential malware, with a zero trust approach, while maintaining high image quality and file usability.","meta":{"url":"http://arxiv.org/abs/2307.14057v1"},"cats":{"new-dataset":0.0733204253,"dev-research":0.2967355114,"prompt-eng":0.4223367993,"data-quality":0.2278182859,"ml-security":0.3928859763}}
{"text":"By extracting the image data, removing it from the rest of the file, and manipulating the image pixels, it is possible to disable or remove the hidden malware inside the file.","meta":{"url":"http://arxiv.org/abs/2307.14057v1"},"cats":{"new-dataset":0.0870213992,"dev-research":0.2657001415,"prompt-eng":0.3382834229,"data-quality":0.2244558084,"ml-security":0.4301724258}}
{"text":"High-accuracy Dichotomous Image Segmentation (DIS) aims to pinpoint category-agnostic foreground objects from natural scenes.","meta":{"url":"http://arxiv.org/abs/2307.14052v1"},"cats":{"new-dataset":0.2299712437,"dev-research":0.2172765989,"prompt-eng":0.4190834364,"data-quality":0.2666154928,"ml-security":0.0990124773}}
{"text":"The main challenge for DIS involves identifying the highly accurate dominant area while rendering detailed object structure.","meta":{"url":"http://arxiv.org/abs/2307.14052v1"},"cats":{"new-dataset":0.1763548744,"dev-research":0.2641030824,"prompt-eng":0.438280748,"data-quality":0.1351828373,"ml-security":0.1025634131}}
{"text":"However, directly using a general encoder-decoder architecture may result in an oversupply of high-level features and neglect the shallow spatial information necessary for partitioning meticulous structures.","meta":{"url":"http://arxiv.org/abs/2307.14052v1"},"cats":{"new-dataset":0.025404817,"dev-research":0.3048887453,"prompt-eng":0.4020313974,"data-quality":0.1895890113,"ml-security":0.151266373}}
{"text":"To fill this gap, we introduce a novel Unite-Divide-Unite Network (UDUN} that restructures and bipartitely arranges complementary features to simultaneously boost the effectiveness of trunk and structure identification.","meta":{"url":"http://arxiv.org/abs/2307.14052v1"},"cats":{"new-dataset":0.1899305459,"dev-research":0.2540979535,"prompt-eng":0.3844860864,"data-quality":0.1985518336,"ml-security":0.0932126626}}
{"text":"The proposed UDUN proceeds from several strengths.","meta":{"url":"http://arxiv.org/abs/2307.14052v1"},"cats":{"new-dataset":0.0639958874,"dev-research":0.271960792,"prompt-eng":0.335992462,"data-quality":0.0791418849,"ml-security":0.0799446348}}
{"text":"First, a dual-size input feeds into the shared backbone to produce more holistic and detailed features while keeping the model lightweight.","meta":{"url":"http://arxiv.org/abs/2307.14052v1"},"cats":{"new-dataset":0.0901867448,"dev-research":0.2612650042,"prompt-eng":0.4530515894,"data-quality":0.0943045641,"ml-security":0.1031717961}}
{"text":"Second, a simple Divide-and-Conquer Module (DCM) is proposed to decouple multiscale low- and high-level features into our structure decoder and trunk decoder to obtain structure and trunk information respectively.","meta":{"url":"http://arxiv.org/abs/2307.14052v1"},"cats":{"new-dataset":0.1482414312,"dev-research":0.2802117856,"prompt-eng":0.4327503867,"data-quality":0.1622979074,"ml-security":0.083691674}}
{"text":"Moreover, we design a Trunk-Structure Aggregation module (TSA) in our union decoder that performs cascade integration for uniform high-accuracy segmentation.","meta":{"url":"http://arxiv.org/abs/2307.14052v1"},"cats":{"new-dataset":0.1791974177,"dev-research":0.2457089049,"prompt-eng":0.4127700474,"data-quality":0.2301222335,"ml-security":0.051752476}}
{"text":"As a result, UDUN performs favorably against state-of-the-art competitors in all six evaluation metrics on overall DIS-TE, i.e., achieving 0.772 weighted F-measure and 977 HCE.","meta":{"url":"http://arxiv.org/abs/2307.14052v1"},"cats":{"new-dataset":0.0688609725,"dev-research":0.2743911929,"prompt-eng":0.3935092649,"data-quality":0.147388061,"ml-security":0.1179382748}}
{"text":"Using 1024*1024 input, our model enables real-time inference at 65.3 fps with ResNet-18.","meta":{"url":"http://arxiv.org/abs/2307.14052v1"},"cats":{"new-dataset":0.2043397184,"dev-research":0.2102246137,"prompt-eng":0.4058191333,"data-quality":0.0886791284,"ml-security":0.0838281081}}
{"text":"Shape generation is the practice of producing 3D shapes as various representations for 3D content creation.","meta":{"url":"http://arxiv.org/abs/2307.14051v1"},"cats":{"new-dataset":0.0289937938,"dev-research":0.3647210749,"prompt-eng":0.3526520349,"data-quality":0.0804141849,"ml-security":0.0687006944}}
{"text":"Previous studies on 3D shape generation have focused on shape quality and structure, without or less considering the importance of semantic information.","meta":{"url":"http://arxiv.org/abs/2307.14051v1"},"cats":{"new-dataset":0.035820516,"dev-research":0.3097241821,"prompt-eng":0.3462323667,"data-quality":0.1214956225,"ml-security":0.0464599261}}
{"text":"Consequently, such generative models often fail to preserve the semantic consistency of shape structure or enable manipulation of the semantic attributes of shapes during generation.","meta":{"url":"http://arxiv.org/abs/2307.14051v1"},"cats":{"new-dataset":0.0093600578,"dev-research":0.2590402778,"prompt-eng":0.4363741404,"data-quality":0.3281723483,"ml-security":0.102768863}}
{"text":"In this paper, we proposed a novel semantic generative model named 3D Semantic Subspace Traverser that utilizes semantic attributes for category-specific 3D shape generation and editing.","meta":{"url":"http://arxiv.org/abs/2307.14051v1"},"cats":{"new-dataset":0.0834592746,"dev-research":0.3080916843,"prompt-eng":0.4084113217,"data-quality":0.1432014931,"ml-security":0.0545815892}}
{"text":"Our method utilizes implicit functions as the 3D shape representation and combines a novel latent-space GAN with a linear subspace model to discover semantic dimensions in the local latent space of 3D shapes.","meta":{"url":"http://arxiv.org/abs/2307.14051v1"},"cats":{"new-dataset":0.0665111836,"dev-research":0.2825524462,"prompt-eng":0.3400486087,"data-quality":0.1768025215,"ml-security":0.1159423131}}
{"text":"Each dimension of the subspace corresponds to a particular semantic attribute, and we can edit the attributes of generated shapes by traversing the coefficients of those dimensions.","meta":{"url":"http://arxiv.org/abs/2307.14051v1"},"cats":{"new-dataset":0.0327945669,"dev-research":0.3011803705,"prompt-eng":0.3944005378,"data-quality":0.1734274644,"ml-security":0.0829141249}}
{"text":"Experimental results demonstrate that our method can produce plausible shapes with complex structures and enable the editing of semantic attributes.","meta":{"url":"http://arxiv.org/abs/2307.14051v1"},"cats":{"new-dataset":0.0493833655,"dev-research":0.3628792194,"prompt-eng":0.448116992,"data-quality":0.2514031503,"ml-security":0.079382745}}
{"text":"The code and trained models are available at https://github.com/TrepangCat/3D_Semantic_Subspace_Traverser","meta":{"url":"http://arxiv.org/abs/2307.14051v1"},"cats":{"new-dataset":0.3306434362,"dev-research":0.2303179795,"prompt-eng":0.4046281939,"data-quality":0.155554047,"ml-security":0.077085763}}
{"text":"In this paper, we consider intelligent reflecting surface (IRS) in a non-orthogonal multiple access (NOMA)-aided Integrated Sensing and Multicast-Unicast Communication (ISMUC) system, where the multicast signal is used for sensing and communications while the unicast signal is used only for communications.","meta":{"url":"http://arxiv.org/abs/2307.14050v1"},"cats":{"new-dataset":0.033658357,"dev-research":0.2201742457,"prompt-eng":0.3629630671,"data-quality":0.0895333863,"ml-security":0.0724561344}}
{"text":"Our goal is to depict whether the IRS improves the performance of NOMA-ISMUC system or not under the imperfect/perfect successive interference cancellation (SIC) scenario.","meta":{"url":"http://arxiv.org/abs/2307.14050v1"},"cats":{"new-dataset":0.0344176712,"dev-research":0.228456836,"prompt-eng":0.4245929071,"data-quality":0.2266707627,"ml-security":0.1209019188}}
{"text":"Towards this end, we formulate a non-convex problem to maximize the unicast rate while ensuring the minimum target illumination power and multicast rate.","meta":{"url":"http://arxiv.org/abs/2307.14050v1"},"cats":{"new-dataset":0.0491428664,"dev-research":0.1692411574,"prompt-eng":0.4259616833,"data-quality":0.1385440767,"ml-security":0.1094686101}}
{"text":"To settle this problem, we employ the Dinkelbach method to transform this original problem into an equivalent one, which is then solved via alternating optimization algorithm and semidefinite relaxation (SDR) with Sequential Rank-One Constraint Relaxation (SROCR).","meta":{"url":"http://arxiv.org/abs/2307.14050v1"},"cats":{"new-dataset":0.1074994774,"dev-research":0.1815261085,"prompt-eng":0.3942022523,"data-quality":0.1442780922,"ml-security":0.0674013106}}
{"text":"Based on this, an iterative algorithm is devised to obtain a near-optimal solution.","meta":{"url":"http://arxiv.org/abs/2307.14050v1"},"cats":{"new-dataset":0.020105705,"dev-research":0.2248340936,"prompt-eng":0.4006786138,"data-quality":0.0944881826,"ml-security":0.0452492851}}
{"text":"Computer simulations verify the quick convergence of the devised iterative algorithm, and provide insightful results.","meta":{"url":"http://arxiv.org/abs/2307.14050v1"},"cats":{"new-dataset":0.0322514481,"dev-research":0.2234331232,"prompt-eng":0.3849837185,"data-quality":0.1094168876,"ml-security":0.0473894458}}
{"text":"Compared to NOMA-ISMUC without IRS, IRS-aided NOMA-ISMUC achieves a higher rate with perfect SIC but keeps the almost same rate in the case of imperfect SIC.","meta":{"url":"http://arxiv.org/abs/2307.14050v1"},"cats":{"new-dataset":0.0076381171,"dev-research":0.218188272,"prompt-eng":0.3488818965,"data-quality":0.1504883583,"ml-security":0.1007576082}}
{"text":"We propose cryptographic protocols to incorporate time provenance guarantees while meeting confidentiality and controlled sharing needs for research data.","meta":{"url":"http://arxiv.org/abs/2307.14041v1"},"cats":{"new-dataset":0.2322593424,"dev-research":0.2734431966,"prompt-eng":0.3812751751,"data-quality":0.1226077533,"ml-security":0.2835366664}}
{"text":"We demonstrate the efficacy of these mechanisms by developing and benchmarking a practical tool, GovernR, which furthermore takes into usability issues and is compatible with a popular open-sourced research data storage platform, Dataverse.","meta":{"url":"http://arxiv.org/abs/2307.14041v1"},"cats":{"new-dataset":0.3708384106,"dev-research":0.3693452819,"prompt-eng":0.4332725796,"data-quality":0.1944321714,"ml-security":0.1096741043}}
{"text":"In doing so, we identify and provide a solution addressing an important gap (though applicable to only niche use cases) in practical research data management.","meta":{"url":"http://arxiv.org/abs/2307.14041v1"},"cats":{"new-dataset":0.1251095274,"dev-research":0.3122441059,"prompt-eng":0.3851183784,"data-quality":0.163277899,"ml-security":0.0807755263}}
{"text":"Recent studies on face forgery detection have shown satisfactory performance for methods involved in training datasets, but are not ideal enough for unknown domains.","meta":{"url":"http://arxiv.org/abs/2307.14039v1"},"cats":{"new-dataset":0.1269988421,"dev-research":0.2671204841,"prompt-eng":0.3509626703,"data-quality":0.3602464581,"ml-security":0.5017427785}}
{"text":"This motivates many works to improve the generalization, but forgery-irrelevant information, such as image background and identity, still exists in different domain features and causes unexpected clustering, limiting the generalization.","meta":{"url":"http://arxiv.org/abs/2307.14039v1"},"cats":{"new-dataset":0.0103326889,"dev-research":0.3166027317,"prompt-eng":0.3668578524,"data-quality":0.3569280462,"ml-security":0.393274577}}
{"text":"In this paper, we propose a controllable guide-space (GS) method to enhance the discrimination of different forgery domains, so as to increase the forgery relevance of features and thereby improve the generalization.","meta":{"url":"http://arxiv.org/abs/2307.14039v1"},"cats":{"new-dataset":0.0498184432,"dev-research":0.3364393235,"prompt-eng":0.4815408367,"data-quality":0.4150080736,"ml-security":0.2767454371}}
{"text":"The well-designed guide-space can simultaneously achieve both the proper separation of forgery domains and the large distance between real-forgery domains in an explicit and controllable manner.","meta":{"url":"http://arxiv.org/abs/2307.14039v1"},"cats":{"new-dataset":0.0597800191,"dev-research":0.3195592797,"prompt-eng":0.4556298503,"data-quality":0.1933874334,"ml-security":0.201749309}}
{"text":"Moreover, for better discrimination, we use a decoupling module to weaken the interference of forgery-irrelevant correlations between domains.","meta":{"url":"http://arxiv.org/abs/2307.14039v1"},"cats":{"new-dataset":0.0397420052,"dev-research":0.2880323631,"prompt-eng":0.4569138324,"data-quality":0.4243626566,"ml-security":0.4058440421}}
{"text":"Furthermore, we make adjustments to the decision boundary manifold according to the clustering degree of the same domain features within the neighborhood.","meta":{"url":"http://arxiv.org/abs/2307.14039v1"},"cats":{"new-dataset":0.0683866142,"dev-research":0.2408841012,"prompt-eng":0.4055967816,"data-quality":0.2401073923,"ml-security":0.1261940984}}
{"text":"Extensive experiments in multiple in-domain and cross-domain settings confirm that our method can achieve state-of-the-art generalization.","meta":{"url":"http://arxiv.org/abs/2307.14039v1"},"cats":{"new-dataset":0.0232420741,"dev-research":0.2571775721,"prompt-eng":0.465460576,"data-quality":0.2274372967,"ml-security":0.0964927704}}
{"text":"We first define appropriate state representation and action space, and then design an adjustment mechanism based on the actions selected by the intelligent agent.","meta":{"url":"http://arxiv.org/abs/2307.14038v1"},"cats":{"new-dataset":0.0857119172,"dev-research":0.243277942,"prompt-eng":0.5257806906,"data-quality":0.10003086,"ml-security":0.1003161516}}
{"text":"The adjustment mechanism outputs the next state and reward value of the agent.","meta":{"url":"http://arxiv.org/abs/2307.14038v1"},"cats":{"new-dataset":0.0161305546,"dev-research":0.2301549036,"prompt-eng":0.4503631155,"data-quality":0.0891919148,"ml-security":0.089637227}}
{"text":"Additionally, the adjustment mechanism calculates the error between the adjusted state and the unadjusted state.","meta":{"url":"http://arxiv.org/abs/2307.14038v1"},"cats":{"new-dataset":0.0078010929,"dev-research":0.3089560701,"prompt-eng":0.4642342524,"data-quality":0.242885443,"ml-security":0.0489461424}}
{"text":"Furthermore, the intelligent agent stores the acquired experience samples containing states and reward values in a buffer and replays the experiences during each iteration to learn the dynamic characteristics of the environment.","meta":{"url":"http://arxiv.org/abs/2307.14038v1"},"cats":{"new-dataset":0.131296504,"dev-research":0.3282400385,"prompt-eng":0.4585439581,"data-quality":0.1068726514,"ml-security":0.1530793696}}
{"text":"We name the improved algorithm as the DQM algorithm.","meta":{"url":"http://arxiv.org/abs/2307.14038v1"},"cats":{"new-dataset":0.0586033789,"dev-research":0.224553811,"prompt-eng":0.401695637,"data-quality":0.1702883497,"ml-security":0.0760904085}}
{"text":"Experimental results demonstrate that the intelligent agent using our proposed algorithm effectively reduces the accumulated errors of inertial navigation in dynamic environments.","meta":{"url":"http://arxiv.org/abs/2307.14038v1"},"cats":{"new-dataset":0.0370746491,"dev-research":0.3510653099,"prompt-eng":0.4363620262,"data-quality":0.1441252422,"ml-security":0.1248630952}}
{"text":"Although our research provides a basis for achieving autonomous navigation of unmanned aerial vehicles, there is still room for significant optimization.","meta":{"url":"http://arxiv.org/abs/2307.14038v1"},"cats":{"new-dataset":0.0306592477,"dev-research":0.2423800489,"prompt-eng":0.4030562229,"data-quality":0.0681200505,"ml-security":0.1124781487}}
{"text":"Further research can include testing unmanned aerial vehicles in simulated environments, testing unmanned aerial vehicles in real-world environments, optimizing the design of reward functions, improving the algorithm workflow to enhance convergence speed and performance, and enhancing the algorithm's generalization ability.","meta":{"url":"http://arxiv.org/abs/2307.14038v1"},"cats":{"new-dataset":0.0242629581,"dev-research":0.3139951577,"prompt-eng":0.3974424903,"data-quality":0.0767459662,"ml-security":0.1695904567}}
{"text":"We present a termination proof for the Battle of Hercules and Hydra represented as a rewrite system with AC symbols.","meta":{"url":"http://arxiv.org/abs/2307.14036v1"},"cats":{"new-dataset":0.102112453,"dev-research":0.2950239472,"prompt-eng":0.4549183566,"data-quality":0.1611737446,"ml-security":0.178948827}}
{"text":"Our proof employs type introduction in connection with many-sorted semantic labeling for AC rewriting and AC-RPO.","meta":{"url":"http://arxiv.org/abs/2307.14036v1"},"cats":{"new-dataset":0.0830906188,"dev-research":0.3423172886,"prompt-eng":0.4254098812,"data-quality":0.3136979204,"ml-security":0.0634642768}}
{"text":"Creating high-quality annotated data for task-oriented dialog (ToD) is known to be notoriously difficult, and the challenges are amplified when the goal is to create equitable, culturally adapted, and large-scale ToD datasets for multiple languages.","meta":{"url":"http://arxiv.org/abs/2307.14031v1"},"cats":{"new-dataset":0.6793630838,"dev-research":0.3482455646,"prompt-eng":0.4374396933,"data-quality":0.2605032377,"ml-security":0.1000902734}}
{"text":"Therefore, the current datasets are still very scarce and suffer from limitations such as translation-based non-native dialogs with translation artefacts, small scale, or lack of cultural adaptation, among others.","meta":{"url":"http://arxiv.org/abs/2307.14031v1"},"cats":{"new-dataset":0.6928565015,"dev-research":0.2184800543,"prompt-eng":0.2935084859,"data-quality":0.2477695844,"ml-security":0.1038004677}}
{"text":"In this work, we first take stock of the current landscape of multilingual ToD datasets, offering a systematic overview of their properties and limitations.","meta":{"url":"http://arxiv.org/abs/2307.14031v1"},"cats":{"new-dataset":0.8600814449,"dev-research":0.2288700878,"prompt-eng":0.3917035624,"data-quality":0.1938468888,"ml-security":0.0640277562}}
{"text":"Aiming to reduce all the detected limitations, we then introduce Multi3WOZ, a novel multilingual, multi-domain, multi-parallel ToD dataset.","meta":{"url":"http://arxiv.org/abs/2307.14031v1"},"cats":{"new-dataset":0.7318367392,"dev-research":0.2649597861,"prompt-eng":0.397911009,"data-quality":0.1728208965,"ml-security":0.0791968519}}
{"text":"It is large-scale and offers culturally adapted dialogs in 4 languages to enable training and evaluation of multilingual and cross-lingual ToD systems.","meta":{"url":"http://arxiv.org/abs/2307.14031v1"},"cats":{"new-dataset":0.4444723701,"dev-research":0.2698483375,"prompt-eng":0.3747177174,"data-quality":0.1245883414,"ml-security":0.05249611}}
{"text":"We describe a complex bottom-up data collection process that yielded the final dataset, and offer the first sets of baseline scores across different ToD-related tasks for future reference, also highlighting its challenging nature.","meta":{"url":"http://arxiv.org/abs/2307.14031v1"},"cats":{"new-dataset":0.8270313085,"dev-research":0.2803225343,"prompt-eng":0.4458802387,"data-quality":0.1688761761,"ml-security":0.0526097609}}
{"text":"RANSAC and its variants are widely used for robust estimation, however, they commonly follow a greedy approach to finding the highest scoring model while ignoring other model hypotheses.","meta":{"url":"http://arxiv.org/abs/2307.14030v1"},"cats":{"new-dataset":0.0709777923,"dev-research":0.2285786857,"prompt-eng":0.4219668387,"data-quality":0.2138304423,"ml-security":0.2797990414}}
{"text":"In contrast, Iteratively Reweighted Least Squares (IRLS) techniques gradually approach the model by iteratively updating the weight of each correspondence based on the residuals from previous iterations.","meta":{"url":"http://arxiv.org/abs/2307.14030v1"},"cats":{"new-dataset":0.0100161875,"dev-research":0.2430145875,"prompt-eng":0.3765028747,"data-quality":0.1119038793,"ml-security":0.0823324832}}
{"text":"Inspired by these methods, we propose a new RANSAC framework that learns to explore the parameter space by considering the residuals seen so far via a novel attention layer.","meta":{"url":"http://arxiv.org/abs/2307.14030v1"},"cats":{"new-dataset":0.2292206111,"dev-research":0.2148403058,"prompt-eng":0.4394103188,"data-quality":0.2300365514,"ml-security":0.2341908179}}
{"text":"The attention mechanism operates on a batch of point-to-model residuals, and updates a per-point estimation state to take into account the consensus found through a lightweight one-step transformer.","meta":{"url":"http://arxiv.org/abs/2307.14030v1"},"cats":{"new-dataset":0.0794136952,"dev-research":0.2337240734,"prompt-eng":0.4884968186,"data-quality":0.175625686,"ml-security":0.0965239315}}
{"text":"This rich state then guides the minimal sampling between iterations as well as the model refinement.","meta":{"url":"http://arxiv.org/abs/2307.14030v1"},"cats":{"new-dataset":0.0452486043,"dev-research":0.1752756283,"prompt-eng":0.444210758,"data-quality":0.1490118858,"ml-security":0.0552115334}}
{"text":"We evaluate the proposed approach on essential and fundamental matrix estimation on a number of indoor and outdoor datasets.","meta":{"url":"http://arxiv.org/abs/2307.14030v1"},"cats":{"new-dataset":0.3802414308,"dev-research":0.2263497929,"prompt-eng":0.3440362712,"data-quality":0.1427107645,"ml-security":0.1054397003}}
{"text":"It outperforms state-of-the-art estimators by a significant margin adding only a small runtime overhead.","meta":{"url":"http://arxiv.org/abs/2307.14030v1"},"cats":{"new-dataset":0.0932179658,"dev-research":0.2366892336,"prompt-eng":0.3861626281,"data-quality":0.1829526192,"ml-security":0.1099562464}}
{"text":"Moreover, we demonstrate good generalization properties of our trained model, indicating its effectiveness across different datasets and tasks.","meta":{"url":"http://arxiv.org/abs/2307.14030v1"},"cats":{"new-dataset":0.0835117167,"dev-research":0.2019202439,"prompt-eng":0.4419163566,"data-quality":0.2515596249,"ml-security":0.2347526406}}
{"text":"The proposed attention mechanism and one-step transformer provide an adaptive behavior that enhances the performance of RANSAC, making it a more effective tool for robust estimation.","meta":{"url":"http://arxiv.org/abs/2307.14030v1"},"cats":{"new-dataset":0.0756600013,"dev-research":0.2526875786,"prompt-eng":0.4860461342,"data-quality":0.1887562339,"ml-security":0.1380196263}}
{"text":"Code is available at https://github.com/cavalli1234/CA-RANSAC.","meta":{"url":"http://arxiv.org/abs/2307.14030v1"},"cats":{"new-dataset":0.4501758265,"dev-research":0.2755646292,"prompt-eng":0.445355502,"data-quality":0.1642705563,"ml-security":0.0869216344}}
{"text":"Diagnosing rare anemia disorders using microscopic images is challenging for skilled specialists and machine-learning methods alike.","meta":{"url":"http://arxiv.org/abs/2307.14025v1"},"cats":{"new-dataset":0.1002639413,"dev-research":0.2689961825,"prompt-eng":0.363589968,"data-quality":0.1774003728,"ml-security":0.1337801577}}
{"text":"Due to thousands of disease-relevant cells in a single blood sample, this constitutes a complex multiple-instance learning (MIL) problem.","meta":{"url":"http://arxiv.org/abs/2307.14025v1"},"cats":{"new-dataset":0.0581191901,"dev-research":0.2067432405,"prompt-eng":0.3533787289,"data-quality":0.1843504132,"ml-security":0.1741934582}}
{"text":"While the spatial neighborhood of red blood cells is not meaningful per se, the topology, i.e., the geometry of blood samples as a whole, contains informative features to remedy typical MIL issues, such as vanishing gradients and overfitting when training on limited data.","meta":{"url":"http://arxiv.org/abs/2307.14025v1"},"cats":{"new-dataset":0.090369766,"dev-research":0.2654746873,"prompt-eng":0.2873178647,"data-quality":0.1637225972,"ml-security":0.171316401}}
{"text":"We thus develop a topology-based approach that extracts multi-scale topological features from bags of single red blood cell images.","meta":{"url":"http://arxiv.org/abs/2307.14025v1"},"cats":{"new-dataset":0.3002433669,"dev-research":0.241072842,"prompt-eng":0.3419778823,"data-quality":0.136266154,"ml-security":0.0798425891}}
{"text":"The topological features are used to regularize the model, enforcing the preservation of characteristic topological properties of the data.","meta":{"url":"http://arxiv.org/abs/2307.14025v1"},"cats":{"new-dataset":0.0477838294,"dev-research":0.2609043812,"prompt-eng":0.4084468566,"data-quality":0.1829563975,"ml-security":0.120522249}}
{"text":"Applied to a dataset of 71 patients suffering from rare anemia disorders with 521 microscopic images of red blood cells, our experiments show that topological regularization is an effective method that leads to more than 3% performance improvements for the automated classification of rare anemia disorders based on single-cell images.","meta":{"url":"http://arxiv.org/abs/2307.14025v1"},"cats":{"new-dataset":0.1417847079,"dev-research":0.2494239268,"prompt-eng":0.3627318312,"data-quality":0.209389441,"ml-security":0.0968225629}}
{"text":"This is the first approach that uses topological properties for regularizing the MIL process.","meta":{"url":"http://arxiv.org/abs/2307.14025v1"},"cats":{"new-dataset":0.0241938491,"dev-research":0.194910619,"prompt-eng":0.443102894,"data-quality":0.1588735056,"ml-security":0.0670979536}}
{"text":"Conversational recommendation systems (CRS) aim to interactively acquire user preferences and accordingly recommend items to users.","meta":{"url":"http://arxiv.org/abs/2307.14024v1"},"cats":{"new-dataset":0.0723854682,"dev-research":0.3026352444,"prompt-eng":0.4343028495,"data-quality":0.121281664,"ml-security":0.0969701002}}
{"text":"Accurately learning the dynamic user preferences is of crucial importance for CRS.","meta":{"url":"http://arxiv.org/abs/2307.14024v1"},"cats":{"new-dataset":0.013852213,"dev-research":0.3365150575,"prompt-eng":0.5156219696,"data-quality":0.1411095611,"ml-security":0.1893121067}}
{"text":"Previous works learn the user preferences with pairwise relations from the interactive conversation and item knowledge, while largely ignoring the fact that factors for a relationship in CRS are multiplex.","meta":{"url":"http://arxiv.org/abs/2307.14024v1"},"cats":{"new-dataset":0.1433417751,"dev-research":0.2828771374,"prompt-eng":0.4576683833,"data-quality":0.1310264993,"ml-security":0.0468387161}}
{"text":"Specifically, the user likes/dislikes the items that satisfy some attributes (Like/Dislike view).","meta":{"url":"http://arxiv.org/abs/2307.14024v1"},"cats":{"new-dataset":0.0078503585,"dev-research":0.3046914481,"prompt-eng":0.4251904756,"data-quality":0.1971014719,"ml-security":0.2141811255}}
{"text":"Moreover social influence is another important factor that affects user preference towards the item (Social view), while is largely ignored by previous works in CRS.","meta":{"url":"http://arxiv.org/abs/2307.14024v1"},"cats":{"new-dataset":0.0045223112,"dev-research":0.2865407419,"prompt-eng":0.3403466934,"data-quality":0.1430557671,"ml-security":0.1092590625}}
{"text":"The user preferences from these three views are inherently different but also correlated as a whole.","meta":{"url":"http://arxiv.org/abs/2307.14024v1"},"cats":{"new-dataset":0.0132546962,"dev-research":0.25192004,"prompt-eng":0.3683452063,"data-quality":0.13662806,"ml-security":0.0752741421}}
{"text":"The user preferences from the same views should be more similar than that from different views.","meta":{"url":"http://arxiv.org/abs/2307.14024v1"},"cats":{"new-dataset":0.0209208288,"dev-research":0.25994986,"prompt-eng":0.40320203,"data-quality":0.1343770118,"ml-security":0.0838844092}}
{"text":"The user preferences from Like View should be similar to Social View while different from Dislike View.","meta":{"url":"http://arxiv.org/abs/2307.14024v1"},"cats":{"new-dataset":0.0082764643,"dev-research":0.2629903994,"prompt-eng":0.3727938716,"data-quality":0.1476218138,"ml-security":0.1083200638}}
{"text":"To this end, we propose a novel model, namely Multi-view Hypergraph Contrastive Policy Learning (MHCPL).","meta":{"url":"http://arxiv.org/abs/2307.14024v1"},"cats":{"new-dataset":0.1109136865,"dev-research":0.1809121783,"prompt-eng":0.3382419716,"data-quality":0.1757300202,"ml-security":0.1062436406}}
{"text":"Specifically, MHCPL timely chooses useful social information according to the interactive history and builds a dynamic hypergraph with three types of multiplex relations from different views.","meta":{"url":"http://arxiv.org/abs/2307.14024v1"},"cats":{"new-dataset":0.1650177388,"dev-research":0.2693743759,"prompt-eng":0.3862846647,"data-quality":0.1012408625,"ml-security":0.0480456206}}
{"text":"The multiplex relations in each view are successively connected according to their generation order.","meta":{"url":"http://arxiv.org/abs/2307.14024v1"},"cats":{"new-dataset":0.1148188025,"dev-research":0.2319519697,"prompt-eng":0.3408984871,"data-quality":0.0955294585,"ml-security":0.031206442}}
{"text":"Existing analyses of the expressive capacity of Transformer models have required excessively deep layers for data memorization, leading to a discrepancy with the Transformers actually used in practice.","meta":{"url":"http://arxiv.org/abs/2307.14023v1"},"cats":{"new-dataset":0.050693442,"dev-research":0.2553376466,"prompt-eng":0.3871241662,"data-quality":0.1544511882,"ml-security":0.1988816177}}
{"text":"This is primarily due to the interpretation of the softmax function as an approximation of the hardmax function.","meta":{"url":"http://arxiv.org/abs/2307.14023v1"},"cats":{"new-dataset":0.0083506855,"dev-research":0.2370435249,"prompt-eng":0.2779573851,"data-quality":0.1894586825,"ml-security":0.1818579387}}
{"text":"By clarifying the connection between the softmax function and the Boltzmann operator, we prove that a single layer of self-attention with low-rank weight matrices possesses the capability to perfectly capture the context of an entire input sequence.","meta":{"url":"http://arxiv.org/abs/2307.14023v1"},"cats":{"new-dataset":0.056559695,"dev-research":0.1602102955,"prompt-eng":0.3752468764,"data-quality":0.1649232108,"ml-security":0.2314358552}}
{"text":"As a consequence, we show that single-layer Transformer has a memorization capacity for finite samples, and that Transformers consisting of one self-attention layer with two feed-forward neural networks are universal approximators for continuous functions on a compact domain.","meta":{"url":"http://arxiv.org/abs/2307.14023v1"},"cats":{"new-dataset":0.0650616414,"dev-research":0.1603358134,"prompt-eng":0.3344954244,"data-quality":0.1277872648,"ml-security":0.2192805393}}
{"text":"Brain encoding models aim to predict brain voxel-wise responses to stimuli images, replicating brain signals captured by neuroimaging techniques.","meta":{"url":"http://arxiv.org/abs/2307.14021v1"},"cats":{"new-dataset":0.0381808804,"dev-research":0.2236298832,"prompt-eng":0.4404347522,"data-quality":0.1596849873,"ml-security":0.1671072183}}
{"text":"There is a large volume of publicly available data, but training a comprehensive brain encoding model is challenging.","meta":{"url":"http://arxiv.org/abs/2307.14021v1"},"cats":{"new-dataset":0.2628097252,"dev-research":0.2003701615,"prompt-eng":0.3968672873,"data-quality":0.1629020003,"ml-security":0.1905981613}}
{"text":"The main difficulties stem from a) diversity within individual brain, with functional heterogeneous brain regions; b) diversity of brains from different subjects, due to genetic and developmental differences; c) diversity of imaging modalities and processing pipelines.","meta":{"url":"http://arxiv.org/abs/2307.14021v1"},"cats":{"new-dataset":0.0119129983,"dev-research":0.2562108106,"prompt-eng":0.3465642884,"data-quality":0.1901699653,"ml-security":0.082857468}}
{"text":"We use this diversity to our advantage by introducing the All-for-One training recipe, which divides the challenging one-big-model problem into multiple small models, with the small models aggregating the knowledge while preserving the distinction between the different functional regions.","meta":{"url":"http://arxiv.org/abs/2307.14021v1"},"cats":{"new-dataset":0.1721449514,"dev-research":0.1485190194,"prompt-eng":0.37058109,"data-quality":0.1691716436,"ml-security":0.1414799904}}
{"text":"Agnostic of the training recipe, we use biological knowledge of the brain, specifically retinotopy, to introduce inductive bias to learn a 3D brain-to-image mapping that ensures a) each neuron knows which image regions and semantic levels to gather information, and b) no neurons are left behind in the model.   ","meta":{"url":"http://arxiv.org/abs/2307.14021v1"},"cats":{"new-dataset":0.0548487754,"dev-research":0.2547523221,"prompt-eng":0.3548957412,"data-quality":0.1762810079,"ml-security":0.1367395359}}
{"text":"We pre-trained a brain encoding model using over one million data points from five public datasets spanning three imaging modalities.","meta":{"url":"http://arxiv.org/abs/2307.14021v1"},"cats":{"new-dataset":0.4824183197,"dev-research":0.1875874902,"prompt-eng":0.4407333343,"data-quality":0.1905006793,"ml-security":0.1780836008}}
{"text":"To the best of our knowledge, this is the most comprehensive brain encoding model to the date.","meta":{"url":"http://arxiv.org/abs/2307.14021v1"},"cats":{"new-dataset":0.1277703541,"dev-research":0.2189559277,"prompt-eng":0.4367487003,"data-quality":0.1477752682,"ml-security":0.1069762464}}
{"text":"We demonstrate the effectiveness of the pre-trained model as a drop-in replacement for commonly used vision backbone models.","meta":{"url":"http://arxiv.org/abs/2307.14021v1"},"cats":{"new-dataset":0.0530733621,"dev-research":0.2076547401,"prompt-eng":0.4687259851,"data-quality":0.1376003917,"ml-security":0.1294153704}}
{"text":"Furthermore, we demonstrate the application of the model to brain decoding.","meta":{"url":"http://arxiv.org/abs/2307.14021v1"},"cats":{"new-dataset":0.0282481434,"dev-research":0.1621190781,"prompt-eng":0.4357865995,"data-quality":0.146799118,"ml-security":0.1420382381}}
{"text":"Code and the model checkpoint will be made available.","meta":{"url":"http://arxiv.org/abs/2307.14021v1"},"cats":{"new-dataset":0.3035263591,"dev-research":0.301294431,"prompt-eng":0.4524553261,"data-quality":0.155765733,"ml-security":0.1354910832}}
{"text":"Strip-decomposable quadrilateral (SDQ) meshes, i.e., quad meshes that can be decomposed into two transversal strip networks, are vital in numerous fabrication processes; examples include woven structures, surfaces from sheets, custom rebar, or cable-net structures.","meta":{"url":"http://arxiv.org/abs/2307.14020v1"},"cats":{"new-dataset":0.0243021483,"dev-research":0.2530620286,"prompt-eng":0.3193627275,"data-quality":0.0937533535,"ml-security":0.0759749954}}
{"text":"However, their design is often challenging and includes tedious manual work, and there is a lack of methodologies for editing such meshes while preserving their strip decomposability.","meta":{"url":"http://arxiv.org/abs/2307.14020v1"},"cats":{"new-dataset":0.0358853124,"dev-research":0.3049762108,"prompt-eng":0.3934452259,"data-quality":0.1350904867,"ml-security":0.087627181}}
{"text":"We present an interactive methodology to generate and edit SDQ meshes aligned to user-defined directions, while also incorporating desirable properties to the strips for fabrication.","meta":{"url":"http://arxiv.org/abs/2307.14020v1"},"cats":{"new-dataset":0.0786503724,"dev-research":0.357624501,"prompt-eng":0.4390381539,"data-quality":0.0828487201,"ml-security":0.0353208991}}
{"text":"Our technique is based on the computation of two coupled transversal tangent direction fields, integrated into two overlapping networks of strips on the surface.","meta":{"url":"http://arxiv.org/abs/2307.14020v1"},"cats":{"new-dataset":0.0507080113,"dev-research":0.2091858665,"prompt-eng":0.3591353708,"data-quality":0.071423592,"ml-security":0.0473299361}}
{"text":"As a case study, we consider the fabrication scenario of robotic non-planar 3D printing of freefrom shell surfaces and apply the presented methodology to design and fabricate non-planar print paths.","meta":{"url":"http://arxiv.org/abs/2307.14020v1"},"cats":{"new-dataset":0.039160965,"dev-research":0.2453924274,"prompt-eng":0.3612838759,"data-quality":0.0329618736,"ml-security":0.0443745524}}
{"text":"The precision of unsupervised point cloud registration methods is typically limited by the lack of reliable inlier estimation and self-supervised signal, especially in partially overlapping scenarios.","meta":{"url":"http://arxiv.org/abs/2307.14019v1"},"cats":{"new-dataset":0.056846413,"dev-research":0.2006295934,"prompt-eng":0.3927434301,"data-quality":0.216812944,"ml-security":0.1426236079}}
{"text":"In this paper, we propose an effective inlier estimation method for unsupervised point cloud registration by capturing geometric structure consistency between the source point cloud and its corresponding reference point cloud copy.","meta":{"url":"http://arxiv.org/abs/2307.14019v1"},"cats":{"new-dataset":0.1284340929,"dev-research":0.2549074396,"prompt-eng":0.3801105044,"data-quality":0.2013384982,"ml-security":0.1075092641}}
{"text":"Specifically, to obtain a high quality reference point cloud copy, an One-Nearest Neighborhood (1-NN) point cloud is generated by input point cloud.","meta":{"url":"http://arxiv.org/abs/2307.14019v1"},"cats":{"new-dataset":0.2215696754,"dev-research":0.2538783543,"prompt-eng":0.3848036938,"data-quality":0.1458552975,"ml-security":0.0600915486}}
{"text":"This facilitates matching map construction and allows for integrating dual neighborhood matching scores of 1-NN point cloud and input point cloud to improve matching confidence.","meta":{"url":"http://arxiv.org/abs/2307.14019v1"},"cats":{"new-dataset":0.1648839727,"dev-research":0.2554494575,"prompt-eng":0.4285958275,"data-quality":0.158260952,"ml-security":0.0636547245}}
{"text":"Benefiting from the high quality reference copy, we argue that the neighborhood graph formed by inlier and its neighborhood should have consistency between source point cloud and its corresponding reference copy.","meta":{"url":"http://arxiv.org/abs/2307.14019v1"},"cats":{"new-dataset":0.2975626209,"dev-research":0.3173538969,"prompt-eng":0.3431042928,"data-quality":0.3592514356,"ml-security":0.1749095591}}
{"text":"Based on this observation, we construct transformation-invariant geometric structure representations and capture geometric structure consistency to score the inlier confidence for estimated correspondences between source point cloud and its reference copy.","meta":{"url":"http://arxiv.org/abs/2307.14019v1"},"cats":{"new-dataset":0.1581738353,"dev-research":0.2210250292,"prompt-eng":0.3905440734,"data-quality":0.2538095064,"ml-security":0.1167951968}}
{"text":"This strategy can simultaneously provide the reliable self-supervised signal for model optimization.","meta":{"url":"http://arxiv.org/abs/2307.14019v1"},"cats":{"new-dataset":0.0165522866,"dev-research":0.2313622057,"prompt-eng":0.5278585942,"data-quality":0.2798244197,"ml-security":0.19262714}}
{"text":"Finally, we further calculate transformation estimation by the weighted SVD algorithm with the estimated correspondences and corresponding inlier confidence.","meta":{"url":"http://arxiv.org/abs/2307.14019v1"},"cats":{"new-dataset":0.1563968746,"dev-research":0.1784990064,"prompt-eng":0.4286378287,"data-quality":0.2596122574,"ml-security":0.0967745422}}
{"text":"We train the proposed model in an unsupervised manner, and extensive experiments on synthetic and real-world datasets illustrate the effectiveness of the proposed method.","meta":{"url":"http://arxiv.org/abs/2307.14019v1"},"cats":{"new-dataset":0.2964406217,"dev-research":0.197890527,"prompt-eng":0.3999686462,"data-quality":0.2174598859,"ml-security":0.194344902}}
{"text":"Palmprint recently shows great potential in recognition applications as it is a privacy-friendly and stable biometric.","meta":{"url":"http://arxiv.org/abs/2307.14016v1"},"cats":{"new-dataset":0.060524748,"dev-research":0.2242916065,"prompt-eng":0.3688125976,"data-quality":0.138558625,"ml-security":0.2209835559}}
{"text":"However, the lack of large-scale public palmprint datasets limits further research and development of palmprint recognition.","meta":{"url":"http://arxiv.org/abs/2307.14016v1"},"cats":{"new-dataset":0.5584706582,"dev-research":0.181418143,"prompt-eng":0.3090842809,"data-quality":0.164700677,"ml-security":0.1405068438}}
{"text":"In this paper, we propose a novel realistic pseudo-palmprint generation (RPG) model to synthesize palmprints with massive identities.","meta":{"url":"http://arxiv.org/abs/2307.14016v1"},"cats":{"new-dataset":0.3363917303,"dev-research":0.2396228206,"prompt-eng":0.4174089151,"data-quality":0.116084828,"ml-security":0.1253040814}}
{"text":"We first introduce a conditional modulation generator to improve the intra-class diversity.","meta":{"url":"http://arxiv.org/abs/2307.14016v1"},"cats":{"new-dataset":0.0824784075,"dev-research":0.2065956393,"prompt-eng":0.468027461,"data-quality":0.1886191407,"ml-security":0.1436925678}}
{"text":"Then an identity-aware loss is proposed to ensure identity consistency against unpaired training.","meta":{"url":"http://arxiv.org/abs/2307.14016v1"},"cats":{"new-dataset":0.0134964899,"dev-research":0.254372169,"prompt-eng":0.4180226403,"data-quality":0.4895891375,"ml-security":0.4450782956}}
{"text":"We further improve the B\\'ezier palm creases generation strategy to guarantee identity independence.","meta":{"url":"http://arxiv.org/abs/2307.14016v1"},"cats":{"new-dataset":0.1054376866,"dev-research":0.2753647153,"prompt-eng":0.4599895774,"data-quality":0.1945966041,"ml-security":0.1592745819}}
{"text":"Extensive experimental results demonstrate that synthetic pretraining significantly boosts the recognition model performance.","meta":{"url":"http://arxiv.org/abs/2307.14016v1"},"cats":{"new-dataset":0.0572267465,"dev-research":0.29000759,"prompt-eng":0.450979695,"data-quality":0.2207474429,"ml-security":0.1634300683}}
{"text":"For example, our model improves the state-of-the-art B\\'ezierPalm by more than $5\\%$ and $14\\%$ in terms of TAR@FAR=1e-6 under the $1:1$ and $1:3$ Open-set protocol.","meta":{"url":"http://arxiv.org/abs/2307.14016v1"},"cats":{"new-dataset":0.0299160862,"dev-research":0.2025471287,"prompt-eng":0.4635910985,"data-quality":0.1197698976,"ml-security":0.1517993523}}
{"text":"When accessing only $10\\%$ of the real training data, our method still outperforms ArcFace with $100\\%$ real training data, indicating that we are closer to real-data-free palmprint recognition.","meta":{"url":"http://arxiv.org/abs/2307.14016v1"},"cats":{"new-dataset":0.1716508879,"dev-research":0.2661118137,"prompt-eng":0.3298120387,"data-quality":0.2939530719,"ml-security":0.3049685937}}
{"text":"Single hyperspectral image super-resolution (single-HSI-SR) aims to restore a high-resolution hyperspectral image from a low-resolution observation.","meta":{"url":"http://arxiv.org/abs/2307.14010v1"},"cats":{"new-dataset":0.0827984253,"dev-research":0.1468554097,"prompt-eng":0.3594080052,"data-quality":0.111700116,"ml-security":0.0384790377}}
{"text":"However, the prevailing CNN-based approaches have shown limitations in building long-range dependencies and capturing interaction information between spectral features.","meta":{"url":"http://arxiv.org/abs/2307.14010v1"},"cats":{"new-dataset":0.158090003,"dev-research":0.2586128659,"prompt-eng":0.3472437126,"data-quality":0.164311158,"ml-security":0.1332763091}}
{"text":"This results in inadequate utilization of spectral information and artifacts after upsampling.","meta":{"url":"http://arxiv.org/abs/2307.14010v1"},"cats":{"new-dataset":0.1071300991,"dev-research":0.2322833351,"prompt-eng":0.3420969554,"data-quality":0.430159232,"ml-security":0.066962961}}
{"text":"To address this issue, we propose ESSAformer, an ESSA attention-embedded Transformer network for single-HSI-SR with an iterative refining structure.","meta":{"url":"http://arxiv.org/abs/2307.14010v1"},"cats":{"new-dataset":0.1091531728,"dev-research":0.2258813557,"prompt-eng":0.4288780334,"data-quality":0.1474553203,"ml-security":0.0704372167}}
{"text":"Specifically, we first introduce a robust and spectral-friendly similarity metric, \\ie, the spectral correlation coefficient of the spectrum (SCC), to replace the original attention matrix and incorporates inductive biases into the model to facilitate training.","meta":{"url":"http://arxiv.org/abs/2307.14010v1"},"cats":{"new-dataset":0.1070242231,"dev-research":0.20153772,"prompt-eng":0.4466423021,"data-quality":0.2576702847,"ml-security":0.0937763179}}
{"text":"Built upon it, we further utilize the kernelizable attention technique with theoretical support to form a novel efficient SCC-kernel-based self-attention (ESSA) and reduce attention computation to linear complexity.","meta":{"url":"http://arxiv.org/abs/2307.14010v1"},"cats":{"new-dataset":0.0649631645,"dev-research":0.2589063797,"prompt-eng":0.4467598305,"data-quality":0.2352241762,"ml-security":0.1124405227}}
{"text":"ESSA enlarges the receptive field for features after upsampling without bringing much computation and allows the model to effectively utilize spatial-spectral information from different scales, resulting in the generation of more natural high-resolution images.","meta":{"url":"http://arxiv.org/abs/2307.14010v1"},"cats":{"new-dataset":0.1130497887,"dev-research":0.1975714748,"prompt-eng":0.3592680355,"data-quality":0.1308498662,"ml-security":0.0823000763}}
{"text":"Without the need for pretraining on large-scale datasets, our experiments demonstrate ESSA's effectiveness in both visual quality and quantitative results.","meta":{"url":"http://arxiv.org/abs/2307.14010v1"},"cats":{"new-dataset":0.3920906891,"dev-research":0.2966244567,"prompt-eng":0.3844365387,"data-quality":0.2345372616,"ml-security":0.1245973684}}
{"text":"Compositional neural scene graph studies have shown that radiance fields can be an efficient tool in an editable autonomous driving simulator.","meta":{"url":"http://arxiv.org/abs/2307.14009v1"},"cats":{"new-dataset":0.1463441366,"dev-research":0.2639623428,"prompt-eng":0.3909355265,"data-quality":0.1500337153,"ml-security":0.1094445099}}
{"text":"However, previous studies learned within a sequence of autonomous driving datasets, resulting in unsatisfactory blurring when rotating the car in the simulator.","meta":{"url":"http://arxiv.org/abs/2307.14009v1"},"cats":{"new-dataset":0.1657492368,"dev-research":0.2637030647,"prompt-eng":0.3329373927,"data-quality":0.2222932285,"ml-security":0.1794841134}}
{"text":"In this letter, we propose a pipeline for learning unconstrained images and building a dataset from processed images.","meta":{"url":"http://arxiv.org/abs/2307.14009v1"},"cats":{"new-dataset":0.6439389726,"dev-research":0.2052373035,"prompt-eng":0.3745077701,"data-quality":0.1783278265,"ml-security":0.0908051184}}
{"text":"To meet the requirements of the simulator, which demands that the vehicle maintain clarity when the perspective changes and that the contour remains sharp from the background to avoid artifacts when editing, we design a radiation field of the vehicle, a crucial part of the urban scene foreground.","meta":{"url":"http://arxiv.org/abs/2307.14009v1"},"cats":{"new-dataset":0.1751040626,"dev-research":0.2684095773,"prompt-eng":0.4511542147,"data-quality":0.1169626515,"ml-security":0.1070735322}}
{"text":"Through experiments, we demonstrate that our model achieves competitive performance compared to baselines.","meta":{"url":"http://arxiv.org/abs/2307.14009v1"},"cats":{"new-dataset":0.017363936,"dev-research":0.2125925071,"prompt-eng":0.4389734398,"data-quality":0.1081191924,"ml-security":0.1114807119}}
{"text":"Using the datasets built from in-the-wild images, our method gradually presents a controllable appearance editing function.","meta":{"url":"http://arxiv.org/abs/2307.14009v1"},"cats":{"new-dataset":0.4555759949,"dev-research":0.2459678416,"prompt-eng":0.4632978912,"data-quality":0.217184018,"ml-security":0.0930111762}}
{"text":"We will release the dataset and code on https://lty2226262.github.io/car-studio/ to facilitate further research in the field.","meta":{"url":"http://arxiv.org/abs/2307.14009v1"},"cats":{"new-dataset":0.8180065922,"dev-research":0.2307124226,"prompt-eng":0.4052394333,"data-quality":0.1928805669,"ml-security":0.07903504}}
{"text":"Recent vision transformers, large-kernel CNNs and MLPs have attained remarkable successes in broad vision tasks thanks to their effective information fusion in the global scope.","meta":{"url":"http://arxiv.org/abs/2307.14008v1"},"cats":{"new-dataset":0.1237553414,"dev-research":0.2080848858,"prompt-eng":0.3763307243,"data-quality":0.1726575369,"ml-security":0.1331177548}}
{"text":"However, their efficient deployments, especially on mobile devices, still suffer from noteworthy challenges due to the heavy computational costs of self-attention mechanisms, large kernels, or fully connected layers.","meta":{"url":"http://arxiv.org/abs/2307.14008v1"},"cats":{"new-dataset":0.085035119,"dev-research":0.3122361448,"prompt-eng":0.4305825135,"data-quality":0.1550670088,"ml-security":0.181775866}}
{"text":"In this work, we apply conventional convolution theorem to deep learning for addressing this and reveal that adaptive frequency filters can serve as efficient global token mixers.","meta":{"url":"http://arxiv.org/abs/2307.14008v1"},"cats":{"new-dataset":0.0675965599,"dev-research":0.2223587511,"prompt-eng":0.3357627174,"data-quality":0.2416974777,"ml-security":0.1604308397}}
{"text":"With this insight, we propose Adaptive Frequency Filtering (AFF) token mixer.","meta":{"url":"http://arxiv.org/abs/2307.14008v1"},"cats":{"new-dataset":0.0440600847,"dev-research":0.2758570289,"prompt-eng":0.4290278216,"data-quality":0.2450321901,"ml-security":0.0675731295}}
{"text":"This neural operator transfers a latent representation to the frequency domain via a Fourier transform and performs semantic-adaptive frequency filtering via an elementwise multiplication, which mathematically equals to a token mixing operation in the original latent space with a dynamic convolution kernel as large as the spatial resolution of this latent representation.","meta":{"url":"http://arxiv.org/abs/2307.14008v1"},"cats":{"new-dataset":0.0564111801,"dev-research":0.2102616212,"prompt-eng":0.3525544264,"data-quality":0.1828289342,"ml-security":0.0953847387}}
{"text":"We take AFF token mixers as primary neural operators to build a lightweight neural network, dubbed AFFNet.","meta":{"url":"http://arxiv.org/abs/2307.14008v1"},"cats":{"new-dataset":0.2006909578,"dev-research":0.2580435743,"prompt-eng":0.4116484756,"data-quality":0.1877165567,"ml-security":0.1821923008}}
{"text":"Extensive experiments demonstrate the effectiveness of our proposed AFF token mixer and show that AFFNet achieve superior accuracy and efficiency trade-offs compared to other lightweight network designs on broad visual tasks, including visual recognition and dense prediction tasks.","meta":{"url":"http://arxiv.org/abs/2307.14008v1"},"cats":{"new-dataset":0.0993170592,"dev-research":0.3106232779,"prompt-eng":0.3843049283,"data-quality":0.2065355324,"ml-security":0.1289536943}}
{"text":"Existing Graph Convolutional Networks to achieve human motion prediction largely adopt a one-step scheme, which output the prediction straight from history input, failing to exploit human motion patterns.","meta":{"url":"http://arxiv.org/abs/2307.14006v1"},"cats":{"new-dataset":0.1587540925,"dev-research":0.2307363614,"prompt-eng":0.3375434479,"data-quality":0.1301729535,"ml-security":0.1293658361}}
{"text":"We observe that human motions have transitional patterns and can be split into snippets representative of each transition.","meta":{"url":"http://arxiv.org/abs/2307.14006v1"},"cats":{"new-dataset":0.1167185711,"dev-research":0.2189017063,"prompt-eng":0.427826356,"data-quality":0.1005625064,"ml-security":0.0441079491}}
{"text":"Each snippet can be reconstructed from its starting and ending poses referred to as the transitional poses.","meta":{"url":"http://arxiv.org/abs/2307.14006v1"},"cats":{"new-dataset":0.3123422623,"dev-research":0.2207508517,"prompt-eng":0.4005051015,"data-quality":0.1425323055,"ml-security":0.0612887619}}
{"text":"We propose a snippet-to-motion multi-stage framework that breaks motion prediction into sub-tasks easier to accomplish.","meta":{"url":"http://arxiv.org/abs/2307.14006v1"},"cats":{"new-dataset":0.2267383647,"dev-research":0.2735066086,"prompt-eng":0.4340889107,"data-quality":0.0948908259,"ml-security":0.0647201332}}
{"text":"Each sub-task integrates three modules: transitional pose prediction, snippet reconstruction, and snippet-to-motion prediction.","meta":{"url":"http://arxiv.org/abs/2307.14006v1"},"cats":{"new-dataset":0.2600582216,"dev-research":0.2404195896,"prompt-eng":0.4178430673,"data-quality":0.1009645874,"ml-security":0.0494980659}}
{"text":"Specifically, we propose to first predict only the transitional poses.","meta":{"url":"http://arxiv.org/abs/2307.14006v1"},"cats":{"new-dataset":0.0934209933,"dev-research":0.1797743045,"prompt-eng":0.4337755645,"data-quality":0.0938268464,"ml-security":0.1020775102}}
{"text":"Then we use them to reconstruct the corresponding snippets, obtaining a close approximation to the true motion sequence.","meta":{"url":"http://arxiv.org/abs/2307.14006v1"},"cats":{"new-dataset":0.0651282442,"dev-research":0.2810239348,"prompt-eng":0.4007108129,"data-quality":0.1571783249,"ml-security":0.0680694427}}
{"text":"Finally we refine them to produce the final prediction output.","meta":{"url":"http://arxiv.org/abs/2307.14006v1"},"cats":{"new-dataset":0.1735470296,"dev-research":0.2731497651,"prompt-eng":0.482082221,"data-quality":0.2570354606,"ml-security":0.1104335888}}
{"text":"To implement the network, we propose a novel unified graph modeling, which allows for direct and effective feature propagation compared to existing approaches which rely on separate space-time modeling.","meta":{"url":"http://arxiv.org/abs/2307.14006v1"},"cats":{"new-dataset":0.0654019858,"dev-research":0.2964458898,"prompt-eng":0.3768407377,"data-quality":0.1632085741,"ml-security":0.1011318851}}
{"text":"Extensive experiments on Human 3.6M, CMU Mocap and 3DPW datasets verify the effectiveness of our method which achieves state-of-the-art performance.","meta":{"url":"http://arxiv.org/abs/2307.14006v1"},"cats":{"new-dataset":0.1580649834,"dev-research":0.2321877376,"prompt-eng":0.4194327023,"data-quality":0.1092539334,"ml-security":0.0985472085}}
{"text":"We propose an unsupervised, corpus-independent method to extract keywords from a single text.","meta":{"url":"http://arxiv.org/abs/2307.14005v1"},"cats":{"new-dataset":0.126110739,"dev-research":0.2152601982,"prompt-eng":0.3918466238,"data-quality":0.3282621667,"ml-security":0.0743627197}}
{"text":"It is based on the spatial distribution of words and the response of this distribution to a random permutation of words.","meta":{"url":"http://arxiv.org/abs/2307.14005v1"},"cats":{"new-dataset":0.0745770351,"dev-research":0.1782692336,"prompt-eng":0.388764774,"data-quality":0.2259570718,"ml-security":0.0765785271}}
{"text":"As compared to existing methods (such as e.g. YAKE)","meta":{"url":"http://arxiv.org/abs/2307.14005v1"},"cats":{"new-dataset":0.0046361683,"dev-research":0.4171926242,"prompt-eng":0.3816122209,"data-quality":0.1356033096,"ml-security":0.1097111674}}
{"text":"our method has three advantages.","meta":{"url":"http://arxiv.org/abs/2307.14005v1"},"cats":{"new-dataset":0.0059652761,"dev-research":0.3449322243,"prompt-eng":0.3258325019,"data-quality":0.0824709516,"ml-security":0.1000138698}}
{"text":"First, it is significantly more effective at extracting keywords from long texts.","meta":{"url":"http://arxiv.org/abs/2307.14005v1"},"cats":{"new-dataset":0.0153835658,"dev-research":0.2858396216,"prompt-eng":0.3210232375,"data-quality":0.2421551102,"ml-security":0.0668025766}}
{"text":"Second, it allows inference of two types of keywords: local and global.","meta":{"url":"http://arxiv.org/abs/2307.14005v1"},"cats":{"new-dataset":0.0146622254,"dev-research":0.3415237713,"prompt-eng":0.3606009164,"data-quality":0.2208515678,"ml-security":0.0718042434}}
{"text":"Third, it uncovers basic themes in texts.","meta":{"url":"http://arxiv.org/abs/2307.14005v1"},"cats":{"new-dataset":0.105177315,"dev-research":0.3331308558,"prompt-eng":0.3609691505,"data-quality":0.2151925718,"ml-security":0.0879911734}}
{"text":"Additionally, our method is language-independent and applies to short texts.","meta":{"url":"http://arxiv.org/abs/2307.14005v1"},"cats":{"new-dataset":0.0275517139,"dev-research":0.3122300023,"prompt-eng":0.3510966582,"data-quality":0.3187210027,"ml-security":0.0655500119}}
{"text":"The results are obtained via human annotators with previous knowledge of texts from our database of classical literary works (the agreement between annotators is from moderate to substantial).","meta":{"url":"http://arxiv.org/abs/2307.14005v1"},"cats":{"new-dataset":0.3786705256,"dev-research":0.2180293136,"prompt-eng":0.3895298938,"data-quality":0.4042321526,"ml-security":0.0643127406}}
{"text":"Our results are supported via human-independent arguments based on the average length of extracted content words and on the average number of nouns in extracted words.","meta":{"url":"http://arxiv.org/abs/2307.14005v1"},"cats":{"new-dataset":0.1342598468,"dev-research":0.1916994813,"prompt-eng":0.412261184,"data-quality":0.3149658571,"ml-security":0.0912094317}}
{"text":"We discuss relations of keywords with higher-order textual features and reveal a connection between keywords and chapter divisions.","meta":{"url":"http://arxiv.org/abs/2307.14005v1"},"cats":{"new-dataset":0.1186341575,"dev-research":0.2760514134,"prompt-eng":0.3892271104,"data-quality":0.3066629289,"ml-security":0.0981543296}}
{"text":"Models for affective text generation have shown a remarkable progress, but they commonly rely only on basic emotion theories or valance/arousal values as conditions.","meta":{"url":"http://arxiv.org/abs/2307.14004v1"},"cats":{"new-dataset":0.0397720027,"dev-research":0.2366510489,"prompt-eng":0.3778942904,"data-quality":0.1573431193,"ml-security":0.1062926336}}
{"text":"This is appropriate when the goal is to create explicit emotion statements (\"The kid is happy.\").","meta":{"url":"http://arxiv.org/abs/2307.14004v1"},"cats":{"new-dataset":0.0460201125,"dev-research":0.3499253202,"prompt-eng":0.4150411417,"data-quality":0.232000592,"ml-security":0.1113389672}}
{"text":"Emotions are, however, commonly communicated implicitly.","meta":{"url":"http://arxiv.org/abs/2307.14004v1"},"cats":{"new-dataset":0.0089867624,"dev-research":0.4198274276,"prompt-eng":0.3338071562,"data-quality":0.2243470538,"ml-security":0.1132634792}}
{"text":"For instance, the emotional interpretation of an event (\"Their dog died.\") does often not require an explicit emotion statement.","meta":{"url":"http://arxiv.org/abs/2307.14004v1"},"cats":{"new-dataset":0.0172425481,"dev-research":0.366330004,"prompt-eng":0.3653859356,"data-quality":0.2744673685,"ml-security":0.1159386738}}
{"text":"In psychology, appraisal theories explain the link between a cognitive evaluation of an event and the potentially developed emotion.","meta":{"url":"http://arxiv.org/abs/2307.14004v1"},"cats":{"new-dataset":0.0056136922,"dev-research":0.3814818609,"prompt-eng":0.3826523661,"data-quality":0.1158481862,"ml-security":0.1073327311}}
{"text":"They put the assessment of the situation on the spot, for instance regarding the own control or the responsibility for what happens.","meta":{"url":"http://arxiv.org/abs/2307.14004v1"},"cats":{"new-dataset":0.0456094222,"dev-research":0.3470839361,"prompt-eng":0.4062518556,"data-quality":0.1560740106,"ml-security":0.1785745506}}
{"text":"We hypothesize and subsequently show that including appraisal variables as conditions in a generation framework comes with two advantages.","meta":{"url":"http://arxiv.org/abs/2307.14004v1"},"cats":{"new-dataset":0.0397604275,"dev-research":0.3480344734,"prompt-eng":0.4429547776,"data-quality":0.1103770094,"ml-security":0.0880751998}}
{"text":"(1) The generation model is informed in greater detail about what makes a specific emotion and what properties it has.","meta":{"url":"http://arxiv.org/abs/2307.14004v1"},"cats":{"new-dataset":0.03917904,"dev-research":0.2746800162,"prompt-eng":0.3979131882,"data-quality":0.1351431551,"ml-security":0.0970917212}}
{"text":"This leads to text generation that better fulfills the condition.","meta":{"url":"http://arxiv.org/abs/2307.14004v1"},"cats":{"new-dataset":0.0451383908,"dev-research":0.3718274565,"prompt-eng":0.4540237123,"data-quality":0.3180117119,"ml-security":0.0908458268}}
{"text":"(2) The variables of appraisal allow a user to perform a more fine-grained control of the generated text, by stating properties of a situation instead of only providing the emotion category.","meta":{"url":"http://arxiv.org/abs/2307.14004v1"},"cats":{"new-dataset":0.0200248608,"dev-research":0.4136872981,"prompt-eng":0.4211393047,"data-quality":0.196277983,"ml-security":0.1306524659}}
{"text":"Our Bart and T5-based experiments with 7 emotions (Anger, Disgust, Fear, Guilt, Joy, Sadness, Shame), and 7 appraisals (Attention, Responsibility, Control, Circumstance, Pleasantness, Effort, Certainty) show that (1) adding appraisals during training improves the accurateness of the generated texts by 10 pp in F1.","meta":{"url":"http://arxiv.org/abs/2307.14004v1"},"cats":{"new-dataset":0.1037596585,"dev-research":0.3335733187,"prompt-eng":0.421885081,"data-quality":0.2828305504,"ml-security":0.1620922048}}
{"text":"Further, (2) the texts with appraisal variables are longer and contain more details.","meta":{"url":"http://arxiv.org/abs/2307.14004v1"},"cats":{"new-dataset":0.1756031417,"dev-research":0.2727961496,"prompt-eng":0.3395839307,"data-quality":0.1671406529,"ml-security":0.0479949663}}
{"text":"This exemplifies the greater control for users.","meta":{"url":"http://arxiv.org/abs/2307.14004v1"},"cats":{"new-dataset":0.0191262563,"dev-research":0.4656115044,"prompt-eng":0.4351140764,"data-quality":0.1292094084,"ml-security":0.1990076247}}
{"text":"In this paper, we apply a Threshold-Decreasing Algorithm to maximize $k$-submodular functions under a matroid constraint, which reduces the query complexity of the algorithm compared to the greedy algorithm with little loss in approximation ratio.","meta":{"url":"http://arxiv.org/abs/2307.13996v1"},"cats":{"new-dataset":0.0471167698,"dev-research":0.2126812396,"prompt-eng":0.347587243,"data-quality":0.1336670922,"ml-security":0.1661990542}}
{"text":"We give a $(\\frac{1}{2} - \\epsilon)$-approximation algorithm for monotone $k$-submodular function maximization, and a $(\\frac{1}{3} - \\epsilon)$-approximation algorithm for non-monotone case, with complexity $O(\\frac{n(k\\cdot EO + IO)}{\\epsilon} \\log \\frac{r}{\\epsilon})$, where $r$ denotes the rank of the matroid, and $IO, EO$ denote the number of oracles to evaluate whether a subset is an independent set and to compute the function value of $f$, respectively.","meta":{"url":"http://arxiv.org/abs/2307.13996v1"},"cats":{"new-dataset":0.0837517392,"dev-research":0.2194466625,"prompt-eng":0.3508242757,"data-quality":0.2114291347,"ml-security":0.1416365159}}
{"text":"Since the constraint of total size can be looked as a special matroid, called uniform matroid, then we present the fast algorithm for maximizing $k$-submodular functions subject to a total size constraint as corollaries.","meta":{"url":"http://arxiv.org/abs/2307.13996v1"},"cats":{"new-dataset":0.1544044677,"dev-research":0.2084770209,"prompt-eng":0.3358734053,"data-quality":0.1151694564,"ml-security":0.1215899765}}
{"text":"corollaries.","meta":{"url":"http://arxiv.org/abs/2307.13996v1"},"cats":{"new-dataset":0.2826877576,"dev-research":0.2952220304,"prompt-eng":0.3695110607,"data-quality":0.1362936996,"ml-security":0.0953273423}}
{"text":"Personalized federated learning (PFL) is a popular framework that allows clients to have different models to address application scenarios where clients' data are in different domains.","meta":{"url":"http://arxiv.org/abs/2307.13995v1"},"cats":{"new-dataset":0.0527976991,"dev-research":0.2380501987,"prompt-eng":0.3875973933,"data-quality":0.124213491,"ml-security":0.2151406836}}
{"text":"The typical model of a client in PFL features a global encoder trained by all clients to extract universal features from the raw data and personalized layers (e.g., a classifier) trained using the client's local data.","meta":{"url":"http://arxiv.org/abs/2307.13995v1"},"cats":{"new-dataset":0.0670797468,"dev-research":0.2563500753,"prompt-eng":0.3848974768,"data-quality":0.1524872612,"ml-security":0.1595165903}}
{"text":"Nonetheless, due to the differences between the data distributions of different clients (aka, domain gaps), the universal features produced by the global encoder largely encompass numerous components irrelevant to a certain client's local task.","meta":{"url":"http://arxiv.org/abs/2307.13995v1"},"cats":{"new-dataset":0.068165823,"dev-research":0.3074852354,"prompt-eng":0.3943862488,"data-quality":0.1976156099,"ml-security":0.1008620032}}
{"text":"Some recent PFL methods address the above problem by personalizing specific parameters within the encoder.","meta":{"url":"http://arxiv.org/abs/2307.13995v1"},"cats":{"new-dataset":0.0705991606,"dev-research":0.1960650635,"prompt-eng":0.4980314715,"data-quality":0.2826284019,"ml-security":0.1071836732}}
{"text":"However, these methods encounter substantial challenges attributed to the high dimensionality and non-linearity of neural network parameter space.","meta":{"url":"http://arxiv.org/abs/2307.13995v1"},"cats":{"new-dataset":0.0171709456,"dev-research":0.2257841274,"prompt-eng":0.3638950365,"data-quality":0.2369409701,"ml-security":0.2671341277}}
{"text":"In contrast, the feature space exhibits a lower dimensionality, providing greater intuitiveness and interpretability as compared to the parameter space.","meta":{"url":"http://arxiv.org/abs/2307.13995v1"},"cats":{"new-dataset":0.0054691334,"dev-research":0.3285546473,"prompt-eng":0.3271961993,"data-quality":0.1328727882,"ml-security":0.1333170957}}
{"text":"To this end, we propose a novel PFL framework named FedPick.","meta":{"url":"http://arxiv.org/abs/2307.13995v1"},"cats":{"new-dataset":0.352091817,"dev-research":0.2557402881,"prompt-eng":0.4264294385,"data-quality":0.1373529604,"ml-security":0.1128637824}}
{"text":"FedPick achieves PFL in the low-dimensional feature space by selecting task-relevant features adaptively for each client from the features generated by the global encoder based on its local data distribution.","meta":{"url":"http://arxiv.org/abs/2307.13995v1"},"cats":{"new-dataset":0.1460269154,"dev-research":0.3015624604,"prompt-eng":0.4270385098,"data-quality":0.1483619293,"ml-security":0.1441794742}}
{"text":"It presents a more accessible and interpretable implementation of PFL compared to those methods working in the parameter space.","meta":{"url":"http://arxiv.org/abs/2307.13995v1"},"cats":{"new-dataset":0.066165336,"dev-research":0.3427800278,"prompt-eng":0.4238315309,"data-quality":0.1335543221,"ml-security":0.0734761393}}
{"text":"Extensive experimental results show that FedPick could effectively select task-relevant features for each client and improve model performance in cross-domain FL.","meta":{"url":"http://arxiv.org/abs/2307.13995v1"},"cats":{"new-dataset":0.0617285093,"dev-research":0.330518034,"prompt-eng":0.4579194592,"data-quality":0.1315289167,"ml-security":0.141563482}}
{"text":"There is a critical need to develop and validate non-invasive animal-based indicators of affective states in livestock species, in order to integrate them into on-farm assessment protocols, potentially via the use of precision livestock farming (PLF) tools.","meta":{"url":"http://arxiv.org/abs/2307.13994v1"},"cats":{"new-dataset":0.0822168316,"dev-research":0.2784175844,"prompt-eng":0.4538752614,"data-quality":0.1679766679,"ml-security":0.1026876127}}
{"text":"One such promising approach is the use of vocal indicators.","meta":{"url":"http://arxiv.org/abs/2307.13994v1"},"cats":{"new-dataset":0.0397368036,"dev-research":0.2534146027,"prompt-eng":0.4428739307,"data-quality":0.2793448352,"ml-security":0.1150277587}}
{"text":"The acoustic structure of vocalizations and their functions were extensively studied in important livestock species, such as pigs, horses, poultry and goats, yet cattle remain understudied in this context to date.","meta":{"url":"http://arxiv.org/abs/2307.13994v1"},"cats":{"new-dataset":0.0948608249,"dev-research":0.1940495924,"prompt-eng":0.3370118176,"data-quality":0.2249880792,"ml-security":0.1036607915}}
{"text":"Cows were shown to produce two types vocalizations: low-frequency calls (LF), produced with the mouth closed, or partially closed, for close distance contacts and open mouth emitted high-frequency calls (HF), produced for long distance communication, with the latter considered to be largely associated with negative affective states.","meta":{"url":"http://arxiv.org/abs/2307.13994v1"},"cats":{"new-dataset":0.1448464393,"dev-research":0.2139493655,"prompt-eng":0.3739417276,"data-quality":0.240238377,"ml-security":0.0816376478}}
{"text":"Moreover, cattle vocalizations were shown to contain information on individuality across a wide range of contexts, both negative and positive.","meta":{"url":"http://arxiv.org/abs/2307.13994v1"},"cats":{"new-dataset":0.0732567621,"dev-research":0.2435111659,"prompt-eng":0.3752400955,"data-quality":0.3168434348,"ml-security":0.1048301159}}
{"text":"Nowadays, dairy cows are facing a series of negative challenges and stressors in a typical production cycle, making vocalizations during negative affective states of special interest for research.","meta":{"url":"http://arxiv.org/abs/2307.13994v1"},"cats":{"new-dataset":0.0604790135,"dev-research":0.3509755987,"prompt-eng":0.3759384928,"data-quality":0.2641986983,"ml-security":0.1484518354}}
{"text":"One contribution of this study is providing the largest to date pre-processed (clean from noises) dataset of lactating adult multiparous dairy cows during negative affective states induced by visual isolation challenges.","meta":{"url":"http://arxiv.org/abs/2307.13994v1"},"cats":{"new-dataset":0.1962228722,"dev-research":0.2736311774,"prompt-eng":0.3765993817,"data-quality":0.2542952086,"ml-security":0.1396723018}}
{"text":"Here we present two computational frameworks - deep learning based and explainable machine learning based, to classify high and low-frequency cattle calls, and individual cow voice recognition.","meta":{"url":"http://arxiv.org/abs/2307.13994v1"},"cats":{"new-dataset":0.379048081,"dev-research":0.1855265563,"prompt-eng":0.3548883758,"data-quality":0.2497522709,"ml-security":0.2012136302}}
{"text":"Our models in these two frameworks reached 87.2% and 89.4% accuracy for LF and HF classification, with 68.9% and 72.5% accuracy rates for the cow individual identification, respectively.","meta":{"url":"http://arxiv.org/abs/2307.13994v1"},"cats":{"new-dataset":0.2156230592,"dev-research":0.1464212304,"prompt-eng":0.4198410483,"data-quality":0.3056362559,"ml-security":0.0664034836}}
{"text":"Deep learning has revolutionized the field of artificial intelligence.","meta":{"url":"http://arxiv.org/abs/2307.13992v1"},"cats":{"new-dataset":0.028886563,"dev-research":0.2012010876,"prompt-eng":0.3088586142,"data-quality":0.0981819821,"ml-security":0.1552527448}}
{"text":"Based on the statistical correlations uncovered by deep learning-based methods, computer vision technology has contributed to tremendous growth in areas such as autonomous driving and robotics.","meta":{"url":"http://arxiv.org/abs/2307.13992v1"},"cats":{"new-dataset":0.0876946441,"dev-research":0.2972801607,"prompt-eng":0.3057770195,"data-quality":0.139437953,"ml-security":0.1732957878}}
{"text":"Despite being the basis of deep learning, such correlation is not stable and is susceptible to uncontrolled factors.","meta":{"url":"http://arxiv.org/abs/2307.13992v1"},"cats":{"new-dataset":0.0225803242,"dev-research":0.230444567,"prompt-eng":0.3330408891,"data-quality":0.2202271638,"ml-security":0.1812968581}}
{"text":"In the absence of the guidance of prior knowledge, statistical correlations can easily turn into spurious correlations and cause confounders.","meta":{"url":"http://arxiv.org/abs/2307.13992v1"},"cats":{"new-dataset":0.005831756,"dev-research":0.3777969821,"prompt-eng":0.3924492446,"data-quality":0.2778135534,"ml-security":0.4001648089}}
{"text":"As a result, researchers are beginning to refine deep learning-based methods with causal theory.","meta":{"url":"http://arxiv.org/abs/2307.13992v1"},"cats":{"new-dataset":0.0132057314,"dev-research":0.3033676481,"prompt-eng":0.3226767292,"data-quality":0.1906218639,"ml-security":0.2442944905}}
{"text":"Causal theory models the intrinsic causal structure unaffected by data bias and is effective in avoiding spurious correlations.","meta":{"url":"http://arxiv.org/abs/2307.13992v1"},"cats":{"new-dataset":0.0073854834,"dev-research":0.2488171,"prompt-eng":0.3492668386,"data-quality":0.2155373392,"ml-security":0.255403132}}
{"text":"This paper aims to comprehensively review the existing causal methods in typical vision and vision-language tasks such as semantic segmentation, object detection, and image captioning.","meta":{"url":"http://arxiv.org/abs/2307.13992v1"},"cats":{"new-dataset":0.0538034653,"dev-research":0.3073197276,"prompt-eng":0.4069548746,"data-quality":0.3663183618,"ml-security":0.1041297457}}
{"text":"The advantages of causality and the approaches for building causal paradigms will be summarized.","meta":{"url":"http://arxiv.org/abs/2307.13992v1"},"cats":{"new-dataset":0.0137357521,"dev-research":0.3941944008,"prompt-eng":0.3536187859,"data-quality":0.0925283697,"ml-security":0.1545088323}}
{"text":"Future roadmaps are also proposed, including facilitating the development of causal theory and its application in other complex scenes and systems.","meta":{"url":"http://arxiv.org/abs/2307.13992v1"},"cats":{"new-dataset":0.0827470391,"dev-research":0.3578427102,"prompt-eng":0.4037266031,"data-quality":0.1093343993,"ml-security":0.1061327073}}
{"text":"Autonomous navigation in off-road conditions requires an accurate estimation of terrain traversability.","meta":{"url":"http://arxiv.org/abs/2307.13991v1"},"cats":{"new-dataset":0.0315890317,"dev-research":0.2436417517,"prompt-eng":0.3904332171,"data-quality":0.0938298266,"ml-security":0.0771820036}}
{"text":"However, traversability estimation in unstructured environments is subject to high uncertainty due to the variability of numerous factors that influence vehicle-terrain interaction.","meta":{"url":"http://arxiv.org/abs/2307.13991v1"},"cats":{"new-dataset":0.0392042701,"dev-research":0.2537914928,"prompt-eng":0.3908714343,"data-quality":0.1151375874,"ml-security":0.1028240011}}
{"text":"Consequently, it is challenging to obtain a generalizable model that can accurately predict traversability in a variety of environments.","meta":{"url":"http://arxiv.org/abs/2307.13991v1"},"cats":{"new-dataset":0.0359429592,"dev-research":0.2221242901,"prompt-eng":0.4679735915,"data-quality":0.1059230545,"ml-security":0.2259872878}}
{"text":"This paper presents METAVerse, a meta-learning framework for learning a global model that accurately and reliably predicts terrain traversability across diverse environments.","meta":{"url":"http://arxiv.org/abs/2307.13991v1"},"cats":{"new-dataset":0.1704767146,"dev-research":0.2175210282,"prompt-eng":0.3904021125,"data-quality":0.1168140968,"ml-security":0.1529406844}}
{"text":"We train the traversability prediction network to generate a dense and continuous-valued cost map from a sparse LiDAR point cloud, leveraging vehicle-terrain interaction feedback in a self-supervised manner.","meta":{"url":"http://arxiv.org/abs/2307.13991v1"},"cats":{"new-dataset":0.1203808627,"dev-research":0.2570681097,"prompt-eng":0.4186221362,"data-quality":0.1406468718,"ml-security":0.1514577833}}
{"text":"Meta-learning is utilized to train a global model with driving data collected from multiple environments, effectively minimizing estimation uncertainty.","meta":{"url":"http://arxiv.org/abs/2307.13991v1"},"cats":{"new-dataset":0.0865344954,"dev-research":0.2366581884,"prompt-eng":0.3949687549,"data-quality":0.1798118526,"ml-security":0.1809251249}}
{"text":"During deployment, online adaptation is performed to rapidly adapt the network to the local environment by exploiting recent interaction experiences.","meta":{"url":"http://arxiv.org/abs/2307.13991v1"},"cats":{"new-dataset":0.0404458149,"dev-research":0.386187745,"prompt-eng":0.4386999008,"data-quality":0.1533474339,"ml-security":0.170381065}}
{"text":"To conduct a comprehensive evaluation, we collect driving data from various terrains and demonstrate that our method can obtain a global model that minimizes uncertainty.","meta":{"url":"http://arxiv.org/abs/2307.13991v1"},"cats":{"new-dataset":0.2424627178,"dev-research":0.2516726741,"prompt-eng":0.4207039008,"data-quality":0.1160259325,"ml-security":0.0990491653}}
{"text":"Moreover, by integrating our model with a model predictive controller, we demonstrate that the reduced uncertainty results in safe and stable navigation in unstructured and unknown terrains.","meta":{"url":"http://arxiv.org/abs/2307.13991v1"},"cats":{"new-dataset":0.0964916829,"dev-research":0.196626991,"prompt-eng":0.3930474243,"data-quality":0.1040009335,"ml-security":0.2050943781}}
{"text":"Large language models underestimate the impact of negations on how much they change the meaning of a sentence.","meta":{"url":"http://arxiv.org/abs/2307.13989v1"},"cats":{"new-dataset":0.0197556232,"dev-research":0.2631126942,"prompt-eng":0.3509785426,"data-quality":0.3043858668,"ml-security":0.148345497}}
{"text":"Therefore, learned evaluation metrics based on these models are insensitive to negations.","meta":{"url":"http://arxiv.org/abs/2307.13989v1"},"cats":{"new-dataset":0.0221503172,"dev-research":0.3077955437,"prompt-eng":0.3884509923,"data-quality":0.3768113683,"ml-security":0.1752877932}}
{"text":"In this paper, we propose NegBLEURT, a negation-aware version of the BLEURT evaluation metric.","meta":{"url":"http://arxiv.org/abs/2307.13989v1"},"cats":{"new-dataset":0.0767606945,"dev-research":0.3591759529,"prompt-eng":0.4409891485,"data-quality":0.3971256566,"ml-security":0.1286784985}}
{"text":"For that, we designed a rule-based sentence negation tool and used it to create the CANNOT negation evaluation dataset.","meta":{"url":"http://arxiv.org/abs/2307.13989v1"},"cats":{"new-dataset":0.2467719734,"dev-research":0.3319641939,"prompt-eng":0.3943398254,"data-quality":0.3797837938,"ml-security":0.1081986196}}
{"text":"Based on this dataset, we fine-tuned a sentence transformer and an evaluation metric to improve their negation sensitivity.","meta":{"url":"http://arxiv.org/abs/2307.13989v1"},"cats":{"new-dataset":0.2417070972,"dev-research":0.2718824281,"prompt-eng":0.439537076,"data-quality":0.4133311423,"ml-security":0.1074390779}}
{"text":"Evaluating these models on existing benchmarks shows that our fine-tuned models outperform existing metrics on the negated sentences by far while preserving their base models' performances on other perturbations.","meta":{"url":"http://arxiv.org/abs/2307.13989v1"},"cats":{"new-dataset":0.061923724,"dev-research":0.2331550239,"prompt-eng":0.4034301252,"data-quality":0.3758694736,"ml-security":0.1272016119}}
{"text":"In this work, we consider the problem of distributed computing of functions of structured sources, focusing on the classical setting of two correlated sources and one user that seeks the outcome of the function while benefiting from low-rate side information provided by a helper node.","meta":{"url":"http://arxiv.org/abs/2307.13987v1"},"cats":{"new-dataset":0.0994756165,"dev-research":0.2675903501,"prompt-eng":0.4041749476,"data-quality":0.1802551765,"ml-security":0.1224246211}}
{"text":"Focusing on the case where the sources are jointly distributed according to a very general mixture model, we here provide an achievable coding scheme that manages to substantially reduce the communication cost of distributed computing by exploiting the nature of the joint distribution of the sources, the side information, as well as the symmetry enjoyed by the desired functions.","meta":{"url":"http://arxiv.org/abs/2307.13987v1"},"cats":{"new-dataset":0.1190051901,"dev-research":0.246540755,"prompt-eng":0.4041536207,"data-quality":0.1588120687,"ml-security":0.1409796062}}
{"text":"Our scheme -- which can readily apply in a variety of real-life scenarios including learning, combinatorics, and graph neural network applications -- is here shown to provide substantial reductions in the communication costs, while simultaneously providing computational savings by reducing the exponential complexity of joint decoding techniques to a complexity that is merely linear.","meta":{"url":"http://arxiv.org/abs/2307.13987v1"},"cats":{"new-dataset":0.0551869637,"dev-research":0.2444762506,"prompt-eng":0.326030135,"data-quality":0.1438553553,"ml-security":0.2452785723}}
{"text":"Deep neural networks (DNNs) are well known to be vulnerable to adversarial examples (AEs).","meta":{"url":"http://arxiv.org/abs/2307.13985v1"},"cats":{"new-dataset":0.0618958466,"dev-research":0.2898488798,"prompt-eng":0.3453930262,"data-quality":0.3392626132,"ml-security":0.8720229804}}
{"text":"In addition, AEs have adversarial transferability, which means AEs generated for a source model can fool another black-box model (target model) with a non-trivial probability.","meta":{"url":"http://arxiv.org/abs/2307.13985v1"},"cats":{"new-dataset":0.0032744475,"dev-research":0.2398531828,"prompt-eng":0.2884534926,"data-quality":0.1501604616,"ml-security":0.6552307952}}
{"text":"In previous studies, it was confirmed that the vision transformer (ViT) is more robust against the property of adversarial transferability than convolutional neural network (CNN) models such as ConvMixer, and moreover encrypted ViT is more robust than ViT without any encryption.","meta":{"url":"http://arxiv.org/abs/2307.13985v1"},"cats":{"new-dataset":0.0201615331,"dev-research":0.2135795253,"prompt-eng":0.3128756089,"data-quality":0.1803030463,"ml-security":0.4917114928}}
{"text":"In this article, we propose a random ensemble of encrypted ViT models to achieve much more robust models.","meta":{"url":"http://arxiv.org/abs/2307.13985v1"},"cats":{"new-dataset":0.180754996,"dev-research":0.1487520776,"prompt-eng":0.4374047529,"data-quality":0.1719245026,"ml-security":0.411044049}}
{"text":"In experiments, the proposed scheme is verified to be more robust against not only black-box attacks but also white-box ones than convention methods.","meta":{"url":"http://arxiv.org/abs/2307.13985v1"},"cats":{"new-dataset":0.0128366841,"dev-research":0.2980645632,"prompt-eng":0.414583267,"data-quality":0.2319214652,"ml-security":0.637115289}}
{"text":"Blind video quality assessment (BVQA) plays an indispensable role in monitoring and improving the end-users' viewing experience in various real-world video-enabled media applications.","meta":{"url":"http://arxiv.org/abs/2307.13981v1"},"cats":{"new-dataset":0.0762940667,"dev-research":0.3009543011,"prompt-eng":0.3375923493,"data-quality":0.2180665058,"ml-security":0.1008362099}}
{"text":"As an experimental field, the improvements of BVQA models have been measured primarily on a few human-rated VQA datasets.","meta":{"url":"http://arxiv.org/abs/2307.13981v1"},"cats":{"new-dataset":0.1242816439,"dev-research":0.2077241244,"prompt-eng":0.3886833587,"data-quality":0.1233440715,"ml-security":0.121868182}}
{"text":"Thus, it is crucial to gain a better understanding of existing VQA datasets in order to properly evaluate the current progress in BVQA.","meta":{"url":"http://arxiv.org/abs/2307.13981v1"},"cats":{"new-dataset":0.2240865451,"dev-research":0.3107048169,"prompt-eng":0.3491223938,"data-quality":0.2305811527,"ml-security":0.1005124344}}
{"text":"Towards this goal, we conduct a first-of-its-kind computational analysis of VQA datasets via designing minimalistic BVQA models.","meta":{"url":"http://arxiv.org/abs/2307.13981v1"},"cats":{"new-dataset":0.3001927793,"dev-research":0.1909437213,"prompt-eng":0.3849531747,"data-quality":0.1767302742,"ml-security":0.1195891991}}
{"text":"By minimalistic, we restrict our family of BVQA models to build only upon basic blocks: a video preprocessor (for aggressive spatiotemporal downsampling), a spatial quality analyzer, an optional temporal quality analyzer, and a quality regressor, all with the simplest possible instantiations.","meta":{"url":"http://arxiv.org/abs/2307.13981v1"},"cats":{"new-dataset":0.205843557,"dev-research":0.2259067279,"prompt-eng":0.3879452673,"data-quality":0.1729796482,"ml-security":0.0984250393}}
{"text":"By comparing the quality prediction performance of different model variants on eight VQA datasets with realistic distortions, we find that nearly all datasets suffer from the easy dataset problem of varying severity, some of which even admit blind image quality assessment (BIQA) solutions.","meta":{"url":"http://arxiv.org/abs/2307.13981v1"},"cats":{"new-dataset":0.3115823797,"dev-research":0.2583947027,"prompt-eng":0.3494205616,"data-quality":0.3856021479,"ml-security":0.1894463587}}
{"text":"We additionally justify our claims by contrasting our model generalizability on these VQA datasets, and by ablating a dizzying set of BVQA design choices related to the basic building blocks.","meta":{"url":"http://arxiv.org/abs/2307.13981v1"},"cats":{"new-dataset":0.2271332758,"dev-research":0.193171869,"prompt-eng":0.3978857506,"data-quality":0.1681161099,"ml-security":0.1478921883}}
{"text":"Our results cast doubt on the current progress in BVQA, and meanwhile shed light on good practices of constructing next-generation VQA datasets and models.","meta":{"url":"http://arxiv.org/abs/2307.13981v1"},"cats":{"new-dataset":0.5522063224,"dev-research":0.2090922063,"prompt-eng":0.3826572776,"data-quality":0.1917909774,"ml-security":0.1250874531}}
{"text":"Generative Adversarial Networks (GAN) have emerged as a formidable AI tool to generate realistic outputs based on training datasets.","meta":{"url":"http://arxiv.org/abs/2307.13978v1"},"cats":{"new-dataset":0.3095826695,"dev-research":0.236076227,"prompt-eng":0.3594420906,"data-quality":0.2065284339,"ml-security":0.3098413787}}
{"text":"However, the challenge of exerting control over the generation process of GANs remains a significant hurdle.","meta":{"url":"http://arxiv.org/abs/2307.13978v1"},"cats":{"new-dataset":0.0166960523,"dev-research":0.3021276735,"prompt-eng":0.4011738308,"data-quality":0.1590637998,"ml-security":0.1986175619}}
{"text":"In this paper, we propose a novel methodology to address this issue by integrating a reinforcement learning (RL) agent with a latent-space GAN (l-GAN), thereby facilitating the generation of desired outputs.","meta":{"url":"http://arxiv.org/abs/2307.13978v1"},"cats":{"new-dataset":0.0592001288,"dev-research":0.221642231,"prompt-eng":0.3930107412,"data-quality":0.1408647254,"ml-security":0.1580389139}}
{"text":"More specifically, we have developed an actor-critic RL agent with a meticulously designed reward policy, enabling it to acquire proficiency in navigating the latent space of the l-GAN and generating outputs based on specified tasks.","meta":{"url":"http://arxiv.org/abs/2307.13978v1"},"cats":{"new-dataset":0.0730438953,"dev-research":0.2343592132,"prompt-eng":0.4117004791,"data-quality":0.1254482309,"ml-security":0.1236538041}}
{"text":"To substantiate the efficacy of our approach, we have conducted a series of experiments employing the MNIST dataset, including arithmetic addition as an illustrative task.","meta":{"url":"http://arxiv.org/abs/2307.13978v1"},"cats":{"new-dataset":0.1410361078,"dev-research":0.2435419246,"prompt-eng":0.3983852593,"data-quality":0.2062365026,"ml-security":0.1356709433}}
{"text":"The outcomes of these experiments serve to validate our methodology.","meta":{"url":"http://arxiv.org/abs/2307.13978v1"},"cats":{"new-dataset":0.0251545633,"dev-research":0.2544288093,"prompt-eng":0.4388554902,"data-quality":0.2168349843,"ml-security":0.1092073961}}
{"text":"Our pioneering integration of an RL agent with a GAN model represents a novel advancement, holding great potential for enhancing generative networks in the future.","meta":{"url":"http://arxiv.org/abs/2307.13978v1"},"cats":{"new-dataset":0.0647234997,"dev-research":0.2215568962,"prompt-eng":0.3885249469,"data-quality":0.1094017448,"ml-security":0.1174563268}}
{"text":"Verifying the correct behavior of robots in contact tasks is challenging due to model uncertainties associated with contacts.","meta":{"url":"http://arxiv.org/abs/2307.13977v1"},"cats":{"new-dataset":0.0468859879,"dev-research":0.2687239403,"prompt-eng":0.5238058038,"data-quality":0.1715727125,"ml-security":0.1285114656}}
{"text":"Standard methods for testing often fall short since all (uncountable many) solutions cannot be obtained.","meta":{"url":"http://arxiv.org/abs/2307.13977v1"},"cats":{"new-dataset":0.0291663706,"dev-research":0.2610048623,"prompt-eng":0.3763469667,"data-quality":0.20614429,"ml-security":0.1597324964}}
{"text":"Instead, we propose to formally and efficiently verify robot behaviors in contact tasks using reachability analysis, which enables checking all the reachable states against user-provided specifications.","meta":{"url":"http://arxiv.org/abs/2307.13977v1"},"cats":{"new-dataset":0.1075956245,"dev-research":0.3470436966,"prompt-eng":0.5217039743,"data-quality":0.1072738772,"ml-security":0.1262896137}}
{"text":"To this end, we extend the state of the art in reachability analysis for hybrid (mixed discrete and continuous) dynamics subject to discrete-time input trajectories.","meta":{"url":"http://arxiv.org/abs/2307.13977v1"},"cats":{"new-dataset":0.0369367143,"dev-research":0.2054687058,"prompt-eng":0.3765367867,"data-quality":0.0884218223,"ml-security":0.1003347773}}
{"text":"In particular, we present a novel and scalable guard intersection approach to reliably compute the complex behavior caused by contacts.","meta":{"url":"http://arxiv.org/abs/2307.13977v1"},"cats":{"new-dataset":0.0697411341,"dev-research":0.29017854,"prompt-eng":0.4524312284,"data-quality":0.0997505599,"ml-security":0.2882612956}}
{"text":"We model robots subject to contacts as hybrid automata in which crucial time delays are included.","meta":{"url":"http://arxiv.org/abs/2307.13977v1"},"cats":{"new-dataset":0.072426748,"dev-research":0.2157959882,"prompt-eng":0.4804434387,"data-quality":0.0587773346,"ml-security":0.0893059332}}
{"text":"The usefulness of our approach is demonstrated by verifying safe human-robot interaction in the presence of constrained collisions, which was out of reach for existing methods.","meta":{"url":"http://arxiv.org/abs/2307.13977v1"},"cats":{"new-dataset":0.1059269693,"dev-research":0.306731516,"prompt-eng":0.4753489655,"data-quality":0.1363130756,"ml-security":0.2277162258}}
{"text":"Visual object tracking is a fundamental video task in computer vision.","meta":{"url":"http://arxiv.org/abs/2307.13974v1"},"cats":{"new-dataset":0.021266259,"dev-research":0.3521994345,"prompt-eng":0.3917692234,"data-quality":0.2081483126,"ml-security":0.0678732933}}
{"text":"Recently, the notably increasing power of perception algorithms allows the unification of single/multiobject and box/mask-based tracking.","meta":{"url":"http://arxiv.org/abs/2307.13974v1"},"cats":{"new-dataset":0.0353921313,"dev-research":0.247872491,"prompt-eng":0.4087563081,"data-quality":0.1367001321,"ml-security":0.1462261719}}
{"text":"Among them, the Segment Anything Model (SAM) attracts much attention.","meta":{"url":"http://arxiv.org/abs/2307.13974v1"},"cats":{"new-dataset":0.0417347226,"dev-research":0.1840576543,"prompt-eng":0.4037781782,"data-quality":0.1061970085,"ml-security":0.0885567772}}
{"text":"In this report, we propose HQTrack, a framework for High Quality Tracking anything in videos.","meta":{"url":"http://arxiv.org/abs/2307.13974v1"},"cats":{"new-dataset":0.3874906571,"dev-research":0.2709378291,"prompt-eng":0.3941678393,"data-quality":0.2687042412,"ml-security":0.0709128703}}
{"text":"HQTrack mainly consists of a video multi-object segmenter (VMOS) and a mask refiner (MR).","meta":{"url":"http://arxiv.org/abs/2307.13974v1"},"cats":{"new-dataset":0.2872995311,"dev-research":0.2176760622,"prompt-eng":0.3726731683,"data-quality":0.1654640736,"ml-security":0.0635260473}}
{"text":"Given the object to be tracked in the initial frame of a video, VMOS propagates the object masks to the current frame.","meta":{"url":"http://arxiv.org/abs/2307.13974v1"},"cats":{"new-dataset":0.0308231822,"dev-research":0.221815942,"prompt-eng":0.3966865175,"data-quality":0.1570722182,"ml-security":0.1239644495}}
{"text":"The mask results at this stage are not accurate enough since VMOS is trained on several closeset video object segmentation (VOS) datasets, which has limited ability to generalize to complex and corner scenes.","meta":{"url":"http://arxiv.org/abs/2307.13974v1"},"cats":{"new-dataset":0.0997346209,"dev-research":0.2140573502,"prompt-eng":0.3490721778,"data-quality":0.3107892231,"ml-security":0.2038782051}}
{"text":"To further improve the quality of tracking masks, a pretrained MR model is employed to refine the tracking results.","meta":{"url":"http://arxiv.org/abs/2307.13974v1"},"cats":{"new-dataset":0.0311112646,"dev-research":0.2559453154,"prompt-eng":0.4481126801,"data-quality":0.1288705081,"ml-security":0.097625198}}
{"text":"As a compelling testament to the effectiveness of our paradigm, without employing any tricks such as test-time data augmentations and model ensemble, HQTrack ranks the 2nd place in the Visual Object Tracking and Segmentation (VOTS2023) challenge.","meta":{"url":"http://arxiv.org/abs/2307.13974v1"},"cats":{"new-dataset":0.3913999954,"dev-research":0.24873155,"prompt-eng":0.4059474921,"data-quality":0.2827260617,"ml-security":0.1106333028}}
{"text":"Code and models are available at https://github.com/jiawen-zhu/HQTrack.","meta":{"url":"http://arxiv.org/abs/2307.13974v1"},"cats":{"new-dataset":0.552072033,"dev-research":0.2528637224,"prompt-eng":0.4365007678,"data-quality":0.122230639,"ml-security":0.0604373035}}
{"text":"In this paper, we measure the linear separability of hidden layer outputs to study the characteristics of deep neural networks.","meta":{"url":"http://arxiv.org/abs/2307.13962v1"},"cats":{"new-dataset":0.0496096511,"dev-research":0.2218035213,"prompt-eng":0.329861111,"data-quality":0.2712422677,"ml-security":0.4252273749}}
{"text":"In particular, we first propose Minkowski difference based linear separability measures (MD-LSMs) to evaluate the linear separability degree of two points sets.","meta":{"url":"http://arxiv.org/abs/2307.13962v1"},"cats":{"new-dataset":0.0902742022,"dev-research":0.1953436104,"prompt-eng":0.349018433,"data-quality":0.1690759192,"ml-security":0.0738563067}}
{"text":"Then, we demonstrate that there is a synchronicity between the linear separability degree of hidden layer outputs and the network training performance, i.e., if the updated weights can enhance the linear separability degree of hidden layer outputs, the updated network will achieve a better training performance, and vice versa.","meta":{"url":"http://arxiv.org/abs/2307.13962v1"},"cats":{"new-dataset":0.0313794055,"dev-research":0.2261437308,"prompt-eng":0.3629599685,"data-quality":0.227552203,"ml-security":0.2872303845}}
{"text":"Moreover, we study the effect of activation function and network size (including width and depth) on the linear separability of hidden layers.","meta":{"url":"http://arxiv.org/abs/2307.13962v1"},"cats":{"new-dataset":0.0129219649,"dev-research":0.1925280607,"prompt-eng":0.2813497011,"data-quality":0.1322507776,"ml-security":0.3104671686}}
{"text":"Finally, we conduct the numerical experiments to validate our findings on some popular deep networks including multilayer perceptron (MLP), convolutional neural network (CNN), deep belief network (DBN), ResNet, VGGNet, AlexNet, vision transformer (ViT) and GoogLeNet.","meta":{"url":"http://arxiv.org/abs/2307.13962v1"},"cats":{"new-dataset":0.1014119341,"dev-research":0.1802679691,"prompt-eng":0.3743988769,"data-quality":0.218250367,"ml-security":0.2416363898}}
{"text":"Recently, vision transformer based multimodal learning methods have been proposed to improve the robustness of face anti-spoofing (FAS) systems.","meta":{"url":"http://arxiv.org/abs/2307.13958v1"},"cats":{"new-dataset":0.0498736483,"dev-research":0.2707983239,"prompt-eng":0.3668354785,"data-quality":0.2424192294,"ml-security":0.4351560789}}
{"text":"However, multimodal face data collected from the real world is often imperfect due to missing modalities from various imaging sensors.","meta":{"url":"http://arxiv.org/abs/2307.13958v1"},"cats":{"new-dataset":0.0984031568,"dev-research":0.2376316087,"prompt-eng":0.3529084518,"data-quality":0.2393238808,"ml-security":0.1276852031}}
{"text":"Recently, flexible-modal FAS~\\cite{yu2023flexible} has attracted more attention, which aims to develop a unified multimodal FAS model using complete multimodal face data but is insensitive to test-time missing modalities.","meta":{"url":"http://arxiv.org/abs/2307.13958v1"},"cats":{"new-dataset":0.2293422551,"dev-research":0.1898112447,"prompt-eng":0.4420856238,"data-quality":0.1797400043,"ml-security":0.0824944548}}
{"text":"In this paper, we tackle one main challenge in flexible-modal FAS, i.e., when missing modality occurs either during training or testing in real-world situations.","meta":{"url":"http://arxiv.org/abs/2307.13958v1"},"cats":{"new-dataset":0.0499359316,"dev-research":0.222384908,"prompt-eng":0.4817743402,"data-quality":0.3169858092,"ml-security":0.1567160266}}
{"text":"Inspired by the recent success of the prompt learning in language models, we propose \\textbf{V}isual \\textbf{P}rompt flexible-modal \\textbf{FAS} (VP-FAS), which learns the modal-relevant prompts to adapt the frozen pre-trained foundation model to downstream flexible-modal FAS task.","meta":{"url":"http://arxiv.org/abs/2307.13958v1"},"cats":{"new-dataset":0.0793596856,"dev-research":0.2025928845,"prompt-eng":0.5399327684,"data-quality":0.2718911181,"ml-security":0.1514804886}}
{"text":"Specifically, both vanilla visual prompts and residual contextual prompts are plugged into multimodal transformers to handle general missing-modality cases, while only requiring less than 4\\% learnable parameters compared to training the entire model.","meta":{"url":"http://arxiv.org/abs/2307.13958v1"},"cats":{"new-dataset":0.0317030039,"dev-research":0.274291771,"prompt-eng":0.5050391989,"data-quality":0.2229329865,"ml-security":0.1619244045}}
{"text":"Furthermore, missing-modality regularization is proposed to force models to learn consistent multimodal feature embeddings when missing partial modalities.","meta":{"url":"http://arxiv.org/abs/2307.13958v1"},"cats":{"new-dataset":0.0707477221,"dev-research":0.1983171924,"prompt-eng":0.3996280754,"data-quality":0.3898718989,"ml-security":0.1375254105}}
{"text":"Extensive experiments conducted on two multimodal FAS benchmark datasets demonstrate the effectiveness of our VP-FAS framework that improves the performance under various missing-modality cases while alleviating the requirement of heavy model re-training.","meta":{"url":"http://arxiv.org/abs/2307.13958v1"},"cats":{"new-dataset":0.2144932942,"dev-research":0.205210332,"prompt-eng":0.4241032241,"data-quality":0.2423719699,"ml-security":0.1020899484}}
{"text":"Multi-agent embodied tasks have recently been studied in complex indoor visual environments.","meta":{"url":"http://arxiv.org/abs/2307.13957v1"},"cats":{"new-dataset":0.0516733182,"dev-research":0.2792819467,"prompt-eng":0.3991494444,"data-quality":0.064717847,"ml-security":0.0739125321}}
{"text":"Collaboration among multiple agents can improve work efficiency and has significant practical value.","meta":{"url":"http://arxiv.org/abs/2307.13957v1"},"cats":{"new-dataset":0.0178956465,"dev-research":0.4010288,"prompt-eng":0.356196325,"data-quality":0.0578521506,"ml-security":0.0518354878}}
{"text":"However, most of the existing research focuses on homogeneous multi-agent tasks.","meta":{"url":"http://arxiv.org/abs/2307.13957v1"},"cats":{"new-dataset":0.0294371215,"dev-research":0.1875173451,"prompt-eng":0.3708280936,"data-quality":0.0588671943,"ml-security":0.0545289922}}
{"text":"Compared with homogeneous agents, heterogeneous agents can leverage their different capabilities to allocate corresponding sub-tasks and cooperate to complete complex tasks.","meta":{"url":"http://arxiv.org/abs/2307.13957v1"},"cats":{"new-dataset":0.0150648199,"dev-research":0.2622165969,"prompt-eng":0.3722982606,"data-quality":0.0500422191,"ml-security":0.0687874692}}
{"text":"Heterogeneous multi-agent tasks are common in real-world scenarios, and the collaboration strategy among heterogeneous agents is a challenging and important problem to be solved.","meta":{"url":"http://arxiv.org/abs/2307.13957v1"},"cats":{"new-dataset":0.0422010296,"dev-research":0.2387028383,"prompt-eng":0.4013850045,"data-quality":0.0613901749,"ml-security":0.0747652075}}
{"text":"To study collaboration among heterogeneous agents, we propose the heterogeneous multi-agent tidying-up task, in which multiple heterogeneous agents with different capabilities collaborate with each other to detect misplaced objects and place them in reasonable locations.","meta":{"url":"http://arxiv.org/abs/2307.13957v1"},"cats":{"new-dataset":0.1217002959,"dev-research":0.2921840309,"prompt-eng":0.4179121328,"data-quality":0.1618459466,"ml-security":0.0822395674}}
{"text":"This is a demanding task since it requires agents to make the best use of their different capabilities to conduct reasonable task planning and complete the whole task.","meta":{"url":"http://arxiv.org/abs/2307.13957v1"},"cats":{"new-dataset":0.020217276,"dev-research":0.3074966419,"prompt-eng":0.4673184059,"data-quality":0.0620644466,"ml-security":0.0657553544}}
{"text":"To solve this task, we build a heterogeneous multi-agent tidying-up benchmark dataset in a large number of houses with multiple rooms based on ProcTHOR-10K.","meta":{"url":"http://arxiv.org/abs/2307.13957v1"},"cats":{"new-dataset":0.8063922577,"dev-research":0.2493019105,"prompt-eng":0.4145181075,"data-quality":0.1186356786,"ml-security":0.0787059797}}
{"text":"We propose the hierarchical decision model based on misplaced object detection, reasonable receptacle prediction, as well as the handshake-based group communication mechanism.","meta":{"url":"http://arxiv.org/abs/2307.13957v1"},"cats":{"new-dataset":0.0509392443,"dev-research":0.2071880955,"prompt-eng":0.4484553986,"data-quality":0.1920266649,"ml-security":0.1874527098}}
{"text":"Extensive experiments are conducted to demonstrate the effectiveness of the proposed model.","meta":{"url":"http://arxiv.org/abs/2307.13957v1"},"cats":{"new-dataset":0.0101020531,"dev-research":0.198294957,"prompt-eng":0.4156633449,"data-quality":0.1052035232,"ml-security":0.1120752174}}
{"text":"The project's website and videos of experiments can be found at https://hetercol.github.io/.","meta":{"url":"http://arxiv.org/abs/2307.13957v1"},"cats":{"new-dataset":0.1679783661,"dev-research":0.1820692277,"prompt-eng":0.4529530336,"data-quality":0.1284709791,"ml-security":0.0593393946}}
{"text":"This work unveils the enigmatic link between phonemes and facial features.","meta":{"url":"http://arxiv.org/abs/2307.13953v1"},"cats":{"new-dataset":0.1784780448,"dev-research":0.2504066349,"prompt-eng":0.3574600103,"data-quality":0.2637090035,"ml-security":0.1783498363}}
{"text":"Traditional studies on voice-face correlations typically involve using a long period of voice input, including generating face images from voices and reconstructing 3D face meshes from voices.","meta":{"url":"http://arxiv.org/abs/2307.13953v1"},"cats":{"new-dataset":0.0612669733,"dev-research":0.2693092997,"prompt-eng":0.3546844203,"data-quality":0.144665242,"ml-security":0.1202910368}}
{"text":"However, in situations like voice-based crimes, the available voice evidence may be short and limited.","meta":{"url":"http://arxiv.org/abs/2307.13953v1"},"cats":{"new-dataset":0.0189925102,"dev-research":0.2175514864,"prompt-eng":0.2962451779,"data-quality":0.1960816468,"ml-security":0.307292995}}
{"text":"Additionally, from a physiological perspective, each segment of speech -- phoneme -- corresponds to different types of airflow and movements in the face.","meta":{"url":"http://arxiv.org/abs/2307.13953v1"},"cats":{"new-dataset":0.0426047874,"dev-research":0.2052901882,"prompt-eng":0.3278517175,"data-quality":0.1079843777,"ml-security":0.0901191761}}
{"text":"Therefore, it is advantageous to discover the hidden link between phonemes and face attributes.","meta":{"url":"http://arxiv.org/abs/2307.13953v1"},"cats":{"new-dataset":0.022449194,"dev-research":0.291450455,"prompt-eng":0.3643617313,"data-quality":0.1559575959,"ml-security":0.203972287}}
{"text":"In this paper, we propose an analysis pipeline to help us explore the voice-face relationship in a fine-grained manner, i.e., phonemes v.s. facial anthropometric measurements (AM).","meta":{"url":"http://arxiv.org/abs/2307.13953v1"},"cats":{"new-dataset":0.2631664743,"dev-research":0.2348884061,"prompt-eng":0.3622064134,"data-quality":0.1716269481,"ml-security":0.106386441}}
{"text":"We build an estimator for each phoneme-AM pair and evaluate the correlation through hypothesis testing.","meta":{"url":"http://arxiv.org/abs/2307.13953v1"},"cats":{"new-dataset":0.2806075364,"dev-research":0.1516514248,"prompt-eng":0.4657711284,"data-quality":0.180495132,"ml-security":0.0605550702}}
{"text":"Our results indicate that AMs are more predictable from vowels compared to consonants, particularly with plosives.","meta":{"url":"http://arxiv.org/abs/2307.13953v1"},"cats":{"new-dataset":0.0273570612,"dev-research":0.2316727513,"prompt-eng":0.3899739122,"data-quality":0.1912159935,"ml-security":0.0991042058}}
{"text":"Additionally, we observe that if a specific AM exhibits more movement during phoneme pronunciation, it is more predictable.","meta":{"url":"http://arxiv.org/abs/2307.13953v1"},"cats":{"new-dataset":0.0067916922,"dev-research":0.2197878776,"prompt-eng":0.3934312224,"data-quality":0.2059167142,"ml-security":0.1135634487}}
{"text":"Our findings support those in physiology regarding correlation and lay the groundwork for future research on speech-face multimodal learning.","meta":{"url":"http://arxiv.org/abs/2307.13953v1"},"cats":{"new-dataset":0.0899562166,"dev-research":0.25117478,"prompt-eng":0.3208755338,"data-quality":0.1556317175,"ml-security":0.1237645419}}
{"text":"A diverse set of Internet of Things (IoT) devices are becoming an integrated part of daily lives, and playing an increasingly vital role in various industry, enterprise and agricultural settings.","meta":{"url":"http://arxiv.org/abs/2307.13952v1"},"cats":{"new-dataset":0.1909966021,"dev-research":0.2608312515,"prompt-eng":0.3516983527,"data-quality":0.0664693645,"ml-security":0.0830723266}}
{"text":"The current IoT ecosystem relies on several IoT management platforms to manage and operate a large number of IoT devices, their data, and their connectivity.","meta":{"url":"http://arxiv.org/abs/2307.13952v1"},"cats":{"new-dataset":0.1466354122,"dev-research":0.2229655228,"prompt-eng":0.3692094256,"data-quality":0.0666750993,"ml-security":0.1054512802}}
{"text":"Considering their key role, these platforms must be properly secured against cyber attacks.","meta":{"url":"http://arxiv.org/abs/2307.13952v1"},"cats":{"new-dataset":0.0801712528,"dev-research":0.2572358842,"prompt-eng":0.3451781767,"data-quality":0.135704685,"ml-security":0.5687011054}}
{"text":"In this work, we first explore the core operations/features of leading platforms to design a framework to perform a systematic security evaluation of these platforms.","meta":{"url":"http://arxiv.org/abs/2307.13952v1"},"cats":{"new-dataset":0.2286886931,"dev-research":0.3562768256,"prompt-eng":0.4678432559,"data-quality":0.1287017976,"ml-security":0.5067524645}}
{"text":"Subsequently, we use our framework to analyze a representative set of 52 IoT management platforms, including 42 web-hosted and 10 locally-deployable platforms.","meta":{"url":"http://arxiv.org/abs/2307.13952v1"},"cats":{"new-dataset":0.5964103347,"dev-research":0.2338748904,"prompt-eng":0.4283451786,"data-quality":0.1169132083,"ml-security":0.1028816112}}
{"text":"We discover a number of high severity unauthorized access vulnerabilities in 9/52 evaluated IoT management platforms, which could be abused to perform attacks such as remote IoT SIM deactivation, IoT SIM overcharging and IoT device data forgery.","meta":{"url":"http://arxiv.org/abs/2307.13952v1"},"cats":{"new-dataset":0.2847209413,"dev-research":0.2754144528,"prompt-eng":0.3934485347,"data-quality":0.1842975205,"ml-security":0.6134743663}}
{"text":"More seriously, we also uncover instances of broken authentication in 13/52 platforms, including complete account takeover on 8/52 platforms along with remote code execution on 2/52 platforms.","meta":{"url":"http://arxiv.org/abs/2307.13952v1"},"cats":{"new-dataset":0.2212621013,"dev-research":0.3311420736,"prompt-eng":0.442474248,"data-quality":0.2426628215,"ml-security":0.4287136272}}
{"text":"In effect, 17/52 platforms were affected by vulnerabilities that could lead to platform-wide attacks.","meta":{"url":"http://arxiv.org/abs/2307.13952v1"},"cats":{"new-dataset":0.1088105439,"dev-research":0.3574160946,"prompt-eng":0.3612984708,"data-quality":0.1899167164,"ml-security":0.6086885759}}
{"text":"Overall, vulnerabilities were uncovered in 33 platforms, out of which 28 platforms responded to our responsible disclosure.","meta":{"url":"http://arxiv.org/abs/2307.13952v1"},"cats":{"new-dataset":0.4408223319,"dev-research":0.3328971373,"prompt-eng":0.3611558207,"data-quality":0.2494382267,"ml-security":0.5222311626}}
{"text":"We were also assigned 11 CVEs and awarded bounty for our findings.","meta":{"url":"http://arxiv.org/abs/2307.13952v1"},"cats":{"new-dataset":0.1525449581,"dev-research":0.2265755245,"prompt-eng":0.3702265144,"data-quality":0.178334993,"ml-security":0.1562241614}}
{"text":"The success of re-localisation has crucial implications for the practical deployment of robots operating within a prior map or relative to one another in real-world scenarios.","meta":{"url":"http://arxiv.org/abs/2307.13950v1"},"cats":{"new-dataset":0.0331241566,"dev-research":0.2925758036,"prompt-eng":0.4258715986,"data-quality":0.1619876871,"ml-security":0.1145526907}}
{"text":"Using single-modality, place recognition and localisation can be compromised in challenging environments such as forests.","meta":{"url":"http://arxiv.org/abs/2307.13950v1"},"cats":{"new-dataset":0.0740472017,"dev-research":0.2531078046,"prompt-eng":0.3954456284,"data-quality":0.2280868947,"ml-security":0.2718225577}}
{"text":"To address this, we propose a strategy to prevent lidar-based re-localisation failure using lidar-image cross-modality.","meta":{"url":"http://arxiv.org/abs/2307.13950v1"},"cats":{"new-dataset":0.0569671038,"dev-research":0.2496272076,"prompt-eng":0.4017385596,"data-quality":0.3106874462,"ml-security":0.1792574043}}
{"text":"Our solution relies on self-supervised 2D-3D feature matching to predict alignment and misalignment.","meta":{"url":"http://arxiv.org/abs/2307.13950v1"},"cats":{"new-dataset":0.0961161276,"dev-research":0.2533450011,"prompt-eng":0.4318432806,"data-quality":0.2369289185,"ml-security":0.0675893259}}
{"text":"Leveraging a deep network for lidar feature extraction and relative pose estimation between point clouds, we train a model to evaluate the estimated transformation.","meta":{"url":"http://arxiv.org/abs/2307.13950v1"},"cats":{"new-dataset":0.1832490251,"dev-research":0.2061223981,"prompt-eng":0.3555659995,"data-quality":0.1574165188,"ml-security":0.1558378919}}
{"text":"A model predicting the presence of misalignment is learned by analysing image-lidar similarity in the embedding space and the geometric constraints available within the region seen in both modalities in Euclidean space.","meta":{"url":"http://arxiv.org/abs/2307.13950v1"},"cats":{"new-dataset":0.0394040123,"dev-research":0.2234081009,"prompt-eng":0.3525086292,"data-quality":0.3010765009,"ml-security":0.1168455824}}
{"text":"Experimental results using real datasets (offline and online modes) demonstrate the effectiveness of the proposed pipeline for robust re-localisation in unstructured, natural environments.","meta":{"url":"http://arxiv.org/abs/2307.13950v1"},"cats":{"new-dataset":0.5251576778,"dev-research":0.2248616302,"prompt-eng":0.3572141737,"data-quality":0.2129046583,"ml-security":0.0822721343}}
{"text":"Transformer-based pretrained language models (PLMs) have achieved great success in modern NLP.","meta":{"url":"http://arxiv.org/abs/2307.13949v1"},"cats":{"new-dataset":0.0637322921,"dev-research":0.1833404592,"prompt-eng":0.4154882801,"data-quality":0.214970249,"ml-security":0.0958655862}}
{"text":"An important advantage of PLMs is good out-of-distribution (OOD) robustness.","meta":{"url":"http://arxiv.org/abs/2307.13949v1"},"cats":{"new-dataset":0.0172257673,"dev-research":0.2389832401,"prompt-eng":0.3897309875,"data-quality":0.2250358052,"ml-security":0.2679877734}}
{"text":"Recently, diffusion models have attracted a lot of work to apply diffusion to PLMs.","meta":{"url":"http://arxiv.org/abs/2307.13949v1"},"cats":{"new-dataset":0.0068516781,"dev-research":0.1628952366,"prompt-eng":0.3668118203,"data-quality":0.0850471813,"ml-security":0.1569635386}}
{"text":"It remains under-explored how diffusion influences PLMs on OOD data.","meta":{"url":"http://arxiv.org/abs/2307.13949v1"},"cats":{"new-dataset":0.0370654563,"dev-research":0.1584974841,"prompt-eng":0.337426298,"data-quality":0.1199324952,"ml-security":0.1378790414}}
{"text":"The core of diffusion models is a forward diffusion process which gradually applies Gaussian noise to inputs, and a reverse denoising process which removes noise.","meta":{"url":"http://arxiv.org/abs/2307.13949v1"},"cats":{"new-dataset":0.0075060373,"dev-research":0.2239854206,"prompt-eng":0.3636505746,"data-quality":0.1297896081,"ml-security":0.2505247743}}
{"text":"The noised input reconstruction is a fundamental ability of diffusion models.","meta":{"url":"http://arxiv.org/abs/2307.13949v1"},"cats":{"new-dataset":0.0345454961,"dev-research":0.1755965166,"prompt-eng":0.3775625014,"data-quality":0.189429877,"ml-security":0.1813094333}}
{"text":"We directly analyze OOD robustness by measuring the reconstruction loss, including testing the abilities to reconstruct OOD data, and to detect OOD samples.","meta":{"url":"http://arxiv.org/abs/2307.13949v1"},"cats":{"new-dataset":0.1413108492,"dev-research":0.2425815767,"prompt-eng":0.4423473141,"data-quality":0.3164155323,"ml-security":0.2143550272}}
{"text":"Experiments are conducted by analyzing different training parameters and data statistical features on eight datasets.","meta":{"url":"http://arxiv.org/abs/2307.13949v1"},"cats":{"new-dataset":0.3290350506,"dev-research":0.2322535026,"prompt-eng":0.3967130398,"data-quality":0.2770545419,"ml-security":0.1861033633}}
{"text":"It shows that finetuning PLMs with diffusion degrades the reconstruction ability on OOD data.","meta":{"url":"http://arxiv.org/abs/2307.13949v1"},"cats":{"new-dataset":0.0985791452,"dev-research":0.1838676781,"prompt-eng":0.3596957252,"data-quality":0.1914346762,"ml-security":0.105310068}}
{"text":"The comparison also shows that diffusion models can effectively detect OOD samples, achieving state-of-the-art performance in most of the datasets with an absolute accuracy improvement up to 18%.","meta":{"url":"http://arxiv.org/abs/2307.13949v1"},"cats":{"new-dataset":0.1033782001,"dev-research":0.1827793807,"prompt-eng":0.376435134,"data-quality":0.1758241122,"ml-security":0.1257578216}}
{"text":"These results indicate that diffusion reduces OOD robustness of PLMs.","meta":{"url":"http://arxiv.org/abs/2307.13949v1"},"cats":{"new-dataset":0.0047580054,"dev-research":0.1867602553,"prompt-eng":0.3609070125,"data-quality":0.1528468242,"ml-security":0.2262536514}}
{"text":"Previous works on voice-face matching and voice-guided face synthesis demonstrate strong correlations between voice and face, but mainly rely on coarse semantic cues such as gender, age, and emotion.","meta":{"url":"http://arxiv.org/abs/2307.13948v1"},"cats":{"new-dataset":0.0509607581,"dev-research":0.2810940463,"prompt-eng":0.3924567149,"data-quality":0.1894059498,"ml-security":0.1055281051}}
{"text":"In this paper, we aim to investigate the capability of reconstructing the 3D facial shape from voice from a geometry perspective without any semantic information.","meta":{"url":"http://arxiv.org/abs/2307.13948v1"},"cats":{"new-dataset":0.1118229396,"dev-research":0.1985637272,"prompt-eng":0.3348160254,"data-quality":0.1226576814,"ml-security":0.078130969}}
{"text":"We propose a voice-anthropometric measurement (AM)-face paradigm, which identifies predictable facial AMs from the voice and uses them to guide 3D face reconstruction.","meta":{"url":"http://arxiv.org/abs/2307.13948v1"},"cats":{"new-dataset":0.1735418216,"dev-research":0.2236622575,"prompt-eng":0.3672700753,"data-quality":0.1154360403,"ml-security":0.0907375724}}
{"text":"By leveraging AMs as a proxy to link the voice and face geometry, we can eliminate the influence of unpredictable AMs and make the face geometry tractable.","meta":{"url":"http://arxiv.org/abs/2307.13948v1"},"cats":{"new-dataset":0.0262530457,"dev-research":0.2276604234,"prompt-eng":0.4285611261,"data-quality":0.1678159809,"ml-security":0.1628248599}}
{"text":"Our approach is evaluated on our proposed dataset with ground-truth 3D face scans and corresponding voice recordings, and we find significant correlations between voice and specific parts of the face geometry, such as the nasal cavity and cranium.","meta":{"url":"http://arxiv.org/abs/2307.13948v1"},"cats":{"new-dataset":0.4593119215,"dev-research":0.2002545521,"prompt-eng":0.3537861034,"data-quality":0.1759369121,"ml-security":0.1241734844}}
{"text":"Our work offers a new perspective on voice-face correlation and can serve as a good empirical study for anthropometry science.","meta":{"url":"http://arxiv.org/abs/2307.13948v1"},"cats":{"new-dataset":0.1124815353,"dev-research":0.2431810202,"prompt-eng":0.3245378107,"data-quality":0.1204408671,"ml-security":0.0841946981}}
{"text":"Cancer grading is an essential task in pathology.","meta":{"url":"http://arxiv.org/abs/2307.13947v1"},"cats":{"new-dataset":0.0143118169,"dev-research":0.3242508232,"prompt-eng":0.4087936076,"data-quality":0.1969013287,"ml-security":0.0693502038}}
{"text":"The recent developments of artificial neural networks in computational pathology have shown that these methods hold great potential for improving the accuracy and quality of cancer diagnosis.","meta":{"url":"http://arxiv.org/abs/2307.13947v1"},"cats":{"new-dataset":0.0306614667,"dev-research":0.309146322,"prompt-eng":0.3435543437,"data-quality":0.2183008443,"ml-security":0.1594383488}}
{"text":"However, the issues with the robustness and reliability of such methods have not been fully resolved yet.","meta":{"url":"http://arxiv.org/abs/2307.13947v1"},"cats":{"new-dataset":0.0308173193,"dev-research":0.2868714174,"prompt-eng":0.4188977258,"data-quality":0.3547055919,"ml-security":0.1190886255}}
{"text":"Herein, we propose a centroid-aware feature recalibration network that can conduct cancer grading in an accurate and robust manner.","meta":{"url":"http://arxiv.org/abs/2307.13947v1"},"cats":{"new-dataset":0.1190689251,"dev-research":0.327123519,"prompt-eng":0.4547260926,"data-quality":0.3156693612,"ml-security":0.0852338684}}
{"text":"The proposed network maps an input pathology image into an embedding space and adjusts it by using centroids embedding vectors of different cancer grades via attention mechanism.","meta":{"url":"http://arxiv.org/abs/2307.13947v1"},"cats":{"new-dataset":0.0916686501,"dev-research":0.2478563274,"prompt-eng":0.4025895014,"data-quality":0.2486164632,"ml-security":0.1195772347}}
{"text":"Equipped with the recalibrated embedding vector, the proposed network classifiers the input pathology image into a pertinent class label, i.e., cancer grade.","meta":{"url":"http://arxiv.org/abs/2307.13947v1"},"cats":{"new-dataset":0.1164255801,"dev-research":0.2472410702,"prompt-eng":0.3888157438,"data-quality":0.4362291893,"ml-security":0.1885385598}}
{"text":"We evaluate the proposed network using colorectal cancer datasets that were collected under different environments.","meta":{"url":"http://arxiv.org/abs/2307.13947v1"},"cats":{"new-dataset":0.5849694983,"dev-research":0.2118536052,"prompt-eng":0.3273337025,"data-quality":0.1489758146,"ml-security":0.1366764266}}
{"text":"The experimental results confirm that the proposed network is able to conduct cancer grading in pathology images with high accuracy regardless of the environmental changes in the datasets.","meta":{"url":"http://arxiv.org/abs/2307.13947v1"},"cats":{"new-dataset":0.2350169705,"dev-research":0.2596760822,"prompt-eng":0.3774033586,"data-quality":0.3245577664,"ml-security":0.1203378654}}
{"text":"Contrastive learning on graphs aims at extracting distinguishable high-level representations of nodes.","meta":{"url":"http://arxiv.org/abs/2307.13944v1"},"cats":{"new-dataset":0.0392929842,"dev-research":0.254624754,"prompt-eng":0.3479551633,"data-quality":0.2534414114,"ml-security":0.1073036176}}
{"text":"In this paper, we theoretically illustrate that the entropy of a dataset can be approximated by maximizing the lower bound of the mutual information across different views of a graph, \\ie, entropy is estimated by a neural network.","meta":{"url":"http://arxiv.org/abs/2307.13944v1"},"cats":{"new-dataset":0.0912455759,"dev-research":0.2160915107,"prompt-eng":0.3195438206,"data-quality":0.2225094088,"ml-security":0.2073926848}}
{"text":"Based on this finding, we propose a simple yet effective subset sampling strategy to contrast pairwise representations between views of a dataset.","meta":{"url":"http://arxiv.org/abs/2307.13944v1"},"cats":{"new-dataset":0.4512705111,"dev-research":0.2236628034,"prompt-eng":0.3323123294,"data-quality":0.2414459036,"ml-security":0.0854157118}}
{"text":"In particular, we randomly sample nodes and edges from a given graph to build the input subset for a view.","meta":{"url":"http://arxiv.org/abs/2307.13944v1"},"cats":{"new-dataset":0.0507423984,"dev-research":0.2719655204,"prompt-eng":0.3771045419,"data-quality":0.1924495402,"ml-security":0.105185582}}
{"text":"Two views are fed into a parameter-shared Siamese network to extract the high-dimensional embeddings and estimate the information entropy of the entire graph.","meta":{"url":"http://arxiv.org/abs/2307.13944v1"},"cats":{"new-dataset":0.1541704892,"dev-research":0.1951012181,"prompt-eng":0.3546413942,"data-quality":0.1928241362,"ml-security":0.0865254306}}
{"text":"For the learning process, we propose to optimize the network using two objectives, simultaneously.","meta":{"url":"http://arxiv.org/abs/2307.13944v1"},"cats":{"new-dataset":0.0080355203,"dev-research":0.3015455911,"prompt-eng":0.4100652281,"data-quality":0.1329907089,"ml-security":0.1371116995}}
{"text":"Concretely, the input of the contrastive loss function consists of positive and negative pairs.","meta":{"url":"http://arxiv.org/abs/2307.13944v1"},"cats":{"new-dataset":0.0243108459,"dev-research":0.2231794924,"prompt-eng":0.3512164015,"data-quality":0.1993143564,"ml-security":0.1255184632}}
{"text":"Our selection strategy of pairs is different from previous works and we present a novel strategy to enhance the representation ability of the graph encoder by selecting nodes based on cross-view similarities.","meta":{"url":"http://arxiv.org/abs/2307.13944v1"},"cats":{"new-dataset":0.1333704838,"dev-research":0.2888194147,"prompt-eng":0.3861175539,"data-quality":0.1568701694,"ml-security":0.0713519158}}
{"text":"We enrich the diversity of the positive and negative pairs by selecting highly similar samples and totally different data with the guidance of cross-view similarity scores, respectively.","meta":{"url":"http://arxiv.org/abs/2307.13944v1"},"cats":{"new-dataset":0.1758298155,"dev-research":0.2223150294,"prompt-eng":0.3678674606,"data-quality":0.2330672352,"ml-security":0.082446247}}
{"text":"We also introduce a cross-view consistency constraint on the representations generated from the different views.","meta":{"url":"http://arxiv.org/abs/2307.13944v1"},"cats":{"new-dataset":0.0674290787,"dev-research":0.2438337507,"prompt-eng":0.3999061595,"data-quality":0.3146398278,"ml-security":0.089165769}}
{"text":"This objective guarantees the learned representations are consistent across views from the perspective of the entire graph.","meta":{"url":"http://arxiv.org/abs/2307.13944v1"},"cats":{"new-dataset":0.0380237307,"dev-research":0.2553481763,"prompt-eng":0.3599346499,"data-quality":0.2705587883,"ml-security":0.1280927217}}
{"text":"We conduct extensive experiments on seven graph benchmarks, and the proposed approach achieves competitive performance compared to the current state-of-the-art methods.","meta":{"url":"http://arxiv.org/abs/2307.13944v1"},"cats":{"new-dataset":0.056957235,"dev-research":0.2606940768,"prompt-eng":0.3986394214,"data-quality":0.1835656597,"ml-security":0.0693466293}}
{"text":"The source code will be publicly released once this paper is accepted.","meta":{"url":"http://arxiv.org/abs/2307.13944v1"},"cats":{"new-dataset":0.3363906365,"dev-research":0.2940289574,"prompt-eng":0.3551991242,"data-quality":0.1926718654,"ml-security":0.1325863076}}
{"text":"Out-of-distribution (OOD) generalization is a challenging machine learning problem yet highly desirable in many high-stake applications.","meta":{"url":"http://arxiv.org/abs/2307.13943v1"},"cats":{"new-dataset":0.0650614262,"dev-research":0.1752496084,"prompt-eng":0.3436858771,"data-quality":0.2874156074,"ml-security":0.3348208755}}
{"text":"Existing methods suffer from overly pessimistic modeling with low generalization confidence.","meta":{"url":"http://arxiv.org/abs/2307.13943v1"},"cats":{"new-dataset":0.0055766755,"dev-research":0.2580083682,"prompt-eng":0.4124281442,"data-quality":0.2085660551,"ml-security":0.2101615926}}
{"text":"As generalizing to arbitrary test distributions is impossible, we hypothesize that further structure on the topology of distributions is crucial in developing strong OOD resilience.","meta":{"url":"http://arxiv.org/abs/2307.13943v1"},"cats":{"new-dataset":0.0632550414,"dev-research":0.2295284113,"prompt-eng":0.4166326155,"data-quality":0.2140342739,"ml-security":0.4027618761}}
{"text":"To this end, we propose topology-aware robust optimization (TRO) that seamlessly integrates distributional topology in a principled optimization framework.","meta":{"url":"http://arxiv.org/abs/2307.13943v1"},"cats":{"new-dataset":0.0213572755,"dev-research":0.2594447027,"prompt-eng":0.4207307817,"data-quality":0.1668455856,"ml-security":0.1597600881}}
{"text":"More specifically, TRO solves two optimization objectives: (1) Topology Learning which explores data manifold to uncover the distributional topology; (2) Learning on Topology which exploits the topology to constrain robust optimization for tightly-bounded generalization risks.","meta":{"url":"http://arxiv.org/abs/2307.13943v1"},"cats":{"new-dataset":0.0090548741,"dev-research":0.2796966541,"prompt-eng":0.3425458253,"data-quality":0.1263523002,"ml-security":0.2266433482}}
{"text":"We theoretically demonstrate the effectiveness of our approach and empirically show that it significantly outperforms the state of the arts in a wide range of tasks including classification, regression, and semantic segmentation.","meta":{"url":"http://arxiv.org/abs/2307.13943v1"},"cats":{"new-dataset":0.107810392,"dev-research":0.2566290765,"prompt-eng":0.3987119403,"data-quality":0.3112997083,"ml-security":0.0720022271}}
{"text":"Moreover, we empirically find the data-driven distributional topology is consistent with domain knowledge, enhancing the explainability of our approach.","meta":{"url":"http://arxiv.org/abs/2307.13943v1"},"cats":{"new-dataset":0.0828567774,"dev-research":0.2721346695,"prompt-eng":0.3851493185,"data-quality":0.1781546771,"ml-security":0.1122262778}}
{"text":"The weighted Euler characteristic transform (WECT) is a new tool for extracting shape information from data equipped with a weight function.","meta":{"url":"http://arxiv.org/abs/2307.13940v1"},"cats":{"new-dataset":0.1241828577,"dev-research":0.2470085183,"prompt-eng":0.4296777364,"data-quality":0.1426255081,"ml-security":0.0761441344}}
{"text":"Image data may benefit from the WECT where the intensity of the pixels are used to define the weight function.","meta":{"url":"http://arxiv.org/abs/2307.13940v1"},"cats":{"new-dataset":0.113791786,"dev-research":0.2387970699,"prompt-eng":0.4002101399,"data-quality":0.1486710297,"ml-security":0.0763622153}}
{"text":"In this work, an empirical assessment of the WECT's ability to distinguish shapes on images with different pixel intensity distributions is considered, along with visualization techniques to improve the intuition and understanding of what is captured by the WECT.","meta":{"url":"http://arxiv.org/abs/2307.13940v1"},"cats":{"new-dataset":0.1392419893,"dev-research":0.2856589988,"prompt-eng":0.4396620818,"data-quality":0.1872631657,"ml-security":0.0776048865}}
{"text":"Additionally, the expected weighted Euler characteristic and the expected WECT are derived.","meta":{"url":"http://arxiv.org/abs/2307.13940v1"},"cats":{"new-dataset":0.0414046684,"dev-research":0.1578425164,"prompt-eng":0.4592643603,"data-quality":0.1213121141,"ml-security":0.0773097312}}
{"text":"Semi-supervised semantic segmentation (SSS) is an important task that utilizes both labeled and unlabeled data to reduce expenses on labeling training examples.","meta":{"url":"http://arxiv.org/abs/2307.13938v1"},"cats":{"new-dataset":0.166533167,"dev-research":0.2444486075,"prompt-eng":0.4126887994,"data-quality":0.4405099631,"ml-security":0.1077590752}}
{"text":"However, the effectiveness of SSS algorithms is limited by the difficulty of fully exploiting the potential of unlabeled data.","meta":{"url":"http://arxiv.org/abs/2307.13938v1"},"cats":{"new-dataset":0.0221165773,"dev-research":0.2301681783,"prompt-eng":0.3782210689,"data-quality":0.1869317111,"ml-security":0.2807419839}}
{"text":"To address this, we propose a dual-level Siamese structure network (DSSN) for pixel-wise contrastive learning.","meta":{"url":"http://arxiv.org/abs/2307.13938v1"},"cats":{"new-dataset":0.2856182762,"dev-research":0.210164527,"prompt-eng":0.3330053137,"data-quality":0.1444849779,"ml-security":0.0886983416}}
{"text":"By aligning positive pairs with a pixel-wise contrastive loss using strong augmented views in both low-level image space and high-level feature space, the proposed DSSN is designed to maximize the utilization of available unlabeled data.","meta":{"url":"http://arxiv.org/abs/2307.13938v1"},"cats":{"new-dataset":0.1608340272,"dev-research":0.2488421065,"prompt-eng":0.4085927337,"data-quality":0.2521696903,"ml-security":0.1434551968}}
{"text":"Additionally, we introduce a novel class-aware pseudo-label selection strategy for weak-to-strong supervision, which addresses the limitations of most existing methods that do not perform selection or apply a predefined threshold for all classes.","meta":{"url":"http://arxiv.org/abs/2307.13938v1"},"cats":{"new-dataset":0.0436268635,"dev-research":0.2833686744,"prompt-eng":0.4894204406,"data-quality":0.5584977965,"ml-security":0.2949744632}}
{"text":"Specifically, our strategy selects the top high-confidence prediction of the weak view for each class to generate pseudo labels that supervise the strong augmented views.","meta":{"url":"http://arxiv.org/abs/2307.13938v1"},"cats":{"new-dataset":0.0675660509,"dev-research":0.2256145086,"prompt-eng":0.4912087371,"data-quality":0.2876479081,"ml-security":0.2176513463}}
{"text":"This strategy is capable of taking into account the class imbalance and improving the performance of long-tailed classes.","meta":{"url":"http://arxiv.org/abs/2307.13938v1"},"cats":{"new-dataset":0.0252097895,"dev-research":0.2174454138,"prompt-eng":0.4450024181,"data-quality":0.1423115155,"ml-security":0.2024205536}}
{"text":"Our proposed method achieves state-of-the-art results on two datasets, PASCAL VOC 2012 and Cityscapes, outperforming other SSS algorithms by a significant margin.","meta":{"url":"http://arxiv.org/abs/2307.13938v1"},"cats":{"new-dataset":0.3793435387,"dev-research":0.2344895299,"prompt-eng":0.3670330928,"data-quality":0.1894329968,"ml-security":0.1190582191}}
{"text":"We consider the Generalized Makespan Problem (GMP) on unrelated machines, where we are given $n$ jobs and $m$ machines and each job $j$ has arbitrary processing time $p_{ij}$ on machine $i$. Additionally, there is a general symmetric monotone norm $\\psi_i$ for each machine $i$, that determines the load on machine $i$ as a function of the sizes of jobs assigned to it.","meta":{"url":"http://arxiv.org/abs/2307.13937v1"},"cats":{"new-dataset":0.0413737049,"dev-research":0.1921394795,"prompt-eng":0.3459638087,"data-quality":0.1328657474,"ml-security":0.145244799}}
{"text":"The goal is to assign the jobs to minimize the maximum machine load.   ","meta":{"url":"http://arxiv.org/abs/2307.13937v1"},"cats":{"new-dataset":0.0285411933,"dev-research":0.2873429855,"prompt-eng":0.3829314483,"data-quality":0.1049164418,"ml-security":0.1244483654}}
{"text":"Recently, Deng, Li, and Rabani (SODA'22) gave a $3$ approximation for GMP when the $\\psi_i$ are top-$k$ norms, and they ask the question whether an $O(1)$ approximation exists for general norms $\\psi$?","meta":{"url":"http://arxiv.org/abs/2307.13937v1"},"cats":{"new-dataset":0.017385259,"dev-research":0.1792327077,"prompt-eng":0.3156007372,"data-quality":0.1428502778,"ml-security":0.095912217}}
{"text":"We answer this negatively and show that, under natural complexity assumptions, there is some fixed constant $\\delta>0$, such that GMP is $\\Omega(\\log^{\\delta} n)$ hard to approximate.","meta":{"url":"http://arxiv.org/abs/2307.13937v1"},"cats":{"new-dataset":0.0150039415,"dev-research":0.1668421332,"prompt-eng":0.3361177751,"data-quality":0.1415171333,"ml-security":0.1327539034}}
{"text":"We also give an $\\Omega(\\log^{1/2} n)$ integrality gap for the natural configuration LP.","meta":{"url":"http://arxiv.org/abs/2307.13937v1"},"cats":{"new-dataset":0.0847361174,"dev-research":0.157408783,"prompt-eng":0.3906070756,"data-quality":0.1604716811,"ml-security":0.051087832}}
{"text":"Driver distraction has become a significant cause of severe traffic accidents over the past decade.","meta":{"url":"http://arxiv.org/abs/2307.13933v1"},"cats":{"new-dataset":0.0174035542,"dev-research":0.4132328869,"prompt-eng":0.3282578985,"data-quality":0.1248361977,"ml-security":0.2374920144}}
{"text":"Despite the growing development of vision-driven driver monitoring systems, the lack of comprehensive perception datasets restricts road safety and traffic security.","meta":{"url":"http://arxiv.org/abs/2307.13933v1"},"cats":{"new-dataset":0.3522416872,"dev-research":0.3038883577,"prompt-eng":0.3743383793,"data-quality":0.1932592674,"ml-security":0.3773235779}}
{"text":"In this paper, we present an AssIstive Driving pErception dataset (AIDE) that considers context information both inside and outside the vehicle in naturalistic scenarios.","meta":{"url":"http://arxiv.org/abs/2307.13933v1"},"cats":{"new-dataset":0.243825458,"dev-research":0.2484228853,"prompt-eng":0.4104912038,"data-quality":0.2215009815,"ml-security":0.1335131227}}
{"text":"AIDE facilitates holistic driver monitoring through three distinctive characteristics, including multi-view settings of driver and scene, multi-modal annotations of face, body, posture, and gesture, and four pragmatic task designs for driving understanding.","meta":{"url":"http://arxiv.org/abs/2307.13933v1"},"cats":{"new-dataset":0.1280531436,"dev-research":0.3987433077,"prompt-eng":0.4433338951,"data-quality":0.1389757481,"ml-security":0.0936244489}}
{"text":"To thoroughly explore AIDE, we provide experimental benchmarks on three kinds of baseline frameworks via extensive methods.","meta":{"url":"http://arxiv.org/abs/2307.13933v1"},"cats":{"new-dataset":0.1148322536,"dev-research":0.3637010036,"prompt-eng":0.4494088872,"data-quality":0.1834965678,"ml-security":0.1066420981}}
{"text":"Moreover, two fusion strategies are introduced to give new insights into learning effective multi-stream/modal representations.","meta":{"url":"http://arxiv.org/abs/2307.13933v1"},"cats":{"new-dataset":0.0205235688,"dev-research":0.2647057031,"prompt-eng":0.3694457469,"data-quality":0.1479233981,"ml-security":0.1067720959}}
{"text":"We also systematically investigate the importance and rationality of the key components in AIDE and benchmarks.","meta":{"url":"http://arxiv.org/abs/2307.13933v1"},"cats":{"new-dataset":0.0624538508,"dev-research":0.3552749957,"prompt-eng":0.4199231268,"data-quality":0.1448805074,"ml-security":0.1041145366}}
{"text":"The project link is https://github.com/ydk122024/AIDE.","meta":{"url":"http://arxiv.org/abs/2307.13933v1"},"cats":{"new-dataset":0.3818162775,"dev-research":0.2750537859,"prompt-eng":0.4299550605,"data-quality":0.164283417,"ml-security":0.0752784244}}
{"text":"Multi-agent collaborative perception as a potential application for vehicle-to-everything communication could significantly improve the perception performance of autonomous vehicles over single-agent perception.","meta":{"url":"http://arxiv.org/abs/2307.13929v1"},"cats":{"new-dataset":0.016258414,"dev-research":0.244014694,"prompt-eng":0.4065427192,"data-quality":0.1007216931,"ml-security":0.0935937411}}
{"text":"However, several challenges remain in achieving pragmatic information sharing in this emerging research.","meta":{"url":"http://arxiv.org/abs/2307.13929v1"},"cats":{"new-dataset":0.0991499476,"dev-research":0.4102246164,"prompt-eng":0.3615569493,"data-quality":0.1847889092,"ml-security":0.1506325664}}
{"text":"In this paper, we propose SCOPE, a novel collaborative perception framework that aggregates the spatio-temporal awareness characteristics across on-road agents in an end-to-end manner.","meta":{"url":"http://arxiv.org/abs/2307.13929v1"},"cats":{"new-dataset":0.1987801264,"dev-research":0.2782745027,"prompt-eng":0.4474825872,"data-quality":0.1113522865,"ml-security":0.0986335909}}
{"text":"Specifically, SCOPE has three distinct strengths: i) it considers effective semantic cues of the temporal context to enhance current representations of the target agent; ii) it aggregates perceptually critical spatial information from heterogeneous agents and overcomes localization errors via multi-scale feature interactions; iii) it integrates multi-source representations of the target agent based on their complementary contributions by an adaptive fusion paradigm.","meta":{"url":"http://arxiv.org/abs/2307.13929v1"},"cats":{"new-dataset":0.0370487159,"dev-research":0.2880288117,"prompt-eng":0.3832566452,"data-quality":0.1321537985,"ml-security":0.1064814883}}
{"text":"To thoroughly evaluate SCOPE, we consider both real-world and simulated scenarios of collaborative 3D object detection tasks on three datasets.","meta":{"url":"http://arxiv.org/abs/2307.13929v1"},"cats":{"new-dataset":0.3557620755,"dev-research":0.2447203991,"prompt-eng":0.3996443272,"data-quality":0.1694381203,"ml-security":0.1238147541}}
{"text":"Extensive experiments demonstrate the superiority of our approach and the necessity of the proposed components.","meta":{"url":"http://arxiv.org/abs/2307.13929v1"},"cats":{"new-dataset":0.0128901279,"dev-research":0.2467925378,"prompt-eng":0.4354264784,"data-quality":0.1311170431,"ml-security":0.0878517181}}
{"text":"The behaviour of multi-agent learning in competitive settings is often considered under the restrictive assumption of a zero-sum game.","meta":{"url":"http://arxiv.org/abs/2307.13928v1"},"cats":{"new-dataset":0.0183824581,"dev-research":0.1722410146,"prompt-eng":0.3137598054,"data-quality":0.113694191,"ml-security":0.340851999}}
{"text":"Only under this strict requirement is the behaviour of learning well understood; beyond this, learning dynamics can often display non-convergent behaviours which prevent fixed-point analysis.","meta":{"url":"http://arxiv.org/abs/2307.13928v1"},"cats":{"new-dataset":0.0235518995,"dev-research":0.2682991463,"prompt-eng":0.320864207,"data-quality":0.1614682687,"ml-security":0.3259699268}}
{"text":"Nonetheless, many relevant competitive games do not satisfy the zero-sum assumption.   ","meta":{"url":"http://arxiv.org/abs/2307.13928v1"},"cats":{"new-dataset":0.0115646316,"dev-research":0.1947820331,"prompt-eng":0.2676402125,"data-quality":0.1383637708,"ml-security":0.2111864655}}
{"text":"Motivated by this, we study a smooth variant of Q-Learning, a popular reinforcement learning dynamics which balances the agents' tendency to maximise their payoffs with their propensity to explore the state space.","meta":{"url":"http://arxiv.org/abs/2307.13928v1"},"cats":{"new-dataset":0.0321004062,"dev-research":0.1569009174,"prompt-eng":0.3513544716,"data-quality":0.0800115427,"ml-security":0.335726687}}
{"text":"We examine this dynamic in games which are `close' to network zero-sum games and find that Q-Learning converges to a neighbourhood around a unique equilibrium.","meta":{"url":"http://arxiv.org/abs/2307.13928v1"},"cats":{"new-dataset":0.0511284931,"dev-research":0.1480468134,"prompt-eng":0.2819863484,"data-quality":0.1033830206,"ml-security":0.3664503321}}
{"text":"The size of the neighbourhood is determined by the `distance' to the zero-sum game, as well as the exploration rates of the agents.","meta":{"url":"http://arxiv.org/abs/2307.13928v1"},"cats":{"new-dataset":0.1132348556,"dev-research":0.2067573481,"prompt-eng":0.2922381594,"data-quality":0.0708074881,"ml-security":0.105431897}}
{"text":"We complement these results by providing a method whereby, given an arbitrary network game, the `nearest' network zero-sum game can be found efficiently.","meta":{"url":"http://arxiv.org/abs/2307.13928v1"},"cats":{"new-dataset":0.1241311445,"dev-research":0.1989072148,"prompt-eng":0.3345553879,"data-quality":0.1457877311,"ml-security":0.1782979456}}
{"text":"As our experiments show, these guarantees are independent of whether the dynamics ultimately reach an equilibrium, or remain non-convergent.","meta":{"url":"http://arxiv.org/abs/2307.13928v1"},"cats":{"new-dataset":0.014606723,"dev-research":0.142832605,"prompt-eng":0.34678052,"data-quality":0.1052427592,"ml-security":0.2472129061}}
{"text":"In image dehazing task, haze density is a key feature and affects the performance of dehazing methods.","meta":{"url":"http://arxiv.org/abs/2307.13927v1"},"cats":{"new-dataset":0.0152716829,"dev-research":0.3845271854,"prompt-eng":0.3422001097,"data-quality":0.1290069911,"ml-security":0.101298871}}
{"text":"However, some of the existing methods lack a comparative image to measure densities, and others create intermediate results but lack the exploitation of their density differences, which can facilitate perception of density.","meta":{"url":"http://arxiv.org/abs/2307.13927v1"},"cats":{"new-dataset":0.0192634854,"dev-research":0.2337797585,"prompt-eng":0.3516716328,"data-quality":0.1769851176,"ml-security":0.070301564}}
{"text":"To address these deficiencies, we propose a density-aware dehazing method named Density Feature Refinement Network (DFR-Net) that extracts haze density features from density differences and leverages density differences to refine density features.","meta":{"url":"http://arxiv.org/abs/2307.13927v1"},"cats":{"new-dataset":0.1365072995,"dev-research":0.4010354842,"prompt-eng":0.3584632393,"data-quality":0.2239240487,"ml-security":0.1279665004}}
{"text":"In DFR-Net, we first generate a proposal image that has lower overall density than the hazy input, bringing in global density differences.","meta":{"url":"http://arxiv.org/abs/2307.13927v1"},"cats":{"new-dataset":0.1405640888,"dev-research":0.2462024405,"prompt-eng":0.4077679445,"data-quality":0.1725798952,"ml-security":0.0684245906}}
{"text":"Additionally, the dehazing residual of the proposal image reflects the level of dehazing performance and provides local density differences that indicate localized hard dehazing or high density areas.","meta":{"url":"http://arxiv.org/abs/2307.13927v1"},"cats":{"new-dataset":0.1448016346,"dev-research":0.3281522162,"prompt-eng":0.3976156311,"data-quality":0.1594063113,"ml-security":0.0836342915}}
{"text":"Subsequently, we introduce a Global Branch (GB) and a Local Branch (LB) to achieve density-awareness.","meta":{"url":"http://arxiv.org/abs/2307.13927v1"},"cats":{"new-dataset":0.1534750688,"dev-research":0.2185669396,"prompt-eng":0.4724760981,"data-quality":0.1666362949,"ml-security":0.0576826335}}
{"text":"In GB, we use Siamese networks for feature extraction of hazy inputs and proposal images, and we propose a Global Density Feature Refinement (GDFR) module that can refine features by pushing features with different global densities further away.","meta":{"url":"http://arxiv.org/abs/2307.13927v1"},"cats":{"new-dataset":0.4608037288,"dev-research":0.3134060746,"prompt-eng":0.3814214505,"data-quality":0.261775331,"ml-security":0.0880002083}}
{"text":"In LB, we explore local density features from the dehazing residuals between hazy inputs and proposal images and introduce an Intermediate Dehazing Residual Feedforward (IDRF) module to update local features and pull them closer to clear image features.","meta":{"url":"http://arxiv.org/abs/2307.13927v1"},"cats":{"new-dataset":0.2806273528,"dev-research":0.3272183648,"prompt-eng":0.4024313509,"data-quality":0.2746972013,"ml-security":0.1454863814}}
{"text":"Sufficient experiments demonstrate that the proposed method achieves results beyond the state-of-the-art methods on various datasets.","meta":{"url":"http://arxiv.org/abs/2307.13927v1"},"cats":{"new-dataset":0.1827674926,"dev-research":0.2090897463,"prompt-eng":0.4001204049,"data-quality":0.2718581535,"ml-security":0.0797531253}}
{"text":"The level-$k$ $\\ell_1$-Fourier weight of a Boolean function refers to the sum of absolute values of its level-$k$ Fourier coefficients.","meta":{"url":"http://arxiv.org/abs/2307.13926v1"},"cats":{"new-dataset":0.0174386159,"dev-research":0.2410955364,"prompt-eng":0.3461064161,"data-quality":0.1369011871,"ml-security":0.1354338921}}
{"text":"Fourier growth refers to the growth of these weights as $k$ grows.","meta":{"url":"http://arxiv.org/abs/2307.13926v1"},"cats":{"new-dataset":0.0316696207,"dev-research":0.1999501668,"prompt-eng":0.2644549042,"data-quality":0.0965367808,"ml-security":0.1044112223}}
{"text":"It has been extensively studied for various computational models, and bounds on the Fourier growth, even for the first few levels, have proven useful in learning theory, circuit lower bounds, pseudorandomness, and quantum-classical separations.   ","meta":{"url":"http://arxiv.org/abs/2307.13926v1"},"cats":{"new-dataset":0.0171981374,"dev-research":0.1393768651,"prompt-eng":0.3133702701,"data-quality":0.098955906,"ml-security":0.1950100118}}
{"text":"We investigate the Fourier growth of certain functions that naturally arise from communication protocols for XOR functions (partial functions evaluated on the bitwise XOR of the inputs to Alice and Bob).","meta":{"url":"http://arxiv.org/abs/2307.13926v1"},"cats":{"new-dataset":0.0768949778,"dev-research":0.263548017,"prompt-eng":0.3601647247,"data-quality":0.125927202,"ml-security":0.2459624117}}
{"text":"If a protocol $\\mathcal C$ computes an XOR function, then $\\mathcal C(x,y)$ is a function of the parity $x\\oplus y$.","meta":{"url":"http://arxiv.org/abs/2307.13926v1"},"cats":{"new-dataset":0.0510243085,"dev-research":0.2976529407,"prompt-eng":0.4041899591,"data-quality":0.1899101734,"ml-security":0.1939563276}}
{"text":"This motivates us to analyze the XOR-fiber of $\\mathcal C$, defined as $h(z):=\\mathbb E_{x,y}[\\mathcal C(x,y)|x\\oplus y=z]$.   We present improved Fourier growth bounds for the XOR-fibers of protocols that communicate $d$ bits.","meta":{"url":"http://arxiv.org/abs/2307.13926v1"},"cats":{"new-dataset":0.0921886275,"dev-research":0.2525490434,"prompt-eng":0.3671178156,"data-quality":0.1073846581,"ml-security":0.1591783756}}
{"text":"For the first level, we show a tight $O(\\sqrt d)$ bound and obtain a new coin theorem, as well as an alternative proof for the tight randomized communication lower bound for Gap-Hamming.","meta":{"url":"http://arxiv.org/abs/2307.13926v1"},"cats":{"new-dataset":0.1128914855,"dev-research":0.1960655087,"prompt-eng":0.3689443679,"data-quality":0.1758369283,"ml-security":0.1805686865}}
{"text":"For the second level, we show an $d^{3/2}\\cdot\\mathrm{polylog}(n)$ bound, which improves the previous $O(d^2)$ bound by Girish, Raz, and Tal (ITCS 2021) and implies a polynomial improvement on the randomized communication lower bound for the XOR-lift of Forrelation, extending its quantum-classical gap.   ","meta":{"url":"http://arxiv.org/abs/2307.13926v1"},"cats":{"new-dataset":0.0518274799,"dev-research":0.1755301084,"prompt-eng":0.3489458589,"data-quality":0.1428614299,"ml-security":0.1736625046}}
{"text":"Our analysis is based on a new way of adaptively partitioning a relatively large set in Gaussian space to control its moments in all directions.","meta":{"url":"http://arxiv.org/abs/2307.13926v1"},"cats":{"new-dataset":0.0798247419,"dev-research":0.1628371921,"prompt-eng":0.3578854854,"data-quality":0.121465407,"ml-security":0.1351653883}}
{"text":"We achieve this via martingale arguments and allowing protocols to transmit real values.","meta":{"url":"http://arxiv.org/abs/2307.13926v1"},"cats":{"new-dataset":0.0771111944,"dev-research":0.1672791065,"prompt-eng":0.4321429042,"data-quality":0.1254086231,"ml-security":0.2297059114}}
{"text":"We also show a connection between Fourier growth and lifting theorems with constant-sized gadgets as a potential approach to prove optimal bounds for the second level and beyond.","meta":{"url":"http://arxiv.org/abs/2307.13926v1"},"cats":{"new-dataset":0.0203414066,"dev-research":0.1958127064,"prompt-eng":0.3491406739,"data-quality":0.0870865358,"ml-security":0.1371730063}}
{"text":"3D anomaly detection is an emerging and vital computer vision task in industrial manufacturing (IM).","meta":{"url":"http://arxiv.org/abs/2307.13925v1"},"cats":{"new-dataset":0.0857629461,"dev-research":0.3332839662,"prompt-eng":0.3618892236,"data-quality":0.2580817334,"ml-security":0.2206669076}}
{"text":"Recently many advanced algorithms have been published, but most of them cannot meet the needs of IM.","meta":{"url":"http://arxiv.org/abs/2307.13925v1"},"cats":{"new-dataset":0.0626891432,"dev-research":0.2104251859,"prompt-eng":0.3446734835,"data-quality":0.1468618377,"ml-security":0.1128469355}}
{"text":"There are several disadvantages: i) difficult to deploy on production lines since their algorithms heavily rely on large pre-trained models; ii) hugely increase storage overhead due to overuse of memory banks; iii) the inference speed cannot be achieved in real-time.","meta":{"url":"http://arxiv.org/abs/2307.13925v1"},"cats":{"new-dataset":0.0075186889,"dev-research":0.3516163813,"prompt-eng":0.3212500763,"data-quality":0.0842971645,"ml-security":0.2194401669}}
{"text":"To overcome these issues, we propose an easy and deployment-friendly network (called EasyNet) without using pre-trained models and memory banks: firstly, we design a multi-scale multi-modality feature encoder-decoder to accurately reconstruct the segmentation maps of anomalous regions and encourage the interaction between RGB images and depth images; secondly, we adopt a multi-modality anomaly segmentation network to achieve a precise anomaly map; thirdly, we propose an attention-based information entropy fusion module for feature fusion during inference, making it suitable for real-time deployment.","meta":{"url":"http://arxiv.org/abs/2307.13925v1"},"cats":{"new-dataset":0.2375507446,"dev-research":0.3280387236,"prompt-eng":0.4358170572,"data-quality":0.2619250696,"ml-security":0.2309710602}}
{"text":"Extensive experiments show that EasyNet achieves an anomaly detection AUROC of 92.6% without using pre-trained models and memory banks.","meta":{"url":"http://arxiv.org/abs/2307.13925v1"},"cats":{"new-dataset":0.0577337719,"dev-research":0.3063425829,"prompt-eng":0.4259141369,"data-quality":0.3394372656,"ml-security":0.5372388718}}
{"text":"In addition, EasyNet is faster than existing methods, with a high frame rate of 94.55 FPS on a Tesla V100 GPU.","meta":{"url":"http://arxiv.org/abs/2307.13925v1"},"cats":{"new-dataset":0.0307557779,"dev-research":0.3194731828,"prompt-eng":0.3536657725,"data-quality":0.1107289004,"ml-security":0.0906419374}}
{"text":"The field of trajectory forecasting has grown significantly in recent years, partially owing to the release of numerous large-scale, real-world human trajectory datasets for autonomous vehicles (AVs) and pedestrian motion tracking.","meta":{"url":"http://arxiv.org/abs/2307.13924v1"},"cats":{"new-dataset":0.3905654261,"dev-research":0.196980075,"prompt-eng":0.3546976931,"data-quality":0.0723612173,"ml-security":0.1505752973}}
{"text":"While such datasets have been a boon for the community, they each use custom and unique data formats and APIs, making it cumbersome for researchers to train and evaluate methods across multiple datasets.","meta":{"url":"http://arxiv.org/abs/2307.13924v1"},"cats":{"new-dataset":0.4764226932,"dev-research":0.3255660354,"prompt-eng":0.3558720394,"data-quality":0.2062864189,"ml-security":0.1136315568}}
{"text":"To remedy this, we present trajdata: a unified interface to multiple human trajectory datasets.","meta":{"url":"http://arxiv.org/abs/2307.13924v1"},"cats":{"new-dataset":0.6634524543,"dev-research":0.1726122496,"prompt-eng":0.3662750413,"data-quality":0.1041777886,"ml-security":0.0693572943}}
{"text":"At its core, trajdata provides a simple, uniform, and efficient representation and API for trajectory and map data.","meta":{"url":"http://arxiv.org/abs/2307.13924v1"},"cats":{"new-dataset":0.2469676395,"dev-research":0.249104281,"prompt-eng":0.3625191334,"data-quality":0.0930565545,"ml-security":0.0658567689}}
{"text":"As a demonstration of its capabilities, in this work we conduct a comprehensive empirical evaluation of existing trajectory datasets, providing users with a rich understanding of the data underpinning much of current pedestrian and AV motion forecasting research, and proposing suggestions for future datasets from these insights.","meta":{"url":"http://arxiv.org/abs/2307.13924v1"},"cats":{"new-dataset":0.5340112149,"dev-research":0.2004590147,"prompt-eng":0.3677168373,"data-quality":0.1009959129,"ml-security":0.1157016091}}
{"text":"trajdata is permissively licensed (Apache 2.0) and can be accessed online at https://github.com/NVlabs/trajdata","meta":{"url":"http://arxiv.org/abs/2307.13924v1"},"cats":{"new-dataset":0.3720733838,"dev-research":0.1935614254,"prompt-eng":0.3119672472,"data-quality":0.1231705869,"ml-security":0.0875358864}}
{"text":"Grammatical error correction aims to correct ungrammatical sentences automatically.","meta":{"url":"http://arxiv.org/abs/2307.13923v1"},"cats":{"new-dataset":0.0094133899,"dev-research":0.4073401964,"prompt-eng":0.4243248585,"data-quality":0.5784111483,"ml-security":0.0829870638}}
{"text":"Recently, some work has demonstrated the excellent capabilities of closed-source Large Language Models (LLMs, e.g., ChatGPT) in grammatical error correction.","meta":{"url":"http://arxiv.org/abs/2307.13923v1"},"cats":{"new-dataset":0.1484847106,"dev-research":0.2376089297,"prompt-eng":0.4182421002,"data-quality":0.4485813272,"ml-security":0.1178518886}}
{"text":"However, the potential of open-source LLMs remains unexplored.","meta":{"url":"http://arxiv.org/abs/2307.13923v1"},"cats":{"new-dataset":0.0891018326,"dev-research":0.1936371605,"prompt-eng":0.316408753,"data-quality":0.176023937,"ml-security":0.2247601835}}
{"text":"In this paper, we introduced GrammarGPT, an open-source LLM, to preliminary explore its potential for native Chinese grammatical error correction.","meta":{"url":"http://arxiv.org/abs/2307.13923v1"},"cats":{"new-dataset":0.1469593558,"dev-research":0.3065657081,"prompt-eng":0.4518318858,"data-quality":0.5015475513,"ml-security":0.0717940064}}
{"text":"The core recipe of GrammarGPT is to leverage the hybrid dataset of ChatGPT-generated and human-annotated.","meta":{"url":"http://arxiv.org/abs/2307.13923v1"},"cats":{"new-dataset":0.704442906,"dev-research":0.3005562395,"prompt-eng":0.4367743124,"data-quality":0.3205788663,"ml-security":0.0701760593}}
{"text":"For grammatical errors with clues, we proposed a heuristic method to guide ChatGPT to generate ungrammatical sentences by providing those clues.","meta":{"url":"http://arxiv.org/abs/2307.13923v1"},"cats":{"new-dataset":0.140945514,"dev-research":0.4080472631,"prompt-eng":0.4900545062,"data-quality":0.3952364069,"ml-security":0.0655678688}}
{"text":"For grammatical errors without clues, we collected ungrammatical sentences from publicly available websites and manually corrected them.","meta":{"url":"http://arxiv.org/abs/2307.13923v1"},"cats":{"new-dataset":0.1928500156,"dev-research":0.3375317243,"prompt-eng":0.4359591876,"data-quality":0.5600296907,"ml-security":0.1042746312}}
{"text":"In addition, we employed an error-invariant augmentation method to enhance the ability of the model to correct native Chinese grammatical errors.","meta":{"url":"http://arxiv.org/abs/2307.13923v1"},"cats":{"new-dataset":0.0362982992,"dev-research":0.2820813312,"prompt-eng":0.4879810956,"data-quality":0.5206263093,"ml-security":0.0869891877}}
{"text":"We ultimately constructed about 1k parallel data and utilized these data to fine-tune open-source LLMs (e.g., Phoenix, released by The Chinese University of Hong Kong, Shenzhen) with instruction tuning.","meta":{"url":"http://arxiv.org/abs/2307.13923v1"},"cats":{"new-dataset":0.4491281108,"dev-research":0.251220697,"prompt-eng":0.4278530445,"data-quality":0.1480411201,"ml-security":0.1029867598}}
{"text":"The experimental results show that GrammarGPT outperforms the existing SOTA system significantly.","meta":{"url":"http://arxiv.org/abs/2307.13923v1"},"cats":{"new-dataset":0.0877637933,"dev-research":0.2709491447,"prompt-eng":0.4272446763,"data-quality":0.2736252292,"ml-security":0.0695824756}}
{"text":"Although model parameters are 20x larger than the SOTA baseline, the required amount of data for instruction tuning is 1200x smaller, illustrating the potential of open-source LLMs on native CGEC.","meta":{"url":"http://arxiv.org/abs/2307.13923v1"},"cats":{"new-dataset":0.1511702288,"dev-research":0.2473075913,"prompt-eng":0.406837208,"data-quality":0.1225759119,"ml-security":0.0767252215}}
{"text":"Our GrammarGPT ranks $3^{rd}$ on NLPCC2023 SharedTask1, demonstrating our approach's effectiveness.","meta":{"url":"http://arxiv.org/abs/2307.13923v1"},"cats":{"new-dataset":0.4366979259,"dev-research":0.2538248368,"prompt-eng":0.4348791154,"data-quality":0.3391188132,"ml-security":0.0774043455}}
{"text":"The code and data are available at \\url{https://github.com/FreedomIntelligence/GrammarGPT}.","meta":{"url":"http://arxiv.org/abs/2307.13923v1"},"cats":{"new-dataset":0.7467050346,"dev-research":0.2636619535,"prompt-eng":0.4138869482,"data-quality":0.2102626699,"ml-security":0.0954390641}}
{"text":"The behaviour of multi-agent learning in many player games has been shown to display complex dynamics outside of restrictive examples such as network zero-sum games.","meta":{"url":"http://arxiv.org/abs/2307.13922v1"},"cats":{"new-dataset":0.0531640495,"dev-research":0.1981083696,"prompt-eng":0.3211448963,"data-quality":0.0953699049,"ml-security":0.2845553154}}
{"text":"In addition, it has been shown that convergent behaviour is less likely to occur as the number of players increase.","meta":{"url":"http://arxiv.org/abs/2307.13922v1"},"cats":{"new-dataset":0.0095505373,"dev-research":0.2363483315,"prompt-eng":0.3558688476,"data-quality":0.1013434072,"ml-security":0.1664855887}}
{"text":"To make progress in resolving this problem, we study Q-Learning dynamics and determine a sufficient condition for the dynamics to converge to a unique equilibrium in any network game.","meta":{"url":"http://arxiv.org/abs/2307.13922v1"},"cats":{"new-dataset":0.0346552428,"dev-research":0.1407920289,"prompt-eng":0.3214545452,"data-quality":0.0962413727,"ml-security":0.3671286159}}
{"text":"We find that this condition depends on the nature of pairwise interactions and on the network structure, but is explicitly independent of the total number of agents in the game.","meta":{"url":"http://arxiv.org/abs/2307.13922v1"},"cats":{"new-dataset":0.0599809004,"dev-research":0.1728278752,"prompt-eng":0.3489384538,"data-quality":0.0764876297,"ml-security":0.1989392861}}
{"text":"We evaluate this result on a number of representative network games and show that, under suitable network conditions, stable learning dynamics can be achieved with an arbitrary number of agents.","meta":{"url":"http://arxiv.org/abs/2307.13922v1"},"cats":{"new-dataset":0.081025798,"dev-research":0.1573506995,"prompt-eng":0.3423398428,"data-quality":0.1356153958,"ml-security":0.3456209212}}
{"text":"We consider the algorithmic problem of finding large \\textit{balanced} independent sets in sparse random bipartite graphs, and more generally the problem of finding independent sets with specified proportions of vertices on each side of the bipartition.","meta":{"url":"http://arxiv.org/abs/2307.13921v1"},"cats":{"new-dataset":0.2003503436,"dev-research":0.1626203566,"prompt-eng":0.3084733981,"data-quality":0.1998092106,"ml-security":0.1673790473}}
{"text":"In a bipartite graph it is trivial to find an independent set of density at least half (take one of the partition classes).","meta":{"url":"http://arxiv.org/abs/2307.13921v1"},"cats":{"new-dataset":0.0677901206,"dev-research":0.159494613,"prompt-eng":0.2842345447,"data-quality":0.1933140029,"ml-security":0.1051036581}}
{"text":"In contrast, in a random bipartite graph of average degree $d$, the largest balanced independent sets (containing equal number of vertices from each class) are typically of density $(2+o_d(1))","meta":{"url":"http://arxiv.org/abs/2307.13921v1"},"cats":{"new-dataset":0.0513009489,"dev-research":0.1767033647,"prompt-eng":0.2512454109,"data-quality":0.1326389319,"ml-security":0.1186385982}}
{"text":"\\frac{\\log d}{d}$. Can we find such large balanced independent sets in these graphs efficiently?","meta":{"url":"http://arxiv.org/abs/2307.13921v1"},"cats":{"new-dataset":0.1634941692,"dev-research":0.1687225754,"prompt-eng":0.3162241374,"data-quality":0.1400649109,"ml-security":0.1222925237}}
{"text":"By utilizing the overlap gap property and the low-degree algorithmic framework, we prove that local and low-degree algorithms (even those that know the bipartition) cannot find balanced independent sets of density greater than $(1+\\epsilon) \\frac{\\log d}{d}$ for any $\\epsilon>0$ fixed and $d$ large but constant.","meta":{"url":"http://arxiv.org/abs/2307.13921v1"},"cats":{"new-dataset":0.0555260466,"dev-research":0.1758350971,"prompt-eng":0.2967003027,"data-quality":0.2098492001,"ml-security":0.1453319774}}
{"text":"This factor $2$ statistical--computational gap between what exists and what local algorithms can achieve is analogous to the gap for finding large independent sets in (non-bipartite) random graphs.","meta":{"url":"http://arxiv.org/abs/2307.13921v1"},"cats":{"new-dataset":0.096800337,"dev-research":0.210241427,"prompt-eng":0.3125270725,"data-quality":0.1844588224,"ml-security":0.1021104074}}
{"text":"Our results therefor suggest that this gap is pervasive in many models, and that hard computational problems can lurk inside otherwise tractable ones.","meta":{"url":"http://arxiv.org/abs/2307.13921v1"},"cats":{"new-dataset":0.0376420668,"dev-research":0.2423212831,"prompt-eng":0.3769174751,"data-quality":0.1997684033,"ml-security":0.1402172898}}
{"text":"A particularly striking aspect of the gap in bipartite graphs is that the algorithm achieving the lower bound is extremely simple and can be implemented as a $1$-local algorithm and a degree-$1$ polynomial (a linear function).","meta":{"url":"http://arxiv.org/abs/2307.13921v1"},"cats":{"new-dataset":0.0235451806,"dev-research":0.2645808521,"prompt-eng":0.2788648179,"data-quality":0.1436194012,"ml-security":0.0922080068}}
{"text":"Bayesian causal discovery aims to infer the posterior distribution over causal models from observed data, quantifying epistemic uncertainty and benefiting downstream tasks.","meta":{"url":"http://arxiv.org/abs/2307.13917v1"},"cats":{"new-dataset":0.0388702082,"dev-research":0.2965010831,"prompt-eng":0.4273405415,"data-quality":0.307414257,"ml-security":0.1663132442}}
{"text":"However, computational challenges arise due to joint inference over combinatorial space of Directed Acyclic Graphs (DAGs) and nonlinear functions.","meta":{"url":"http://arxiv.org/abs/2307.13917v1"},"cats":{"new-dataset":0.0687615403,"dev-research":0.2763229925,"prompt-eng":0.3584270622,"data-quality":0.1465027316,"ml-security":0.1195567103}}
{"text":"Despite recent progress towards efficient posterior inference over DAGs, existing methods are either limited to variational inference on node permutation matrices for linear causal models, leading to compromised inference accuracy, or continuous relaxation of adjacency matrices constrained by a DAG regularizer, which cannot ensure resulting graphs are DAGs.","meta":{"url":"http://arxiv.org/abs/2307.13917v1"},"cats":{"new-dataset":0.0273312779,"dev-research":0.230314219,"prompt-eng":0.4249090879,"data-quality":0.2030022438,"ml-security":0.1948310229}}
{"text":"In this work, we introduce a scalable Bayesian causal discovery framework based on stochastic gradient Markov Chain Monte Carlo (SG-MCMC) that overcomes these limitations.","meta":{"url":"http://arxiv.org/abs/2307.13917v1"},"cats":{"new-dataset":0.1061040391,"dev-research":0.1987538352,"prompt-eng":0.3964677093,"data-quality":0.1446017279,"ml-security":0.1679030272}}
{"text":"Our approach directly samples DAGs from the posterior without requiring any DAG regularization, simultaneously draws function parameter samples and is applicable to both linear and nonlinear causal models.","meta":{"url":"http://arxiv.org/abs/2307.13917v1"},"cats":{"new-dataset":0.0490264836,"dev-research":0.2061104459,"prompt-eng":0.4808800764,"data-quality":0.2151277484,"ml-security":0.1761806068}}
{"text":"To enable our approach, we derive a novel equivalence to the permutation-based DAG learning, which opens up possibilities of using any relaxed gradient estimator defined over permutations.","meta":{"url":"http://arxiv.org/abs/2307.13917v1"},"cats":{"new-dataset":0.0629708919,"dev-research":0.1780597297,"prompt-eng":0.3768686532,"data-quality":0.2210991044,"ml-security":0.2099191299}}
{"text":"To our knowledge, this is the first framework applying gradient-based MCMC sampling for causal discovery.","meta":{"url":"http://arxiv.org/abs/2307.13917v1"},"cats":{"new-dataset":0.0488522311,"dev-research":0.1934244796,"prompt-eng":0.4055213402,"data-quality":0.182480928,"ml-security":0.1214982858}}
{"text":"Empirical evaluations on synthetic and real-world datasets demonstrate our approach's effectiveness compared to state-of-the-art baselines.","meta":{"url":"http://arxiv.org/abs/2307.13917v1"},"cats":{"new-dataset":0.6639714321,"dev-research":0.3104952184,"prompt-eng":0.3453350396,"data-quality":0.2530860752,"ml-security":0.166861848}}
{"text":"This article aims to describe and explain the theoretical foundations of concurrent and set concurrent algorithms, considering an asynchronous shared memory system where any number of processes can crash.","meta":{"url":"http://arxiv.org/abs/2307.13915v1"},"cats":{"new-dataset":0.1154286814,"dev-research":0.3000804143,"prompt-eng":0.3683700242,"data-quality":0.1310306923,"ml-security":0.1913554902}}
{"text":"Verification of concurrent algorithms is often described in terms of their progress condition, which guarantees that eventually something good will happen, also called the security of the algorithms, and correctness, which guarantees that nothing bad will happen, also called liveliness.","meta":{"url":"http://arxiv.org/abs/2307.13915v1"},"cats":{"new-dataset":0.0275222914,"dev-research":0.3686631678,"prompt-eng":0.3766802588,"data-quality":0.1791927251,"ml-security":0.2854078099}}
{"text":"of the algorithms.","meta":{"url":"http://arxiv.org/abs/2307.13915v1"},"cats":{"new-dataset":0.086783123,"dev-research":0.2819927204,"prompt-eng":0.3317295061,"data-quality":0.1445676295,"ml-security":0.1347664775}}
{"text":"The meaning of correctness of a concurrent algorithm is explained in detail, focusing on linearizability, and a generalization is addressed, concurrency by sets; which is much more recent and less well known.","meta":{"url":"http://arxiv.org/abs/2307.13915v1"},"cats":{"new-dataset":0.0163679367,"dev-research":0.3676530246,"prompt-eng":0.3500998693,"data-quality":0.2241376223,"ml-security":0.1386813297}}
{"text":"The {\\it SetStackLogic} algorithm is shown, which is a set-concurrent algorithm and is also an implementation of a stack with multiplicity.","meta":{"url":"http://arxiv.org/abs/2307.13915v1"},"cats":{"new-dataset":0.22243377,"dev-research":0.3272832212,"prompt-eng":0.4172819609,"data-quality":0.1283620114,"ml-security":0.0927922596}}
{"text":"The properties of the algorithm {\\it SetStackLogic} are demonstrated in a formal and detailed way, in order to present a rigorous scheme in the formalization of this type of algorithm; same that could be used for other algorithms.","meta":{"url":"http://arxiv.org/abs/2307.13915v1"},"cats":{"new-dataset":0.0789886799,"dev-research":0.3187434981,"prompt-eng":0.4340238229,"data-quality":0.1866883498,"ml-security":0.0879341448}}
{"text":"In addition, the operation of the algorithm is explained through scenario examples that illustrate its dynamics in some possible executions.","meta":{"url":"http://arxiv.org/abs/2307.13915v1"},"cats":{"new-dataset":0.0152731371,"dev-research":0.3610202231,"prompt-eng":0.3845929723,"data-quality":0.1042491234,"ml-security":0.1565632773}}
{"text":"Can we design artificial intelligence (AI) systems that rank our social media feeds to consider democratic values such as mitigating partisan animosity as part of their objective functions?","meta":{"url":"http://arxiv.org/abs/2307.13912v1"},"cats":{"new-dataset":0.0283173099,"dev-research":0.2240386103,"prompt-eng":0.3686179005,"data-quality":0.2022570751,"ml-security":0.2811678321}}
{"text":"We introduce a method for translating established, vetted social scientific constructs into AI objective functions, which we term societal objective functions, and demonstrate the method with application to the political science construct of anti-democratic attitudes.","meta":{"url":"http://arxiv.org/abs/2307.13912v1"},"cats":{"new-dataset":0.0601715981,"dev-research":0.2781508624,"prompt-eng":0.3405288652,"data-quality":0.1570612266,"ml-security":0.2377818027}}
{"text":"Traditionally, we have lacked observable outcomes to use to train such models, however, the social sciences have developed survey instruments and qualitative codebooks for these constructs, and their precision facilitates translation into detailed prompts for large language models.","meta":{"url":"http://arxiv.org/abs/2307.13912v1"},"cats":{"new-dataset":0.2048024771,"dev-research":0.2575455529,"prompt-eng":0.3793287692,"data-quality":0.2452439094,"ml-security":0.092289122}}
{"text":"We apply this method to create a democratic attitude model that estimates the extent to which a social media post promotes anti-democratic attitudes, and test this democratic attitude model across three studies.","meta":{"url":"http://arxiv.org/abs/2307.13912v1"},"cats":{"new-dataset":0.0360722444,"dev-research":0.1973337412,"prompt-eng":0.3427244078,"data-quality":0.1397578755,"ml-security":0.1657341925}}
{"text":"In Study 1, we first test the attitudinal and behavioral effectiveness of the intervention among US partisans (N=1,380) by manually annotating (alpha=.895) social media posts with anti-democratic attitude scores and testing several feed ranking conditions based on these scores.","meta":{"url":"http://arxiv.org/abs/2307.13912v1"},"cats":{"new-dataset":0.0520909806,"dev-research":0.2268693216,"prompt-eng":0.4356738139,"data-quality":0.2589604626,"ml-security":0.1109639101}}
{"text":"Removal (d=.20) and downranking feeds (d=.25) reduced participants' partisan animosity without compromising their experience and engagement.","meta":{"url":"http://arxiv.org/abs/2307.13912v1"},"cats":{"new-dataset":0.0125059216,"dev-research":0.243511341,"prompt-eng":0.3441040835,"data-quality":0.2322672108,"ml-security":0.185234658}}
{"text":"In Study 2, we scale up the manual labels by creating the democratic attitude model, finding strong agreement with manual labels (rho=.75).","meta":{"url":"http://arxiv.org/abs/2307.13912v1"},"cats":{"new-dataset":0.2853360733,"dev-research":0.2069880514,"prompt-eng":0.4385402743,"data-quality":0.3746641081,"ml-security":0.0754726351}}
{"text":"Finally, in Study 3, we replicate Study 1 using the democratic attitude model instead of manual labels to test its attitudinal and behavioral impact (N=558), and again find that the feed downranking using the societal objective function reduced partisan animosity (d=.25).","meta":{"url":"http://arxiv.org/abs/2307.13912v1"},"cats":{"new-dataset":0.0658777277,"dev-research":0.2019278865,"prompt-eng":0.3899582734,"data-quality":0.2427800605,"ml-security":0.1552477422}}
{"text":"This method presents a novel strategy to draw on social science theory and methods to mitigate societal harms in social media AIs.","meta":{"url":"http://arxiv.org/abs/2307.13912v1"},"cats":{"new-dataset":0.0217549011,"dev-research":0.3173679907,"prompt-eng":0.3244037603,"data-quality":0.2414807563,"ml-security":0.4534612452}}
{"text":"The conventional single-target Cross-Domain Recommendation (CDR) aims to improve the recommendation performance on a sparser target domain by transferring the knowledge from a source domain that contains relatively richer information.","meta":{"url":"http://arxiv.org/abs/2307.13910v1"},"cats":{"new-dataset":0.0192904151,"dev-research":0.2692206686,"prompt-eng":0.4386417543,"data-quality":0.1628503323,"ml-security":0.1101477955}}
{"text":"By contrast, in recent years, dual-target CDR has been proposed to improve the recommendation performance on both domains simultaneously.","meta":{"url":"http://arxiv.org/abs/2307.13910v1"},"cats":{"new-dataset":0.0102545443,"dev-research":0.2692726667,"prompt-eng":0.4734867615,"data-quality":0.1716507527,"ml-security":0.0647973349}}
{"text":"However, to this end, there are two challenges in dual-target CDR: (1) how to generate both relevant and diverse augmented user representations, and (2) how to effectively decouple domain-independent information from domain-specific information, in addition to domain-shared information, to capture comprehensive user preferences.","meta":{"url":"http://arxiv.org/abs/2307.13910v1"},"cats":{"new-dataset":0.02927374,"dev-research":0.3085089531,"prompt-eng":0.5151079109,"data-quality":0.1993699249,"ml-security":0.1360602324}}
{"text":"To address the above two challenges, we propose a Disentanglement-based framework with Interpolative Data Augmentation for dual-target Cross-Domain Recommendation, called DIDA-CDR.","meta":{"url":"http://arxiv.org/abs/2307.13910v1"},"cats":{"new-dataset":0.0583969596,"dev-research":0.2443704239,"prompt-eng":0.4256167531,"data-quality":0.2170311315,"ml-security":0.1233857947}}
{"text":"In DIDA-CDR, we first propose an interpolative data augmentation approach to generating both relevant and diverse augmented user representations to augment sparser domain and explore potential user preferences.","meta":{"url":"http://arxiv.org/abs/2307.13910v1"},"cats":{"new-dataset":0.1214249004,"dev-research":0.261069115,"prompt-eng":0.4686569752,"data-quality":0.1539639045,"ml-security":0.0790389941}}
{"text":"We then propose a disentanglement module to effectively decouple domain-specific and domain-independent information to capture comprehensive user preferences.","meta":{"url":"http://arxiv.org/abs/2307.13910v1"},"cats":{"new-dataset":0.0587712124,"dev-research":0.3822150342,"prompt-eng":0.5235435136,"data-quality":0.2022973045,"ml-security":0.1794886623}}
{"text":"Both steps significantly contribute to capturing more comprehensive user preferences, thereby improving the recommendation performance on each domain.","meta":{"url":"http://arxiv.org/abs/2307.13910v1"},"cats":{"new-dataset":0.0105525609,"dev-research":0.3585210162,"prompt-eng":0.4695627219,"data-quality":0.1124558626,"ml-security":0.0691569084}}
{"text":"Extensive experiments conducted on five real-world datasets show the significant superiority of DIDA-CDR over the state-of-the-art methods.","meta":{"url":"http://arxiv.org/abs/2307.13910v1"},"cats":{"new-dataset":0.1319112437,"dev-research":0.2789545578,"prompt-eng":0.3943076835,"data-quality":0.1840880762,"ml-security":0.0866410748}}
{"text":"Graph Neural Networks have emerged as an effective machine learning tool for multi-disciplinary tasks such as pharmaceutical molecule classification and chemical reaction prediction, because they can model non-euclidean relationships between different entities.","meta":{"url":"http://arxiv.org/abs/2307.13909v1"},"cats":{"new-dataset":0.0391153758,"dev-research":0.2174730704,"prompt-eng":0.2834906839,"data-quality":0.2392459694,"ml-security":0.1811433302}}
{"text":"Particle crushing, as a significant field of civil engineering, describes the breakage of granular materials caused by the breakage of particle fragment bonds under the modeling of numerical simulations, which motivates us to characterize the mechanical behaviors of particle crushing through the connectivity of particle fragments with Graph Neural Networks (GNNs).","meta":{"url":"http://arxiv.org/abs/2307.13909v1"},"cats":{"new-dataset":0.0785272045,"dev-research":0.2701819102,"prompt-eng":0.3178506282,"data-quality":0.2083059724,"ml-security":0.2158005903}}
{"text":"However, there lacks an open-source large-scale particle crushing dataset for research due to the expensive costs of laboratory tests or numerical simulations.","meta":{"url":"http://arxiv.org/abs/2307.13909v1"},"cats":{"new-dataset":0.4559894121,"dev-research":0.1854713179,"prompt-eng":0.3362238688,"data-quality":0.1274153579,"ml-security":0.1385736041}}
{"text":"Therefore, we firstly generate a dataset with 45,000 numerical simulations and 900 particle types to facilitate the research progress of machine learning for particle crushing.","meta":{"url":"http://arxiv.org/abs/2307.13909v1"},"cats":{"new-dataset":0.4284950994,"dev-research":0.2130109357,"prompt-eng":0.3866508739,"data-quality":0.1727820285,"ml-security":0.2288613704}}
{"text":"Secondly, we devise a hybrid framework based on GNNs to predict particle crushing strength in a particle fragment view with the advances of state of the art GNNs.","meta":{"url":"http://arxiv.org/abs/2307.13909v1"},"cats":{"new-dataset":0.1316909277,"dev-research":0.2142833572,"prompt-eng":0.3893651633,"data-quality":0.1799231375,"ml-security":0.1871961}}
{"text":"Finally, we compare our hybrid framework against traditional machine learning methods and the plain MLP to verify its effectiveness.","meta":{"url":"http://arxiv.org/abs/2307.13909v1"},"cats":{"new-dataset":0.0802835675,"dev-research":0.2354803896,"prompt-eng":0.4208960577,"data-quality":0.2669328955,"ml-security":0.221089158}}
{"text":"The usefulness of different features is further discussed through the gradient attribution explanation w.r.t the predictions.","meta":{"url":"http://arxiv.org/abs/2307.13909v1"},"cats":{"new-dataset":0.0183551153,"dev-research":0.2888120597,"prompt-eng":0.396603431,"data-quality":0.2216560073,"ml-security":0.2246439145}}
{"text":"Our data and code are released at https://github.com/doujiang-zheng/GNN-For-Particle-Crushing.","meta":{"url":"http://arxiv.org/abs/2307.13909v1"},"cats":{"new-dataset":0.4889075922,"dev-research":0.2390356313,"prompt-eng":0.4062717546,"data-quality":0.1564597979,"ml-security":0.106178933}}
{"text":"Text-to-3D generation has recently garnered significant attention, fueled by 2D diffusion models trained on billions of image-text pairs.","meta":{"url":"http://arxiv.org/abs/2307.13908v1"},"cats":{"new-dataset":0.299597062,"dev-research":0.2089135591,"prompt-eng":0.363114539,"data-quality":0.1504977606,"ml-security":0.0909194249}}
{"text":"Existing methods primarily rely on score distillation to leverage the 2D diffusion priors to supervise the generation of 3D models, e.g., NeRF.","meta":{"url":"http://arxiv.org/abs/2307.13908v1"},"cats":{"new-dataset":0.0279312292,"dev-research":0.2701826489,"prompt-eng":0.405650969,"data-quality":0.1184460076,"ml-security":0.1180698136}}
{"text":"However, score distillation is prone to suffer the view inconsistency problem, and implicit NeRF modeling can also lead to an arbitrary shape, thus leading to less realistic and uncontrollable 3D generation.","meta":{"url":"http://arxiv.org/abs/2307.13908v1"},"cats":{"new-dataset":0.0062662062,"dev-research":0.3704814339,"prompt-eng":0.3700530961,"data-quality":0.2241296413,"ml-security":0.1900509924}}
{"text":"In this work, we propose a flexible framework of Points-to-3D to bridge the gap between sparse yet freely available 3D points and realistic shape-controllable 3D generation by distilling the knowledge from both 2D and 3D diffusion models.","meta":{"url":"http://arxiv.org/abs/2307.13908v1"},"cats":{"new-dataset":0.1590138614,"dev-research":0.2092587702,"prompt-eng":0.3754684972,"data-quality":0.0727259254,"ml-security":0.0959159835}}
{"text":"The core idea of Points-to-3D is to introduce controllable sparse 3D points to guide the text-to-3D generation.","meta":{"url":"http://arxiv.org/abs/2307.13908v1"},"cats":{"new-dataset":0.0707469108,"dev-research":0.2955272908,"prompt-eng":0.3976420806,"data-quality":0.132333033,"ml-security":0.0633393993}}
{"text":"Specifically, we use the sparse point cloud generated from the 3D diffusion model, Point-E, as the geometric prior, conditioned on a single reference image.","meta":{"url":"http://arxiv.org/abs/2307.13908v1"},"cats":{"new-dataset":0.0698573762,"dev-research":0.1701498664,"prompt-eng":0.392841678,"data-quality":0.0969986676,"ml-security":0.0805751745}}
{"text":"To better utilize the sparse 3D points, we propose an efficient point cloud guidance loss to adaptively drive the NeRF's geometry to align with the shape of the sparse 3D points.","meta":{"url":"http://arxiv.org/abs/2307.13908v1"},"cats":{"new-dataset":0.0351939315,"dev-research":0.2440562525,"prompt-eng":0.3980321046,"data-quality":0.1754765551,"ml-security":0.1422179161}}
{"text":"In addition to controlling the geometry, we propose to optimize the NeRF for a more view-consistent appearance.","meta":{"url":"http://arxiv.org/abs/2307.13908v1"},"cats":{"new-dataset":0.030140837,"dev-research":0.2863746722,"prompt-eng":0.4402274324,"data-quality":0.1947051133,"ml-security":0.1329303515}}
{"text":"To be specific, we perform score distillation to the publicly available 2D image diffusion model ControlNet, conditioned on text as well as depth map of the learned compact geometry.","meta":{"url":"http://arxiv.org/abs/2307.13908v1"},"cats":{"new-dataset":0.2167843818,"dev-research":0.1818278913,"prompt-eng":0.3945548671,"data-quality":0.1415000023,"ml-security":0.1299225947}}
{"text":"Qualitative and quantitative comparisons demonstrate that Points-to-3D improves view consistency and achieves good shape controllability for text-to-3D generation.","meta":{"url":"http://arxiv.org/abs/2307.13908v1"},"cats":{"new-dataset":0.0448664297,"dev-research":0.3458792027,"prompt-eng":0.4084471108,"data-quality":0.1735778649,"ml-security":0.0575290942}}
{"text":"Points-to-3D provides users with a new way to improve and control text-to-3D generation.","meta":{"url":"http://arxiv.org/abs/2307.13908v1"},"cats":{"new-dataset":0.1103760324,"dev-research":0.3777791229,"prompt-eng":0.4087104367,"data-quality":0.1261679712,"ml-security":0.0604340553}}
{"text":"Data-driven, neural network (NN) based anomaly detection and predictive maintenance are emerging research areas.","meta":{"url":"http://arxiv.org/abs/2307.13907v1"},"cats":{"new-dataset":0.2331298805,"dev-research":0.320822409,"prompt-eng":0.3330391478,"data-quality":0.290463322,"ml-security":0.4597595564}}
{"text":"NN-based analytics of time-series data offer valuable insights into past behaviors and estimates of critical parameters like remaining useful life (RUL) of equipment and state-of-charge (SOC) of batteries.","meta":{"url":"http://arxiv.org/abs/2307.13907v1"},"cats":{"new-dataset":0.2264803523,"dev-research":0.2192806847,"prompt-eng":0.3364248702,"data-quality":0.1378642306,"ml-security":0.0958720712}}
{"text":"However, input time series data can be exposed to intentional or unintentional noise when passing through sensors, necessitating robust validation and verification of these NNs.","meta":{"url":"http://arxiv.org/abs/2307.13907v1"},"cats":{"new-dataset":0.0717114503,"dev-research":0.2781195991,"prompt-eng":0.4177761294,"data-quality":0.3428713851,"ml-security":0.3780244343}}
{"text":"This paper presents a case study of the robustness verification approach for time series regression NNs (TSRegNN) using set-based formal methods.","meta":{"url":"http://arxiv.org/abs/2307.13907v1"},"cats":{"new-dataset":0.0778912256,"dev-research":0.2922003757,"prompt-eng":0.402248464,"data-quality":0.2715939142,"ml-security":0.2270920566}}
{"text":"It focuses on utilizing variable-length input data to streamline input manipulation and enhance network architecture generalizability.","meta":{"url":"http://arxiv.org/abs/2307.13907v1"},"cats":{"new-dataset":0.1422587087,"dev-research":0.3408346826,"prompt-eng":0.3942910085,"data-quality":0.1344218544,"ml-security":0.1378278104}}
{"text":"The method is applied to two data sets in the Prognostics and Health Management (PHM) application areas: (1) SOC estimation of a Lithium-ion battery and (2) RUL estimation of a turbine engine.","meta":{"url":"http://arxiv.org/abs/2307.13907v1"},"cats":{"new-dataset":0.118517747,"dev-research":0.2409778301,"prompt-eng":0.4009264641,"data-quality":0.1372796889,"ml-security":0.0887985399}}
{"text":"The NNs' robustness is checked using star-based reachability analysis, and several performance measures evaluate the effect of bounded perturbations in the input on network outputs, i.e., future outcomes.","meta":{"url":"http://arxiv.org/abs/2307.13907v1"},"cats":{"new-dataset":0.0150599313,"dev-research":0.2533672366,"prompt-eng":0.4052713657,"data-quality":0.2351603069,"ml-security":0.2115016508}}
{"text":"Overall, the paper offers a comprehensive case study for validating and verifying NN-based analytics of time-series data in real-world applications, emphasizing the importance of robustness testing for accurate and reliable predictions, especially considering the impact of noise on future outcomes.","meta":{"url":"http://arxiv.org/abs/2307.13907v1"},"cats":{"new-dataset":0.1012210119,"dev-research":0.2884377608,"prompt-eng":0.3757688881,"data-quality":0.3123174445,"ml-security":0.2248468535}}
{"text":"In this work, we propose reinforcement learning (RL) for sequential decoding of moderate length generalized low-density parity-check (GLDPC) codes.","meta":{"url":"http://arxiv.org/abs/2307.13905v1"},"cats":{"new-dataset":0.1203440489,"dev-research":0.1910638656,"prompt-eng":0.3856503246,"data-quality":0.1592930157,"ml-security":0.1028294312}}
{"text":"Here, sequential decoding refers to scheduling all the generalized constraint nodes (GCNs) and single parity-check nodes (SPCNs) of a GLDPC code serially in each iteration.","meta":{"url":"http://arxiv.org/abs/2307.13905v1"},"cats":{"new-dataset":0.0921088744,"dev-research":0.2684322524,"prompt-eng":0.3903452608,"data-quality":0.1192714004,"ml-security":0.0827018029}}
{"text":"A GLDPC decoding environment is modeled as a finite Markov decision process (MDP) in which the state-space comprises of all possible sequences of hard-decision values of the variables nodes (VNs) connected to the scheduled GCN or SPCN, and the action-space of the MDP consists of all possible actions (GCN and SPCN scheduling).","meta":{"url":"http://arxiv.org/abs/2307.13905v1"},"cats":{"new-dataset":0.0624469094,"dev-research":0.2219283002,"prompt-eng":0.4023158385,"data-quality":0.1132559083,"ml-security":0.1209179446}}
{"text":"The goal of RL is to determine an optimized scheduling policy, i.e., one that results in a decoded codeword by minimizing the complexity of the belief propagation (BP) decoder.","meta":{"url":"http://arxiv.org/abs/2307.13905v1"},"cats":{"new-dataset":0.0272094037,"dev-research":0.3326074805,"prompt-eng":0.4081518493,"data-quality":0.1326566821,"ml-security":0.1096600005}}
{"text":"For training, we consider the proportion of correct bits at the output of the GCN or SPCN as a reward once it is scheduled.","meta":{"url":"http://arxiv.org/abs/2307.13905v1"},"cats":{"new-dataset":0.0639735596,"dev-research":0.2173143112,"prompt-eng":0.4374106968,"data-quality":0.1655739941,"ml-security":0.1369244868}}
{"text":"The expected rewards for scheduling all the GCNs/SPCNs in the code's Tanner graph are earned via BP decoding during the RL phase.","meta":{"url":"http://arxiv.org/abs/2307.13905v1"},"cats":{"new-dataset":0.0809919128,"dev-research":0.2284999948,"prompt-eng":0.3927466529,"data-quality":0.1426148743,"ml-security":0.1049130797}}
{"text":"The proposed RL-based decoding scheme is shown to significantly outperform the standard BP flooding decoder, as well as a sequential decoder in which the GCNs/SPCNs are scheduled randomly.","meta":{"url":"http://arxiv.org/abs/2307.13905v1"},"cats":{"new-dataset":0.1857451315,"dev-research":0.2493361048,"prompt-eng":0.4214277786,"data-quality":0.1673051761,"ml-security":0.1366279673}}
{"text":"I study the problem of learning a Lipschitz function with corrupted binary signals.","meta":{"url":"http://arxiv.org/abs/2307.13903v1"},"cats":{"new-dataset":0.0839787286,"dev-research":0.2116668475,"prompt-eng":0.3718106848,"data-quality":0.3889982482,"ml-security":0.2636115488}}
{"text":"The learner tries to learn a Lipschitz function $f$ that the adversary chooses.","meta":{"url":"http://arxiv.org/abs/2307.13903v1"},"cats":{"new-dataset":0.0243559492,"dev-research":0.2583907707,"prompt-eng":0.302578237,"data-quality":0.2559312824,"ml-security":0.5570514138}}
{"text":"In each round, the adversary selects a context vector $x_t$ in the input space, and the learner makes a guess to the true function value $f(x_t)$ and receives a binary signal indicating whether the guess was high or low.","meta":{"url":"http://arxiv.org/abs/2307.13903v1"},"cats":{"new-dataset":0.0073067951,"dev-research":0.2745777245,"prompt-eng":0.4165010604,"data-quality":0.2196687032,"ml-security":0.5654637073}}
{"text":"In a total of $C$ rounds, the signal may be corrupted, though the value of $C$ is unknown to the learner.","meta":{"url":"http://arxiv.org/abs/2307.13903v1"},"cats":{"new-dataset":0.0257602937,"dev-research":0.2688250688,"prompt-eng":0.376903714,"data-quality":0.4372209199,"ml-security":0.3682629344}}
{"text":"The learner's goal is to incur a small cumulative loss.","meta":{"url":"http://arxiv.org/abs/2307.13903v1"},"cats":{"new-dataset":0.01004589,"dev-research":0.320107118,"prompt-eng":0.3382082309,"data-quality":0.2200425053,"ml-security":0.2381300742}}
{"text":"I present a natural yet powerful technique sanity check, which proves useful in designing corruption-robust algorithms.","meta":{"url":"http://arxiv.org/abs/2307.13903v1"},"cats":{"new-dataset":0.0858960099,"dev-research":0.4145927501,"prompt-eng":0.4653712477,"data-quality":0.4457410158,"ml-security":0.37288378}}
{"text":"I design algorithms which (treating the Lipschitz parameter $L$ as constant): for the symmetric loss, the learner achieves regret $O(C\\log T)$ with $d = 1$ and $O_d(C\\log T + T^{(d-1)/d})$ with $d > 1$; for the pricing loss the learner achieves regret $\\widetilde{O} (T^{d/(d+1)} + C\\cdot T^{1/(d+1)})$.","meta":{"url":"http://arxiv.org/abs/2307.13903v1"},"cats":{"new-dataset":0.00519041,"dev-research":0.2280639148,"prompt-eng":0.3120325584,"data-quality":0.1882202332,"ml-security":0.3583759751}}
{"text":"We present YOLOBench, a benchmark comprised of 550+ YOLO-based object detection models on 4 different datasets and 4 different embedded hardware platforms (x86 CPU, ARM CPU, Nvidia GPU, NPU).","meta":{"url":"http://arxiv.org/abs/2307.13901v1"},"cats":{"new-dataset":0.4286279093,"dev-research":0.2336006352,"prompt-eng":0.3924856786,"data-quality":0.1885276993,"ml-security":0.1849095136}}
{"text":"We collect accuracy and latency numbers for a variety of YOLO-based one-stage detectors at different model scales by performing a fair, controlled comparison of these detectors with a fixed training environment (code and training hyperparameters).","meta":{"url":"http://arxiv.org/abs/2307.13901v1"},"cats":{"new-dataset":0.1643247524,"dev-research":0.2051140222,"prompt-eng":0.4823549483,"data-quality":0.29673362,"ml-security":0.1684980092}}
{"text":"Pareto-optimality analysis of the collected data reveals that, if modern detection heads and training techniques are incorporated into the learning process, multiple architectures of the YOLO series achieve a good accuracy-latency trade-off, including older models like YOLOv3 and YOLOv4.","meta":{"url":"http://arxiv.org/abs/2307.13901v1"},"cats":{"new-dataset":0.0742039161,"dev-research":0.1923476396,"prompt-eng":0.4129828578,"data-quality":0.2059943243,"ml-security":0.2031841628}}
{"text":"We also evaluate training-free accuracy estimators used in neural architecture search on YOLOBench and demonstrate that, while most state-of-the-art zero-cost accuracy estimators are outperformed by a simple baseline like MAC count, some of them can be effectively used to predict Pareto-optimal detection models.","meta":{"url":"http://arxiv.org/abs/2307.13901v1"},"cats":{"new-dataset":0.0590813957,"dev-research":0.1976267175,"prompt-eng":0.4291701899,"data-quality":0.2819914549,"ml-security":0.372778448}}
{"text":"We showcase that by using a zero-cost proxy to identify a YOLO architecture competitive against a state-of-the-art YOLOv8 model on a Raspberry Pi 4 CPU.","meta":{"url":"http://arxiv.org/abs/2307.13901v1"},"cats":{"new-dataset":0.1807706059,"dev-research":0.2405290599,"prompt-eng":0.4178438535,"data-quality":0.1005529297,"ml-security":0.1653486434}}
{"text":"The code and data are available at https://github.com/Deeplite/deeplite-torch-zoo","meta":{"url":"http://arxiv.org/abs/2307.13901v1"},"cats":{"new-dataset":0.7050232902,"dev-research":0.2437104911,"prompt-eng":0.4099843148,"data-quality":0.1427751405,"ml-security":0.0677883563}}
{"text":"We present FinTree, Financial Dataset Pretrain Transformer Encoder for Relation Extraction.","meta":{"url":"http://arxiv.org/abs/2307.13900v1"},"cats":{"new-dataset":0.442250815,"dev-research":0.2263261862,"prompt-eng":0.3786268732,"data-quality":0.1918512025,"ml-security":0.0811519267}}
{"text":"Utilizing an encoder language model, we further pretrain FinTree on the financial dataset, adapting the model in financial domain tasks.","meta":{"url":"http://arxiv.org/abs/2307.13900v1"},"cats":{"new-dataset":0.2509872269,"dev-research":0.2116768362,"prompt-eng":0.4223229364,"data-quality":0.1995338666,"ml-security":0.1024676034}}
{"text":"FinTree stands out with its novel structure that predicts a masked token instead of the conventional [CLS] token, inspired by the Pattern Exploiting Training methodology.","meta":{"url":"http://arxiv.org/abs/2307.13900v1"},"cats":{"new-dataset":0.0628403823,"dev-research":0.2501447583,"prompt-eng":0.4571156019,"data-quality":0.2382024752,"ml-security":0.3116830811}}
{"text":"This structure allows for more accurate relation predictions between two given entities.","meta":{"url":"http://arxiv.org/abs/2307.13900v1"},"cats":{"new-dataset":0.0398772136,"dev-research":0.2702155563,"prompt-eng":0.4464814475,"data-quality":0.1345618628,"ml-security":0.0578394066}}
{"text":"The model is trained with a unique input pattern to provide contextual and positional information about the entities of interest, and a post-processing step ensures accurate predictions in line with the entity types.","meta":{"url":"http://arxiv.org/abs/2307.13900v1"},"cats":{"new-dataset":0.0369225947,"dev-research":0.2530987708,"prompt-eng":0.4577868409,"data-quality":0.1339639758,"ml-security":0.09212819}}
{"text":"Our experiments demonstrate that FinTree outperforms on the REFinD, a large-scale financial relation extraction dataset.","meta":{"url":"http://arxiv.org/abs/2307.13900v1"},"cats":{"new-dataset":0.2440745498,"dev-research":0.1901569094,"prompt-eng":0.3514590643,"data-quality":0.1939923835,"ml-security":0.0830371484}}
{"text":"The code and pretrained models are available at https://github.com/HJ-Ok/FinTree.","meta":{"url":"http://arxiv.org/abs/2307.13900v1"},"cats":{"new-dataset":0.3772789678,"dev-research":0.2443078993,"prompt-eng":0.4608491528,"data-quality":0.1096580147,"ml-security":0.0757751083}}
{"text":"This paper investigates methods for improving generative data augmentation for deep learning.","meta":{"url":"http://arxiv.org/abs/2307.13899v1"},"cats":{"new-dataset":0.0697645426,"dev-research":0.2813131619,"prompt-eng":0.4077322518,"data-quality":0.317046934,"ml-security":0.1280644844}}
{"text":"Generative data augmentation leverages the synthetic samples produced by generative models as an additional dataset for classification with small dataset settings.","meta":{"url":"http://arxiv.org/abs/2307.13899v1"},"cats":{"new-dataset":0.2697925726,"dev-research":0.2599828837,"prompt-eng":0.3996542668,"data-quality":0.2849128177,"ml-security":0.1494495038}}
{"text":"A key challenge of generative data augmentation is that the synthetic data contain uninformative samples that degrade accuracy.","meta":{"url":"http://arxiv.org/abs/2307.13899v1"},"cats":{"new-dataset":0.2022710723,"dev-research":0.2867304525,"prompt-eng":0.403347557,"data-quality":0.4197365782,"ml-security":0.153038499}}
{"text":"This is because the synthetic samples do not perfectly represent class categories in real data and uniform sampling does not necessarily provide useful samples for tasks.","meta":{"url":"http://arxiv.org/abs/2307.13899v1"},"cats":{"new-dataset":0.1222507048,"dev-research":0.2267731263,"prompt-eng":0.3041648348,"data-quality":0.2785838594,"ml-security":0.1458400579}}
{"text":"In this paper, we present a novel strategy for generative data augmentation called meta generative regularization (MGR).","meta":{"url":"http://arxiv.org/abs/2307.13899v1"},"cats":{"new-dataset":0.0922275982,"dev-research":0.2589935085,"prompt-eng":0.4700285704,"data-quality":0.304349945,"ml-security":0.0952562208}}
{"text":"To avoid the degradation of generative data augmentation, MGR utilizes synthetic samples in the regularization term for feature extractors instead of in the loss function, e.g., cross-entropy.","meta":{"url":"http://arxiv.org/abs/2307.13899v1"},"cats":{"new-dataset":0.033206411,"dev-research":0.2260349422,"prompt-eng":0.4112788406,"data-quality":0.2733139156,"ml-security":0.1046945642}}
{"text":"These synthetic samples are dynamically determined to minimize the validation losses through meta-learning.","meta":{"url":"http://arxiv.org/abs/2307.13899v1"},"cats":{"new-dataset":0.0605078245,"dev-research":0.246515343,"prompt-eng":0.4027386926,"data-quality":0.285073927,"ml-security":0.2496465138}}
{"text":"We observed that MGR can avoid the performance degradation of na\\\"ive generative data augmentation and boost the baselines.","meta":{"url":"http://arxiv.org/abs/2307.13899v1"},"cats":{"new-dataset":0.0606082392,"dev-research":0.2531495525,"prompt-eng":0.4426292023,"data-quality":0.2207282303,"ml-security":0.0867330104}}
{"text":"Experiments on six datasets showed that MGR is effective particularly when datasets are smaller and stably outperforms baselines.","meta":{"url":"http://arxiv.org/abs/2307.13899v1"},"cats":{"new-dataset":0.0637465616,"dev-research":0.2657454107,"prompt-eng":0.4066839538,"data-quality":0.2495214675,"ml-security":0.0961736333}}
{"text":"Skin lesion segmentation (SLS) plays an important role in skin lesion analysis.","meta":{"url":"http://arxiv.org/abs/2307.13897v1"},"cats":{"new-dataset":0.035287924,"dev-research":0.277331823,"prompt-eng":0.3026756143,"data-quality":0.1630708655,"ml-security":0.1125427644}}
{"text":"Vision transformers (ViTs) are considered an auspicious solution for SLS, but they require more training data compared to convolutional neural networks (CNNs) due to their inherent parameter-heavy structure and lack of some inductive biases.","meta":{"url":"http://arxiv.org/abs/2307.13897v1"},"cats":{"new-dataset":0.0363315708,"dev-research":0.2267840611,"prompt-eng":0.3493445919,"data-quality":0.1339400628,"ml-security":0.1754419362}}
{"text":"To alleviate this issue, current approaches fine-tune pre-trained ViT backbones on SLS datasets, aiming to leverage the knowledge learned from a larger set of natural images to lower the amount of skin training data needed.","meta":{"url":"http://arxiv.org/abs/2307.13897v1"},"cats":{"new-dataset":0.3880750545,"dev-research":0.2359745187,"prompt-eng":0.4146920738,"data-quality":0.1903342002,"ml-security":0.191956996}}
{"text":"However, fully fine-tuning all parameters of large backbones is computationally expensive and memory intensive.","meta":{"url":"http://arxiv.org/abs/2307.13897v1"},"cats":{"new-dataset":0.024127499,"dev-research":0.2420116166,"prompt-eng":0.4173860112,"data-quality":0.080270228,"ml-security":0.1292553712}}
{"text":"In this paper, we propose AViT, a novel efficient strategy to mitigate ViTs' data-hunger by transferring any pre-trained ViTs to the SLS task.","meta":{"url":"http://arxiv.org/abs/2307.13897v1"},"cats":{"new-dataset":0.1043278924,"dev-research":0.2690788874,"prompt-eng":0.4625823045,"data-quality":0.184205492,"ml-security":0.2008337445}}
{"text":"Specifically, we integrate lightweight modules (adapters) within the transformer layers, which modulate the feature representation of a ViT without updating its pre-trained weights.","meta":{"url":"http://arxiv.org/abs/2307.13897v1"},"cats":{"new-dataset":0.0257894893,"dev-research":0.2854560154,"prompt-eng":0.4534602298,"data-quality":0.139592544,"ml-security":0.1677645535}}
{"text":"In addition, we employ a shallow CNN as a prompt generator to create a prompt embedding from the input image, which grasps fine-grained information and CNN's inductive biases to guide the segmentation task on small datasets.","meta":{"url":"http://arxiv.org/abs/2307.13897v1"},"cats":{"new-dataset":0.3808411843,"dev-research":0.2188951529,"prompt-eng":0.5111677502,"data-quality":0.2604109707,"ml-security":0.1743301051}}
{"text":"Our quantitative experiments on 4 skin lesion datasets demonstrate that AViT achieves competitive, and at times superior, performance to SOTA but with significantly fewer trainable parameters.","meta":{"url":"http://arxiv.org/abs/2307.13897v1"},"cats":{"new-dataset":0.1527965645,"dev-research":0.2346005095,"prompt-eng":0.372940668,"data-quality":0.1389782534,"ml-security":0.1712393838}}
{"text":"Our code is available at https://github.com/siyi-wind/AViT.","meta":{"url":"http://arxiv.org/abs/2307.13897v1"},"cats":{"new-dataset":0.3721971467,"dev-research":0.2776015219,"prompt-eng":0.4485557915,"data-quality":0.1436057946,"ml-security":0.0687960441}}
{"text":"We study few-shot Natural Language Understanding (NLU) tasks with Large Language Models (LLMs) in federated learning (FL) scenarios.","meta":{"url":"http://arxiv.org/abs/2307.13896v1"},"cats":{"new-dataset":0.1595690314,"dev-research":0.2073759661,"prompt-eng":0.3941211743,"data-quality":0.2443908807,"ml-security":0.1140272705}}
{"text":"It is a challenging task due to limited labeled data and communication capacities in FL, especially with mobile devices.","meta":{"url":"http://arxiv.org/abs/2307.13896v1"},"cats":{"new-dataset":0.2572233609,"dev-research":0.2648570662,"prompt-eng":0.4182155194,"data-quality":0.1846080431,"ml-security":0.0902525687}}
{"text":"Recent studies show LLMs can be prompted to perform few-shot NLU tasks like sentiment analysis and arithmetic reasoning.","meta":{"url":"http://arxiv.org/abs/2307.13896v1"},"cats":{"new-dataset":0.0282524653,"dev-research":0.2925968597,"prompt-eng":0.4013000485,"data-quality":0.1980458124,"ml-security":0.1045234062}}
{"text":"However, the huge sizes of LLMs result in high computation and communication costs, making classical FL schemes impractical.","meta":{"url":"http://arxiv.org/abs/2307.13896v1"},"cats":{"new-dataset":0.0145779875,"dev-research":0.2111512103,"prompt-eng":0.324080759,"data-quality":0.0666186147,"ml-security":0.1440534378}}
{"text":"To address these challenges, we propose Low-Parameter Federated Learning (LP-FL).","meta":{"url":"http://arxiv.org/abs/2307.13896v1"},"cats":{"new-dataset":0.0618952423,"dev-research":0.1728924237,"prompt-eng":0.4002256233,"data-quality":0.1921018726,"ml-security":0.2111391189}}
{"text":"LP-FL combines few-shot prompt learning from LLMs with efficient communication and federating techniques.","meta":{"url":"http://arxiv.org/abs/2307.13896v1"},"cats":{"new-dataset":0.1178302137,"dev-research":0.2387237645,"prompt-eng":0.5137080475,"data-quality":0.167833196,"ml-security":0.1500464245}}
{"text":"Our approach enables federated clients to assign soft labels to unlabeled data using gradually learned knowledge from the global model.","meta":{"url":"http://arxiv.org/abs/2307.13896v1"},"cats":{"new-dataset":0.1711825584,"dev-research":0.2577068007,"prompt-eng":0.4370145458,"data-quality":0.466234327,"ml-security":0.1714580665}}
{"text":"Through iterative soft-label assigning, we continually expand the labeled set during the FL process.","meta":{"url":"http://arxiv.org/abs/2307.13896v1"},"cats":{"new-dataset":0.1382147365,"dev-research":0.247916309,"prompt-eng":0.4575336398,"data-quality":0.3584471561,"ml-security":0.0607567632}}
{"text":"Additionally, to reduce computation and communication costs, LP-FL utilizes the Low-Rank Adaptation (LoRA) technique for compact learnable parameter construction, efficient local model fine-tuning, and affordable global model federation.","meta":{"url":"http://arxiv.org/abs/2307.13896v1"},"cats":{"new-dataset":0.0676079794,"dev-research":0.2090894998,"prompt-eng":0.4059211203,"data-quality":0.1353683207,"ml-security":0.0993104689}}
{"text":"LP-FL consistently outperforms Full-Parameter Federated Learning (FP-FL) in sentiment analysis tasks across various FL settings.","meta":{"url":"http://arxiv.org/abs/2307.13896v1"},"cats":{"new-dataset":0.0546998005,"dev-research":0.2186105848,"prompt-eng":0.3941380615,"data-quality":0.3632481689,"ml-security":0.1080406094}}
{"text":"Its resistance to overfitting allows LP-FL to equal or surpass centralized training in few-shot scenarios.","meta":{"url":"http://arxiv.org/abs/2307.13896v1"},"cats":{"new-dataset":0.0107907498,"dev-research":0.2520031442,"prompt-eng":0.3191793557,"data-quality":0.1735255865,"ml-security":0.2501110057}}
{"text":"We present a critical analysis of the simulation framework RICE-N, an integrated assessment model (IAM) for evaluating the impacts of climate change on the economy.","meta":{"url":"http://arxiv.org/abs/2307.13894v1"},"cats":{"new-dataset":0.1864576178,"dev-research":0.2165791156,"prompt-eng":0.3788470079,"data-quality":0.1105684834,"ml-security":0.0660004502}}
{"text":"We identify key issues with RICE-N, including action masking and irrelevant actions, and suggest improvements such as utilizing tariff revenue and penalizing overproduction.","meta":{"url":"http://arxiv.org/abs/2307.13894v1"},"cats":{"new-dataset":0.1945243773,"dev-research":0.222876817,"prompt-eng":0.3726993945,"data-quality":0.2682531891,"ml-security":0.2145979643}}
{"text":"We also critically engage with features of IAMs in general, namely overly optimistic damage functions and unrealistic abatement cost functions.","meta":{"url":"http://arxiv.org/abs/2307.13894v1"},"cats":{"new-dataset":0.0469818546,"dev-research":0.3447535955,"prompt-eng":0.4264847801,"data-quality":0.1917735417,"ml-security":0.3161633852}}
{"text":"Our findings contribute to the ongoing efforts to further develop the RICE-N framework in an effort to improve the simulation, making it more useful as an inspiration for policymakers.","meta":{"url":"http://arxiv.org/abs/2307.13894v1"},"cats":{"new-dataset":0.3071429759,"dev-research":0.2200809827,"prompt-eng":0.3657758036,"data-quality":0.1183088768,"ml-security":0.0866019318}}
{"text":"In this paper, we propose a dynamic grouping negotiation model for climate mitigation based on real-world business and political negotiation protocols.","meta":{"url":"http://arxiv.org/abs/2307.13893v1"},"cats":{"new-dataset":0.218195293,"dev-research":0.2432162679,"prompt-eng":0.3801119842,"data-quality":0.0910693987,"ml-security":0.1466785666}}
{"text":"Within the AI4GCC competition framework, we develop a three-stage process: group formation and updates, intra-group negotiation, and inter-group negotiation.","meta":{"url":"http://arxiv.org/abs/2307.13893v1"},"cats":{"new-dataset":0.2734124451,"dev-research":0.2888653364,"prompt-eng":0.4385852255,"data-quality":0.1014642546,"ml-security":0.078974392}}
{"text":"Our model promotes efficient and effective cooperation between various stakeholders to achieve global climate change objectives.","meta":{"url":"http://arxiv.org/abs/2307.13893v1"},"cats":{"new-dataset":0.0604471882,"dev-research":0.2799124037,"prompt-eng":0.3966049392,"data-quality":0.0603706723,"ml-security":0.0638197218}}
{"text":"By implementing a group-forming method and group updating strategy, we address the complexities and imbalances in multi-region climate negotiations.","meta":{"url":"http://arxiv.org/abs/2307.13893v1"},"cats":{"new-dataset":0.178004183,"dev-research":0.2613721346,"prompt-eng":0.387746547,"data-quality":0.1102337837,"ml-security":0.1167793278}}
{"text":"Intra-group negotiations ensure that all members contribute to mitigation efforts, while inter-group negotiations use the proposal-evaluation framework to set mitigation and savings rates.","meta":{"url":"http://arxiv.org/abs/2307.13893v1"},"cats":{"new-dataset":0.0276805271,"dev-research":0.3228152567,"prompt-eng":0.3688393491,"data-quality":0.0973463161,"ml-security":0.1033521947}}
{"text":"We demonstrate our negotiation model within the RICE-N framework, illustrating a promising approach for facilitating international cooperation on climate change mitigation.","meta":{"url":"http://arxiv.org/abs/2307.13893v1"},"cats":{"new-dataset":0.3770091572,"dev-research":0.2019071375,"prompt-eng":0.3790582483,"data-quality":0.1085636858,"ml-security":0.1126203155}}
{"text":"As our submission for track three of the AI for Global Climate Cooperation (AI4GCC) competition, we propose a negotiation protocol for use in the RICE-N climate-economic simulation.","meta":{"url":"http://arxiv.org/abs/2307.13892v1"},"cats":{"new-dataset":0.3234007695,"dev-research":0.1892442047,"prompt-eng":0.3705501037,"data-quality":0.0798116112,"ml-security":0.0755238866}}
{"text":"Our proposal seeks to address the challenges of carbon leakage through methods inspired by the Carbon Border Adjustment Mechanism (CBAM) and Climate Clubs (CC).","meta":{"url":"http://arxiv.org/abs/2307.13892v1"},"cats":{"new-dataset":0.1453285862,"dev-research":0.3048817564,"prompt-eng":0.3576376852,"data-quality":0.2720304284,"ml-security":0.1617396181}}
{"text":"We demonstrate the effectiveness of our approach by comparing simulated outcomes to representative concentration pathways (RCP) and shared socioeconomic pathways (SSP).","meta":{"url":"http://arxiv.org/abs/2307.13892v1"},"cats":{"new-dataset":0.031049744,"dev-research":0.2200557443,"prompt-eng":0.3946710895,"data-quality":0.0744517843,"ml-security":0.0928684253}}
{"text":"Our protocol results in a temperature rise comparable to RCP 3.4/4.5 and SSP 2.","meta":{"url":"http://arxiv.org/abs/2307.13892v1"},"cats":{"new-dataset":0.2735066955,"dev-research":0.2134568812,"prompt-eng":0.4539999294,"data-quality":0.1062677075,"ml-security":0.0834202196}}
{"text":"Furthermore, we provide an analysis of our protocol's World Trade Organization compliance, administrative and political feasibility, and ethical concerns.","meta":{"url":"http://arxiv.org/abs/2307.13892v1"},"cats":{"new-dataset":0.4274747348,"dev-research":0.2163007152,"prompt-eng":0.3718540864,"data-quality":0.1334373996,"ml-security":0.1381071647}}
{"text":"We recognize that our proposal risks hurting the least developing countries, and we suggest specific corrective measures to avoid exacerbating existing inequalities, such as technology sharing and wealth redistribution.","meta":{"url":"http://arxiv.org/abs/2307.13892v1"},"cats":{"new-dataset":0.0693700365,"dev-research":0.2939249414,"prompt-eng":0.3868950776,"data-quality":0.1849381382,"ml-security":0.215653968}}
{"text":"Future research should improve the RICE-N tariff mechanism and implement actions allowing for the aforementioned corrective measures.","meta":{"url":"http://arxiv.org/abs/2307.13892v1"},"cats":{"new-dataset":0.0495869341,"dev-research":0.186067177,"prompt-eng":0.3861801863,"data-quality":0.1636071669,"ml-security":0.0748804003}}
{"text":"The current framework for climate change negotiation models presents several limitations that warrant further research and development.","meta":{"url":"http://arxiv.org/abs/2307.13886v1"},"cats":{"new-dataset":0.1173497924,"dev-research":0.210039551,"prompt-eng":0.3367349475,"data-quality":0.0656674745,"ml-security":0.0925566764}}
{"text":"In this track, we discuss mainly two key areas for improvement, focusing on the geographical impacts and utility framework.","meta":{"url":"http://arxiv.org/abs/2307.13886v1"},"cats":{"new-dataset":0.2457437588,"dev-research":0.2886245063,"prompt-eng":0.3528125971,"data-quality":0.1162241306,"ml-security":0.0512691225}}
{"text":"In the aspects of geographical impacts, We explore five critical aspects: (1) the shift from local to global impact, (2) variability in climate change effects across regions, (3) heterogeneity in geographical location and political structures, and (4) collaborations between adjacent nations, (5) the importance of including historical and cultural factors influencing climate negotiations.","meta":{"url":"http://arxiv.org/abs/2307.13886v1"},"cats":{"new-dataset":0.156073164,"dev-research":0.2754704261,"prompt-eng":0.324177592,"data-quality":0.1050975428,"ml-security":0.0869978363}}
{"text":"Furthermore, we emphasize the need to refine the utility and rewards framework to reduce the homogeneity and the level of overestimating the climate mitigation by integrating the positive effects of saving rates into the reward function and heterogeneity among all regions.","meta":{"url":"http://arxiv.org/abs/2307.13886v1"},"cats":{"new-dataset":0.05713507,"dev-research":0.2291693195,"prompt-eng":0.4067934442,"data-quality":0.1222073371,"ml-security":0.140820302}}
{"text":"By addressing these limitations, we hope to enhance the accuracy and effectiveness of climate change negotiation models, enabling policymakers and stakeholders to devise targeted and appropriate strategies to tackle climate change at both regional and global levels.","meta":{"url":"http://arxiv.org/abs/2307.13886v1"},"cats":{"new-dataset":0.1745971864,"dev-research":0.275764687,"prompt-eng":0.3992476973,"data-quality":0.1048741136,"ml-security":0.0781011822}}
{"text":"Machine learning models often need to be robust to noisy input data.","meta":{"url":"http://arxiv.org/abs/2307.13885v1"},"cats":{"new-dataset":0.0180321766,"dev-research":0.3176805053,"prompt-eng":0.3971184545,"data-quality":0.4764905056,"ml-security":0.5729719982}}
{"text":"The effect of real-world noise (which is often random) on model predictions is captured by a model's local robustness, i.e., the consistency of model predictions in a local region around an input.","meta":{"url":"http://arxiv.org/abs/2307.13885v1"},"cats":{"new-dataset":0.0379232582,"dev-research":0.2982896147,"prompt-eng":0.3835310197,"data-quality":0.3951826441,"ml-security":0.303072047}}
{"text":"However, the na\\\"ive approach to computing local robustness based on Monte-Carlo sampling is statistically inefficient, leading to prohibitive computational costs for large-scale applications.","meta":{"url":"http://arxiv.org/abs/2307.13885v1"},"cats":{"new-dataset":0.0431020323,"dev-research":0.2755084368,"prompt-eng":0.4037740254,"data-quality":0.2219489326,"ml-security":0.1913430585}}
{"text":"In this work, we develop the first analytical estimators to efficiently compute local robustness of multi-class discriminative models using local linear function approximation and the multivariate Normal CDF.","meta":{"url":"http://arxiv.org/abs/2307.13885v1"},"cats":{"new-dataset":0.1153691234,"dev-research":0.1759556033,"prompt-eng":0.4343801856,"data-quality":0.3956535128,"ml-security":0.2671373328}}
{"text":"Through the derivation of these estimators, we show how local robustness is connected to concepts such as randomized smoothing and softmax probability.","meta":{"url":"http://arxiv.org/abs/2307.13885v1"},"cats":{"new-dataset":0.0619471883,"dev-research":0.2257960701,"prompt-eng":0.3825496765,"data-quality":0.3672629469,"ml-security":0.3348191434}}
{"text":"We also confirm empirically that these estimators accurately and efficiently compute the local robustness of standard deep learning models.","meta":{"url":"http://arxiv.org/abs/2307.13885v1"},"cats":{"new-dataset":0.0784547963,"dev-research":0.2301489767,"prompt-eng":0.392340899,"data-quality":0.4729192685,"ml-security":0.3752328623}}
{"text":"In addition, we demonstrate these estimators' usefulness for various tasks involving local robustness, such as measuring robustness bias and identifying examples that are vulnerable to noise perturbation in a dataset.","meta":{"url":"http://arxiv.org/abs/2307.13885v1"},"cats":{"new-dataset":0.2660626604,"dev-research":0.2680042931,"prompt-eng":0.4293660259,"data-quality":0.5615699413,"ml-security":0.3837385942}}
{"text":"By developing these analytical estimators, this work not only advances conceptual understanding of local robustness, but also makes its computation practical, enabling the use of local robustness in critical downstream applications.","meta":{"url":"http://arxiv.org/abs/2307.13885v1"},"cats":{"new-dataset":0.0571085667,"dev-research":0.2982932416,"prompt-eng":0.4242318319,"data-quality":0.3582950984,"ml-security":0.301807309}}
{"text":"When writing programs, people have the ability to tackle a new complex task by decomposing it into smaller and more familiar subtasks.","meta":{"url":"http://arxiv.org/abs/2307.13883v1"},"cats":{"new-dataset":0.0192302258,"dev-research":0.5431747854,"prompt-eng":0.3942046998,"data-quality":0.1063157253,"ml-security":0.1134094667}}
{"text":"While it is difficult to measure whether neural program synthesis methods have similar capabilities, we can measure whether they compositionally generalize, that is, whether a model that has been trained on the simpler subtasks is subsequently able to solve more complex tasks.","meta":{"url":"http://arxiv.org/abs/2307.13883v1"},"cats":{"new-dataset":0.0127803591,"dev-research":0.3648999121,"prompt-eng":0.4016290738,"data-quality":0.1320791005,"ml-security":0.1997925729}}
{"text":"In this paper, we characterize several different forms of compositional generalization that are desirable in program synthesis, forming a meta-benchmark which we use to create generalization tasks for two popular datasets, RobustFill and DeepCoder.","meta":{"url":"http://arxiv.org/abs/2307.13883v1"},"cats":{"new-dataset":0.1658703594,"dev-research":0.4076808225,"prompt-eng":0.4190200035,"data-quality":0.2531933314,"ml-security":0.1912207286}}
{"text":"We then propose ExeDec, a novel decomposition-based synthesis strategy that predicts execution subgoals to solve problems step-by-step informed by program execution at each step.","meta":{"url":"http://arxiv.org/abs/2307.13883v1"},"cats":{"new-dataset":0.0984983622,"dev-research":0.5111476228,"prompt-eng":0.4632865048,"data-quality":0.1432205539,"ml-security":0.2387337797}}
{"text":"ExeDec has better synthesis performance and greatly improved compositional generalization ability compared to baselines.","meta":{"url":"http://arxiv.org/abs/2307.13883v1"},"cats":{"new-dataset":0.0473701734,"dev-research":0.3864486467,"prompt-eng":0.4132808753,"data-quality":0.1526115626,"ml-security":0.0870907525}}
{"text":"Human culture research has witnessed an opportunity of revolution thanks to the big data and social network revolution.","meta":{"url":"http://arxiv.org/abs/2307.13882v1"},"cats":{"new-dataset":0.2374102409,"dev-research":0.2269931109,"prompt-eng":0.2881838261,"data-quality":0.0879572878,"ml-security":0.0582425295}}
{"text":"Websites such as Douban.com, Goodreads.com, Pandora and IMDB become the new gold mine for cultural researchers.","meta":{"url":"http://arxiv.org/abs/2307.13882v1"},"cats":{"new-dataset":0.3670594166,"dev-research":0.2182039422,"prompt-eng":0.2773282126,"data-quality":0.1710804177,"ml-security":0.0913201474}}
{"text":"In 2021 and 2022, the author of this paper invented 2 data-free recommender systems for AI cold-start problem.","meta":{"url":"http://arxiv.org/abs/2307.13882v1"},"cats":{"new-dataset":0.1790239854,"dev-research":0.2725622877,"prompt-eng":0.3805660877,"data-quality":0.1085986924,"ml-security":0.1929036104}}
{"text":"The algorithms can recommend cultural and commercial products to users without reference to users' past preferences.","meta":{"url":"http://arxiv.org/abs/2307.13882v1"},"cats":{"new-dataset":0.0379117126,"dev-research":0.2571984278,"prompt-eng":0.3656939104,"data-quality":0.1558604987,"ml-security":0.1282398853}}
{"text":"The social implications of the new inventions are human cultural tastes can be predicted very precisely without any information related to human individuals.","meta":{"url":"http://arxiv.org/abs/2307.13882v1"},"cats":{"new-dataset":0.0437113673,"dev-research":0.2730352816,"prompt-eng":0.2910310812,"data-quality":0.1883949428,"ml-security":0.1810841762}}
{"text":"In this paper, we analyze the AI technologies and its cultural implications together with other AI algorithms.","meta":{"url":"http://arxiv.org/abs/2307.13882v1"},"cats":{"new-dataset":0.0825233609,"dev-research":0.2748966065,"prompt-eng":0.3285649586,"data-quality":0.1778171109,"ml-security":0.1354740041}}
{"text":"We show that human culture is (mostly) a history irrelevant and predictable experience.","meta":{"url":"http://arxiv.org/abs/2307.13882v1"},"cats":{"new-dataset":0.0373392129,"dev-research":0.2817402181,"prompt-eng":0.331138451,"data-quality":0.2388205506,"ml-security":0.0920898464}}
{"text":"Physics-informed neural networks (PINNs) offer a novel and efficient approach to solving partial differential equations (PDEs).","meta":{"url":"http://arxiv.org/abs/2307.13869v1"},"cats":{"new-dataset":0.0531712707,"dev-research":0.2388949951,"prompt-eng":0.3403642085,"data-quality":0.0823117398,"ml-security":0.206784552}}
{"text":"Their success lies in the physics-informed loss, which trains a neural network to satisfy a given PDE at specific points and to approximate the solution.","meta":{"url":"http://arxiv.org/abs/2307.13869v1"},"cats":{"new-dataset":0.0120233566,"dev-research":0.2019041501,"prompt-eng":0.3616831774,"data-quality":0.206203501,"ml-security":0.2086378457}}
{"text":"However, the solutions to PDEs are inherently infinite-dimensional, and the distance between the output and the solution is defined by an integral over the domain.","meta":{"url":"http://arxiv.org/abs/2307.13869v1"},"cats":{"new-dataset":0.0082183144,"dev-research":0.2514174516,"prompt-eng":0.2634471881,"data-quality":0.0677382145,"ml-security":0.1952931529}}
{"text":"Therefore, the physics-informed loss only provides a finite approximation, and selecting appropriate collocation points becomes crucial to suppress the discretization errors, although this aspect has often been overlooked.","meta":{"url":"http://arxiv.org/abs/2307.13869v1"},"cats":{"new-dataset":0.0035247295,"dev-research":0.2118517191,"prompt-eng":0.3515732889,"data-quality":0.2295614,"ml-security":0.138817073}}
{"text":"In this paper, we propose a new technique called good lattice training (GLT) for PINNs, inspired by number theoretic methods for numerical analysis.","meta":{"url":"http://arxiv.org/abs/2307.13869v1"},"cats":{"new-dataset":0.0560612281,"dev-research":0.2100439663,"prompt-eng":0.3575679823,"data-quality":0.1212313105,"ml-security":0.1488602084}}
{"text":"GLT offers a set of collocation points that are effective even with a small number of points and for multi-dimensional spaces.","meta":{"url":"http://arxiv.org/abs/2307.13869v1"},"cats":{"new-dataset":0.0842263515,"dev-research":0.2210799795,"prompt-eng":0.3720708198,"data-quality":0.123430076,"ml-security":0.079524194}}
{"text":"Our experiments demonstrate that GLT requires 2--20 times fewer collocation points (resulting in lower computational cost) than uniformly random sampling or Latin hypercube sampling, while achieving competitive performance.","meta":{"url":"http://arxiv.org/abs/2307.13869v1"},"cats":{"new-dataset":0.0582574837,"dev-research":0.1994641857,"prompt-eng":0.3818643332,"data-quality":0.1787940278,"ml-security":0.0725153978}}
{"text":"Advancements in large pre-trained generative models have expanded their potential as effective data generators in visual recognition.","meta":{"url":"http://arxiv.org/abs/2307.13697v1"},"cats":{"new-dataset":0.1090096574,"dev-research":0.2227268346,"prompt-eng":0.4210437452,"data-quality":0.1635460755,"ml-security":0.1106118022}}
{"text":"This work delves into the impact of generative images, primarily comparing paradigms that harness external data (\\ie generative \\vs retrieval \\vs original).   ","meta":{"url":"http://arxiv.org/abs/2307.13697v1"},"cats":{"new-dataset":0.1104503581,"dev-research":0.2998544228,"prompt-eng":0.3955038342,"data-quality":0.2671750017,"ml-security":0.0927263306}}
{"text":"Our key contributions are: \\textbf{1) GenBench Construction:}","meta":{"url":"http://arxiv.org/abs/2307.13697v1"},"cats":{"new-dataset":0.1451473841,"dev-research":0.2486704956,"prompt-eng":0.4057240418,"data-quality":0.279286964,"ml-security":0.0791114225}}
{"text":"We devise \\textbf{GenBench}, a broad benchmark comprising 22 datasets with 2548 categories, to appraise generative data across various visual recognition tasks.","meta":{"url":"http://arxiv.org/abs/2307.13697v1"},"cats":{"new-dataset":0.7486092734,"dev-research":0.2461986331,"prompt-eng":0.4107253268,"data-quality":0.2370881031,"ml-security":0.0829608235}}
{"text":"\\textbf{2) CLER Score:} To address the insufficient correlation of existing metrics (\\eg, FID, CLIP score) with downstream recognition performance, we propose \\textbf{CLER}, a training-free metric indicating generative data's efficiency for recognition tasks prior to training.","meta":{"url":"http://arxiv.org/abs/2307.13697v1"},"cats":{"new-dataset":0.1967185502,"dev-research":0.2542097934,"prompt-eng":0.4330171499,"data-quality":0.3129658492,"ml-security":0.0974926418}}
{"text":"\\textbf{3) New Baselines:} Comparisons of generative data with retrieved data from the same external pool help to elucidate the unique traits of generative data.","meta":{"url":"http://arxiv.org/abs/2307.13697v1"},"cats":{"new-dataset":0.3780757727,"dev-research":0.2839923189,"prompt-eng":0.4318801113,"data-quality":0.3292514191,"ml-security":0.0742647654}}
{"text":"\\textbf{4)","meta":{"url":"http://arxiv.org/abs/2307.13697v1"},"cats":{"new-dataset":0.0704093412,"dev-research":0.2872907214,"prompt-eng":0.3519943407,"data-quality":0.3243063232,"ml-security":0.1088784636}}
{"text":"External Knowledge Injection:} By fine-tuning special token embeddings for each category via Textual Inversion, performance improves across 17 datasets, except when dealing with low-resolution reference images.   ","meta":{"url":"http://arxiv.org/abs/2307.13697v1"},"cats":{"new-dataset":0.3160488794,"dev-research":0.2752151348,"prompt-eng":0.3885202179,"data-quality":0.3870884935,"ml-security":0.1777106476}}
{"text":"Our exhaustive benchmark and analysis spotlight generative data's promise in visual recognition, while identifying key challenges for future investigation.","meta":{"url":"http://arxiv.org/abs/2307.13697v1"},"cats":{"new-dataset":0.248387469,"dev-research":0.2608463217,"prompt-eng":0.407818959,"data-quality":0.282662621,"ml-security":0.0963839036}}
{"text":"Maximal Common Subsequences (MCSs) between two strings X and Y are subsequences of both X and Y that are maximal under inclusion.","meta":{"url":"http://arxiv.org/abs/2307.13695v1"},"cats":{"new-dataset":0.2991857477,"dev-research":0.2153264706,"prompt-eng":0.3622359919,"data-quality":0.1977031434,"ml-security":0.0693848839}}
{"text":"MCSs relax and generalize the well known and widely used concept of Longest Common Subsequences (LCSs), which can be seen as MCSs of maximum length.","meta":{"url":"http://arxiv.org/abs/2307.13695v1"},"cats":{"new-dataset":0.3008461341,"dev-research":0.2022289509,"prompt-eng":0.3511101738,"data-quality":0.1298746962,"ml-security":0.0914407253}}
{"text":"While the number both LCSs and MCSs can be exponential in the length of the strings, LCSs have been long exploited for string and text analysis, as simple compact representations of all LCSs between two strings, built via dynamic programming or automata, have been known since the '70s.","meta":{"url":"http://arxiv.org/abs/2307.13695v1"},"cats":{"new-dataset":0.1146313306,"dev-research":0.233011931,"prompt-eng":0.3659668354,"data-quality":0.1683764891,"ml-security":0.1286542535}}
{"text":"MCSs appear to have a more challenging structure: even listing them efficiently was an open problem open until recently, thus narrowing the complexity difference between the two problems, but the gap remained significant.","meta":{"url":"http://arxiv.org/abs/2307.13695v1"},"cats":{"new-dataset":0.0868902745,"dev-research":0.2744295718,"prompt-eng":0.3539310486,"data-quality":0.1551730963,"ml-security":0.0809108823}}
{"text":"In this paper we close the complexity gap: we show how to build DAG of polynomial size-in polynomial time-which allows for efficient operations on the set of all MCSs such as enumeration in Constant Amortized Time per solution (CAT), counting, and random access to the i-th element (i.e., rank and select operations).","meta":{"url":"http://arxiv.org/abs/2307.13695v1"},"cats":{"new-dataset":0.1315224113,"dev-research":0.286181072,"prompt-eng":0.3694297574,"data-quality":0.0818983332,"ml-security":0.1047302626}}
{"text":"Other than improving known algorithmic results, this work paves the way for new sequence analysis methods based on MCSs.","meta":{"url":"http://arxiv.org/abs/2307.13695v1"},"cats":{"new-dataset":0.196798575,"dev-research":0.236580453,"prompt-eng":0.3793231388,"data-quality":0.189369034,"ml-security":0.0692808327}}
{"text":"The rise of large language models (LLMs) has marked a pivotal shift in the field of natural language processing (NLP).","meta":{"url":"http://arxiv.org/abs/2307.13693v1"},"cats":{"new-dataset":0.0549554401,"dev-research":0.1683483016,"prompt-eng":0.3676013431,"data-quality":0.1874655899,"ml-security":0.0971966192}}
{"text":"LLMs have revolutionized a multitude of domains, and they have made a significant impact in the medical field.","meta":{"url":"http://arxiv.org/abs/2307.13693v1"},"cats":{"new-dataset":0.0162203322,"dev-research":0.1902267349,"prompt-eng":0.328342367,"data-quality":0.0797144987,"ml-security":0.1080682834}}
{"text":"Large language models are now more abundant than ever, and many of these models exhibit bilingual capabilities, proficient in both English and Chinese.","meta":{"url":"http://arxiv.org/abs/2307.13693v1"},"cats":{"new-dataset":0.0311804807,"dev-research":0.1948049395,"prompt-eng":0.3052397692,"data-quality":0.1199775755,"ml-security":0.076442526}}
{"text":"However, a comprehensive evaluation of these models remains to be conducted.","meta":{"url":"http://arxiv.org/abs/2307.13693v1"},"cats":{"new-dataset":0.0398186305,"dev-research":0.1587766076,"prompt-eng":0.3983402709,"data-quality":0.1448588997,"ml-security":0.0569034584}}
{"text":"This lack of assessment is especially apparent within the context of radiology NLP.","meta":{"url":"http://arxiv.org/abs/2307.13693v1"},"cats":{"new-dataset":0.0158550744,"dev-research":0.2925256672,"prompt-eng":0.3591494804,"data-quality":0.3923585422,"ml-security":0.0686599745}}
{"text":"This study seeks to bridge this gap by critically evaluating thirty two LLMs in interpreting radiology reports, a crucial component of radiology NLP.","meta":{"url":"http://arxiv.org/abs/2307.13693v1"},"cats":{"new-dataset":0.074952728,"dev-research":0.2982470532,"prompt-eng":0.3851323373,"data-quality":0.2605796746,"ml-security":0.0445967607}}
{"text":"Specifically, the ability to derive impressions from radiologic findings is assessed.","meta":{"url":"http://arxiv.org/abs/2307.13693v1"},"cats":{"new-dataset":0.0138251813,"dev-research":0.3060456297,"prompt-eng":0.4641536831,"data-quality":0.1930459253,"ml-security":0.0833782552}}
{"text":"The outcomes of this evaluation provide key insights into the performance, strengths, and weaknesses of these LLMs, informing their practical applications within the medical domain.","meta":{"url":"http://arxiv.org/abs/2307.13693v1"},"cats":{"new-dataset":0.0260167893,"dev-research":0.1959161463,"prompt-eng":0.3849335563,"data-quality":0.1130448781,"ml-security":0.0958333807}}
{"text":"Large Language Models (LLMs) have demonstrated remarkable performance on various quantitative reasoning and knowledge benchmarks.","meta":{"url":"http://arxiv.org/abs/2307.13692v1"},"cats":{"new-dataset":0.1005982577,"dev-research":0.2221125962,"prompt-eng":0.4064073389,"data-quality":0.206373661,"ml-security":0.0944546216}}
{"text":"However, many of these benchmarks are losing utility as LLMs get increasingly high scores, despite not yet reaching expert performance in these domains.","meta":{"url":"http://arxiv.org/abs/2307.13692v1"},"cats":{"new-dataset":0.0279944307,"dev-research":0.2610801825,"prompt-eng":0.4209955949,"data-quality":0.2753629384,"ml-security":0.1775042324}}
{"text":"We introduce ARB, a novel benchmark composed of advanced reasoning problems in multiple fields.","meta":{"url":"http://arxiv.org/abs/2307.13692v1"},"cats":{"new-dataset":0.1270028317,"dev-research":0.3267605013,"prompt-eng":0.4415052943,"data-quality":0.1917605469,"ml-security":0.0752366216}}
{"text":"ARB presents a more challenging test than prior benchmarks, featuring problems in mathematics, physics, biology, chemistry, and law.","meta":{"url":"http://arxiv.org/abs/2307.13692v1"},"cats":{"new-dataset":0.0787857324,"dev-research":0.2685926328,"prompt-eng":0.4120830526,"data-quality":0.1570369711,"ml-security":0.0982698994}}
{"text":"As a subset of ARB, we introduce a challenging set of math and physics problems which require advanced symbolic reasoning and domain knowledge.","meta":{"url":"http://arxiv.org/abs/2307.13692v1"},"cats":{"new-dataset":0.1198382001,"dev-research":0.2737864682,"prompt-eng":0.3995053769,"data-quality":0.1322363546,"ml-security":0.0944152358}}
{"text":"We evaluate recent models such as GPT-4 and Claude on ARB and demonstrate that current models score well below 50% on more demanding tasks.","meta":{"url":"http://arxiv.org/abs/2307.13692v1"},"cats":{"new-dataset":0.0896957648,"dev-research":0.2082512572,"prompt-eng":0.4807671882,"data-quality":0.1044351891,"ml-security":0.0748004586}}
{"text":"In order to improve both automatic and assisted evaluation capabilities, we introduce a rubric-based evaluation approach, allowing GPT-4 to score its own intermediate reasoning steps.","meta":{"url":"http://arxiv.org/abs/2307.13692v1"},"cats":{"new-dataset":0.0777197104,"dev-research":0.42381208,"prompt-eng":0.5028742022,"data-quality":0.1955787771,"ml-security":0.0460210253}}
{"text":"Further, we conduct a human evaluation of the symbolic subset of ARB, finding promising agreement between annotators and GPT-4 rubric evaluation scores.","meta":{"url":"http://arxiv.org/abs/2307.13692v1"},"cats":{"new-dataset":0.2131916589,"dev-research":0.31677866,"prompt-eng":0.4775760923,"data-quality":0.3243457942,"ml-security":0.0577234862}}
{"text":"The growing interest in unmanned aerial vehicles (UAVs) from both scientific and industrial sectors has attracted a wave of new researchers and substantial investments in this expansive field.","meta":{"url":"http://arxiv.org/abs/2307.13691v1"},"cats":{"new-dataset":0.1093615818,"dev-research":0.2660381292,"prompt-eng":0.2994316468,"data-quality":0.0608963808,"ml-security":0.1193787028}}
{"text":"However, due to the wide range of topics and subdomains within UAV research, newcomers may find themselves overwhelmed by the numerous options available.","meta":{"url":"http://arxiv.org/abs/2307.13691v1"},"cats":{"new-dataset":0.0463034555,"dev-research":0.2911055818,"prompt-eng":0.4111346624,"data-quality":0.0833293665,"ml-security":0.1131264885}}
{"text":"It is therefore crucial for those involved in UAV research to recognize its interdisciplinary nature and its connections with other disciplines.","meta":{"url":"http://arxiv.org/abs/2307.13691v1"},"cats":{"new-dataset":0.0462593372,"dev-research":0.2513926065,"prompt-eng":0.3240076007,"data-quality":0.0782951675,"ml-security":0.0687522779}}
{"text":"This paper presents a comprehensive overview of the UAV field, highlighting recent trends and advancements.","meta":{"url":"http://arxiv.org/abs/2307.13691v1"},"cats":{"new-dataset":0.161806145,"dev-research":0.185390033,"prompt-eng":0.366824448,"data-quality":0.0573496811,"ml-security":0.1180239869}}
{"text":"Drawing on recent literature reviews and surveys, the review begins by classifying UAVs based on their flight characteristics.","meta":{"url":"http://arxiv.org/abs/2307.13691v1"},"cats":{"new-dataset":0.1540833587,"dev-research":0.2108840666,"prompt-eng":0.3866383391,"data-quality":0.1160098169,"ml-security":0.1555228964}}
{"text":"It then provides an overview of current research trends in UAVs, utilizing data from the Scopus database to quantify the number of scientific documents associated with each research direction and their interconnections.","meta":{"url":"http://arxiv.org/abs/2307.13691v1"},"cats":{"new-dataset":0.2838605779,"dev-research":0.2269695825,"prompt-eng":0.354037751,"data-quality":0.0800363123,"ml-security":0.0700417787}}
{"text":"The paper also explores potential areas for further development in UAVs, including communication, artificial intelligence, remote sensing, miniaturization, swarming and cooperative control, and transformability.","meta":{"url":"http://arxiv.org/abs/2307.13691v1"},"cats":{"new-dataset":0.1128455983,"dev-research":0.2482640757,"prompt-eng":0.3534183662,"data-quality":0.0630888941,"ml-security":0.1269529577}}
{"text":"Additionally, it discusses the development of aircraft control, commonly used control techniques, and appropriate control algorithms in UAV research.","meta":{"url":"http://arxiv.org/abs/2307.13691v1"},"cats":{"new-dataset":0.1954471992,"dev-research":0.2998752981,"prompt-eng":0.3732247422,"data-quality":0.0731614316,"ml-security":0.1201582785}}
{"text":"Furthermore, the paper addresses the general hardware and software architecture of UAVs, their applications, and the key issues associated with them.","meta":{"url":"http://arxiv.org/abs/2307.13691v1"},"cats":{"new-dataset":0.1280293888,"dev-research":0.2752698667,"prompt-eng":0.3812378812,"data-quality":0.0821406458,"ml-security":0.12111201}}
{"text":"It also provides an overview of current open-source software and hardware projects in the UAV field.","meta":{"url":"http://arxiv.org/abs/2307.13691v1"},"cats":{"new-dataset":0.3301437512,"dev-research":0.2988636241,"prompt-eng":0.3538705773,"data-quality":0.0698109134,"ml-security":0.0982529944}}
{"text":"By presenting a comprehensive view of the UAV field, this paper aims to enhance understanding of this rapidly evolving and highly interdisciplinary area of research.","meta":{"url":"http://arxiv.org/abs/2307.13691v1"},"cats":{"new-dataset":0.1496334056,"dev-research":0.2299098222,"prompt-eng":0.3471367011,"data-quality":0.0506661248,"ml-security":0.1035365828}}
{"text":"The $k$-means++ algorithm by Arthur and Vassilvitskii [SODA 2007] is a classical and time-tested algorithm for the $k$-means problem.","meta":{"url":"http://arxiv.org/abs/2307.13685v1"},"cats":{"new-dataset":0.1010872642,"dev-research":0.2210500684,"prompt-eng":0.3210930119,"data-quality":0.1484675337,"ml-security":0.1491692963}}
{"text":"While being very practical, the algorithm also has good theoretical guarantees: its solution is $O(\\log k)$-approximate, in expectation.   ","meta":{"url":"http://arxiv.org/abs/2307.13685v1"},"cats":{"new-dataset":0.0106801642,"dev-research":0.2094317019,"prompt-eng":0.3359973346,"data-quality":0.1125647808,"ml-security":0.1119116687}}
{"text":"In a recent work, Bhattacharya, Eube, Roglin, and Schmidt [ESA 2020] considered the following question: does the algorithm retain its guarantees if we allow for a slight adversarial noise in the sampling probability distributions used by the algorithm?","meta":{"url":"http://arxiv.org/abs/2307.13685v1"},"cats":{"new-dataset":0.0231644433,"dev-research":0.1824752149,"prompt-eng":0.3146663274,"data-quality":0.2957260486,"ml-security":0.5602591805}}
{"text":"This is motivated e.g. by the fact that computations with real numbers in $k$-means++ implementations are inexact.   ","meta":{"url":"http://arxiv.org/abs/2307.13685v1"},"cats":{"new-dataset":0.0107612538,"dev-research":0.3764895488,"prompt-eng":0.2888262813,"data-quality":0.1510474924,"ml-security":0.2480439443}}
{"text":"Surprisingly, the analysis under this scenario gets substantially more difficult and the authors were able to prove only a weaker approximation guarantee of $O(\\log^2 k)$.","meta":{"url":"http://arxiv.org/abs/2307.13685v1"},"cats":{"new-dataset":0.0608877186,"dev-research":0.1578474937,"prompt-eng":0.3080781023,"data-quality":0.1955443998,"ml-security":0.1360737157}}
{"text":"In this paper, we close the gap by providing a tight, $O(\\log k)$-approximate guarantee for the $k$-means++ algorithm with noise.","meta":{"url":"http://arxiv.org/abs/2307.13685v1"},"cats":{"new-dataset":0.1154031234,"dev-research":0.2104723375,"prompt-eng":0.3450613949,"data-quality":0.2992365339,"ml-security":0.1518488148}}
{"text":"We introduce text2fabric, a novel dataset that links free-text descriptions to various fabric materials.","meta":{"url":"http://arxiv.org/abs/2307.13681v1"},"cats":{"new-dataset":0.7264067705,"dev-research":0.2604037058,"prompt-eng":0.3836181534,"data-quality":0.2121270766,"ml-security":0.0690109788}}
{"text":"The dataset comprises 15,000 natural language descriptions associated to 3,000 corresponding images of fabric materials.","meta":{"url":"http://arxiv.org/abs/2307.13681v1"},"cats":{"new-dataset":0.9143442145,"dev-research":0.2382201253,"prompt-eng":0.3377193908,"data-quality":0.2499744495,"ml-security":0.068860387}}
{"text":"Traditionally, material descriptions come in the form of tags/keywords, which limits their expressivity, induces pre-existing knowledge of the appropriate vocabulary, and ultimately leads to a chopped description system.","meta":{"url":"http://arxiv.org/abs/2307.13681v1"},"cats":{"new-dataset":0.067070607,"dev-research":0.3590637634,"prompt-eng":0.3584624015,"data-quality":0.3465781051,"ml-security":0.0788110149}}
{"text":"Therefore, we study the use of free-text as a more appropriate way to describe material appearance, taking the use case of fabrics as a common item that non-experts may often deal with.","meta":{"url":"http://arxiv.org/abs/2307.13681v1"},"cats":{"new-dataset":0.069710643,"dev-research":0.3194520081,"prompt-eng":0.3656299776,"data-quality":0.1998489948,"ml-security":0.1196376583}}
{"text":"Based on the analysis of the dataset, we identify a compact lexicon, set of attributes and key structure that emerge from the descriptions.","meta":{"url":"http://arxiv.org/abs/2307.13681v1"},"cats":{"new-dataset":0.8655532058,"dev-research":0.2810718505,"prompt-eng":0.3824800233,"data-quality":0.2392665883,"ml-security":0.0842976873}}
{"text":"This allows us to accurately understand how people describe fabrics and draw directions for generalization to other types of materials.","meta":{"url":"http://arxiv.org/abs/2307.13681v1"},"cats":{"new-dataset":0.0257604863,"dev-research":0.365087579,"prompt-eng":0.4030818739,"data-quality":0.1561495438,"ml-security":0.062765584}}
{"text":"We also show that our dataset enables specializing large vision-language models such as CLIP, creating a meaningful latent space for fabric appearance, and significantly improving applications such as fine-grained material retrieval and automatic captioning.","meta":{"url":"http://arxiv.org/abs/2307.13681v1"},"cats":{"new-dataset":0.6070793256,"dev-research":0.2380063171,"prompt-eng":0.4032865885,"data-quality":0.2423972901,"ml-security":0.0793239041}}
{"text":"Gradient clipping is a commonly used technique to stabilize the training process of neural networks.","meta":{"url":"http://arxiv.org/abs/2307.13680v1"},"cats":{"new-dataset":0.0042199459,"dev-research":0.2899844183,"prompt-eng":0.3628011745,"data-quality":0.2179348957,"ml-security":0.2938474543}}
{"text":"A growing body of studies has shown that gradient clipping is a promising technique for dealing with the heavy-tailed behavior that emerged in stochastic optimization as well.","meta":{"url":"http://arxiv.org/abs/2307.13680v1"},"cats":{"new-dataset":0.0041320292,"dev-research":0.2199545238,"prompt-eng":0.424496887,"data-quality":0.1349627445,"ml-security":0.2944198761}}
{"text":"While gradient clipping is significant, its theoretical guarantees are scarce.","meta":{"url":"http://arxiv.org/abs/2307.13680v1"},"cats":{"new-dataset":0.0025214163,"dev-research":0.1999609526,"prompt-eng":0.3462434418,"data-quality":0.1533033149,"ml-security":0.2575469683}}
{"text":"Most theoretical guarantees only provide an in-expectation analysis and only focus on optimization performance.","meta":{"url":"http://arxiv.org/abs/2307.13680v1"},"cats":{"new-dataset":0.0035423382,"dev-research":0.1820728888,"prompt-eng":0.314172378,"data-quality":0.0707450209,"ml-security":0.2136300578}}
{"text":"In this paper, we provide high probability analysis in the non-convex setting and derive the optimization bound and the generalization bound simultaneously for popular stochastic optimization algorithms with gradient clipping, including stochastic gradient descent and its variants of momentum and adaptive stepsizes.","meta":{"url":"http://arxiv.org/abs/2307.13680v1"},"cats":{"new-dataset":0.0209390241,"dev-research":0.1640797781,"prompt-eng":0.3816301958,"data-quality":0.122849854,"ml-security":0.1914786795}}
{"text":"With the gradient clipping, we study a heavy-tailed assumption that the gradients only have bounded $\\alpha$-th moments for some $\\alpha \\in (1, 2]$, which is much weaker than the standard bounded second-moment assumption.","meta":{"url":"http://arxiv.org/abs/2307.13680v1"},"cats":{"new-dataset":0.0275973038,"dev-research":0.135284629,"prompt-eng":0.3634796316,"data-quality":0.1597383698,"ml-security":0.2780976254}}
{"text":"Overall, our study provides a relatively complete picture for the theoretical guarantee of stochastic optimization algorithms with clipping.","meta":{"url":"http://arxiv.org/abs/2307.13680v1"},"cats":{"new-dataset":0.0088326117,"dev-research":0.1811101926,"prompt-eng":0.4014490854,"data-quality":0.1517427029,"ml-security":0.1605062855}}
{"text":"Multivariate time series classification is a rapidly growing research field with practical applications in finance, healthcare, engineering, and more.","meta":{"url":"http://arxiv.org/abs/2307.13679v1"},"cats":{"new-dataset":0.071921914,"dev-research":0.1781054969,"prompt-eng":0.3204436368,"data-quality":0.1029211371,"ml-security":0.150483214}}
{"text":"The complexity of classifying multivariate time series data arises from its high dimensionality, temporal dependencies, and varying lengths.","meta":{"url":"http://arxiv.org/abs/2307.13679v1"},"cats":{"new-dataset":0.1218425326,"dev-research":0.1819365355,"prompt-eng":0.2964108119,"data-quality":0.1015356055,"ml-security":0.1352925203}}
{"text":"This paper introduces a novel ensemble classifier called RED CoMETS (Random Enhanced Co-eye for Multivariate Time Series), which addresses these challenges.","meta":{"url":"http://arxiv.org/abs/2307.13679v1"},"cats":{"new-dataset":0.3350659277,"dev-research":0.1933741149,"prompt-eng":0.3736839128,"data-quality":0.2284584287,"ml-security":0.1653260477}}
{"text":"RED CoMETS builds upon the success of Co-eye, an ensemble classifier specifically designed for symbolically represented univariate time series, and extends its capabilities to handle multivariate data.","meta":{"url":"http://arxiv.org/abs/2307.13679v1"},"cats":{"new-dataset":0.4252627875,"dev-research":0.2505511571,"prompt-eng":0.398737313,"data-quality":0.1756101011,"ml-security":0.1222775649}}
{"text":"The performance of RED CoMETS is evaluated on benchmark datasets from the UCR archive, where it demonstrates competitive accuracy when compared to state-of-the-art techniques in multivariate settings.","meta":{"url":"http://arxiv.org/abs/2307.13679v1"},"cats":{"new-dataset":0.4273464588,"dev-research":0.223761694,"prompt-eng":0.396968295,"data-quality":0.1725676576,"ml-security":0.072509325}}
{"text":"Notably, it achieves the highest reported accuracy in the literature for the 'HandMovementDirection' dataset.","meta":{"url":"http://arxiv.org/abs/2307.13679v1"},"cats":{"new-dataset":0.260663083,"dev-research":0.2418159277,"prompt-eng":0.3901695218,"data-quality":0.1814186235,"ml-security":0.0539582826}}
{"text":"Moreover, the proposed method significantly reduces computation time compared to Co-eye, making it an efficient and effective choice for multivariate time series classification.","meta":{"url":"http://arxiv.org/abs/2307.13679v1"},"cats":{"new-dataset":0.0700787417,"dev-research":0.2455007762,"prompt-eng":0.3659132244,"data-quality":0.1394489768,"ml-security":0.1022628902}}
{"text":"Many data analytic systems have adopted a newly emerging compute resource, serverless (SL), to handle data analytics queries in a timely and cost-efficient manner, i.e., serverless data analytics.","meta":{"url":"http://arxiv.org/abs/2307.13677v1"},"cats":{"new-dataset":0.0989441188,"dev-research":0.2665462191,"prompt-eng":0.3189706253,"data-quality":0.0937600405,"ml-security":0.1138759169}}
{"text":"While these systems can start processing queries quickly thanks to the agility and scalability of SL, they may encounter performance-","meta":{"url":"http://arxiv.org/abs/2307.13677v1"},"cats":{"new-dataset":0.0212606492,"dev-research":0.2714923831,"prompt-eng":0.4288758893,"data-quality":0.0897771735,"ml-security":0.0947164898}}
{"text":"and cost-bottlenecks based on workloads due to SL's worse performance and more expensive cost than traditional compute resources, e.g., virtual machine (VM).","meta":{"url":"http://arxiv.org/abs/2307.13677v1"},"cats":{"new-dataset":0.0093635433,"dev-research":0.3649208705,"prompt-eng":0.3326488296,"data-quality":0.0991755267,"ml-security":0.136220287}}
{"text":"In this project, we introduce Smartpick, a SL-enabled scalable data analytics system that exploits SL and VM together to realize composite benefits, i.e., agility from SL and better performance with reduced cost from VM.","meta":{"url":"http://arxiv.org/abs/2307.13677v1"},"cats":{"new-dataset":0.1811582598,"dev-research":0.3617398631,"prompt-eng":0.3918922061,"data-quality":0.1398522186,"ml-security":0.145723221}}
{"text":"Smartpick uses a machine learning prediction scheme, decision-tree based Random Forest with Bayesian Optimizer, to determine SL and VM configurations, i.e., how many SL and VM instances for queries, that meet cost-performance goals.","meta":{"url":"http://arxiv.org/abs/2307.13677v1"},"cats":{"new-dataset":0.0461953593,"dev-research":0.2965259337,"prompt-eng":0.3804069931,"data-quality":0.1379680978,"ml-security":0.1842419296}}
{"text":"Smartpick offers a knob for applications to allow them to explore a richer cost-performance tradeoff space opened by exploiting SL and VM together.","meta":{"url":"http://arxiv.org/abs/2307.13677v1"},"cats":{"new-dataset":0.0433476823,"dev-research":0.3965418905,"prompt-eng":0.4108944977,"data-quality":0.1205415353,"ml-security":0.2011401613}}
{"text":"To maximize the benefits of SL, Smartpick supports a simple but strong mechanism, called relay-instances.","meta":{"url":"http://arxiv.org/abs/2307.13677v1"},"cats":{"new-dataset":0.0221115398,"dev-research":0.3073783913,"prompt-eng":0.4549905438,"data-quality":0.0938819389,"ml-security":0.2458911109}}
{"text":"Smartpick also supports event-driven prediction model retraining to deal with workload dynamics.","meta":{"url":"http://arxiv.org/abs/2307.13677v1"},"cats":{"new-dataset":0.1218278061,"dev-research":0.3593755786,"prompt-eng":0.4575925666,"data-quality":0.1170383133,"ml-security":0.1665118237}}
{"text":"A Smartpick prototype was implemented on Spark and deployed on live test-beds, Amazon AWS and Google Cloud Platform.","meta":{"url":"http://arxiv.org/abs/2307.13677v1"},"cats":{"new-dataset":0.3302111099,"dev-research":0.3015960328,"prompt-eng":0.4259573001,"data-quality":0.1419662246,"ml-security":0.1748115241}}
{"text":"Evaluation results indicate 97.05% and 83.49% prediction accuracies respectively with up to 50% cost reduction as opposed to the baselines.","meta":{"url":"http://arxiv.org/abs/2307.13677v1"},"cats":{"new-dataset":0.0393017641,"dev-research":0.2919796989,"prompt-eng":0.4503845273,"data-quality":0.1951878339,"ml-security":0.065453235}}
{"text":"The results also confirm that Smartpick allows data analytics applications to navigate the richer cost-performance tradeoff space efficiently and to handle workload dynamics effectively and automatically.","meta":{"url":"http://arxiv.org/abs/2307.13677v1"},"cats":{"new-dataset":0.1688526888,"dev-research":0.4166969847,"prompt-eng":0.3884980836,"data-quality":0.1580522608,"ml-security":0.1321949516}}
{"text":"We study the interaction of structural subtyping with parametric polymorphism and recursively defined type constructors.","meta":{"url":"http://arxiv.org/abs/2307.13661v1"},"cats":{"new-dataset":0.0229669781,"dev-research":0.2746678786,"prompt-eng":0.417350492,"data-quality":0.12111203,"ml-security":0.1061096689}}
{"text":"Although structural subtyping is undecidable in this setting, we describe a notion of parametricity for type constructors and then exploit it to define parametric subtyping, a conceptually simple, decidable, and expressive fragment of structural subtyping that strictly generalizes nominal subtyping.","meta":{"url":"http://arxiv.org/abs/2307.13661v1"},"cats":{"new-dataset":0.0209232209,"dev-research":0.3668927776,"prompt-eng":0.429763779,"data-quality":0.1735933241,"ml-security":0.1243531732}}
{"text":"We present and prove correct an effective saturation-based decision procedure for parametric subtyping, demonstrating its applicability using a variety of examples.","meta":{"url":"http://arxiv.org/abs/2307.13661v1"},"cats":{"new-dataset":0.014117902,"dev-research":0.2670503065,"prompt-eng":0.4800546427,"data-quality":0.1499481851,"ml-security":0.1009792748}}
{"text":"An implementation of this decision procedure is available in an online repository.","meta":{"url":"http://arxiv.org/abs/2307.13661v1"},"cats":{"new-dataset":0.1655892121,"dev-research":0.255217858,"prompt-eng":0.4312502849,"data-quality":0.1408951339,"ml-security":0.0490923568}}
{"text":"The Gromov--Hausdorff distance measures the difference in shape between compact metric spaces and poses a notoriously difficult problem in combinatorial optimization.","meta":{"url":"http://arxiv.org/abs/2307.13660v1"},"cats":{"new-dataset":0.0415552871,"dev-research":0.2293051653,"prompt-eng":0.3375169479,"data-quality":0.1262340312,"ml-security":0.0739493128}}
{"text":"We introduce its quadratic relaxation over a convex polytope whose solutions provably deliver the Gromov--Hausdorff distance.","meta":{"url":"http://arxiv.org/abs/2307.13660v1"},"cats":{"new-dataset":0.0329678724,"dev-research":0.2073329205,"prompt-eng":0.3470544999,"data-quality":0.1308881681,"ml-security":0.1384080751}}
{"text":"The optimality guarantee is enabled by the fact that the search space of our approach is not constrained to a generalization of bijections, unlike in other relaxations such as the Gromov--Wasserstein distance.   ","meta":{"url":"http://arxiv.org/abs/2307.13660v1"},"cats":{"new-dataset":0.0149541332,"dev-research":0.1510624433,"prompt-eng":0.361511497,"data-quality":0.1415322328,"ml-security":0.1156494187}}
{"text":"We suggest the Frank--Wolfe algorithm with $O(n^3)$-time iterations for solving the relaxation and numerically demonstrate its performance on metric spaces of hundreds of points.","meta":{"url":"http://arxiv.org/abs/2307.13660v1"},"cats":{"new-dataset":0.1156896785,"dev-research":0.2453716229,"prompt-eng":0.3457723383,"data-quality":0.1292190477,"ml-security":0.0790936563}}
{"text":"In particular, we obtain a new upper bound of the Gromov--Hausdorff distance between the unit circle and the unit hemisphere equipped with Euclidean metric.","meta":{"url":"http://arxiv.org/abs/2307.13660v1"},"cats":{"new-dataset":0.163732067,"dev-research":0.1838715839,"prompt-eng":0.3518877239,"data-quality":0.1234273662,"ml-security":0.0800156453}}
{"text":"Our approach is implemented as a Python package dGH.","meta":{"url":"http://arxiv.org/abs/2307.13660v1"},"cats":{"new-dataset":0.199829732,"dev-research":0.3049918406,"prompt-eng":0.4601365981,"data-quality":0.1884149726,"ml-security":0.0747438137}}
{"text":"This white paper is a response to the \"AI Accountability Policy Request for Comments\" by the National Telecommunications and Information Administration of the United States.","meta":{"url":"http://arxiv.org/abs/2307.13658v1"},"cats":{"new-dataset":0.1479663135,"dev-research":0.2369083251,"prompt-eng":0.3668004486,"data-quality":0.2536441436,"ml-security":0.2508124998}}
{"text":"The question numbers for which comments were requested are provided in superscripts at the end of key sentences answering the respective questions.","meta":{"url":"http://arxiv.org/abs/2307.13658v1"},"cats":{"new-dataset":0.2372787378,"dev-research":0.256500282,"prompt-eng":0.4671687684,"data-quality":0.1511228803,"ml-security":0.0755411568}}
{"text":"The white paper offers a set of interconnected recommendations for an AI accountability policy.","meta":{"url":"http://arxiv.org/abs/2307.13658v1"},"cats":{"new-dataset":0.0575574346,"dev-research":0.2575686294,"prompt-eng":0.3462852765,"data-quality":0.1207189755,"ml-security":0.1856429084}}
{"text":"The human hand has an inherent ability to manipulate and re-orientate objects without external assistance.","meta":{"url":"http://arxiv.org/abs/2307.13657v1"},"cats":{"new-dataset":0.0158725408,"dev-research":0.2361044748,"prompt-eng":0.3508046888,"data-quality":0.0560137445,"ml-security":0.0713107387}}
{"text":"As a consequence, we are able to operate tools and perform an array of actions using just one hand, without having to continuously re-grasp objects.","meta":{"url":"http://arxiv.org/abs/2307.13657v1"},"cats":{"new-dataset":0.0356843159,"dev-research":0.285518326,"prompt-eng":0.3491055988,"data-quality":0.0514864627,"ml-security":0.1229461805}}
{"text":"Emulating this functionality in robotic end-effectors remains a key area of study with efforts being made to create advanced control systems that could be used to operate complex manipulators.","meta":{"url":"http://arxiv.org/abs/2307.13657v1"},"cats":{"new-dataset":0.030843077,"dev-research":0.2601001082,"prompt-eng":0.472591589,"data-quality":0.0750518238,"ml-security":0.1117086544}}
{"text":"In this paper, a three fingered soft gripper with an active rotary palm is presented as a simpler, alternative method of performing in-hand rotations.","meta":{"url":"http://arxiv.org/abs/2307.13657v1"},"cats":{"new-dataset":0.0296626369,"dev-research":0.2219121141,"prompt-eng":0.3444376294,"data-quality":0.0873294988,"ml-security":0.0538047275}}
{"text":"The gripper, complete with its pneumatic suction cup to prevent object slippage, was tested and found to be able to effectively grasp and rotate a variety of objects both quickly and precisely.","meta":{"url":"http://arxiv.org/abs/2307.13657v1"},"cats":{"new-dataset":0.0243801378,"dev-research":0.2426840622,"prompt-eng":0.3717154187,"data-quality":0.0741986793,"ml-security":0.0691825559}}
{"text":"With the development of pre-trained models and the incorporation of phonetic and graphic information, neural models have achieved high scores in Chinese Spelling Check (CSC).","meta":{"url":"http://arxiv.org/abs/2307.13655v1"},"cats":{"new-dataset":0.0800939452,"dev-research":0.2565810017,"prompt-eng":0.4418032051,"data-quality":0.2728660438,"ml-security":0.1445525048}}
{"text":"However, it does not provide a comprehensive reflection of the models' capability due to the limited test sets.","meta":{"url":"http://arxiv.org/abs/2307.13655v1"},"cats":{"new-dataset":0.0439582013,"dev-research":0.1734079218,"prompt-eng":0.3885321846,"data-quality":0.1116937191,"ml-security":0.0904852578}}
{"text":"In this study, we abstract the representative model paradigm, implement it with nine structures and experiment them on comprehensive test sets we constructed with different purposes.","meta":{"url":"http://arxiv.org/abs/2307.13655v1"},"cats":{"new-dataset":0.1058945275,"dev-research":0.2190078862,"prompt-eng":0.5143163036,"data-quality":0.1409558214,"ml-security":0.120014421}}
{"text":"We perform a detailed analysis of the results and find that: 1) Fusing phonetic and graphic information reasonably is effective for CSC.","meta":{"url":"http://arxiv.org/abs/2307.13655v1"},"cats":{"new-dataset":0.4035644415,"dev-research":0.3054029792,"prompt-eng":0.4573572842,"data-quality":0.358064693,"ml-security":0.1156511709}}
{"text":"2) Models are sensitive to the error distribution of the test set, which reflects the shortcomings of models and reveals the direction we should work on.","meta":{"url":"http://arxiv.org/abs/2307.13655v1"},"cats":{"new-dataset":0.0327870309,"dev-research":0.2693560908,"prompt-eng":0.4454346641,"data-quality":0.4018820066,"ml-security":0.2594589814}}
{"text":"3) Whether or not the errors and contexts have been seen has a significant impact on models.","meta":{"url":"http://arxiv.org/abs/2307.13655v1"},"cats":{"new-dataset":0.0153777848,"dev-research":0.3005494226,"prompt-eng":0.3790979547,"data-quality":0.3755856526,"ml-security":0.149148081}}
{"text":"4) The commonly used benchmark, SIGHAN, can not reliably evaluate models' performance.","meta":{"url":"http://arxiv.org/abs/2307.13655v1"},"cats":{"new-dataset":0.0391727273,"dev-research":0.2755077733,"prompt-eng":0.3752381025,"data-quality":0.2181308392,"ml-security":0.121100167}}
{"text":"Object detection has been widely applied for construction safety management, especially personal protective equipment (PPE) detection.","meta":{"url":"http://arxiv.org/abs/2307.13654v1"},"cats":{"new-dataset":0.0629469306,"dev-research":0.3154319785,"prompt-eng":0.4088211585,"data-quality":0.1847407849,"ml-security":0.2741819238}}
{"text":"Though the existing PPE detection models trained on conventional datasets have achieved excellent results, their performance dramatically declines in extreme construction conditions.","meta":{"url":"http://arxiv.org/abs/2307.13654v1"},"cats":{"new-dataset":0.0935371984,"dev-research":0.2496878536,"prompt-eng":0.4128096543,"data-quality":0.2599734756,"ml-security":0.243457242}}
{"text":"A robust detection model NST-YOLOv5 is developed by combining the neural style transfer (NST) and YOLOv5 technologies.","meta":{"url":"http://arxiv.org/abs/2307.13654v1"},"cats":{"new-dataset":0.0678498492,"dev-research":0.2480845217,"prompt-eng":0.4506114394,"data-quality":0.331057377,"ml-security":0.2249721575}}
{"text":"Five extreme conditions are considered and simulated via the NST module to endow the detection model with excellent robustness, including low light, intense light, sand dust, fog, and rain.","meta":{"url":"http://arxiv.org/abs/2307.13654v1"},"cats":{"new-dataset":0.2124779098,"dev-research":0.2490062438,"prompt-eng":0.4705887897,"data-quality":0.160072496,"ml-security":0.1698858684}}
{"text":"Experiments show that the NST has great potential as a tool for extreme data synthesis since it is better at simulating extreme conditions than other traditional image processing algorithms and helps the NST-YOLOv5 achieve 0.141 and 0.083 mAP_(05:95) improvements in synthesized and real-world extreme data.","meta":{"url":"http://arxiv.org/abs/2307.13654v1"},"cats":{"new-dataset":0.25440435,"dev-research":0.2518160826,"prompt-eng":0.3799855668,"data-quality":0.1229298049,"ml-security":0.1240052023}}
{"text":"This study provides a new feasible way to obtain a more robust detection model for extreme construction conditions.","meta":{"url":"http://arxiv.org/abs/2307.13654v1"},"cats":{"new-dataset":0.1329661057,"dev-research":0.3117636266,"prompt-eng":0.432458951,"data-quality":0.2207579766,"ml-security":0.2916304553}}
{"text":"Image quality remains a key problem for both traditional and deep learning (DL)-based approaches to retinal image analysis, but identifying poor quality images can be time consuming and subjective.","meta":{"url":"http://arxiv.org/abs/2307.13646v1"},"cats":{"new-dataset":0.0518980489,"dev-research":0.3082767095,"prompt-eng":0.3461661032,"data-quality":0.3268708252,"ml-security":0.1203671911}}
{"text":"Thus, automated methods for retinal image quality scoring (RIQS) are needed.","meta":{"url":"http://arxiv.org/abs/2307.13646v1"},"cats":{"new-dataset":0.0394465874,"dev-research":0.2745106884,"prompt-eng":0.4404425548,"data-quality":0.2576936041,"ml-security":0.0509509228}}
{"text":"The current state-of-the-art is MCFNet, composed of three Densenet121 backbones each operating in a different colour space.","meta":{"url":"http://arxiv.org/abs/2307.13646v1"},"cats":{"new-dataset":0.3230205113,"dev-research":0.1971557201,"prompt-eng":0.3750423112,"data-quality":0.1282636738,"ml-security":0.1068981532}}
{"text":"MCFNet, and the EyeQ dataset released by the same authors, was a huge step forward for RIQS.","meta":{"url":"http://arxiv.org/abs/2307.13646v1"},"cats":{"new-dataset":0.4131194307,"dev-research":0.2305888642,"prompt-eng":0.3681357363,"data-quality":0.1501567929,"ml-security":0.0803938837}}
{"text":"We present QuickQual, a simple approach to RIQS, consisting of a single off-the-shelf ImageNet-pretrained Densenet121 backbone plus a Support Vector Machine (SVM).","meta":{"url":"http://arxiv.org/abs/2307.13646v1"},"cats":{"new-dataset":0.3216136364,"dev-research":0.2025703075,"prompt-eng":0.3723721745,"data-quality":0.2141523343,"ml-security":0.1519079766}}
{"text":"QuickQual performs very well, setting a new state-of-the-art for EyeQ (Accuracy: 88.50% vs 88.00% for MCFNet; AUC: 0.9687 vs 0.9588).","meta":{"url":"http://arxiv.org/abs/2307.13646v1"},"cats":{"new-dataset":0.0784465827,"dev-research":0.314711344,"prompt-eng":0.4481262703,"data-quality":0.1934297792,"ml-security":0.0476131009}}
{"text":"This suggests that RIQS can be solved with generic perceptual features learned on natural images, as opposed to requiring DL models trained on large amounts of fundus images.","meta":{"url":"http://arxiv.org/abs/2307.13646v1"},"cats":{"new-dataset":0.093322539,"dev-research":0.1796728314,"prompt-eng":0.4070645632,"data-quality":0.2051609873,"ml-security":0.113244713}}
{"text":"Additionally, we propose a Fixed Prior linearisation scheme, that converts EyeQ from a 3-way classification to a continuous logistic regression task.","meta":{"url":"http://arxiv.org/abs/2307.13646v1"},"cats":{"new-dataset":0.1157374016,"dev-research":0.2305377799,"prompt-eng":0.4505273895,"data-quality":0.1837779392,"ml-security":0.1076047547}}
{"text":"For this task, we present a second model, QuickQual MEga Minified Estimator (QuickQual-MEME), that consists of only 10 parameters on top of an off-the-shelf Densenet121 and can distinguish between gradable and ungradable images with an accuracy of 89.18% (AUC: 0.9537).","meta":{"url":"http://arxiv.org/abs/2307.13646v1"},"cats":{"new-dataset":0.3493538896,"dev-research":0.1754555303,"prompt-eng":0.3987725805,"data-quality":0.2816899749,"ml-security":0.1097764167}}
{"text":"Code and model are available on GitHub: https://github.com/justinengelmann/QuickQual .","meta":{"url":"http://arxiv.org/abs/2307.13646v1"},"cats":{"new-dataset":0.2905530798,"dev-research":0.2702360939,"prompt-eng":0.4424128592,"data-quality":0.1136337311,"ml-security":0.067980387}}
{"text":"QuickQual is so lightweight, that the entire inference code (and even the parameters for QuickQual-MEME) is already contained in this paper.","meta":{"url":"http://arxiv.org/abs/2307.13646v1"},"cats":{"new-dataset":0.0596691521,"dev-research":0.2835523513,"prompt-eng":0.4413453488,"data-quality":0.1782142033,"ml-security":0.093879918}}
{"text":"Obtaining labelled data in medical image segmentation is challenging due to the need for pixel-level annotations by experts.","meta":{"url":"http://arxiv.org/abs/2307.13645v1"},"cats":{"new-dataset":0.3339504956,"dev-research":0.2672267914,"prompt-eng":0.4264082189,"data-quality":0.412402049,"ml-security":0.1085108702}}
{"text":"Recent works have shown that augmenting the object of interest with deformable transformations can help mitigate this challenge.","meta":{"url":"http://arxiv.org/abs/2307.13645v1"},"cats":{"new-dataset":0.0850104333,"dev-research":0.2227190064,"prompt-eng":0.4138387018,"data-quality":0.1650433558,"ml-security":0.1361537666}}
{"text":"However, these transformations have been learned globally for the image, limiting their transferability across datasets or applicability in problems where image alignment is difficult.","meta":{"url":"http://arxiv.org/abs/2307.13645v1"},"cats":{"new-dataset":0.145670205,"dev-research":0.2259473087,"prompt-eng":0.3585410331,"data-quality":0.2054170077,"ml-security":0.1347282554}}
{"text":"While object-centric augmentations provide a great opportunity to overcome these issues, existing works are only focused on position and random transformations without considering shape variations of the objects.","meta":{"url":"http://arxiv.org/abs/2307.13645v1"},"cats":{"new-dataset":0.0376923555,"dev-research":0.2293199882,"prompt-eng":0.4175832172,"data-quality":0.150905111,"ml-security":0.0872978725}}
{"text":"To this end, we propose a novel object-centric data augmentation model that is able to learn the shape variations for the objects of interest and augment the object in place without modifying the rest of the image.","meta":{"url":"http://arxiv.org/abs/2307.13645v1"},"cats":{"new-dataset":0.1453896035,"dev-research":0.21283001,"prompt-eng":0.4086217483,"data-quality":0.1772835793,"ml-security":0.1088889974}}
{"text":"We demonstrated its effectiveness in improving kidney tumour segmentation when leveraging shape variations learned both from within the same dataset and transferred from external datasets.","meta":{"url":"http://arxiv.org/abs/2307.13645v1"},"cats":{"new-dataset":0.2246266759,"dev-research":0.2097026172,"prompt-eng":0.3582298346,"data-quality":0.1963711075,"ml-security":0.1481754722}}
{"text":"Any autonomous controller will be unsafe in some situations.","meta":{"url":"http://arxiv.org/abs/2307.13642v1"},"cats":{"new-dataset":0.0268784604,"dev-research":0.2450725932,"prompt-eng":0.3948073113,"data-quality":0.1276159317,"ml-security":0.4970665738}}
{"text":"The ability to quantitatively identify when these unsafe situations are about to occur is crucial for drawing timely human oversight in, e.g., freight transportation applications.","meta":{"url":"http://arxiv.org/abs/2307.13642v1"},"cats":{"new-dataset":0.0392399586,"dev-research":0.4080887139,"prompt-eng":0.4531753812,"data-quality":0.2069596303,"ml-security":0.2925518875}}
{"text":"In this work, we demonstrate that the true criticality of an agent's situation can be robustly defined as the mean reduction in reward given some number of random actions.","meta":{"url":"http://arxiv.org/abs/2307.13642v1"},"cats":{"new-dataset":0.0625863336,"dev-research":0.1790642892,"prompt-eng":0.4428439936,"data-quality":0.1963650754,"ml-security":0.3415895417}}
{"text":"Proxy criticality metrics that are computable in real-time (i.e., without actually simulating the effects of random actions) can be compared to the true criticality, and we show how to leverage these proxy metrics to generate safety margins, which directly tie the consequences of potentially incorrect actions to an anticipated loss in overall performance.","meta":{"url":"http://arxiv.org/abs/2307.13642v1"},"cats":{"new-dataset":0.0475819158,"dev-research":0.3664128516,"prompt-eng":0.4496695482,"data-quality":0.2216774445,"ml-security":0.423994262}}
{"text":"We evaluate our approach on learned policies from APE-X and A3C within an Atari environment, and demonstrate how safety margins decrease as agents approach failure states.","meta":{"url":"http://arxiv.org/abs/2307.13642v1"},"cats":{"new-dataset":0.118062895,"dev-research":0.3083568968,"prompt-eng":0.415922039,"data-quality":0.2173508832,"ml-security":0.4895519557}}
{"text":"The integration of safety margins into programs for monitoring deployed agents allows for the real-time identification of potentially catastrophic situations.","meta":{"url":"http://arxiv.org/abs/2307.13642v1"},"cats":{"new-dataset":0.2474071647,"dev-research":0.4161939927,"prompt-eng":0.4461142088,"data-quality":0.1801299383,"ml-security":0.3513945085}}
{"text":"Unsupervised localization and segmentation are long-standing robot vision challenges that describe the critical ability for an autonomous robot to learn to decompose images into individual objects without labeled data.","meta":{"url":"http://arxiv.org/abs/2307.13640v1"},"cats":{"new-dataset":0.1516133179,"dev-research":0.2038463156,"prompt-eng":0.4060373998,"data-quality":0.2570418853,"ml-security":0.1499624781}}
{"text":"These tasks are important because of the limited availability of dense image manual annotation and the promising vision of adapting to an evolving set of object categories in lifelong learning.","meta":{"url":"http://arxiv.org/abs/2307.13640v1"},"cats":{"new-dataset":0.2619493643,"dev-research":0.2432981875,"prompt-eng":0.3948824337,"data-quality":0.259691764,"ml-security":0.1068024509}}
{"text":"Most recent methods focus on using visual appearance continuity as object cues by spatially clustering features obtained from self-supervised vision transformers (ViT).","meta":{"url":"http://arxiv.org/abs/2307.13640v1"},"cats":{"new-dataset":0.0485074073,"dev-research":0.2611711758,"prompt-eng":0.4352475646,"data-quality":0.2300455429,"ml-security":0.1075989516}}
{"text":"In this work, we leverage motion cues, inspired by the common fate principle that pixels that share similar movements tend to belong to the same object.","meta":{"url":"http://arxiv.org/abs/2307.13640v1"},"cats":{"new-dataset":0.1247695478,"dev-research":0.2316864632,"prompt-eng":0.4268214282,"data-quality":0.1243174459,"ml-security":0.0831281332}}
{"text":"We propose a new loss term formulation that uses optical flow in unlabeled videos to encourage self-supervised ViT features to become closer to each other if their corresponding spatial locations share similar movements, and vice versa.","meta":{"url":"http://arxiv.org/abs/2307.13640v1"},"cats":{"new-dataset":0.0527548839,"dev-research":0.2159461198,"prompt-eng":0.3868761111,"data-quality":0.2494447997,"ml-security":0.0832840168}}
{"text":"We use the proposed loss function to finetune vision transformers that were originally trained on static images.","meta":{"url":"http://arxiv.org/abs/2307.13640v1"},"cats":{"new-dataset":0.0891393847,"dev-research":0.2324541023,"prompt-eng":0.4030719792,"data-quality":0.2457016129,"ml-security":0.1489333614}}
{"text":"Our fine-tuning procedure outperforms state-of-the-art techniques for unsupervised semantic segmentation through linear probing, without the use of any labeled data.","meta":{"url":"http://arxiv.org/abs/2307.13640v1"},"cats":{"new-dataset":0.2188406639,"dev-research":0.2428860015,"prompt-eng":0.4561066763,"data-quality":0.3979818372,"ml-security":0.1056468725}}
{"text":"This procedure also demonstrates increased performance over original ViT networks across unsupervised object localization and semantic segmentation benchmarks.","meta":{"url":"http://arxiv.org/abs/2307.13640v1"},"cats":{"new-dataset":0.1773525736,"dev-research":0.2478157453,"prompt-eng":0.3979891912,"data-quality":0.2771658215,"ml-security":0.0956521018}}
{"text":"Accurate 3D face shape estimation is an enabling technology with applications in healthcare, security, and creative industries, yet current state-of-the-art methods either rely on self-supervised training with 2D image data or supervised training with very limited 3D data.","meta":{"url":"http://arxiv.org/abs/2307.13639v1"},"cats":{"new-dataset":0.038405217,"dev-research":0.2302723096,"prompt-eng":0.3424336671,"data-quality":0.1398152796,"ml-security":0.225575715}}
{"text":"To bridge this gap, we present a novel approach which uses a conditioned stable diffusion model for face image generation, leveraging the abundance of 2D facial information to inform 3D space.","meta":{"url":"http://arxiv.org/abs/2307.13639v1"},"cats":{"new-dataset":0.1718740711,"dev-research":0.2157234439,"prompt-eng":0.3725492231,"data-quality":0.0911531538,"ml-security":0.1288400281}}
{"text":"By conditioning stable diffusion on depth maps sampled from a 3D Morphable Model (3DMM) of the human face, we generate diverse and shape-consistent images, forming the basis of SynthFace.","meta":{"url":"http://arxiv.org/abs/2307.13639v1"},"cats":{"new-dataset":0.1160903912,"dev-research":0.1900300437,"prompt-eng":0.3679235797,"data-quality":0.0814884933,"ml-security":0.1436232517}}
{"text":"We introduce this large-scale synthesised dataset of 250K photorealistic images and corresponding 3DMM parameters.","meta":{"url":"http://arxiv.org/abs/2307.13639v1"},"cats":{"new-dataset":0.8763439023,"dev-research":0.2045786501,"prompt-eng":0.3565220377,"data-quality":0.1008363423,"ml-security":0.0754867658}}
{"text":"We further propose ControlFace, a deep neural network, trained on SynthFace, which achieves competitive performance on the NoW benchmark, without requiring 3D supervision or manual 3D asset creation.","meta":{"url":"http://arxiv.org/abs/2307.13639v1"},"cats":{"new-dataset":0.3531545738,"dev-research":0.2940531085,"prompt-eng":0.377807642,"data-quality":0.1025262574,"ml-security":0.158444897}}
{"text":"The analysis of brain signals holds considerable importance in enhancing our comprehension of diverse learning techniques and cognitive mechanisms.","meta":{"url":"http://arxiv.org/abs/2307.13637v1"},"cats":{"new-dataset":0.0249880206,"dev-research":0.3438860401,"prompt-eng":0.3787263223,"data-quality":0.1346274512,"ml-security":0.1446670651}}
{"text":"Game-based learning is increasingly being recognized for its interactive and engaging educational approach.","meta":{"url":"http://arxiv.org/abs/2307.13637v1"},"cats":{"new-dataset":0.0739951873,"dev-research":0.3102360961,"prompt-eng":0.3339699226,"data-quality":0.0720380753,"ml-security":0.1323366024}}
{"text":"A pilot study of twelve participants divided into experimental and control groups was conducted to understand its effects on cognitive processes.","meta":{"url":"http://arxiv.org/abs/2307.13637v1"},"cats":{"new-dataset":0.0169719253,"dev-research":0.3859820357,"prompt-eng":0.4167231306,"data-quality":0.0906301634,"ml-security":0.1226911168}}
{"text":"Both groups were provided with the same contents regarding the basic structure of the graph.","meta":{"url":"http://arxiv.org/abs/2307.13637v1"},"cats":{"new-dataset":0.1262278739,"dev-research":0.2239134082,"prompt-eng":0.3135441968,"data-quality":0.1359561798,"ml-security":0.0392491477}}
{"text":"The participants in the experimental group engaged in a quiz-based game, while those in the control group watched a pre-recorded video.","meta":{"url":"http://arxiv.org/abs/2307.13637v1"},"cats":{"new-dataset":0.2425728292,"dev-research":0.3163253931,"prompt-eng":0.4367289427,"data-quality":0.1321425935,"ml-security":0.1306966925}}
{"text":"Functional Near-Infrared Spectroscopy (fNIRS) was employed to acquire cerebral signals, and a series of pre and post-tests were administered.","meta":{"url":"http://arxiv.org/abs/2307.13637v1"},"cats":{"new-dataset":0.0471560092,"dev-research":0.265199019,"prompt-eng":0.367154947,"data-quality":0.0977874765,"ml-security":0.0729348798}}
{"text":"The findings of our study indicate that the group engaged in the game activity displayed elevated levels of oxygenated hemoglobin compared to the group involved in watching videos.","meta":{"url":"http://arxiv.org/abs/2307.13637v1"},"cats":{"new-dataset":0.2073221356,"dev-research":0.3391849403,"prompt-eng":0.3483899175,"data-quality":0.1105337607,"ml-security":0.0944345452}}
{"text":"Conversely, the deoxygenated hemoglobin levels remained relatively consistent across both groups throughout the learning process.","meta":{"url":"http://arxiv.org/abs/2307.13637v1"},"cats":{"new-dataset":0.0093260058,"dev-research":0.2728142122,"prompt-eng":0.3201740031,"data-quality":0.1821445052,"ml-security":0.0856124859}}
{"text":"The aforementioned findings suggest that the use of game-based learning has a substantial influence on cognitive processes.","meta":{"url":"http://arxiv.org/abs/2307.13637v1"},"cats":{"new-dataset":0.0123225979,"dev-research":0.3867789011,"prompt-eng":0.3275808017,"data-quality":0.0967472595,"ml-security":0.1398297294}}
{"text":"Furthermore, it is evident that both the game and video groups exhibited higher neural activity in the Lateral Prefrontal cortex (PFC).","meta":{"url":"http://arxiv.org/abs/2307.13637v1"},"cats":{"new-dataset":0.0644649627,"dev-research":0.2435027212,"prompt-eng":0.360788842,"data-quality":0.0930325571,"ml-security":0.0897838048}}
{"text":"The oxygenated hemoglobin ratio demonstrates that the game group had 2.33 times more neural processing in the Lateral PFC than the video group.","meta":{"url":"http://arxiv.org/abs/2307.13637v1"},"cats":{"new-dataset":0.0634717752,"dev-research":0.250361527,"prompt-eng":0.3553298334,"data-quality":0.1259596509,"ml-security":0.0587133897}}
{"text":"This data is further supported by the knowledge gain analysis, which indicates that the game-based approach resulted in a 47.74% higher knowledge gain than the video group, as calculated from the difference in pre-and post-test scores.","meta":{"url":"http://arxiv.org/abs/2307.13637v1"},"cats":{"new-dataset":0.1467729693,"dev-research":0.3155669399,"prompt-eng":0.3860591449,"data-quality":0.1479187784,"ml-security":0.1031558989}}
{"text":"Mainstream bias, where some users receive poor recommendations because their preferences are uncommon or simply because they are less active, is an important aspect to consider regarding fairness in recommender systems.","meta":{"url":"http://arxiv.org/abs/2307.13632v1"},"cats":{"new-dataset":0.0042282664,"dev-research":0.2719789206,"prompt-eng":0.3758961468,"data-quality":0.2240659049,"ml-security":0.2481052669}}
{"text":"Existing methods to mitigate mainstream bias do not explicitly model the importance of these non-mainstream users or, when they do, it is in a way that is not necessarily compatible with the data and recommendation model at hand.","meta":{"url":"http://arxiv.org/abs/2307.13632v1"},"cats":{"new-dataset":0.0059906994,"dev-research":0.2232598018,"prompt-eng":0.3330718064,"data-quality":0.2314142268,"ml-security":0.2544330821}}
{"text":"In contrast, we use the recommendation utility as a more generic and implicit proxy to quantify mainstreamness, and propose a simple user-weighting approach to incorporate it into the training process while taking the cost of potential recommendation errors into account.","meta":{"url":"http://arxiv.org/abs/2307.13632v1"},"cats":{"new-dataset":0.0299622112,"dev-research":0.2901667678,"prompt-eng":0.4451424945,"data-quality":0.3203361202,"ml-security":0.1847947268}}
{"text":"We provide extensive experimental results showing that quantifying mainstreamness via utility is better able at identifying non-mainstream users, and that they are indeed better served when training the model in a cost-sensitive way.","meta":{"url":"http://arxiv.org/abs/2307.13632v1"},"cats":{"new-dataset":0.0420208377,"dev-research":0.2307190176,"prompt-eng":0.4647490211,"data-quality":0.2897048407,"ml-security":0.3266965703}}
{"text":"This is achieved with negligible or no loss in overall recommendation accuracy, meaning that the models learn a better balance across users.","meta":{"url":"http://arxiv.org/abs/2307.13632v1"},"cats":{"new-dataset":0.0023945502,"dev-research":0.25739105,"prompt-eng":0.4171603696,"data-quality":0.23676579,"ml-security":0.1594785447}}
{"text":"In addition, we show that research of this kind, which evaluates recommendation quality at the individual user level, may not be reliable if not using enough interactions when assessing model performance.","meta":{"url":"http://arxiv.org/abs/2307.13632v1"},"cats":{"new-dataset":0.015075268,"dev-research":0.3134746967,"prompt-eng":0.4884640544,"data-quality":0.1934120327,"ml-security":0.1230677094}}
{"text":"This thesis work falls within the framework of question answering (QA) in the biomedical domain where several specific challenges are addressed, such as specialized lexicons and terminologies, the types of treated questions, and the characteristics of targeted documents.","meta":{"url":"http://arxiv.org/abs/2307.13631v1"},"cats":{"new-dataset":0.1167128395,"dev-research":0.2417935873,"prompt-eng":0.4147726236,"data-quality":0.1985587576,"ml-security":0.1239537743}}
{"text":"We are particularly interested in studying and improving methods that aim at finding accurate and short answers to biomedical natural language questions from a large scale of biomedical textual documents in English.","meta":{"url":"http://arxiv.org/abs/2307.13631v1"},"cats":{"new-dataset":0.1560648589,"dev-research":0.2510260539,"prompt-eng":0.4128252897,"data-quality":0.2967306521,"ml-security":0.0651343273}}
{"text":"QA aims at providing inquirers with direct, short and precise answers to their natural language questions.","meta":{"url":"http://arxiv.org/abs/2307.13631v1"},"cats":{"new-dataset":0.0626826422,"dev-research":0.2869123774,"prompt-eng":0.4171727611,"data-quality":0.1715155705,"ml-security":0.078063092}}
{"text":"In this Ph.D. thesis, we propose four contributions to improve the performance of QA in the biomedical domain.","meta":{"url":"http://arxiv.org/abs/2307.13631v1"},"cats":{"new-dataset":0.0294357447,"dev-research":0.2082639535,"prompt-eng":0.3807528758,"data-quality":0.1193328192,"ml-security":0.0737614309}}
{"text":"In our first contribution, we propose a machine learning-based method for question type classification to determine the types of given questions which enable to a biomedical QA system to use the appropriate answer extraction method.","meta":{"url":"http://arxiv.org/abs/2307.13631v1"},"cats":{"new-dataset":0.0588279683,"dev-research":0.2486286531,"prompt-eng":0.4414249782,"data-quality":0.2428304394,"ml-security":0.1502321473}}
{"text":"We also propose an another machine learning-based method to assign one or more topics (e.g., pharmacological, test, treatment, etc.) to given questions in order to determine the semantic types of the expected answers which are very useful in generating specific answer retrieval strategies.","meta":{"url":"http://arxiv.org/abs/2307.13631v1"},"cats":{"new-dataset":0.0213185732,"dev-research":0.259309863,"prompt-eng":0.4261782989,"data-quality":0.246221266,"ml-security":0.1739266506}}
{"text":"In the second contribution, we first propose a document retrieval method to retrieve a set of relevant documents that are likely to contain the answers to biomedical questions from the MEDLINE database.","meta":{"url":"http://arxiv.org/abs/2307.13631v1"},"cats":{"new-dataset":0.3047221088,"dev-research":0.1991672393,"prompt-eng":0.417255927,"data-quality":0.1900773155,"ml-security":0.0481694952}}
{"text":"We then present a passage retrieval method to retrieve a set of relevant passages to questions.","meta":{"url":"http://arxiv.org/abs/2307.13631v1"},"cats":{"new-dataset":0.181725686,"dev-research":0.2416055167,"prompt-eng":0.466276427,"data-quality":0.2428532196,"ml-security":0.0480918179}}
{"text":"In the third contribution, we propose specific answer extraction methods to generate both exact and ideal answers.","meta":{"url":"http://arxiv.org/abs/2307.13631v1"},"cats":{"new-dataset":0.0822981109,"dev-research":0.2582948343,"prompt-eng":0.4518427227,"data-quality":0.1985918179,"ml-security":0.0607056847}}
{"text":"Finally, in the fourth contribution, we develop a fully automated semantic biomedical QA system called SemBioNLQA which is able to deal with a variety of natural language questions and to generate appropriate answers by providing both exact and ideal answers.","meta":{"url":"http://arxiv.org/abs/2307.13631v1"},"cats":{"new-dataset":0.2814789954,"dev-research":0.300741251,"prompt-eng":0.4550875719,"data-quality":0.2386343408,"ml-security":0.0604546374}}
{"text":"Idealized first-principles models of chemical plants can be inaccurate.","meta":{"url":"http://arxiv.org/abs/2307.13621v1"},"cats":{"new-dataset":0.0066473301,"dev-research":0.2674535304,"prompt-eng":0.364641203,"data-quality":0.284260469,"ml-security":0.1091597474}}
{"text":"An alternative is to fit a Machine Learning (ML) model directly to plant sensor data.","meta":{"url":"http://arxiv.org/abs/2307.13621v1"},"cats":{"new-dataset":0.1486591369,"dev-research":0.2328689112,"prompt-eng":0.3989060816,"data-quality":0.1587872471,"ml-security":0.153688599}}
{"text":"We use a structured approach: Each unit within the plant gets represented by one ML model.","meta":{"url":"http://arxiv.org/abs/2307.13621v1"},"cats":{"new-dataset":0.1317691944,"dev-research":0.1899981085,"prompt-eng":0.4688096565,"data-quality":0.1539704449,"ml-security":0.054383701}}
{"text":"After fitting the models to the data, the models are connected into a flowsheet-like directed graph.","meta":{"url":"http://arxiv.org/abs/2307.13621v1"},"cats":{"new-dataset":0.1046157807,"dev-research":0.2439093718,"prompt-eng":0.3986142256,"data-quality":0.1351016511,"ml-security":0.0778381298}}
{"text":"We find that for smaller plants, this approach works well, but for larger plants, the complex dynamics arising from large and nested cycles in the flowsheet lead to instabilities in the cycle solver.","meta":{"url":"http://arxiv.org/abs/2307.13621v1"},"cats":{"new-dataset":0.0640455728,"dev-research":0.2410454217,"prompt-eng":0.3720148293,"data-quality":0.0893962603,"ml-security":0.0778453023}}
{"text":"We analyze this problem in depth and show that it is not merely a specialized concern but rather a more pervasive challenge that will likely occur whenever ML is applied to larger plants.","meta":{"url":"http://arxiv.org/abs/2307.13621v1"},"cats":{"new-dataset":0.1084685923,"dev-research":0.1956021776,"prompt-eng":0.4253337215,"data-quality":0.2648095898,"ml-security":0.1441941915}}
{"text":"To address this problem, we present a way to fine-tune ML models such that solving cycles with the usual methods becomes robust again.","meta":{"url":"http://arxiv.org/abs/2307.13621v1"},"cats":{"new-dataset":0.0830293536,"dev-research":0.237715891,"prompt-eng":0.4499350522,"data-quality":0.3258096158,"ml-security":0.2708720994}}
{"text":"End-to-end region-based object detectors like Sparse R-CNN usually have multiple cascade bounding box decoding stages, which refine the current predictions according to their previous results.","meta":{"url":"http://arxiv.org/abs/2307.13619v1"},"cats":{"new-dataset":0.1141046708,"dev-research":0.1907342944,"prompt-eng":0.3856263113,"data-quality":0.2178082805,"ml-security":0.1441393609}}
{"text":"Model parameters within each stage are independent, evolving a huge cost.","meta":{"url":"http://arxiv.org/abs/2307.13619v1"},"cats":{"new-dataset":0.0149881034,"dev-research":0.2129443313,"prompt-eng":0.3936615167,"data-quality":0.0957012505,"ml-security":0.1726356718}}
{"text":"In this paper, we find the general setting of decoding stages is actually redundant.","meta":{"url":"http://arxiv.org/abs/2307.13619v1"},"cats":{"new-dataset":0.0437067144,"dev-research":0.2239822776,"prompt-eng":0.4439556429,"data-quality":0.2618836894,"ml-security":0.15068726}}
{"text":"By simply sharing parameters and making a recursive decoder, the detector already obtains a significant improvement.","meta":{"url":"http://arxiv.org/abs/2307.13619v1"},"cats":{"new-dataset":0.0172011117,"dev-research":0.2926061161,"prompt-eng":0.4969353466,"data-quality":0.2331192408,"ml-security":0.182116809}}
{"text":"The recursive decoder can be further enhanced by positional encoding (PE) of the proposal box, which makes it aware of the exact locations and sizes of input bounding boxes, thus becoming adaptive to proposals from different stages during the recursion.","meta":{"url":"http://arxiv.org/abs/2307.13619v1"},"cats":{"new-dataset":0.0554728837,"dev-research":0.3216428506,"prompt-eng":0.4851865109,"data-quality":0.1430814979,"ml-security":0.0893590136}}
{"text":"Moreover, we also design centerness-based PE to distinguish the RoI feature element and dynamic convolution kernels at different positions within the bounding box.","meta":{"url":"http://arxiv.org/abs/2307.13619v1"},"cats":{"new-dataset":0.0625378941,"dev-research":0.2255885237,"prompt-eng":0.4129457451,"data-quality":0.1971989499,"ml-security":0.1222077087}}
{"text":"To validate the effectiveness of the proposed method, we conduct intensive ablations and build the full model on three recent mainstream region-based detectors.","meta":{"url":"http://arxiv.org/abs/2307.13619v1"},"cats":{"new-dataset":0.1560927132,"dev-research":0.1898794705,"prompt-eng":0.4600756562,"data-quality":0.2396759932,"ml-security":0.0652999321}}
{"text":"The RecusiveDet is able to achieve obvious performance boosts with even fewer model parameters and slightly increased computation cost.","meta":{"url":"http://arxiv.org/abs/2307.13619v1"},"cats":{"new-dataset":0.0173066324,"dev-research":0.2979695379,"prompt-eng":0.4395804562,"data-quality":0.1168205767,"ml-security":0.1212676809}}
{"text":"Codes are available at https://github.com/bravezzzzzz/RecursiveDet.","meta":{"url":"http://arxiv.org/abs/2307.13619v1"},"cats":{"new-dataset":0.4842787981,"dev-research":0.2955720857,"prompt-eng":0.4669684972,"data-quality":0.1542799096,"ml-security":0.0748145118}}
{"text":"Financial analysis is an important tool for evaluating company performance.","meta":{"url":"http://arxiv.org/abs/2307.13617v1"},"cats":{"new-dataset":0.0078752993,"dev-research":0.4373117708,"prompt-eng":0.340652466,"data-quality":0.1109043969,"ml-security":0.0575926404}}
{"text":"Practitioners work to answer financial questions to make profitable investment decisions, and use advanced quantitative analyses to do so.","meta":{"url":"http://arxiv.org/abs/2307.13617v1"},"cats":{"new-dataset":0.0196929821,"dev-research":0.3676359419,"prompt-eng":0.3474597159,"data-quality":0.0704188018,"ml-security":0.0684137277}}
{"text":"As a result, Financial Question Answering (QA) is a question answering task that requires deep reasoning about numbers.","meta":{"url":"http://arxiv.org/abs/2307.13617v1"},"cats":{"new-dataset":0.0596165994,"dev-research":0.2331359784,"prompt-eng":0.3332907104,"data-quality":0.152962786,"ml-security":0.0927779301}}
{"text":"Furthermore, it is unknown how well pre-trained language models can reason in the financial domain.","meta":{"url":"http://arxiv.org/abs/2307.13617v1"},"cats":{"new-dataset":0.0233560018,"dev-research":0.2680234089,"prompt-eng":0.352353374,"data-quality":0.3260283376,"ml-security":0.1984804961}}
{"text":"The current state-of-the-art requires a retriever to collect relevant facts about the financial question from the text and a generator to produce a valid financial program and a final answer.","meta":{"url":"http://arxiv.org/abs/2307.13617v1"},"cats":{"new-dataset":0.2202480174,"dev-research":0.2460988232,"prompt-eng":0.4464413671,"data-quality":0.1940953003,"ml-security":0.0740202595}}
{"text":"However, recently large language models like GPT-3 have achieved state-of-the-art performance on wide variety of tasks with just a few shot examples.","meta":{"url":"http://arxiv.org/abs/2307.13617v1"},"cats":{"new-dataset":0.1393764166,"dev-research":0.2566905606,"prompt-eng":0.4170640624,"data-quality":0.1554023813,"ml-security":0.0789722012}}
{"text":"We run several experiments with GPT-3 and find that a separate retrieval model and logic engine continue to be essential components to achieving SOTA performance in this task, particularly due to the precise nature of financial questions and the complex information stored in financial documents.","meta":{"url":"http://arxiv.org/abs/2307.13617v1"},"cats":{"new-dataset":0.1047822252,"dev-research":0.2226630384,"prompt-eng":0.423607894,"data-quality":0.1335161675,"ml-security":0.0619623162}}
{"text":"With this understanding, our refined prompt-engineering approach on GPT-3 achieves near SOTA accuracy without any fine-tuning.","meta":{"url":"http://arxiv.org/abs/2307.13617v1"},"cats":{"new-dataset":0.078933759,"dev-research":0.2428650072,"prompt-eng":0.5230461286,"data-quality":0.1599621621,"ml-security":0.0863872275}}
{"text":"Similarity analysis using neural networks has emerged as a powerful technique for understanding and categorizing complex patterns in various domains.","meta":{"url":"http://arxiv.org/abs/2307.13606v1"},"cats":{"new-dataset":0.042096798,"dev-research":0.3023791963,"prompt-eng":0.3272709669,"data-quality":0.1727620656,"ml-security":0.1139921347}}
{"text":"By leveraging the latent representations learned by neural networks, data objects such as images can be compared effectively.","meta":{"url":"http://arxiv.org/abs/2307.13606v1"},"cats":{"new-dataset":0.0415459953,"dev-research":0.3018127717,"prompt-eng":0.3853387862,"data-quality":0.2639988774,"ml-security":0.1375864482}}
{"text":"This research explores the utilization of latent information generated by fully convolutional networks (FCNs) in similarity analysis, notably to estimate the visual resemblance of objects segmented in 2D pictures.","meta":{"url":"http://arxiv.org/abs/2307.13606v1"},"cats":{"new-dataset":0.193700135,"dev-research":0.2336823044,"prompt-eng":0.3286784446,"data-quality":0.1970837094,"ml-security":0.0718693192}}
{"text":"To do this, the analytical scheme comprises two steps: (1) extracting and transforming feature patterns per 2D object from a trained FCN, and (2) identifying the most similar patterns through fuzzy inference.","meta":{"url":"http://arxiv.org/abs/2307.13606v1"},"cats":{"new-dataset":0.1607624458,"dev-research":0.2733876178,"prompt-eng":0.3936938403,"data-quality":0.1333229423,"ml-security":0.0765047628}}
{"text":"The step (2) can be further enhanced by incorporating a weighting scheme that considers the significance of latent variables in the analysis.","meta":{"url":"http://arxiv.org/abs/2307.13606v1"},"cats":{"new-dataset":0.0159292788,"dev-research":0.2310329405,"prompt-eng":0.4559784785,"data-quality":0.1908388127,"ml-security":0.0671791093}}
{"text":"The results provide valuable insights into the benefits and challenges of employing neural network-based similarity analysis for discerning data patterns effectively.","meta":{"url":"http://arxiv.org/abs/2307.13606v1"},"cats":{"new-dataset":0.0854800809,"dev-research":0.315620165,"prompt-eng":0.3466396822,"data-quality":0.3352962407,"ml-security":0.1811277639}}
{"text":"In recent years the use of Artificial Intelligence (AI) has become increasingly prevalent in a growing number of fields.","meta":{"url":"http://arxiv.org/abs/2307.13601v1"},"cats":{"new-dataset":0.0242364467,"dev-research":0.3239858346,"prompt-eng":0.3025408547,"data-quality":0.0812312968,"ml-security":0.1626969735}}
{"text":"As AI systems are being adopted in more high-stakes areas such as medicine and finance, ensuring that they are trustworthy is of increasing importance.","meta":{"url":"http://arxiv.org/abs/2307.13601v1"},"cats":{"new-dataset":0.0213645837,"dev-research":0.3546864214,"prompt-eng":0.3690878428,"data-quality":0.1252114886,"ml-security":0.394963015}}
{"text":"A concern that is prominently addressed by the development and application of explainability methods, which are purported to increase trust from its users and wider society.","meta":{"url":"http://arxiv.org/abs/2307.13601v1"},"cats":{"new-dataset":0.01218641,"dev-research":0.5182535911,"prompt-eng":0.4129463178,"data-quality":0.2562767118,"ml-security":0.3917505292}}
{"text":"While an increase in trust may be desirable, an analysis of literature from different research fields shows that an exclusive focus on increasing trust may not be warranted.","meta":{"url":"http://arxiv.org/abs/2307.13601v1"},"cats":{"new-dataset":0.0021727066,"dev-research":0.2615726101,"prompt-eng":0.3507631463,"data-quality":0.1730941507,"ml-security":0.3372515936}}
{"text":"Something which is well exemplified by the recent development in AI chatbots, which while highly coherent tend to make up facts.","meta":{"url":"http://arxiv.org/abs/2307.13601v1"},"cats":{"new-dataset":0.1741453204,"dev-research":0.4018573309,"prompt-eng":0.3449882449,"data-quality":0.1958838053,"ml-security":0.1449236269}}
{"text":"In this contribution, we investigate the concepts of trust, trustworthiness, and user reliance.   ","meta":{"url":"http://arxiv.org/abs/2307.13601v1"},"cats":{"new-dataset":0.0224329094,"dev-research":0.2790427783,"prompt-eng":0.403298251,"data-quality":0.1611771459,"ml-security":0.1792262756}}
{"text":"In order to foster appropriate reliance on AI we need to prevent both disuse of these systems as well as overtrust.","meta":{"url":"http://arxiv.org/abs/2307.13601v1"},"cats":{"new-dataset":0.0190754845,"dev-research":0.2956753717,"prompt-eng":0.3451108461,"data-quality":0.1243355614,"ml-security":0.5472963886}}
{"text":"From our analysis of research on interpersonal trust, trust in automation, and trust in (X)AI, we identify the potential merit of the distinction between trust and distrust (in AI).","meta":{"url":"http://arxiv.org/abs/2307.13601v1"},"cats":{"new-dataset":0.0286490092,"dev-research":0.3903099406,"prompt-eng":0.4044362949,"data-quality":0.1891192058,"ml-security":0.2855245352}}
{"text":"We propose that alongside trust a healthy amount of distrust is of additional value for mitigating disuse and overtrust.","meta":{"url":"http://arxiv.org/abs/2307.13601v1"},"cats":{"new-dataset":0.0601258775,"dev-research":0.3065920067,"prompt-eng":0.4221557534,"data-quality":0.1896862737,"ml-security":0.3805260009}}
{"text":"We argue that by considering and evaluating both trust and distrust, we can ensure that users can rely appropriately on trustworthy AI, which can both be useful as well as fallible.","meta":{"url":"http://arxiv.org/abs/2307.13601v1"},"cats":{"new-dataset":0.0233170412,"dev-research":0.3648276981,"prompt-eng":0.445272227,"data-quality":0.2280966281,"ml-security":0.4439928921}}
{"text":"Optical sensors have played a pivotal role in acquiring real world data for critical applications.","meta":{"url":"http://arxiv.org/abs/2307.13600v1"},"cats":{"new-dataset":0.22555007,"dev-research":0.2364248812,"prompt-eng":0.3763500294,"data-quality":0.1188938164,"ml-security":0.100895856}}
{"text":"This data, when integrated with advanced machine learning algorithms provides meaningful information thus enhancing human vision.","meta":{"url":"http://arxiv.org/abs/2307.13600v1"},"cats":{"new-dataset":0.3794180748,"dev-research":0.2426272466,"prompt-eng":0.3892666487,"data-quality":0.1361862902,"ml-security":0.1015331415}}
{"text":"This paper focuses on various optical technologies for design and development of state-of-the-art out-cabin forward vision systems and in-cabin driver monitoring systems.","meta":{"url":"http://arxiv.org/abs/2307.13600v1"},"cats":{"new-dataset":0.0799593936,"dev-research":0.2345198794,"prompt-eng":0.3935220023,"data-quality":0.1397795373,"ml-security":0.0639574782}}
{"text":"The focused optical sensors include Longwave Thermal Imaging (LWIR) cameras, Near Infrared (NIR), Neuromorphic/ event cameras, Visible CMOS cameras and Depth cameras.","meta":{"url":"http://arxiv.org/abs/2307.13600v1"},"cats":{"new-dataset":0.2705282072,"dev-research":0.1998441959,"prompt-eng":0.3647510982,"data-quality":0.0965044786,"ml-security":0.0561164918}}
{"text":"Further the paper discusses different potential applications which can be employed using the unique strengths of each these optical modalities in real time environment.","meta":{"url":"http://arxiv.org/abs/2307.13600v1"},"cats":{"new-dataset":0.0258327618,"dev-research":0.1903865107,"prompt-eng":0.3758076097,"data-quality":0.0708575949,"ml-security":0.0355551783}}
{"text":"Mesh-based numerical solvers are an important part in many design tool chains.","meta":{"url":"http://arxiv.org/abs/2307.13592v1"},"cats":{"new-dataset":0.0071296657,"dev-research":0.4086963252,"prompt-eng":0.3763085647,"data-quality":0.0746241189,"ml-security":0.0904048765}}
{"text":"However, accurate simulations like computational fluid dynamics are time and resource consuming which is why surrogate models are employed to speed-up the solution process.","meta":{"url":"http://arxiv.org/abs/2307.13592v1"},"cats":{"new-dataset":0.0091204442,"dev-research":0.29037822,"prompt-eng":0.3441299338,"data-quality":0.0465459361,"ml-security":0.1154715028}}
{"text":"Machine Learning based surrogate models on the other hand are fast in predicting approximate solutions but often lack accuracy.","meta":{"url":"http://arxiv.org/abs/2307.13592v1"},"cats":{"new-dataset":0.0093638902,"dev-research":0.2391093786,"prompt-eng":0.3350551082,"data-quality":0.1311220271,"ml-security":0.2075535657}}
{"text":"Thus, the development of the predictor in a predictor-corrector approach is the focus here, where the surrogate model predicts a flow field and the numerical solver corrects it.","meta":{"url":"http://arxiv.org/abs/2307.13592v1"},"cats":{"new-dataset":0.0210705427,"dev-research":0.3115496004,"prompt-eng":0.4177470993,"data-quality":0.1664039262,"ml-security":0.1653091868}}
{"text":"This paper scales a state-of-the-art surrogate model from the domain of graph-based machine learning to industry-relevant mesh sizes of a numerical flow simulation.","meta":{"url":"http://arxiv.org/abs/2307.13592v1"},"cats":{"new-dataset":0.0577401576,"dev-research":0.2403667451,"prompt-eng":0.3052672852,"data-quality":0.0910918043,"ml-security":0.1956621207}}
{"text":"The approach partitions and distributes the flow domain to multiple GPUs and provides halo exchange between these partitions during training.","meta":{"url":"http://arxiv.org/abs/2307.13592v1"},"cats":{"new-dataset":0.0725884981,"dev-research":0.2129847668,"prompt-eng":0.3906143151,"data-quality":0.0952465523,"ml-security":0.1312320537}}
{"text":"The utilized graph neural network operates directly on the numerical mesh and is able to preserve complex geometries as well as all other properties of the mesh.","meta":{"url":"http://arxiv.org/abs/2307.13592v1"},"cats":{"new-dataset":0.0244494349,"dev-research":0.2485555557,"prompt-eng":0.2793448469,"data-quality":0.0950800966,"ml-security":0.1196071592}}
{"text":"The proposed surrogate model is evaluated with an application on a three dimensional turbomachinery setup and compared to a traditionally trained distributed model.","meta":{"url":"http://arxiv.org/abs/2307.13592v1"},"cats":{"new-dataset":0.0260031985,"dev-research":0.1903775991,"prompt-eng":0.4071680812,"data-quality":0.0776645112,"ml-security":0.1927101323}}
{"text":"The results show that the traditional approach produces superior predictions and outperforms the proposed surrogate model.","meta":{"url":"http://arxiv.org/abs/2307.13592v1"},"cats":{"new-dataset":0.0294599387,"dev-research":0.1952447155,"prompt-eng":0.41077033,"data-quality":0.1268866459,"ml-security":0.1292749708}}
{"text":"Possible explanations, improvements and future directions are outlined.","meta":{"url":"http://arxiv.org/abs/2307.13592v1"},"cats":{"new-dataset":0.011156788,"dev-research":0.3376003397,"prompt-eng":0.4064487737,"data-quality":0.140560374,"ml-security":0.0896883911}}
{"text":"A central issue lying at the heart of online reinforcement learning (RL) is data efficiency.","meta":{"url":"http://arxiv.org/abs/2307.13586v1"},"cats":{"new-dataset":0.0152532137,"dev-research":0.2704729791,"prompt-eng":0.3287026411,"data-quality":0.1184214452,"ml-security":0.1727842476}}
{"text":"While a number of recent works achieved asymptotically minimal regret in online RL, the optimality of these results is only guaranteed in a ``large-sample'' regime, imposing enormous burn-in cost in order for their algorithms to operate optimally.","meta":{"url":"http://arxiv.org/abs/2307.13586v1"},"cats":{"new-dataset":0.0223074667,"dev-research":0.1688991111,"prompt-eng":0.3409160094,"data-quality":0.1283724581,"ml-security":0.1748836291}}
{"text":"How to achieve minimax-optimal regret without incurring any burn-in cost has been an open problem in RL theory.   ","meta":{"url":"http://arxiv.org/abs/2307.13586v1"},"cats":{"new-dataset":0.0121773778,"dev-research":0.1755948665,"prompt-eng":0.3381047519,"data-quality":0.1069219784,"ml-security":0.2020570322}}
{"text":"We settle this problem for the context of finite-horizon inhomogeneous Markov decision processes.","meta":{"url":"http://arxiv.org/abs/2307.13586v1"},"cats":{"new-dataset":0.0385453701,"dev-research":0.1527139403,"prompt-eng":0.3912141913,"data-quality":0.1145843355,"ml-security":0.1892770959}}
{"text":"Specifically, we prove that a modified version of Monotonic Value Propagation (MVP), a model-based algorithm proposed by \\cite{zhang2020reinforcement}, achieves a regret on the order of (modulo log factors)","meta":{"url":"http://arxiv.org/abs/2307.13586v1"},"cats":{"new-dataset":0.0349064792,"dev-research":0.2236728845,"prompt-eng":0.4262000177,"data-quality":0.1701722532,"ml-security":0.1195373602}}
{"text":"\\begin{equation*}   \\min\\big\\{ \\sqrt{SAH^3K}, \\,HK \\big\\}, \\end{equation*} where $S$ is the number of states, $A$ is the number of actions, $H$ is the planning horizon, and $K$ is the total number of episodes.","meta":{"url":"http://arxiv.org/abs/2307.13586v1"},"cats":{"new-dataset":0.2250565738,"dev-research":0.1845729351,"prompt-eng":0.3593179466,"data-quality":0.0543354872,"ml-security":0.0679474411}}
{"text":"This regret matches the minimax lower bound for the entire range of sample size $K\\geq 1$, essentially eliminating any burn-in requirement.","meta":{"url":"http://arxiv.org/abs/2307.13586v1"},"cats":{"new-dataset":0.0415395306,"dev-research":0.1853041205,"prompt-eng":0.3410680353,"data-quality":0.2186538212,"ml-security":0.1443828438}}
{"text":"It also translates to a PAC sample complexity (i.e., the number of episodes needed to yield $\\varepsilon$-accuracy) of $\\frac{SAH^3}{\\varepsilon^2}$ up to log factor, which is minimax-optimal for the full $\\varepsilon$-range.   ","meta":{"url":"http://arxiv.org/abs/2307.13586v1"},"cats":{"new-dataset":0.025731251,"dev-research":0.1747621551,"prompt-eng":0.3003482285,"data-quality":0.1328383345,"ml-security":0.0633382045}}
{"text":"Further, we extend our theory to unveil the influences of problem-dependent quantities like the optimal value/cost and certain variances.","meta":{"url":"http://arxiv.org/abs/2307.13586v1"},"cats":{"new-dataset":0.0092003612,"dev-research":0.1829519487,"prompt-eng":0.3831424459,"data-quality":0.1479650252,"ml-security":0.2099034739}}
{"text":"The key technical innovation lies in the development of a new regret decomposition strategy and a novel analysis paradigm to decouple complicated statistical dependency -- a long-standing challenge facing the analysis of online RL in the sample-hungry regime.","meta":{"url":"http://arxiv.org/abs/2307.13586v1"},"cats":{"new-dataset":0.040036713,"dev-research":0.2419042053,"prompt-eng":0.3932254913,"data-quality":0.1922034063,"ml-security":0.1519114134}}
{"text":"Argumentative explainable AI has been advocated by several in recent years, with an increasing interest on explaining the reasoning outcomes of Argumentation Frameworks (AFs).","meta":{"url":"http://arxiv.org/abs/2307.13582v1"},"cats":{"new-dataset":0.034250779,"dev-research":0.33089949,"prompt-eng":0.3956834287,"data-quality":0.1705654269,"ml-security":0.2209342661}}
{"text":"While there is a considerable body of research on qualitatively explaining the reasoning outcomes of AFs with debates/disputes/dialogues in the spirit of \\emph{extension-based semantics}, explaining the quantitative reasoning outcomes of AFs under \\emph{gradual semantics} has not received much attention, despite widespread use in applications.","meta":{"url":"http://arxiv.org/abs/2307.13582v1"},"cats":{"new-dataset":0.0166780448,"dev-research":0.3972547715,"prompt-eng":0.3828527198,"data-quality":0.2472273626,"ml-security":0.1279458029}}
{"text":"In this paper, we contribute to filling this gap by proposing a novel theory of \\emph{Argument Attribution Explanations (AAEs)} by incorporating the spirit of feature attribution from machine learning in the context of Quantitative Bipolar Argumentation Frameworks (QBAFs): whereas feature attribution is used to determine the influence of features towards outputs of machine learning models, AAEs are used to determine the influence of arguments towards \\emph{topic argument}s of interest.","meta":{"url":"http://arxiv.org/abs/2307.13582v1"},"cats":{"new-dataset":0.0217477095,"dev-research":0.3895871315,"prompt-eng":0.3836235447,"data-quality":0.3494853029,"ml-security":0.4323991924}}
{"text":"We study desirable properties of AAEs, including some new ones and some partially adapted from the literature to our setting.","meta":{"url":"http://arxiv.org/abs/2307.13582v1"},"cats":{"new-dataset":0.0610422897,"dev-research":0.1608876354,"prompt-eng":0.4010631467,"data-quality":0.1485205012,"ml-security":0.0910080274}}
{"text":"To demonstrate the applicability of our AAEs in practice, we conclude by carrying out two case studies in the scenarios of fake news detection and movie recommender systems.","meta":{"url":"http://arxiv.org/abs/2307.13582v1"},"cats":{"new-dataset":0.0639056574,"dev-research":0.2398891197,"prompt-eng":0.3644141497,"data-quality":0.3127883895,"ml-security":0.2936545775}}
{"text":"Survival analysis is an integral part of the statistical toolbox.","meta":{"url":"http://arxiv.org/abs/2307.13579v1"},"cats":{"new-dataset":0.0796694211,"dev-research":0.2287813683,"prompt-eng":0.3399286998,"data-quality":0.0919550509,"ml-security":0.1103784958}}
{"text":"However, while most domains of classical statistics have embraced deep learning, survival analysis only recently gained some minor attention from the deep learning community.","meta":{"url":"http://arxiv.org/abs/2307.13579v1"},"cats":{"new-dataset":0.0910303144,"dev-research":0.1864312533,"prompt-eng":0.2666425223,"data-quality":0.1327776872,"ml-security":0.1875408468}}
{"text":"This recent development is likely in part motivated by the COVID-19 pandemic.","meta":{"url":"http://arxiv.org/abs/2307.13579v1"},"cats":{"new-dataset":0.1113887963,"dev-research":0.3102285963,"prompt-eng":0.4107686001,"data-quality":0.1062602831,"ml-security":0.1990750864}}
{"text":"We aim to provide the tools needed to fully harness the potential of survival analysis in deep learning.","meta":{"url":"http://arxiv.org/abs/2307.13579v1"},"cats":{"new-dataset":0.1740272964,"dev-research":0.2300419481,"prompt-eng":0.3648089209,"data-quality":0.172451522,"ml-security":0.2067646571}}
{"text":"On the one hand, we discuss how survival analysis connects to classification and regression.","meta":{"url":"http://arxiv.org/abs/2307.13579v1"},"cats":{"new-dataset":0.086060541,"dev-research":0.2524870834,"prompt-eng":0.3490489405,"data-quality":0.1973494067,"ml-security":0.1999058254}}
{"text":"On the other hand, we provide technical tools.","meta":{"url":"http://arxiv.org/abs/2307.13579v1"},"cats":{"new-dataset":0.0570681511,"dev-research":0.4741740358,"prompt-eng":0.3410160418,"data-quality":0.1013210306,"ml-security":0.063030213}}
{"text":"We provide a new loss function, evaluation metrics, and the first universal approximating network that provably produces survival curves without numeric integration.","meta":{"url":"http://arxiv.org/abs/2307.13579v1"},"cats":{"new-dataset":0.1313069783,"dev-research":0.1644689807,"prompt-eng":0.3860998171,"data-quality":0.1695449215,"ml-security":0.1458294948}}
{"text":"We show that the loss function and model outperform other approaches using a large numerical study.","meta":{"url":"http://arxiv.org/abs/2307.13579v1"},"cats":{"new-dataset":0.01617194,"dev-research":0.186262882,"prompt-eng":0.369084502,"data-quality":0.2318407355,"ml-security":0.1522961962}}
{"text":"Optimal transport and its related problems, including optimal partial transport, have proven to be valuable tools in machine learning for computing meaningful distances between probability or positive measures.","meta":{"url":"http://arxiv.org/abs/2307.13571v1"},"cats":{"new-dataset":0.0212940037,"dev-research":0.2067405708,"prompt-eng":0.3924393008,"data-quality":0.1364285812,"ml-security":0.1720843439}}
{"text":"This success has led to a growing interest in defining transport-based distances that allow for comparing signed measures and, more generally, multi-channeled signals.","meta":{"url":"http://arxiv.org/abs/2307.13571v1"},"cats":{"new-dataset":0.0413529521,"dev-research":0.2511083396,"prompt-eng":0.4335740822,"data-quality":0.1137841904,"ml-security":0.0511380582}}
{"text":"Transport $\\mathrm{L}^{p}$ distances are notable extensions of the optimal transport framework to signed and possibly multi-channeled signals.","meta":{"url":"http://arxiv.org/abs/2307.13571v1"},"cats":{"new-dataset":0.0283864064,"dev-research":0.1744844816,"prompt-eng":0.3875378779,"data-quality":0.0940976296,"ml-security":0.085051413}}
{"text":"In this paper, we introduce partial transport $\\mathrm{L}^{p}$ distances as a new family of metrics for comparing generic signals, benefiting from the robustness of partial transport distances.","meta":{"url":"http://arxiv.org/abs/2307.13571v1"},"cats":{"new-dataset":0.053818917,"dev-research":0.2240408852,"prompt-eng":0.4040779792,"data-quality":0.1783232111,"ml-security":0.1396892933}}
{"text":"We provide theoretical background such as the existence of optimal plans and the behavior of the distance in various limits.","meta":{"url":"http://arxiv.org/abs/2307.13571v1"},"cats":{"new-dataset":0.0377940608,"dev-research":0.2099038018,"prompt-eng":0.3327814897,"data-quality":0.0703358952,"ml-security":0.1191270334}}
{"text":"Furthermore, we introduce the sliced variation of these distances, which allows for rapid comparison of generic signals.","meta":{"url":"http://arxiv.org/abs/2307.13571v1"},"cats":{"new-dataset":0.0857237948,"dev-research":0.1955154955,"prompt-eng":0.4176375388,"data-quality":0.1442786048,"ml-security":0.0700730862}}
{"text":"Finally, we demonstrate the application of the proposed distances in signal class separability and nearest neighbor classification.","meta":{"url":"http://arxiv.org/abs/2307.13571v1"},"cats":{"new-dataset":0.0870821415,"dev-research":0.2265270661,"prompt-eng":0.3624347336,"data-quality":0.3042816532,"ml-security":0.2020678196}}
{"text":"Next-generation wireless communication systems impose much stricter requirements for transmission rate, latency, and reliability.","meta":{"url":"http://arxiv.org/abs/2307.13570v1"},"cats":{"new-dataset":0.0090243589,"dev-research":0.2373461906,"prompt-eng":0.3953934591,"data-quality":0.1529289806,"ml-security":0.1627443835}}
{"text":"The peak data rate of 6G networks should be no less than 1 Tb/s, which is comparable to existing long-haul optical transport networks.","meta":{"url":"http://arxiv.org/abs/2307.13570v1"},"cats":{"new-dataset":0.0666951207,"dev-research":0.1483446846,"prompt-eng":0.3009799932,"data-quality":0.1237207627,"ml-security":0.0712384638}}
{"text":"It is believed that using long error-correcting codes (ECC) with soft-decision decoding (SDD) is not feasible in this case due to the resulting high power consumption.","meta":{"url":"http://arxiv.org/abs/2307.13570v1"},"cats":{"new-dataset":0.0293468673,"dev-research":0.3577093737,"prompt-eng":0.4178364027,"data-quality":0.3822775068,"ml-security":0.1926332162}}
{"text":"On the other hand, ECC with hard-decision decoding (HDD) suffers from significant performance degradation.","meta":{"url":"http://arxiv.org/abs/2307.13570v1"},"cats":{"new-dataset":0.010105721,"dev-research":0.3525389081,"prompt-eng":0.3556389855,"data-quality":0.1961431546,"ml-security":0.1653542852}}
{"text":"In this paper, we consider a concatenated solution consisting of an outer long HDD code and an inner short SDD code.","meta":{"url":"http://arxiv.org/abs/2307.13570v1"},"cats":{"new-dataset":0.227190227,"dev-research":0.2945071198,"prompt-eng":0.3840898811,"data-quality":0.1475154247,"ml-security":0.0910500012}}
{"text":"The latter code is a crucial component of the system and the focus of our research.","meta":{"url":"http://arxiv.org/abs/2307.13570v1"},"cats":{"new-dataset":0.0730509703,"dev-research":0.4127550833,"prompt-eng":0.3990317484,"data-quality":0.1537482295,"ml-security":0.075750877}}
{"text":"Due to its short length, the code cannot correct all errors, but it is designed to minimize the number of errors.","meta":{"url":"http://arxiv.org/abs/2307.13570v1"},"cats":{"new-dataset":0.0243841597,"dev-research":0.4379690536,"prompt-eng":0.4025069099,"data-quality":0.5088698019,"ml-security":0.1380751921}}
{"text":"Such codes are known as error-reducing codes.","meta":{"url":"http://arxiv.org/abs/2307.13570v1"},"cats":{"new-dataset":0.0480380675,"dev-research":0.3814372762,"prompt-eng":0.4214143611,"data-quality":0.4797259741,"ml-security":0.1493466414}}
{"text":"We investigate the error-reducing properties of superposition codes.","meta":{"url":"http://arxiv.org/abs/2307.13570v1"},"cats":{"new-dataset":0.0207289073,"dev-research":0.286374839,"prompt-eng":0.3941808229,"data-quality":0.3660584896,"ml-security":0.1803211644}}
{"text":"Initially, we explore sparse regression codes (SPARCs) with Gaussian signals.","meta":{"url":"http://arxiv.org/abs/2307.13570v1"},"cats":{"new-dataset":0.1284664688,"dev-research":0.221801666,"prompt-eng":0.4073181485,"data-quality":0.2166190175,"ml-security":0.1752029606}}
{"text":"This approach outperforms error-reducing binary LDPC codes optimized by Barakatain, et al.","meta":{"url":"http://arxiv.org/abs/2307.13570v1"},"cats":{"new-dataset":0.0746589118,"dev-research":0.3222702471,"prompt-eng":0.4463502589,"data-quality":0.3386766846,"ml-security":0.1193049705}}
{"text":"(2018) in terms of performance","meta":{"url":"http://arxiv.org/abs/2307.13570v1"},"cats":{"new-dataset":0.0480016411,"dev-research":0.2793708415,"prompt-eng":0.3566556266,"data-quality":0.085370678,"ml-security":0.0579662078}}
{"text":"but faces limitations in practical applicability due to high implementation complexity.","meta":{"url":"http://arxiv.org/abs/2307.13570v1"},"cats":{"new-dataset":0.0072004904,"dev-research":0.4537872402,"prompt-eng":0.3191259169,"data-quality":0.0894359245,"ml-security":0.152736302}}
{"text":"Subsequently, we propose an LDPC-based superposition code scheme with low-complexity soft successive interference cancellation (SIC) decoding.","meta":{"url":"http://arxiv.org/abs/2307.13570v1"},"cats":{"new-dataset":0.045839066,"dev-research":0.2932860301,"prompt-eng":0.3700980793,"data-quality":0.1823419904,"ml-security":0.1243729555}}
{"text":"This scheme demonstrates comparable performance to SPARCs while maintaining manageable complexity.","meta":{"url":"http://arxiv.org/abs/2307.13570v1"},"cats":{"new-dataset":0.0384627675,"dev-research":0.3888479237,"prompt-eng":0.4810826799,"data-quality":0.1219331744,"ml-security":0.1071295815}}
{"text":"Numerical results were obtained for inner codes with an overhead (OH) of 8.24% within a concatenated scheme (15% OH) with an outer hard-decision decoded staircase code (6.25% OH).","meta":{"url":"http://arxiv.org/abs/2307.13570v1"},"cats":{"new-dataset":0.05043729,"dev-research":0.313434528,"prompt-eng":0.397603197,"data-quality":0.2004630002,"ml-security":0.1103570938}}
{"text":"To facilitate the reuse of existing charts, previous research has examined how to obtain a semantic understanding of a chart by deconstructing its visual representation into reusable components, such as encodings.","meta":{"url":"http://arxiv.org/abs/2307.13567v1"},"cats":{"new-dataset":0.1587637598,"dev-research":0.4170595664,"prompt-eng":0.3958032188,"data-quality":0.227203066,"ml-security":0.082506128}}
{"text":"However, existing deconstruction approaches primarily focus on chart styles, handling only basic layouts.","meta":{"url":"http://arxiv.org/abs/2307.13567v1"},"cats":{"new-dataset":0.0266118255,"dev-research":0.415410073,"prompt-eng":0.3538238817,"data-quality":0.1784232855,"ml-security":0.1329453212}}
{"text":"In this paper, we investigate how to deconstruct chart layouts, focusing on rectangle-based ones as they cover not only 17 chart types but also advanced layouts (e.g., small multiples, nested layouts).","meta":{"url":"http://arxiv.org/abs/2307.13567v1"},"cats":{"new-dataset":0.2576208231,"dev-research":0.3548404917,"prompt-eng":0.3726267259,"data-quality":0.1617547641,"ml-security":0.1112577816}}
{"text":"We develop an interactive tool, called Mystique, adopting a mixed-initiative approach to extract the axes and legend, and deconstruct a chart's layout into four semantic components: mark groups, spatial relationships, data encodings, and graphical constraints.","meta":{"url":"http://arxiv.org/abs/2307.13567v1"},"cats":{"new-dataset":0.4727799113,"dev-research":0.3698010605,"prompt-eng":0.4244703897,"data-quality":0.1814141044,"ml-security":0.0462661833}}
{"text":"Mystique employs a wizard interface that guides chart authors through a series of steps to specify how the deconstructed components map to their own data.","meta":{"url":"http://arxiv.org/abs/2307.13567v1"},"cats":{"new-dataset":0.3148855147,"dev-research":0.3856710866,"prompt-eng":0.4298568826,"data-quality":0.1765144215,"ml-security":0.0682508413}}
{"text":"On 150 rectangle-based SVG charts, Mystique achieves above 85% accuracy for axis and legend extraction and 96% accuracy for layout deconstruction.","meta":{"url":"http://arxiv.org/abs/2307.13567v1"},"cats":{"new-dataset":0.2134965714,"dev-research":0.328233084,"prompt-eng":0.4039389632,"data-quality":0.2084988141,"ml-security":0.0536174862}}
{"text":"In a chart reproduction study, participants could easily reuse existing charts on new datasets.","meta":{"url":"http://arxiv.org/abs/2307.13567v1"},"cats":{"new-dataset":0.2214194097,"dev-research":0.3605504703,"prompt-eng":0.3740862352,"data-quality":0.1518816957,"ml-security":0.1068932199}}
{"text":"We discuss the current limitations of Mystique and future research directions.","meta":{"url":"http://arxiv.org/abs/2307.13567v1"},"cats":{"new-dataset":0.0943831356,"dev-research":0.2039859435,"prompt-eng":0.3606017831,"data-quality":0.0874553144,"ml-security":0.0878898505}}
{"text":"Explainability techniques are rapidly being developed to improve human-AI decision-making across various cooperative work settings.","meta":{"url":"http://arxiv.org/abs/2307.13566v1"},"cats":{"new-dataset":0.0196463338,"dev-research":0.5043150894,"prompt-eng":0.3992836072,"data-quality":0.2066813736,"ml-security":0.1841341716}}
{"text":"Consequently, previous research has evaluated how decision-makers collaborate with imperfect AI by investigating appropriate reliance and task performance with the aim of designing more human-centered computer-supported collaborative tools.","meta":{"url":"http://arxiv.org/abs/2307.13566v1"},"cats":{"new-dataset":0.0154241815,"dev-research":0.5104156058,"prompt-eng":0.4076427952,"data-quality":0.145852626,"ml-security":0.0860349979}}
{"text":"Several human-centered explainable AI (XAI) techniques have been proposed in hopes of improving decision-makers' collaboration with AI; however, these techniques are grounded in findings from previous studies that primarily focus on the impact of incorrect AI advice.","meta":{"url":"http://arxiv.org/abs/2307.13566v1"},"cats":{"new-dataset":0.0220550223,"dev-research":0.5011942942,"prompt-eng":0.3882930302,"data-quality":0.292828171,"ml-security":0.2356358666}}
{"text":"Few studies acknowledge the possibility for the explanations to be incorrect even if the AI advice is correct.","meta":{"url":"http://arxiv.org/abs/2307.13566v1"},"cats":{"new-dataset":0.0045277251,"dev-research":0.3329923825,"prompt-eng":0.3721173532,"data-quality":0.3194903015,"ml-security":0.1864828798}}
{"text":"Thus, it is crucial to understand how imperfect XAI affects human-AI decision-making.","meta":{"url":"http://arxiv.org/abs/2307.13566v1"},"cats":{"new-dataset":0.0065066026,"dev-research":0.4048645177,"prompt-eng":0.3595058932,"data-quality":0.214902028,"ml-security":0.1895485697}}
{"text":"In this work, we contribute a robust, mixed-methods user study with 136 participants to evaluate how incorrect explanations influence humans' decision-making behavior in a bird species identification task taking into account their level of expertise and an explanation's level of assertiveness.","meta":{"url":"http://arxiv.org/abs/2307.13566v1"},"cats":{"new-dataset":0.0499735508,"dev-research":0.4277095045,"prompt-eng":0.4856808727,"data-quality":0.4669457977,"ml-security":0.2156300686}}
{"text":"Our findings reveal the influence of imperfect XAI and humans' level of expertise on their reliance on AI and human-AI team performance.","meta":{"url":"http://arxiv.org/abs/2307.13566v1"},"cats":{"new-dataset":0.0426336487,"dev-research":0.3924691664,"prompt-eng":0.3657598802,"data-quality":0.1604590181,"ml-security":0.1539826677}}
{"text":"We also discuss how explanations can deceive decision-makers during human-AI collaboration.","meta":{"url":"http://arxiv.org/abs/2307.13566v1"},"cats":{"new-dataset":0.0420195293,"dev-research":0.4545704571,"prompt-eng":0.4227741526,"data-quality":0.2422849591,"ml-security":0.4251944634}}
{"text":"Hence, we shed light on the impacts of imperfect XAI in the field of computer-supported cooperative work and provide guidelines for designers of human-AI collaboration systems.","meta":{"url":"http://arxiv.org/abs/2307.13566v1"},"cats":{"new-dataset":0.0738995739,"dev-research":0.4262530778,"prompt-eng":0.4102889287,"data-quality":0.1807476951,"ml-security":0.1518865175}}
{"text":"Decision-focused learning (DFL) is an emerging paradigm in machine learning which trains a model to optimize decisions, integrating prediction and optimization in an end-to-end system.","meta":{"url":"http://arxiv.org/abs/2307.13565v1"},"cats":{"new-dataset":0.0358632437,"dev-research":0.352688587,"prompt-eng":0.3743435596,"data-quality":0.1432908553,"ml-security":0.2133598465}}
{"text":"This paradigm holds the promise to revolutionize decision-making in many real-world applications which operate under uncertainty, where the estimation of unknown parameters within these decision models often becomes a substantial roadblock.","meta":{"url":"http://arxiv.org/abs/2307.13565v1"},"cats":{"new-dataset":0.019538436,"dev-research":0.2767543315,"prompt-eng":0.4180918436,"data-quality":0.1797162787,"ml-security":0.3260776227}}
{"text":"This paper presents a comprehensive review of DFL.","meta":{"url":"http://arxiv.org/abs/2307.13565v1"},"cats":{"new-dataset":0.1093140649,"dev-research":0.3401386808,"prompt-eng":0.3732575666,"data-quality":0.1719933947,"ml-security":0.0948237365}}
{"text":"It provides an in-depth analysis of the various techniques devised to integrate machine learning and optimization models introduces a taxonomy of DFL methods distinguished by their unique characteristics, and conducts an extensive empirical evaluation of these methods proposing suitable benchmark dataset and tasks for DFL.","meta":{"url":"http://arxiv.org/abs/2307.13565v1"},"cats":{"new-dataset":0.0869582453,"dev-research":0.3076977628,"prompt-eng":0.4188100546,"data-quality":0.2195750642,"ml-security":0.1567877423}}
{"text":"Finally, the study provides valuable insights into current and potential future avenues in DFL research.","meta":{"url":"http://arxiv.org/abs/2307.13565v1"},"cats":{"new-dataset":0.0817559412,"dev-research":0.2859835705,"prompt-eng":0.3565095414,"data-quality":0.1282113431,"ml-security":0.0788682424}}
{"text":"Recently, diffusion models have excelled in image generation tasks and have also been applied to neural language processing (NLP) for controllable text generation.","meta":{"url":"http://arxiv.org/abs/2307.13560v1"},"cats":{"new-dataset":0.0572388393,"dev-research":0.2217492839,"prompt-eng":0.3998093638,"data-quality":0.2173873428,"ml-security":0.1151543862}}
{"text":"However, the application of diffusion models in a cross-lingual setting is less unexplored.","meta":{"url":"http://arxiv.org/abs/2307.13560v1"},"cats":{"new-dataset":0.0225711079,"dev-research":0.1767537038,"prompt-eng":0.2786172753,"data-quality":0.2225529547,"ml-security":0.1029432301}}
{"text":"Additionally, while pretraining with diffusion models has been studied within a single language, the potential of cross-lingual pretraining remains understudied.","meta":{"url":"http://arxiv.org/abs/2307.13560v1"},"cats":{"new-dataset":0.0293948676,"dev-research":0.2047991706,"prompt-eng":0.3845912694,"data-quality":0.2093182488,"ml-security":0.1127260821}}
{"text":"To address these gaps, we propose XDLM, a novel Cross-lingual diffusion model for machine translation, consisting of pretraining and fine-tuning stages.","meta":{"url":"http://arxiv.org/abs/2307.13560v1"},"cats":{"new-dataset":0.1015215602,"dev-research":0.2007199197,"prompt-eng":0.4217867861,"data-quality":0.231928105,"ml-security":0.0709373974}}
{"text":"In the pretraining stage, we propose TLDM, a new training objective for mastering the mapping between different languages; in the fine-tuning stage, we build up the translation system based on the pretrained model.","meta":{"url":"http://arxiv.org/abs/2307.13560v1"},"cats":{"new-dataset":0.2081056697,"dev-research":0.2943234434,"prompt-eng":0.4565249529,"data-quality":0.2244975579,"ml-security":0.0783019595}}
{"text":"We evaluate the result on several machine translation benchmarks and outperformed both diffusion and Transformer baselines.","meta":{"url":"http://arxiv.org/abs/2307.13560v1"},"cats":{"new-dataset":0.166991589,"dev-research":0.2093054261,"prompt-eng":0.4055001504,"data-quality":0.2320264757,"ml-security":0.0622044379}}
{"text":"Rubik's Cube (RC) is a well-known and computationally challenging puzzle that has motivated AI researchers to explore efficient alternative representations and problem-solving methods.","meta":{"url":"http://arxiv.org/abs/2307.13552v1"},"cats":{"new-dataset":0.0612548656,"dev-research":0.3202143668,"prompt-eng":0.3403729912,"data-quality":0.0822934717,"ml-security":0.0937183408}}
{"text":"The ideal situation for planning here is that a problem be solved optimally and efficiently represented in a standard notation using a general-purpose solver and heuristics.","meta":{"url":"http://arxiv.org/abs/2307.13552v1"},"cats":{"new-dataset":0.0309340027,"dev-research":0.3412319095,"prompt-eng":0.4677672827,"data-quality":0.0773320991,"ml-security":0.0607438537}}
{"text":"The fastest solver today for RC is DeepCubeA with a custom representation, and another approach is with Scorpion planner with State-Action-Space+ (SAS+) representation.","meta":{"url":"http://arxiv.org/abs/2307.13552v1"},"cats":{"new-dataset":0.178032392,"dev-research":0.3356498127,"prompt-eng":0.4433663984,"data-quality":0.066315035,"ml-security":0.083665612}}
{"text":"In this paper, we present the first RC representation in the popular PDDL language so that the domain becomes more accessible to PDDL planners, competitions, and knowledge engineering tools, and is more human-readable.","meta":{"url":"http://arxiv.org/abs/2307.13552v1"},"cats":{"new-dataset":0.4029613917,"dev-research":0.4413298033,"prompt-eng":0.4736152026,"data-quality":0.1500086009,"ml-security":0.0886331998}}
{"text":"We then bridge across existing approaches and compare performance.","meta":{"url":"http://arxiv.org/abs/2307.13552v1"},"cats":{"new-dataset":0.0108591355,"dev-research":0.4048480458,"prompt-eng":0.3924494952,"data-quality":0.1267584626,"ml-security":0.0724478613}}
{"text":"We find that in one comparable experiment, DeepCubeA solves all problems with varying complexities, albeit only 18\\% are optimal plans.","meta":{"url":"http://arxiv.org/abs/2307.13552v1"},"cats":{"new-dataset":0.0331937933,"dev-research":0.2433920849,"prompt-eng":0.3732387792,"data-quality":0.0906753477,"ml-security":0.1043209243}}
{"text":"For the same problem set, Scorpion with SAS+ representation and pattern database heuristics solves 61.50\\% problems, while FastDownward with PDDL representation and FF heuristic solves 56.50\\% problems, out of which all the plans generated were optimal.","meta":{"url":"http://arxiv.org/abs/2307.13552v1"},"cats":{"new-dataset":0.0731179323,"dev-research":0.3479658058,"prompt-eng":0.4396134727,"data-quality":0.1115170858,"ml-security":0.0774666225}}
{"text":"Our study provides valuable insights into the trade-offs between representational choice and plan optimality that can help researchers design future strategies for challenging domains combining general-purpose solving methods (planning, reinforcement learning), heuristics, and representations (standard or custom).","meta":{"url":"http://arxiv.org/abs/2307.13552v1"},"cats":{"new-dataset":0.0144054075,"dev-research":0.2768789754,"prompt-eng":0.3825862082,"data-quality":0.0449600825,"ml-security":0.1108410476}}
{"text":"Ontologies are known for their ability to organize rich metadata, support the identification of novel insights via semantic queries, and promote reuse.","meta":{"url":"http://arxiv.org/abs/2307.13549v1"},"cats":{"new-dataset":0.158543446,"dev-research":0.3452843798,"prompt-eng":0.3789437261,"data-quality":0.1962166455,"ml-security":0.0793280108}}
{"text":"In this paper, we consider the problem of automated planning, where the objective is to find a sequence of actions that will move an agent from an initial state of the world to a desired goal state.","meta":{"url":"http://arxiv.org/abs/2307.13549v1"},"cats":{"new-dataset":0.1626839186,"dev-research":0.2449660902,"prompt-eng":0.4432092076,"data-quality":0.0702838978,"ml-security":0.0691074426}}
{"text":"We hypothesize that given a large number of available planners and diverse planning domains; they carry essential information that can be leveraged to identify suitable planners and improve their performance for a domain.","meta":{"url":"http://arxiv.org/abs/2307.13549v1"},"cats":{"new-dataset":0.1272591336,"dev-research":0.3550698737,"prompt-eng":0.4565859862,"data-quality":0.0866084497,"ml-security":0.0913451167}}
{"text":"We use data on planning domains and planners from the International Planning Competition (IPC) to construct a planning ontology and demonstrate via experiments in two use cases that the ontology can lead to the selection of promising planners and improving their performance using macros - a form of action ordering constraints extracted from planning ontology.","meta":{"url":"http://arxiv.org/abs/2307.13549v1"},"cats":{"new-dataset":0.1690680451,"dev-research":0.317315675,"prompt-eng":0.4356717735,"data-quality":0.079641364,"ml-security":0.0460702106}}
{"text":"We also make the planning ontology and associated resources available to the community to promote further research.","meta":{"url":"http://arxiv.org/abs/2307.13549v1"},"cats":{"new-dataset":0.2294211299,"dev-research":0.2944746275,"prompt-eng":0.3990793386,"data-quality":0.0887151884,"ml-security":0.0314774791}}
{"text":"In this paper, we present a stealthy and effective attack that exposes privacy vulnerabilities in Graph Neural Networks (GNNs) by inferring private links within graph-structured data.","meta":{"url":"http://arxiv.org/abs/2307.13548v1"},"cats":{"new-dataset":0.2035257803,"dev-research":0.2649948855,"prompt-eng":0.3047959564,"data-quality":0.3078832254,"ml-security":0.8358703791}}
{"text":"Focusing on the inductive setting where new nodes join the graph and an API is used to query predictions, we investigate the potential leakage of private edge information.","meta":{"url":"http://arxiv.org/abs/2307.13548v1"},"cats":{"new-dataset":0.05752336,"dev-research":0.2558838786,"prompt-eng":0.3637244068,"data-quality":0.2548359286,"ml-security":0.4986737871}}
{"text":"We also propose methods to preserve privacy while maintaining model utility.","meta":{"url":"http://arxiv.org/abs/2307.13548v1"},"cats":{"new-dataset":0.0374727611,"dev-research":0.2372684538,"prompt-eng":0.44373197,"data-quality":0.1604356845,"ml-security":0.4543994379}}
{"text":"Our attack demonstrates superior performance in inferring the links compared to the state of the art.","meta":{"url":"http://arxiv.org/abs/2307.13548v1"},"cats":{"new-dataset":0.1000350745,"dev-research":0.3098639781,"prompt-eng":0.4398647912,"data-quality":0.189265564,"ml-security":0.3770931387}}
{"text":"Furthermore, we examine the application of differential privacy (DP) mechanisms to mitigate the impact of our proposed attack, we analyze the trade-off between privacy preservation and model utility.","meta":{"url":"http://arxiv.org/abs/2307.13548v1"},"cats":{"new-dataset":0.0943916539,"dev-research":0.2892855939,"prompt-eng":0.3715848431,"data-quality":0.1728470564,"ml-security":0.8205115703}}
{"text":"Our work highlights the privacy vulnerabilities inherent in GNNs, underscoring the importance of developing robust privacy-preserving mechanisms for their application.","meta":{"url":"http://arxiv.org/abs/2307.13548v1"},"cats":{"new-dataset":0.2256368979,"dev-research":0.2960518373,"prompt-eng":0.3446499377,"data-quality":0.2601415483,"ml-security":0.7955972756}}
{"text":"Group activity recognition is a hot topic in computer vision.","meta":{"url":"http://arxiv.org/abs/2307.13541v1"},"cats":{"new-dataset":0.1080773993,"dev-research":0.2792682515,"prompt-eng":0.368320722,"data-quality":0.1533120743,"ml-security":0.1234978373}}
{"text":"Recognizing activities through group relationships plays a vital role in group activity recognition.","meta":{"url":"http://arxiv.org/abs/2307.13541v1"},"cats":{"new-dataset":0.0714976382,"dev-research":0.2800437525,"prompt-eng":0.3362548994,"data-quality":0.144254456,"ml-security":0.0986025364}}
{"text":"It holds practical implications in various scenarios, such as video analysis, surveillance, automatic driving, and understanding social activities.","meta":{"url":"http://arxiv.org/abs/2307.13541v1"},"cats":{"new-dataset":0.039233927,"dev-research":0.315726059,"prompt-eng":0.3392047382,"data-quality":0.141204177,"ml-security":0.1931727152}}
{"text":"The model's key capabilities encompass efficiently modeling hierarchical relationships within a scene and accurately extracting distinctive spatiotemporal features from groups.","meta":{"url":"http://arxiv.org/abs/2307.13541v1"},"cats":{"new-dataset":0.1608802307,"dev-research":0.2355604856,"prompt-eng":0.3793617442,"data-quality":0.1096057781,"ml-security":0.0667287086}}
{"text":"Given this technology's extensive applicability, identifying group activities has garnered significant research attention.","meta":{"url":"http://arxiv.org/abs/2307.13541v1"},"cats":{"new-dataset":0.1725922233,"dev-research":0.2495600181,"prompt-eng":0.4047811636,"data-quality":0.1259459546,"ml-security":0.0678084909}}
{"text":"This work examines the current progress in technology for recognizing group activities, with a specific focus on global interactivity and activities.","meta":{"url":"http://arxiv.org/abs/2307.13541v1"},"cats":{"new-dataset":0.3004704333,"dev-research":0.2598610575,"prompt-eng":0.387545141,"data-quality":0.1443341985,"ml-security":0.0624976208}}
{"text":"Firstly, we comprehensively review the pertinent literature and various group activity recognition approaches, from traditional methodologies to the latest methods based on spatial structure, descriptors, non-deep learning, hierarchical recurrent neural networks (HRNN), relationship models, and attention mechanisms.","meta":{"url":"http://arxiv.org/abs/2307.13541v1"},"cats":{"new-dataset":0.3047611,"dev-research":0.1948410004,"prompt-eng":0.3439977402,"data-quality":0.1059723983,"ml-security":0.1163261936}}
{"text":"Subsequently, we present the relational network and relational architectures for each module.","meta":{"url":"http://arxiv.org/abs/2307.13541v1"},"cats":{"new-dataset":0.2418913961,"dev-research":0.2594538098,"prompt-eng":0.4174649671,"data-quality":0.1031458704,"ml-security":0.0596785344}}
{"text":"Thirdly, we investigate methods for recognizing group activity and compare their performance with state-of-the-art technologies.","meta":{"url":"http://arxiv.org/abs/2307.13541v1"},"cats":{"new-dataset":0.1114588014,"dev-research":0.2989863461,"prompt-eng":0.4170323284,"data-quality":0.1926151756,"ml-security":0.0893645987}}
{"text":"We summarize the existing challenges and provide comprehensive guidance for newcomers to understand group activity recognition.","meta":{"url":"http://arxiv.org/abs/2307.13541v1"},"cats":{"new-dataset":0.1993472566,"dev-research":0.3218443828,"prompt-eng":0.3855407767,"data-quality":0.2122415708,"ml-security":0.1068755172}}
{"text":"Furthermore, we review emerging perspectives in group activity recognition to explore new directions and possibilities.","meta":{"url":"http://arxiv.org/abs/2307.13541v1"},"cats":{"new-dataset":0.1308873381,"dev-research":0.2452887445,"prompt-eng":0.3595786966,"data-quality":0.1499501426,"ml-security":0.1240241477}}
{"text":"For safety-related applications, it is crucial to produce trustworthy deep neural networks whose prediction is associated with confidence that can represent the likelihood of correctness for subsequent decision-making.","meta":{"url":"http://arxiv.org/abs/2307.13539v1"},"cats":{"new-dataset":0.0287496829,"dev-research":0.3284356117,"prompt-eng":0.4072264107,"data-quality":0.3429487902,"ml-security":0.6973177763}}
{"text":"Existing dense binary classification models are prone to being over-confident.","meta":{"url":"http://arxiv.org/abs/2307.13539v1"},"cats":{"new-dataset":0.0314562386,"dev-research":0.2588165199,"prompt-eng":0.4307845323,"data-quality":0.3627893462,"ml-security":0.441271008}}
{"text":"To improve model calibration, we propose Adaptive Stochastic Label Perturbation (ASLP) which learns a unique label perturbation level for each training image.","meta":{"url":"http://arxiv.org/abs/2307.13539v1"},"cats":{"new-dataset":0.1035100301,"dev-research":0.192693963,"prompt-eng":0.4399108866,"data-quality":0.5865190265,"ml-security":0.1952860208}}
{"text":"ASLP employs our proposed Self-Calibrating Binary Cross Entropy (SC-BCE) loss, which unifies label perturbation processes including stochastic approaches (like DisturbLabel), and label smoothing, to correct calibration while maintaining classification rates.","meta":{"url":"http://arxiv.org/abs/2307.13539v1"},"cats":{"new-dataset":0.0709642883,"dev-research":0.2628356131,"prompt-eng":0.5007419748,"data-quality":0.5685610292,"ml-security":0.2068105063}}
{"text":"ASLP follows Maximum Entropy Inference of classic statistical mechanics to maximise prediction entropy with respect to missing information.","meta":{"url":"http://arxiv.org/abs/2307.13539v1"},"cats":{"new-dataset":0.0572429482,"dev-research":0.1770619217,"prompt-eng":0.4476299638,"data-quality":0.1724587334,"ml-security":0.139428817}}
{"text":"It performs this while: (1) preserving classification accuracy on known data as a conservative solution, or (2) specifically improves model calibration degree by minimising the gap between the prediction accuracy and expected confidence of the target training label.","meta":{"url":"http://arxiv.org/abs/2307.13539v1"},"cats":{"new-dataset":0.005958261,"dev-research":0.2823388272,"prompt-eng":0.416595287,"data-quality":0.3874693814,"ml-security":0.1629898653}}
{"text":"Extensive results demonstrate that ASLP can significantly improve calibration degrees of dense binary classification models on both in-distribution and out-of-distribution data.","meta":{"url":"http://arxiv.org/abs/2307.13539v1"},"cats":{"new-dataset":0.1986412803,"dev-research":0.2083401178,"prompt-eng":0.4321737696,"data-quality":0.3585991458,"ml-security":0.1924255139}}
{"text":"The code is available on https://github.com/Carlisle-Liu/ASLP.","meta":{"url":"http://arxiv.org/abs/2307.13539v1"},"cats":{"new-dataset":0.3428833747,"dev-research":0.2781803196,"prompt-eng":0.4407337436,"data-quality":0.1496577496,"ml-security":0.0679333}}
{"text":"For numerical design, the development of efficient and accurate surrogate models is paramount.","meta":{"url":"http://arxiv.org/abs/2307.13538v1"},"cats":{"new-dataset":0.0118149263,"dev-research":0.2201050431,"prompt-eng":0.4164795926,"data-quality":0.0560214122,"ml-security":0.1248016442}}
{"text":"They allow us to approximate complex physical phenomena, thereby reducing the computational burden of direct numerical simulations.","meta":{"url":"http://arxiv.org/abs/2307.13538v1"},"cats":{"new-dataset":0.007422335,"dev-research":0.3358674277,"prompt-eng":0.3327535794,"data-quality":0.0590860673,"ml-security":0.0915634814}}
{"text":"We propose INFINITY, a deep learning model that utilizes implicit neural representations (INRs) to address this challenge.","meta":{"url":"http://arxiv.org/abs/2307.13538v1"},"cats":{"new-dataset":0.1465267848,"dev-research":0.2011228175,"prompt-eng":0.3608227699,"data-quality":0.1956257015,"ml-security":0.2439970066}}
{"text":"Our framework encodes geometric information and physical fields into compact representations and learns a mapping between them to infer the physical fields.","meta":{"url":"http://arxiv.org/abs/2307.13538v1"},"cats":{"new-dataset":0.1903925239,"dev-research":0.2234668469,"prompt-eng":0.3862683867,"data-quality":0.1163339597,"ml-security":0.0891176665}}
{"text":"We use an airfoil design optimization problem as an example task and we evaluate our approach on the challenging AirfRANS dataset, which closely resembles real-world industrial use-cases.","meta":{"url":"http://arxiv.org/abs/2307.13538v1"},"cats":{"new-dataset":0.4288788952,"dev-research":0.2585965414,"prompt-eng":0.3756648883,"data-quality":0.0877222203,"ml-security":0.1392718363}}
{"text":"The experimental results demonstrate that our framework achieves state-of-the-art performance by accurately inferring physical fields throughout the volume and surface.","meta":{"url":"http://arxiv.org/abs/2307.13538v1"},"cats":{"new-dataset":0.0760479647,"dev-research":0.2485177837,"prompt-eng":0.4489046903,"data-quality":0.1351846707,"ml-security":0.0644134265}}
{"text":"Additionally we demonstrate its applicability in contexts such as design exploration and shape optimization: our model can correctly predict drag and lift coefficients while adhering to the equations.","meta":{"url":"http://arxiv.org/abs/2307.13538v1"},"cats":{"new-dataset":0.0186518048,"dev-research":0.2795491129,"prompt-eng":0.4085735393,"data-quality":0.0748530746,"ml-security":0.1325200645}}
{"text":"Current referring video object segmentation (R-VOS) techniques extract conditional kernels from encoded (low-resolution) vision-language features to segment the decoded high-resolution features.","meta":{"url":"http://arxiv.org/abs/2307.13537v1"},"cats":{"new-dataset":0.2369325545,"dev-research":0.2423095738,"prompt-eng":0.3976315779,"data-quality":0.3337648089,"ml-security":0.0797348961}}
{"text":"We discovered that this causes significant feature drift, which the segmentation kernels struggle to perceive during the forward computation.","meta":{"url":"http://arxiv.org/abs/2307.13537v1"},"cats":{"new-dataset":0.05180429,"dev-research":0.3359072184,"prompt-eng":0.3941966507,"data-quality":0.3380515275,"ml-security":0.2261521641}}
{"text":"This negatively affects the ability of segmentation kernels.","meta":{"url":"http://arxiv.org/abs/2307.13537v1"},"cats":{"new-dataset":0.0102187418,"dev-research":0.2968008565,"prompt-eng":0.3663766413,"data-quality":0.3594136863,"ml-security":0.2656452815}}
{"text":"To address the drift problem, we propose a Spectrum-guided Multi-granularity (SgMg) approach, which performs direct segmentation on the encoded features and employs visual details to further optimize the masks.","meta":{"url":"http://arxiv.org/abs/2307.13537v1"},"cats":{"new-dataset":0.2163067313,"dev-research":0.2672836477,"prompt-eng":0.4133126509,"data-quality":0.2651773678,"ml-security":0.1021666999}}
{"text":"In addition, we propose Spectrum-guided Cross-modal Fusion (SCF) to perform intra-frame global interactions in the spectral domain for effective multimodal representation.","meta":{"url":"http://arxiv.org/abs/2307.13537v1"},"cats":{"new-dataset":0.0756516015,"dev-research":0.2288258015,"prompt-eng":0.4374961377,"data-quality":0.1159267816,"ml-security":0.0524491905}}
{"text":"Finally, we extend SgMg to perform multi-object R-VOS, a new paradigm that enables simultaneous segmentation of multiple referred objects in a video.","meta":{"url":"http://arxiv.org/abs/2307.13537v1"},"cats":{"new-dataset":0.2224004351,"dev-research":0.1915649772,"prompt-eng":0.3948987541,"data-quality":0.1868993494,"ml-security":0.0429258229}}
{"text":"This not only makes R-VOS faster, but also more practical.","meta":{"url":"http://arxiv.org/abs/2307.13537v1"},"cats":{"new-dataset":0.0073577637,"dev-research":0.2923843941,"prompt-eng":0.3577285914,"data-quality":0.1020100258,"ml-security":0.0821922283}}
{"text":"Extensive experiments show that SgMg achieves state-of-the-art performance on four video benchmark datasets, outperforming the nearest competitor by 2.8% points on Ref-YouTube-VOS.","meta":{"url":"http://arxiv.org/abs/2307.13537v1"},"cats":{"new-dataset":0.2202968356,"dev-research":0.2425880302,"prompt-eng":0.3770467625,"data-quality":0.2530801715,"ml-security":0.1035305868}}
{"text":"Our extended SgMg enables multi-object R-VOS, runs about 3 times faster while maintaining satisfactory performance.","meta":{"url":"http://arxiv.org/abs/2307.13537v1"},"cats":{"new-dataset":0.0278129085,"dev-research":0.2138671456,"prompt-eng":0.3829074799,"data-quality":0.0820956304,"ml-security":0.0730768767}}
{"text":"Code is available at https://github.com/bo-miao/SgMg.","meta":{"url":"http://arxiv.org/abs/2307.13537v1"},"cats":{"new-dataset":0.3317971596,"dev-research":0.2917895419,"prompt-eng":0.4393312999,"data-quality":0.1501975509,"ml-security":0.0824658624}}
{"text":"Human-Object Interaction (HOI) detection is a challenging computer vision task that requires visual models to address the complex interactive relationship between humans and objects and predict HOI triplets.","meta":{"url":"http://arxiv.org/abs/2307.13529v1"},"cats":{"new-dataset":0.1323803565,"dev-research":0.2870846394,"prompt-eng":0.4763688409,"data-quality":0.1478353561,"ml-security":0.1091657196}}
{"text":"Despite the challenges posed by the numerous interaction combinations, they also offer opportunities for multimodal learning of visual texts.","meta":{"url":"http://arxiv.org/abs/2307.13529v1"},"cats":{"new-dataset":0.1280276434,"dev-research":0.3152454225,"prompt-eng":0.3557335621,"data-quality":0.1569906107,"ml-security":0.0598061981}}
{"text":"In this paper, we present a systematic and unified framework (RmLR) that enhances HOI detection by incorporating structured text knowledge.","meta":{"url":"http://arxiv.org/abs/2307.13529v1"},"cats":{"new-dataset":0.2134610621,"dev-research":0.323084011,"prompt-eng":0.4932514103,"data-quality":0.3942623516,"ml-security":0.1638979855}}
{"text":"Firstly, we qualitatively and quantitatively analyze the loss of interaction information in the two-stage HOI detector and propose a re-mining strategy to generate more comprehensive visual representation.","meta":{"url":"http://arxiv.org/abs/2307.13529v1"},"cats":{"new-dataset":0.1715982427,"dev-research":0.2943890479,"prompt-eng":0.5008719929,"data-quality":0.2275989864,"ml-security":0.100835536}}
{"text":"Secondly, we design more fine-grained sentence-","meta":{"url":"http://arxiv.org/abs/2307.13529v1"},"cats":{"new-dataset":0.0346116375,"dev-research":0.3447317548,"prompt-eng":0.4026851096,"data-quality":0.2895107388,"ml-security":0.0957750332}}
{"text":"and word-level alignment and knowledge transfer strategies to effectively address the many-to-many matching problem between multiple interactions and multiple texts.","meta":{"url":"http://arxiv.org/abs/2307.13529v1"},"cats":{"new-dataset":0.0831991978,"dev-research":0.2458609094,"prompt-eng":0.3999970193,"data-quality":0.2040350274,"ml-security":0.0592095099}}
{"text":"These strategies alleviate the matching confusion problem that arises when multiple interactions occur simultaneously, thereby improving the effectiveness of the alignment process.","meta":{"url":"http://arxiv.org/abs/2307.13529v1"},"cats":{"new-dataset":0.0095314546,"dev-research":0.3559544636,"prompt-eng":0.4697278198,"data-quality":0.2661320978,"ml-security":0.0710038342}}
{"text":"Finally, HOI reasoning by visual features augmented with textual knowledge substantially improves the understanding of interactions.","meta":{"url":"http://arxiv.org/abs/2307.13529v1"},"cats":{"new-dataset":0.1007094952,"dev-research":0.4564791291,"prompt-eng":0.4526426269,"data-quality":0.1948511117,"ml-security":0.0789776982}}
{"text":"Experimental results illustrate the effectiveness of our approach, where state-of-the-art performance is achieved on public benchmarks.","meta":{"url":"http://arxiv.org/abs/2307.13529v1"},"cats":{"new-dataset":0.1019757359,"dev-research":0.2819042673,"prompt-eng":0.4583906159,"data-quality":0.1888234939,"ml-security":0.0979548474}}
{"text":"We further analyze the effects of different components of our approach to provide insights into its efficacy.","meta":{"url":"http://arxiv.org/abs/2307.13529v1"},"cats":{"new-dataset":0.0064663093,"dev-research":0.3119935994,"prompt-eng":0.3621649823,"data-quality":0.1328705852,"ml-security":0.1270876071}}
{"text":"The emergence of generative pre-trained models has facilitated the synthesis of high-quality text, but it has also posed challenges in identifying factual errors in the generated text.","meta":{"url":"http://arxiv.org/abs/2307.13528v1"},"cats":{"new-dataset":0.0854109915,"dev-research":0.3265254083,"prompt-eng":0.4339326792,"data-quality":0.4507891036,"ml-security":0.1563554417}}
{"text":"In particular: (1) A wider range of tasks now face an increasing risk of containing factual errors when handled by generative models.","meta":{"url":"http://arxiv.org/abs/2307.13528v1"},"cats":{"new-dataset":0.0137983066,"dev-research":0.3459021947,"prompt-eng":0.4261723085,"data-quality":0.285051453,"ml-security":0.2151095599}}
{"text":"(2) Generated texts tend to be lengthy and lack a clearly defined granularity for individual facts.","meta":{"url":"http://arxiv.org/abs/2307.13528v1"},"cats":{"new-dataset":0.0685935242,"dev-research":0.3521847517,"prompt-eng":0.3113391231,"data-quality":0.3333751774,"ml-security":0.0607541521}}
{"text":"(3) There is a scarcity of explicit evidence available during the process of fact checking.","meta":{"url":"http://arxiv.org/abs/2307.13528v1"},"cats":{"new-dataset":0.0529608945,"dev-research":0.2981518005,"prompt-eng":0.3329163272,"data-quality":0.2631986131,"ml-security":0.208930208}}
{"text":"With the above challenges in mind, in this paper, we propose FacTool, a task and domain agnostic framework for detecting factual errors of texts generated by large language models (e.g., ChatGPT).","meta":{"url":"http://arxiv.org/abs/2307.13528v1"},"cats":{"new-dataset":0.2026527813,"dev-research":0.3307185377,"prompt-eng":0.4272924893,"data-quality":0.5304108403,"ml-security":0.1298555132}}
{"text":"Experiments on four different tasks (knowledge-based QA, code generation, mathematical reasoning, and scientific literature review) show the efficacy of the proposed method.","meta":{"url":"http://arxiv.org/abs/2307.13528v1"},"cats":{"new-dataset":0.0283801697,"dev-research":0.4012457386,"prompt-eng":0.4355564561,"data-quality":0.1444726253,"ml-security":0.0606171126}}
{"text":"Diffusion Models (DM) are highly effective at generating realistic, high-quality images.","meta":{"url":"http://arxiv.org/abs/2307.13527v1"},"cats":{"new-dataset":0.0555392332,"dev-research":0.1688892107,"prompt-eng":0.3839724983,"data-quality":0.1102187486,"ml-security":0.1076759923}}
{"text":"However, these models lack creativity and merely compose outputs based on their training data, guided by a textual input provided at creation time.","meta":{"url":"http://arxiv.org/abs/2307.13527v1"},"cats":{"new-dataset":0.0633678395,"dev-research":0.2734972409,"prompt-eng":0.3855630333,"data-quality":0.3216349943,"ml-security":0.1510847348}}
{"text":"Is it acceptable to generate images reminiscent of an artist, employing his name as input?","meta":{"url":"http://arxiv.org/abs/2307.13527v1"},"cats":{"new-dataset":0.0270312928,"dev-research":0.2812364733,"prompt-eng":0.4133376588,"data-quality":0.2714469766,"ml-security":0.1739751378}}
{"text":"This imply that if the DM is able to replicate an artist's work then it was trained on some or all of his artworks thus violating copyright.","meta":{"url":"http://arxiv.org/abs/2307.13527v1"},"cats":{"new-dataset":0.0241353057,"dev-research":0.2613118112,"prompt-eng":0.2926871584,"data-quality":0.2375684888,"ml-security":0.2301082305}}
{"text":"In this paper, a preliminary study to infer the probability of use of an artist's name in the input string of a generated image is presented.","meta":{"url":"http://arxiv.org/abs/2307.13527v1"},"cats":{"new-dataset":0.0788322375,"dev-research":0.2247440579,"prompt-eng":0.4693674862,"data-quality":0.4010145436,"ml-security":0.1382787596}}
{"text":"To this aim we focused only on images generated by the famous DALL-E 2 and collected images (both original and generated) of five renowned artists.","meta":{"url":"http://arxiv.org/abs/2307.13527v1"},"cats":{"new-dataset":0.592220359,"dev-research":0.2027034487,"prompt-eng":0.3440796451,"data-quality":0.1773820678,"ml-security":0.0671027112}}
{"text":"Finally, a dedicated Siamese Neural Network was employed to have a first kind of probability.","meta":{"url":"http://arxiv.org/abs/2307.13527v1"},"cats":{"new-dataset":0.096523751,"dev-research":0.1796799201,"prompt-eng":0.3733637656,"data-quality":0.1432553036,"ml-security":0.0990797804}}
{"text":"Experimental results demonstrate that our approach is an optimal starting point and can be employed as a prior for predicting a complete input string of an investigated image.","meta":{"url":"http://arxiv.org/abs/2307.13527v1"},"cats":{"new-dataset":0.1113689974,"dev-research":0.1821576897,"prompt-eng":0.4924678158,"data-quality":0.2361721527,"ml-security":0.0995215677}}
{"text":"Dataset and code are available at: https://github.com/ictlab-unict/not-with-my-name .","meta":{"url":"http://arxiv.org/abs/2307.13527v1"},"cats":{"new-dataset":0.8545656603,"dev-research":0.2778479093,"prompt-eng":0.3958038874,"data-quality":0.2127096702,"ml-security":0.1237567404}}
{"text":"Logically constrained term rewriting systems (LCTRSs) are a program analyzing formalism with native support for data types which are not (co)inductively defined.","meta":{"url":"http://arxiv.org/abs/2307.13519v1"},"cats":{"new-dataset":0.060071246,"dev-research":0.4168313898,"prompt-eng":0.4102889964,"data-quality":0.1606288811,"ml-security":0.1028917559}}
{"text":"As a first-order formalism, LCTRSs have accommodated only analysis of imperative programs so far.","meta":{"url":"http://arxiv.org/abs/2307.13519v1"},"cats":{"new-dataset":0.0391380285,"dev-research":0.2805442727,"prompt-eng":0.3705621903,"data-quality":0.0833103577,"ml-security":0.1113684872}}
{"text":"In this paper, we present a higher-order variant of the LCTRS formalism, which can be used to analyze functional programs.","meta":{"url":"http://arxiv.org/abs/2307.13519v1"},"cats":{"new-dataset":0.031401441,"dev-research":0.3796797725,"prompt-eng":0.468178737,"data-quality":0.1228173629,"ml-security":0.1167607311}}
{"text":"Then we study the termination problem and define a higher-order recursive path ordering (HORPO) for this new formalism.","meta":{"url":"http://arxiv.org/abs/2307.13519v1"},"cats":{"new-dataset":0.0650821534,"dev-research":0.263933879,"prompt-eng":0.4599478289,"data-quality":0.1322580773,"ml-security":0.0837562169}}
{"text":"Dragonfly is scheduled to begin exploring Titan by 2034 using a series of multi-kilometer surface flights.","meta":{"url":"http://arxiv.org/abs/2307.13513v1"},"cats":{"new-dataset":0.1511759021,"dev-research":0.2198294673,"prompt-eng":0.3739601325,"data-quality":0.0752435725,"ml-security":0.0890967376}}
{"text":"This paper outlines the preliminary design of the navigation filter for the Dragonfly Mobility subsystem.","meta":{"url":"http://arxiv.org/abs/2307.13513v1"},"cats":{"new-dataset":0.0760584354,"dev-research":0.2368057242,"prompt-eng":0.4567380591,"data-quality":0.0889842554,"ml-security":0.0734354222}}
{"text":"The software architecture and filter formulation for lidar, visual odometry, pressure sensors, and redundant IMUs are described in detail.","meta":{"url":"http://arxiv.org/abs/2307.13513v1"},"cats":{"new-dataset":0.0786393863,"dev-research":0.2962044098,"prompt-eng":0.3661723002,"data-quality":0.1166695633,"ml-security":0.078314712}}
{"text":"Special discussion is given to developments to achieve multi-kilometer surface flights, including optimizing sequential image baselines, modeling correlating image processing errors, and an efficient approximation to the Simultaneous Localization and Mapping (SLAM) problem.","meta":{"url":"http://arxiv.org/abs/2307.13513v1"},"cats":{"new-dataset":0.1509161238,"dev-research":0.2259547021,"prompt-eng":0.3679518842,"data-quality":0.089987152,"ml-security":0.0375970911}}
{"text":"Vision-based Bird's Eye View (BEV) representation is an emerging perception formulation for autonomous driving.","meta":{"url":"http://arxiv.org/abs/2307.13510v1"},"cats":{"new-dataset":0.0426966618,"dev-research":0.2520066131,"prompt-eng":0.3977331812,"data-quality":0.1109099189,"ml-security":0.0836585608}}
{"text":"The core challenge is to construct BEV space with multi-camera features, which is a one-to-many ill-posed problem.","meta":{"url":"http://arxiv.org/abs/2307.13510v1"},"cats":{"new-dataset":0.2136085079,"dev-research":0.2162355447,"prompt-eng":0.3919877351,"data-quality":0.1274975759,"ml-security":0.0889912838}}
{"text":"Diving into all previous BEV representation generation methods, we found that most of them fall into two types: modeling depths in image views or modeling heights in the BEV space, mostly in an implicit way.","meta":{"url":"http://arxiv.org/abs/2307.13510v1"},"cats":{"new-dataset":0.0799074751,"dev-research":0.2512272065,"prompt-eng":0.3979012532,"data-quality":0.1241813072,"ml-security":0.0724326396}}
{"text":"In this work, we propose to explicitly model heights in the BEV space, which needs no extra data like LiDAR and can fit arbitrary camera rigs and types compared to modeling depths.","meta":{"url":"http://arxiv.org/abs/2307.13510v1"},"cats":{"new-dataset":0.3237129976,"dev-research":0.1755782454,"prompt-eng":0.390480265,"data-quality":0.0888956398,"ml-security":0.0960572549}}
{"text":"Theoretically, we give proof of the equivalence between height-based methods and depth-based methods.","meta":{"url":"http://arxiv.org/abs/2307.13510v1"},"cats":{"new-dataset":0.0237572452,"dev-research":0.247423941,"prompt-eng":0.361379887,"data-quality":0.1301550741,"ml-security":0.0965693082}}
{"text":"Considering the equivalence and some advantages of modeling heights, we propose HeightFormer, which models heights and uncertainties in a self-recursive way.","meta":{"url":"http://arxiv.org/abs/2307.13510v1"},"cats":{"new-dataset":0.062485906,"dev-research":0.2165681761,"prompt-eng":0.4631852686,"data-quality":0.0991841416,"ml-security":0.0910994643}}
{"text":"Without any extra data, the proposed HeightFormer could estimate heights in BEV accurately.","meta":{"url":"http://arxiv.org/abs/2307.13510v1"},"cats":{"new-dataset":0.0693121561,"dev-research":0.2157637043,"prompt-eng":0.4245022716,"data-quality":0.1121298278,"ml-security":0.0613590473}}
{"text":"Benchmark results show that the performance of HeightFormer achieves SOTA compared with those camera-only methods.","meta":{"url":"http://arxiv.org/abs/2307.13510v1"},"cats":{"new-dataset":0.0539599815,"dev-research":0.244910982,"prompt-eng":0.3969902543,"data-quality":0.0799829157,"ml-security":0.0432474595}}
{"text":"In this paper, we shall give an explicit proof that constacyclic codes over finite commutative rings can be realized as ideals in some twisted group rings.","meta":{"url":"http://arxiv.org/abs/2307.13507v1"},"cats":{"new-dataset":0.0999948821,"dev-research":0.2906665843,"prompt-eng":0.3521666145,"data-quality":0.1404348236,"ml-security":0.1493661255}}
{"text":"Also, we shall study isometries between those codes and, finally, we shall study k-Galois LCD constacyclic codes over finite fields.","meta":{"url":"http://arxiv.org/abs/2307.13507v1"},"cats":{"new-dataset":0.4710504272,"dev-research":0.2440646379,"prompt-eng":0.4296715007,"data-quality":0.1762386638,"ml-security":0.0661923419}}
{"text":"In particular, we shall characterize constacyclic LCD codes with respect to Euclidean inner product in terms of its idempotent generators and the classical involution using the twisted group algebras structures and find some good LCD codes.","meta":{"url":"http://arxiv.org/abs/2307.13507v1"},"cats":{"new-dataset":0.3711952252,"dev-research":0.262247408,"prompt-eng":0.3979887981,"data-quality":0.1572889455,"ml-security":0.1135994545}}
{"text":"Weighted automata (WA) are an extension of finite automata that defines functions from words to values in a given semi-ring.","meta":{"url":"http://arxiv.org/abs/2307.13505v1"},"cats":{"new-dataset":0.0397588346,"dev-research":0.2217435876,"prompt-eng":0.4186365091,"data-quality":0.1584380062,"ml-security":0.1053389216}}
{"text":"An alternative model that is deterministic, called Cost Register Automata (CRA), was introduced by Alur et al.","meta":{"url":"http://arxiv.org/abs/2307.13505v1"},"cats":{"new-dataset":0.0561486088,"dev-research":0.3014355016,"prompt-eng":0.4330722915,"data-quality":0.1384741587,"ml-security":0.1516957548}}
{"text":"It enriches deterministic finite automata with a finite number of registers, which store values, are updated at each transition using the operations of the semi-ring, and are combined to produce the output.","meta":{"url":"http://arxiv.org/abs/2307.13505v1"},"cats":{"new-dataset":0.1186748783,"dev-research":0.29438621,"prompt-eng":0.4355533404,"data-quality":0.1077085631,"ml-security":0.0911515604}}
{"text":"The expressiveness of a CRA depends on the number of its registers and the type of register updates allowed for each of its transitions.","meta":{"url":"http://arxiv.org/abs/2307.13505v1"},"cats":{"new-dataset":0.0280066362,"dev-research":0.2663522855,"prompt-eng":0.3850049591,"data-quality":0.1217286054,"ml-security":0.0836183518}}
{"text":"In particular, the class of functions computable by a CRA with register updates defined by linear (or affine) maps correspond exactly with rational functions (functions computable by a WA).","meta":{"url":"http://arxiv.org/abs/2307.13505v1"},"cats":{"new-dataset":0.0618470712,"dev-research":0.3173243844,"prompt-eng":0.3383633778,"data-quality":0.1463715619,"ml-security":0.1519352621}}
{"text":"A natural problem for CRA is the register minimization problem: given a function defined by a CRA, what is the minimal number of registers needed to realize this function?   ","meta":{"url":"http://arxiv.org/abs/2307.13505v1"},"cats":{"new-dataset":0.0487376453,"dev-research":0.2981763665,"prompt-eng":0.3416899749,"data-quality":0.1301176015,"ml-security":0.0801020636}}
{"text":"In this paper, we solve the register minimization problem for CRA over a field with linear (or affine) register updates, using an algebraic invariant of a WA introduced recently by Bell and Smertnig, the so-called the linear hull of the automaton.","meta":{"url":"http://arxiv.org/abs/2307.13505v1"},"cats":{"new-dataset":0.0908746358,"dev-research":0.2992054442,"prompt-eng":0.4237026337,"data-quality":0.1987838399,"ml-security":0.1138656404}}
{"text":"This invariant being computable, we are able to explicitly compute a CRA with linear (or affine) updates, using the minimal number of registers.   ","meta":{"url":"http://arxiv.org/abs/2307.13505v1"},"cats":{"new-dataset":0.1150421326,"dev-research":0.2355004554,"prompt-eng":0.4262749863,"data-quality":0.1532381416,"ml-security":0.1067482899}}
{"text":"Using these techniques, we are also able to solve the more general CRA minimisation problem: given a CRA and integers $k,d$, is there an equivalent linear (resp.~affine) CRA using at most $k$ states and $d$ registers?","meta":{"url":"http://arxiv.org/abs/2307.13505v1"},"cats":{"new-dataset":0.0605893262,"dev-research":0.1811736877,"prompt-eng":0.3713820419,"data-quality":0.1265243363,"ml-security":0.0759982609}}
{"text":"Prevalent in many real-world settings such as healthcare, irregular time series are challenging to formulate predictions from.","meta":{"url":"http://arxiv.org/abs/2307.13503v1"},"cats":{"new-dataset":0.0217832215,"dev-research":0.2427773821,"prompt-eng":0.3568542512,"data-quality":0.1582367636,"ml-security":0.2654531612}}
{"text":"It is difficult to infer the value of a feature at any given time when observations are sporadic, as it could take on a range of values depending on when it was last observed.","meta":{"url":"http://arxiv.org/abs/2307.13503v1"},"cats":{"new-dataset":0.0255189835,"dev-research":0.2414154822,"prompt-eng":0.4521662387,"data-quality":0.2310999277,"ml-security":0.1229023969}}
{"text":"To characterize this uncertainty we present EDICT, a strategy that learns an evidential distribution over irregular time series in continuous time.","meta":{"url":"http://arxiv.org/abs/2307.13503v1"},"cats":{"new-dataset":0.0818771329,"dev-research":0.1882236874,"prompt-eng":0.4139683686,"data-quality":0.297129802,"ml-security":0.2558073316}}
{"text":"This distribution enables well-calibrated and flexible inference of partially observed features at any time of interest, while expanding uncertainty temporally for sparse, irregular observations.","meta":{"url":"http://arxiv.org/abs/2307.13503v1"},"cats":{"new-dataset":0.2093639309,"dev-research":0.2086993006,"prompt-eng":0.4135283605,"data-quality":0.1938157919,"ml-security":0.1101806538}}
{"text":"We demonstrate that EDICT attains competitive performance on challenging time series classification tasks and enabling uncertainty-guided inference when encountering noisy data.","meta":{"url":"http://arxiv.org/abs/2307.13503v1"},"cats":{"new-dataset":0.0959267454,"dev-research":0.251399765,"prompt-eng":0.4058796352,"data-quality":0.3996594057,"ml-security":0.2561170596}}
{"text":"Current anti-money laundering (AML) systems, predominantly rule-based, exhibit notable shortcomings in efficiently and precisely detecting instances of money laundering.","meta":{"url":"http://arxiv.org/abs/2307.13499v1"},"cats":{"new-dataset":0.0506601215,"dev-research":0.2768765877,"prompt-eng":0.3470138453,"data-quality":0.2534286736,"ml-security":0.2996778254}}
{"text":"As a result, there has been a recent surge toward exploring alternative approaches, particularly those utilizing machine learning.","meta":{"url":"http://arxiv.org/abs/2307.13499v1"},"cats":{"new-dataset":0.0208775425,"dev-research":0.287318731,"prompt-eng":0.3683747803,"data-quality":0.1668416775,"ml-security":0.2348860362}}
{"text":"Since criminals often collaborate in their money laundering endeavors, accounting for diverse types of customer relations and links becomes crucial.","meta":{"url":"http://arxiv.org/abs/2307.13499v1"},"cats":{"new-dataset":0.0395891668,"dev-research":0.2875802342,"prompt-eng":0.3237694305,"data-quality":0.1788703937,"ml-security":0.2579701113}}
{"text":"In line with this, the present paper introduces a graph neural network (GNN) approach to identify money laundering activities within a large heterogeneous network constructed from real-world bank transactions and business role data belonging to DNB, Norway's largest bank.","meta":{"url":"http://arxiv.org/abs/2307.13499v1"},"cats":{"new-dataset":0.2754067913,"dev-research":0.2166355645,"prompt-eng":0.2688267467,"data-quality":0.2303284978,"ml-security":0.2916635948}}
{"text":"Specifically, we extend the homogeneous GNN method known as the Message Passing Neural Network (MPNN) to operate effectively on a heterogeneous graph.","meta":{"url":"http://arxiv.org/abs/2307.13499v1"},"cats":{"new-dataset":0.0577651005,"dev-research":0.2106295807,"prompt-eng":0.3143220925,"data-quality":0.2113443773,"ml-security":0.1354129467}}
{"text":"As part of this procedure, we propose a novel method for aggregating messages across different edges of the graph.","meta":{"url":"http://arxiv.org/abs/2307.13499v1"},"cats":{"new-dataset":0.0992033196,"dev-research":0.2641949092,"prompt-eng":0.3889982183,"data-quality":0.2809908927,"ml-security":0.078493011}}
{"text":"Our findings highlight the importance of using an appropriate GNN architecture when combining information in heterogeneous graphs.","meta":{"url":"http://arxiv.org/abs/2307.13499v1"},"cats":{"new-dataset":0.052495538,"dev-research":0.2493560949,"prompt-eng":0.3151586453,"data-quality":0.2313966863,"ml-security":0.1010363925}}
{"text":"The performance results of our model demonstrate great potential in enhancing the quality of electronic surveillance systems employed by banks to detect instances of money laundering.","meta":{"url":"http://arxiv.org/abs/2307.13499v1"},"cats":{"new-dataset":0.0769993015,"dev-research":0.2384818018,"prompt-eng":0.4153392626,"data-quality":0.2346421067,"ml-security":0.3502229656}}
{"text":"To the best of our knowledge, this is the first published work applying GNN on a large real-world heterogeneous network for anti-money laundering purposes.","meta":{"url":"http://arxiv.org/abs/2307.13499v1"},"cats":{"new-dataset":0.3750566884,"dev-research":0.1830897963,"prompt-eng":0.296223185,"data-quality":0.1670694761,"ml-security":0.2389699881}}
{"text":"The Zero-Shot Learning (ZSL) task pertains to the identification of entities or relations in texts that were not seen during training.","meta":{"url":"http://arxiv.org/abs/2307.13497v1"},"cats":{"new-dataset":0.0880302434,"dev-research":0.204820523,"prompt-eng":0.3622294081,"data-quality":0.3026012006,"ml-security":0.150846666}}
{"text":"ZSL has emerged as a critical research area due to the scarcity of labeled data in specific domains, and its applications have grown significantly in recent years.","meta":{"url":"http://arxiv.org/abs/2307.13497v1"},"cats":{"new-dataset":0.243148495,"dev-research":0.2794769661,"prompt-eng":0.3949100163,"data-quality":0.2698089404,"ml-security":0.1059230706}}
{"text":"With the advent of large pretrained language models, several novel methods have been proposed, resulting in substantial improvements in ZSL performance.","meta":{"url":"http://arxiv.org/abs/2307.13497v1"},"cats":{"new-dataset":0.0681863289,"dev-research":0.2444529362,"prompt-eng":0.4429771463,"data-quality":0.2156141291,"ml-security":0.0800607991}}
{"text":"There is a growing demand, both in the research community and industry, for a comprehensive ZSL framework that facilitates the development and accessibility of the latest methods and pretrained models.","meta":{"url":"http://arxiv.org/abs/2307.13497v1"},"cats":{"new-dataset":0.1154034723,"dev-research":0.2764904555,"prompt-eng":0.3938473602,"data-quality":0.0621052306,"ml-security":0.0517295949}}
{"text":"In this study, we propose a novel ZSL framework called Zshot that aims to address the aforementioned challenges.","meta":{"url":"http://arxiv.org/abs/2307.13497v1"},"cats":{"new-dataset":0.2650046837,"dev-research":0.3154878709,"prompt-eng":0.4219848163,"data-quality":0.1106591157,"ml-security":0.0854752862}}
{"text":"Our primary objective is to provide a platform that allows researchers to compare different state-of-the-art ZSL methods with standard benchmark datasets.","meta":{"url":"http://arxiv.org/abs/2307.13497v1"},"cats":{"new-dataset":0.2444289798,"dev-research":0.2788081602,"prompt-eng":0.3951656851,"data-quality":0.1220577775,"ml-security":0.0658128407}}
{"text":"Additionally, we have designed our framework to support the industry with readily available APIs for production under the standard SpaCy NLP pipeline.","meta":{"url":"http://arxiv.org/abs/2307.13497v1"},"cats":{"new-dataset":0.2258544675,"dev-research":0.3223608042,"prompt-eng":0.3934602261,"data-quality":0.1966136925,"ml-security":0.0840453853}}
{"text":"Our API is extendible and evaluable, moreover, we include numerous enhancements such as boosting the accuracy with pipeline ensembling and visualization utilities available as a SpaCy extension.","meta":{"url":"http://arxiv.org/abs/2307.13497v1"},"cats":{"new-dataset":0.2814751018,"dev-research":0.3238271915,"prompt-eng":0.4546817992,"data-quality":0.220536485,"ml-security":0.1209022142}}
{"text":"Learned cardinality estimation methods have achieved high precision compared to traditional methods.","meta":{"url":"http://arxiv.org/abs/2307.13494v2"},"cats":{"new-dataset":0.0409464943,"dev-research":0.2471650303,"prompt-eng":0.3773497226,"data-quality":0.2970490134,"ml-security":0.1271568946}}
{"text":"Among learned methods, query-driven approaches face the data and workload drift problem for a long time.","meta":{"url":"http://arxiv.org/abs/2307.13494v2"},"cats":{"new-dataset":0.0719100363,"dev-research":0.3574499631,"prompt-eng":0.3514234004,"data-quality":0.1942659086,"ml-security":0.2587588663}}
{"text":"Although both query-driven and hybrid methods are proposed to avoid this problem, even the state-of-art of them suffer from high training and estimation costs, limited scalability, instability, and long-tailed distribution problem on high cardinality and high dimensional tables, which seriously affects the practical application of learned cardinality estimators.","meta":{"url":"http://arxiv.org/abs/2307.13494v2"},"cats":{"new-dataset":0.0896480883,"dev-research":0.1769878377,"prompt-eng":0.3800411124,"data-quality":0.209401326,"ml-security":0.205137624}}
{"text":"In this paper, we prove that most of these problems are directly caused by the widely used progressive sampling.","meta":{"url":"http://arxiv.org/abs/2307.13494v2"},"cats":{"new-dataset":0.0999179978,"dev-research":0.1943938546,"prompt-eng":0.3763029875,"data-quality":0.2666516981,"ml-security":0.1363589775}}
{"text":"We solve this problem by introducing predicates into the autoregressive model and propose Duet, a stable, efficient, and scalable hybrid method to estimate cardinality directly without sampling or any non-differentiable process, which can not only reduces the inference complexity from $O(n)$ to $O(1)$ compared to Naru and UAE but also achieve higher accuracy on high cardinality and high dimensional tables.","meta":{"url":"http://arxiv.org/abs/2307.13494v2"},"cats":{"new-dataset":0.0980330555,"dev-research":0.1847657767,"prompt-eng":0.4030662316,"data-quality":0.1278878828,"ml-security":0.0716480147}}
{"text":"Experimental results show that Duet can achieve all the design goals above and be much more practical and even has a lower inference cost on CPU than that of most learned methods on GPU.","meta":{"url":"http://arxiv.org/abs/2307.13494v2"},"cats":{"new-dataset":0.0204311189,"dev-research":0.3208290853,"prompt-eng":0.3912875214,"data-quality":0.0903462704,"ml-security":0.1087591162}}
{"text":"Deep learning has made significant advancements in supervised learning.","meta":{"url":"http://arxiv.org/abs/2307.13492v1"},"cats":{"new-dataset":0.0415744286,"dev-research":0.2439154227,"prompt-eng":0.3272077048,"data-quality":0.2088348612,"ml-security":0.1434846888}}
{"text":"However, models trained in this setting often face challenges due to domain shift between training and test sets, resulting in a significant drop in performance during testing.","meta":{"url":"http://arxiv.org/abs/2307.13492v1"},"cats":{"new-dataset":0.0188395922,"dev-research":0.3040824149,"prompt-eng":0.416798443,"data-quality":0.2890512221,"ml-security":0.2258507214}}
{"text":"To address this issue, several domain generalization methods have been developed to learn robust and domain-invariant features from multiple training domains that can generalize well to unseen test domains.","meta":{"url":"http://arxiv.org/abs/2307.13492v1"},"cats":{"new-dataset":0.0741108539,"dev-research":0.2768096939,"prompt-eng":0.4558158087,"data-quality":0.4040011134,"ml-security":0.3391463832}}
{"text":"Data augmentation plays a crucial role in achieving this goal by enhancing the diversity of the training data.","meta":{"url":"http://arxiv.org/abs/2307.13492v1"},"cats":{"new-dataset":0.1034422347,"dev-research":0.2591101106,"prompt-eng":0.3677424427,"data-quality":0.2410911788,"ml-security":0.1509234353}}
{"text":"In this paper, inspired by the observation that normalizing an image with different statistics generated by different batches with various domains can perturb its feature, we propose a simple yet effective method called NormAUG (Normalization-guided Augmentation).","meta":{"url":"http://arxiv.org/abs/2307.13492v1"},"cats":{"new-dataset":0.0499528245,"dev-research":0.2347387351,"prompt-eng":0.4321447542,"data-quality":0.3387592741,"ml-security":0.1205673758}}
{"text":"Our method includes two paths: the main path and the auxiliary (augmented) path.","meta":{"url":"http://arxiv.org/abs/2307.13492v1"},"cats":{"new-dataset":0.0291133868,"dev-research":0.2552011912,"prompt-eng":0.4303251597,"data-quality":0.1376078115,"ml-security":0.0444201933}}
{"text":"During training, the auxiliary path includes multiple sub-paths, each corresponding to batch normalization for a single domain or a random combination of multiple domains.","meta":{"url":"http://arxiv.org/abs/2307.13492v1"},"cats":{"new-dataset":0.0472744087,"dev-research":0.2208082458,"prompt-eng":0.4225311405,"data-quality":0.204431236,"ml-security":0.1165123348}}
{"text":"This introduces diverse information at the feature level and improves the generalization of the main path.","meta":{"url":"http://arxiv.org/abs/2307.13492v1"},"cats":{"new-dataset":0.0656494886,"dev-research":0.3276186045,"prompt-eng":0.4381524488,"data-quality":0.1501605389,"ml-security":0.085997136}}
{"text":"Moreover, our NormAUG method effectively reduces the existing upper boundary for generalization based on theoretical perspectives.","meta":{"url":"http://arxiv.org/abs/2307.13492v1"},"cats":{"new-dataset":0.003322144,"dev-research":0.2042877059,"prompt-eng":0.3849879649,"data-quality":0.2202370814,"ml-security":0.1316191235}}
{"text":"During the test stage, we leverage an ensemble strategy to combine the predictions from the auxiliary path of our model, further boosting performance.","meta":{"url":"http://arxiv.org/abs/2307.13492v1"},"cats":{"new-dataset":0.0482995796,"dev-research":0.2432430477,"prompt-eng":0.52097229,"data-quality":0.1795489865,"ml-security":0.1511863955}}
{"text":"Extensive experiments are conducted on multiple benchmark datasets to validate the effectiveness of our proposed method.","meta":{"url":"http://arxiv.org/abs/2307.13492v1"},"cats":{"new-dataset":0.1847122768,"dev-research":0.2490779506,"prompt-eng":0.4074034202,"data-quality":0.2735083156,"ml-security":0.0986149588}}
{"text":"We propose Cos R-CNN, a simple exemplar-based R-CNN formulation that is designed for online few-shot object detection.","meta":{"url":"http://arxiv.org/abs/2307.13485v1"},"cats":{"new-dataset":0.1847041326,"dev-research":0.2005170775,"prompt-eng":0.3258296235,"data-quality":0.1734464275,"ml-security":0.1434148402}}
{"text":"That is, it is able to localise and classify novel object categories in images with few examples without fine-tuning.","meta":{"url":"http://arxiv.org/abs/2307.13485v1"},"cats":{"new-dataset":0.0472894106,"dev-research":0.2755868989,"prompt-eng":0.3974461823,"data-quality":0.3101870922,"ml-security":0.0840880344}}
{"text":"Cos R-CNN frames detection as a learning-to-compare task: unseen classes are represented as exemplar images, and objects are detected based on their similarity to these exemplars.","meta":{"url":"http://arxiv.org/abs/2307.13485v1"},"cats":{"new-dataset":0.1630710394,"dev-research":0.2666759655,"prompt-eng":0.3576258057,"data-quality":0.3202371697,"ml-security":0.1568296299}}
{"text":"The cosine-based classification head allows for dynamic adaptation of classification parameters to the exemplar embedding, and encourages the clustering of similar classes in embedding space without the need for manual tuning of distance-metric hyperparameters.","meta":{"url":"http://arxiv.org/abs/2307.13485v1"},"cats":{"new-dataset":0.0324084211,"dev-research":0.2982752544,"prompt-eng":0.4141404619,"data-quality":0.2585320309,"ml-security":0.152308997}}
{"text":"This simple formulation achieves best results on the recently proposed 5-way ImageNet few-shot detection benchmark, beating the online 1/5/10-shot scenarios by more than 8/3/1%, as well as performing up to 20% better in online 20-way few-shot VOC across all shots on novel classes.","meta":{"url":"http://arxiv.org/abs/2307.13485v1"},"cats":{"new-dataset":0.1054595966,"dev-research":0.2128241471,"prompt-eng":0.353800187,"data-quality":0.2422618715,"ml-security":0.1531692639}}
{"text":"This work is concerned with the kernel-based approximation of a complex-valued function from data, where the frequency response function of a partial differential equation in the frequency domain is of particular interest.","meta":{"url":"http://arxiv.org/abs/2307.13484v1"},"cats":{"new-dataset":0.0935161178,"dev-research":0.1881170495,"prompt-eng":0.3614457383,"data-quality":0.1878473773,"ml-security":0.1289662464}}
{"text":"In this setting, kernel methods are employed more and more frequently, however, standard kernels do not perform well.","meta":{"url":"http://arxiv.org/abs/2307.13484v1"},"cats":{"new-dataset":0.0063927109,"dev-research":0.3150791218,"prompt-eng":0.3501102296,"data-quality":0.2242505875,"ml-security":0.1352857127}}
{"text":"Moreover, the role and mathematical implications of the underlying pair of kernels, which arises naturally in the complex-valued case, remain to be addressed.","meta":{"url":"http://arxiv.org/abs/2307.13484v1"},"cats":{"new-dataset":0.0184968326,"dev-research":0.2323506266,"prompt-eng":0.3104021151,"data-quality":0.1627442809,"ml-security":0.2070091736}}
{"text":"We introduce new reproducing kernel Hilbert spaces of complex-valued functions, and formulate the problem of complex-valued interpolation with a kernel pair as minimum norm interpolation in these spaces.","meta":{"url":"http://arxiv.org/abs/2307.13484v1"},"cats":{"new-dataset":0.0817481771,"dev-research":0.2401185302,"prompt-eng":0.3552329971,"data-quality":0.2102012597,"ml-security":0.1306418756}}
{"text":"Moreover, we combine the interpolant with a low-order rational function, where the order is adaptively selected based on a new model selection criterion.","meta":{"url":"http://arxiv.org/abs/2307.13484v1"},"cats":{"new-dataset":0.0136822224,"dev-research":0.1327557606,"prompt-eng":0.386064549,"data-quality":0.103039813,"ml-security":0.0816224901}}
{"text":"Numerical results on examples from different fields, including electromagnetics and acoustic examples, illustrate the performance of the method, also in comparison to available rational approximation methods.","meta":{"url":"http://arxiv.org/abs/2307.13484v1"},"cats":{"new-dataset":0.0178539755,"dev-research":0.1912816859,"prompt-eng":0.3361668096,"data-quality":0.1279884633,"ml-security":0.0660395451}}
{"text":"Secure aggregation usually aims at securely computing the sum of the inputs from $K$ users at a server.","meta":{"url":"http://arxiv.org/abs/2307.13474v1"},"cats":{"new-dataset":0.0305710126,"dev-research":0.2554565355,"prompt-eng":0.3551384271,"data-quality":0.1188451137,"ml-security":0.3896572932}}
{"text":"Noticing that the sum might inevitably reveal information about the inputs (when the inputs are non-uniform) and typically the users (not the server) desire the sum (in applications such as federated learning), we consider a variant of secure aggregation where the server is oblivious, i.e., the server only serves as a communication facilitator/helper to enable the users to securely compute the sum and learns nothing in the process.","meta":{"url":"http://arxiv.org/abs/2307.13474v1"},"cats":{"new-dataset":0.0308123145,"dev-research":0.2228082709,"prompt-eng":0.3728679257,"data-quality":0.1483126688,"ml-security":0.5088105305}}
{"text":"Our communication protocol involves one round of messages from the users to the server and one round of messages from the server to each user such that in the end each user only learns the sum of all $K$ inputs and the server learns no information about the inputs.","meta":{"url":"http://arxiv.org/abs/2307.13474v1"},"cats":{"new-dataset":0.1550372445,"dev-research":0.2382090514,"prompt-eng":0.3962861277,"data-quality":0.1871416098,"ml-security":0.3562102243}}
{"text":"For this secure aggregation with an oblivious server problem, we show that to compute $1$ bit of the sum securely, each user needs to send at least $1$ bit to the server, the server needs to send at least $1$ bit to each user, each user needs to hold a key of at least $2$ bits, and all users need to collectively hold at least $K$ key bits.","meta":{"url":"http://arxiv.org/abs/2307.13474v1"},"cats":{"new-dataset":0.162216284,"dev-research":0.1983438329,"prompt-eng":0.3581069039,"data-quality":0.1019776605,"ml-security":0.3160316991}}
{"text":"In addition, when user dropouts are allowed, the optimal performance remains the same, except that the minimum size of the key held by each user increases to $K$ bits, per sum bit.","meta":{"url":"http://arxiv.org/abs/2307.13474v1"},"cats":{"new-dataset":0.004310653,"dev-research":0.2833243218,"prompt-eng":0.3506553581,"data-quality":0.0943130229,"ml-security":0.1301317838}}
{"text":"This paper proposes a new combinatorial auction framework for local energy flexibility markets, which addresses the issue of prosumers' inability to bundle multiple flexibility time intervals.","meta":{"url":"http://arxiv.org/abs/2307.13470v1"},"cats":{"new-dataset":0.0637230404,"dev-research":0.2069888902,"prompt-eng":0.3727767315,"data-quality":0.087926309,"ml-security":0.0767784893}}
{"text":"To solve the underlying NP-complete winner determination problems, we present a simple yet powerful heterogeneous tri-partite graph representation and design graph neural network-based models.","meta":{"url":"http://arxiv.org/abs/2307.13470v1"},"cats":{"new-dataset":0.1056790786,"dev-research":0.2467498016,"prompt-eng":0.3189054378,"data-quality":0.1896911395,"ml-security":0.1595413571}}
{"text":"Our models achieve an average optimal value deviation of less than 5\\% from an off-the-shelf optimization tool and show linear inference time complexity compared to the exponential complexity of the commercial solver.","meta":{"url":"http://arxiv.org/abs/2307.13470v1"},"cats":{"new-dataset":0.0279280935,"dev-research":0.2626535879,"prompt-eng":0.4173888732,"data-quality":0.1286702068,"ml-security":0.1335013076}}
{"text":"Contributions and results demonstrate the potential of using machine learning to efficiently allocate energy flexibility resources in local markets and solving optimization problems in general.","meta":{"url":"http://arxiv.org/abs/2307.13470v1"},"cats":{"new-dataset":0.027847465,"dev-research":0.2757353637,"prompt-eng":0.3463177266,"data-quality":0.1293483755,"ml-security":0.2081058589}}
{"text":"Bundle recommendation aims to provide a bundle of items to satisfy the user preference on e-commerce platform.","meta":{"url":"http://arxiv.org/abs/2307.13468v1"},"cats":{"new-dataset":0.0289182594,"dev-research":0.2808762919,"prompt-eng":0.4328518616,"data-quality":0.0851269172,"ml-security":0.065685337}}
{"text":"Existing successful solutions are based on the contrastive graph learning paradigm where graph neural networks (GNNs) are employed to learn representations from user-level and bundle-level graph views with a contrastive learning module to enhance the cooperative association between different views.","meta":{"url":"http://arxiv.org/abs/2307.13468v1"},"cats":{"new-dataset":0.0831179683,"dev-research":0.2572891301,"prompt-eng":0.308746587,"data-quality":0.2057278765,"ml-security":0.1175656708}}
{"text":"Nevertheless, they ignore the uncertainty issue which has a significant impact in real bundle recommendation scenarios due to the lack of discriminative information caused by highly sparsity or diversity.","meta":{"url":"http://arxiv.org/abs/2307.13468v1"},"cats":{"new-dataset":0.0260760506,"dev-research":0.2319964431,"prompt-eng":0.3697912251,"data-quality":0.3687941136,"ml-security":0.1458649992}}
{"text":"We further suggest that their instancewise contrastive learning fails to distinguish the semantically similar negatives (i.e., sampling bias issue), resulting in performance degradation.","meta":{"url":"http://arxiv.org/abs/2307.13468v1"},"cats":{"new-dataset":0.0277610773,"dev-research":0.2400141392,"prompt-eng":0.3527031387,"data-quality":0.43680121,"ml-security":0.1751127439}}
{"text":"In this paper, we propose a novel Gaussian Graph with Prototypical Contrastive Learning (GPCL) framework to overcome these challenges.","meta":{"url":"http://arxiv.org/abs/2307.13468v1"},"cats":{"new-dataset":0.1774782276,"dev-research":0.2043377947,"prompt-eng":0.3346451054,"data-quality":0.1642869033,"ml-security":0.1184431861}}
{"text":"In particular, GPCL embeds each user/bundle/item as a Gaussian distribution rather than a fixed vector.","meta":{"url":"http://arxiv.org/abs/2307.13468v1"},"cats":{"new-dataset":0.0210444328,"dev-research":0.2295325817,"prompt-eng":0.3835406215,"data-quality":0.1121061945,"ml-security":0.1199920796}}
{"text":"We further design a prototypical contrastive learning module to capture the contextual information and mitigate the sampling bias issue.","meta":{"url":"http://arxiv.org/abs/2307.13468v1"},"cats":{"new-dataset":0.1123545413,"dev-research":0.2557717692,"prompt-eng":0.3893331599,"data-quality":0.292463273,"ml-security":0.1357990335}}
{"text":"Extensive experiments demonstrate that benefiting from the proposed components, we achieve new state-of-the-art performance compared to previous methods on several public datasets.","meta":{"url":"http://arxiv.org/abs/2307.13468v1"},"cats":{"new-dataset":0.3995715434,"dev-research":0.2229803922,"prompt-eng":0.4033119895,"data-quality":0.234592735,"ml-security":0.1104271532}}
{"text":"Moreover, GPCL has been deployed on real-world e-commerce platform and achieved substantial improvements.","meta":{"url":"http://arxiv.org/abs/2307.13468v1"},"cats":{"new-dataset":0.1335969576,"dev-research":0.2906515263,"prompt-eng":0.3508627514,"data-quality":0.0925594742,"ml-security":0.0770546482}}
{"text":"Holographic MIMO refers to an array (possibly large) with a massive number of antennas that are individually controlled and densely deployed.","meta":{"url":"http://arxiv.org/abs/2307.13467v1"},"cats":{"new-dataset":0.0834340447,"dev-research":0.2037879569,"prompt-eng":0.3347019632,"data-quality":0.0782618235,"ml-security":0.0697855106}}
{"text":"The aim of this paper is to provide further insights into the advantages (if any) of having closely spaced antennas in the uplink and downlink of a multi-user Holographic MIMO system.","meta":{"url":"http://arxiv.org/abs/2307.13467v1"},"cats":{"new-dataset":0.0164526606,"dev-research":0.228330936,"prompt-eng":0.3805961668,"data-quality":0.0822383804,"ml-security":0.0587347925}}
{"text":"To this end, we make use of the multiport communication theory, which ensures physically consistent uplink and downlink models.","meta":{"url":"http://arxiv.org/abs/2307.13467v1"},"cats":{"new-dataset":0.0236467274,"dev-research":0.2180908321,"prompt-eng":0.4090776713,"data-quality":0.1006788175,"ml-security":0.085493006}}
{"text":"We first consider a simple uplink scenario with two side-by-side half-wavelength dipoles, two users and single path line-of-sight propagation, and show both analytically and numerically that the channel gain and average spectral efficiency depend strongly on the directions from which the signals are received and on the array matching network used.","meta":{"url":"http://arxiv.org/abs/2307.13467v1"},"cats":{"new-dataset":0.0100532894,"dev-research":0.2017854971,"prompt-eng":0.3794654286,"data-quality":0.1208760611,"ml-security":0.0872525595}}
{"text":"Numerical results are then used to extend the analysis to more practical scenarios with a larger number of dipoles and users.","meta":{"url":"http://arxiv.org/abs/2307.13467v1"},"cats":{"new-dataset":0.0222975615,"dev-research":0.294899368,"prompt-eng":0.3980152508,"data-quality":0.0974988881,"ml-security":0.1002934919}}
{"text":"The case in which the antennas are densely packed in a space-constrained factor form is also considered.","meta":{"url":"http://arxiv.org/abs/2307.13467v1"},"cats":{"new-dataset":0.026324946,"dev-research":0.1640999244,"prompt-eng":0.3541606063,"data-quality":0.1202790662,"ml-security":0.0699195762}}
{"text":"It turns out that the spectral efficiency increases as the antenna distance reduces thanks to the larger number of antennas that allow to collect more energy, not because of the mutual coupling.","meta":{"url":"http://arxiv.org/abs/2307.13467v1"},"cats":{"new-dataset":0.0038291564,"dev-research":0.2372036029,"prompt-eng":0.330150204,"data-quality":0.0934159906,"ml-security":0.0424401451}}
{"text":"Crop yield prediction typically involves the utilization of either theory-driven process-based crop growth models, which have proven to be difficult to calibrate for local conditions, or data-driven machine learning methods, which are known to require large datasets.","meta":{"url":"http://arxiv.org/abs/2307.13466v1"},"cats":{"new-dataset":0.0617247825,"dev-research":0.3071800659,"prompt-eng":0.3502585443,"data-quality":0.1709570235,"ml-security":0.2002023051}}
{"text":"In this work we investigate potato yield prediction using a hybrid meta-modeling approach.","meta":{"url":"http://arxiv.org/abs/2307.13466v1"},"cats":{"new-dataset":0.0668702988,"dev-research":0.1845580148,"prompt-eng":0.4021237151,"data-quality":0.1321319905,"ml-security":0.103373243}}
{"text":"A crop growth model is employed to generate synthetic data for (pre)training a convolutional neural net, which is then fine-tuned with observational data.","meta":{"url":"http://arxiv.org/abs/2307.13466v1"},"cats":{"new-dataset":0.4073168901,"dev-research":0.1942346184,"prompt-eng":0.336465538,"data-quality":0.1358552758,"ml-security":0.1294274753}}
{"text":"When applied in silico, our meta-modeling approach yields better predictions than a baseline comprising a purely data-driven approach.","meta":{"url":"http://arxiv.org/abs/2307.13466v1"},"cats":{"new-dataset":0.0575453935,"dev-research":0.2615041624,"prompt-eng":0.4221587786,"data-quality":0.1582978783,"ml-security":0.1068873204}}
{"text":"When tested on real-world data from field trials (n=303) and commercial fields (n=77), the meta-modeling approach yields competitive results with respect to the crop growth model.","meta":{"url":"http://arxiv.org/abs/2307.13466v1"},"cats":{"new-dataset":0.0769469998,"dev-research":0.157920656,"prompt-eng":0.3490660659,"data-quality":0.1003320576,"ml-security":0.0574198047}}
{"text":"In the latter set, however, both models perform worse than a simple linear regression with a hand-picked feature set and dedicated preprocessing designed by domain experts.","meta":{"url":"http://arxiv.org/abs/2307.13466v1"},"cats":{"new-dataset":0.0226773712,"dev-research":0.3374098587,"prompt-eng":0.3778341757,"data-quality":0.2109271619,"ml-security":0.1551020514}}
{"text":"Our findings indicate the potential of meta-modeling for accurate crop yield prediction; however, further advancements and validation using extensive real-world datasets is recommended to solidify its practical effectiveness.","meta":{"url":"http://arxiv.org/abs/2307.13466v1"},"cats":{"new-dataset":0.1093546784,"dev-research":0.2112758082,"prompt-eng":0.3905783133,"data-quality":0.1840277651,"ml-security":0.1178948448}}
{"text":"The emergence of artificial emotional intelligence technology is revolutionizing the fields of computers and robotics, allowing for a new level of communication and understanding of human behavior that was once thought impossible.","meta":{"url":"http://arxiv.org/abs/2307.13463v1"},"cats":{"new-dataset":0.0449808577,"dev-research":0.3178626692,"prompt-eng":0.3273818916,"data-quality":0.0915678441,"ml-security":0.1518474021}}
{"text":"While recent advancements in deep learning have transformed the field of computer vision, automated understanding of evoked or expressed emotions in visual media remains in its infancy.","meta":{"url":"http://arxiv.org/abs/2307.13463v1"},"cats":{"new-dataset":0.0834437635,"dev-research":0.2944791796,"prompt-eng":0.3551135558,"data-quality":0.2341954433,"ml-security":0.1666611744}}
{"text":"This foundering stems from the absence of a universally accepted definition of \"emotion\", coupled with the inherently subjective nature of emotions and their intricate nuances.","meta":{"url":"http://arxiv.org/abs/2307.13463v1"},"cats":{"new-dataset":0.0256460911,"dev-research":0.3249883835,"prompt-eng":0.2792759905,"data-quality":0.1701502167,"ml-security":0.0727256946}}
{"text":"In this article, we provide a comprehensive, multidisciplinary overview of the field of emotion analysis in visual media, drawing on insights from psychology, engineering, and the arts.","meta":{"url":"http://arxiv.org/abs/2307.13463v1"},"cats":{"new-dataset":0.1571905579,"dev-research":0.3203297824,"prompt-eng":0.3583021452,"data-quality":0.1802405587,"ml-security":0.0704269946}}
{"text":"We begin by exploring the psychological foundations of emotion and the computational principles that underpin the understanding of emotions from images and videos.","meta":{"url":"http://arxiv.org/abs/2307.13463v1"},"cats":{"new-dataset":0.1512301105,"dev-research":0.2773884239,"prompt-eng":0.338486063,"data-quality":0.1523358044,"ml-security":0.1037641361}}
{"text":"We then review the latest research and systems within the field, accentuating the most promising approaches.","meta":{"url":"http://arxiv.org/abs/2307.13463v1"},"cats":{"new-dataset":0.0800557259,"dev-research":0.2400327252,"prompt-eng":0.4239321911,"data-quality":0.103178083,"ml-security":0.043458615}}
{"text":"We also discuss the current technological challenges and limitations of emotion analysis, underscoring the necessity for continued investigation and innovation.","meta":{"url":"http://arxiv.org/abs/2307.13463v1"},"cats":{"new-dataset":0.0989404756,"dev-research":0.320154043,"prompt-eng":0.3471931871,"data-quality":0.1945338991,"ml-security":0.0998794537}}
{"text":"We contend that this represents a \"Holy Grail\" research problem in computing and delineate pivotal directions for future inquiry.","meta":{"url":"http://arxiv.org/abs/2307.13463v1"},"cats":{"new-dataset":0.0901054118,"dev-research":0.2756622875,"prompt-eng":0.4289647243,"data-quality":0.0990204675,"ml-security":0.1024928483}}
{"text":"Finally, we examine the ethical ramifications of emotion-understanding technologies and contemplate their potential societal impacts.","meta":{"url":"http://arxiv.org/abs/2307.13463v1"},"cats":{"new-dataset":0.1000106498,"dev-research":0.378938703,"prompt-eng":0.3495069218,"data-quality":0.1550285381,"ml-security":0.2123317797}}
{"text":"Overall, this article endeavors to equip readers with a deeper understanding of the domain of emotion analysis in visual media and to inspire further research and development in this captivating and rapidly evolving field.","meta":{"url":"http://arxiv.org/abs/2307.13463v1"},"cats":{"new-dataset":0.1331114096,"dev-research":0.3245447393,"prompt-eng":0.348049861,"data-quality":0.1914893539,"ml-security":0.0822161766}}
{"text":"The main challenges of 3D pose transfer are: 1) Lack of paired training data with different characters performing the same pose; 2) Disentangling pose and shape information from the target mesh; 3) Difficulty in applying to meshes with different topologies.","meta":{"url":"http://arxiv.org/abs/2307.13459v1"},"cats":{"new-dataset":0.0260006944,"dev-research":0.2835417852,"prompt-eng":0.2972416108,"data-quality":0.1003604585,"ml-security":0.1902934065}}
{"text":"We thus propose a novel weakly-supervised keypoint-based framework to overcome these difficulties.","meta":{"url":"http://arxiv.org/abs/2307.13459v1"},"cats":{"new-dataset":0.1142570237,"dev-research":0.2526044014,"prompt-eng":0.4666749547,"data-quality":0.393738866,"ml-security":0.1206055273}}
{"text":"Specifically, we use a topology-agnostic keypoint detector with inverse kinematics to compute transformations between the source and target meshes.","meta":{"url":"http://arxiv.org/abs/2307.13459v1"},"cats":{"new-dataset":0.065064924,"dev-research":0.2418330932,"prompt-eng":0.4300439686,"data-quality":0.1148987911,"ml-security":0.1088570376}}
{"text":"Our method only requires supervision on the keypoints, can be applied to meshes with different topologies and is shape-invariant for the target which allows extraction of pose-only information from the target meshes without transferring shape information.","meta":{"url":"http://arxiv.org/abs/2307.13459v1"},"cats":{"new-dataset":0.038844643,"dev-research":0.2373263559,"prompt-eng":0.403271784,"data-quality":0.1311569936,"ml-security":0.1379961177}}
{"text":"We further design a cycle reconstruction to perform self-supervised pose transfer without the need for ground truth deformed mesh with the same pose and shape as the target and source, respectively.","meta":{"url":"http://arxiv.org/abs/2307.13459v1"},"cats":{"new-dataset":0.1527519929,"dev-research":0.196215382,"prompt-eng":0.4085170213,"data-quality":0.1504550268,"ml-security":0.1853354803}}
{"text":"We evaluate our approach on benchmark human and animal datasets, where we achieve superior performance compared to the state-of-the-art unsupervised approaches and even comparable performance with the fully supervised approaches.","meta":{"url":"http://arxiv.org/abs/2307.13459v1"},"cats":{"new-dataset":0.6800897218,"dev-research":0.2337078685,"prompt-eng":0.419985863,"data-quality":0.2468512699,"ml-security":0.1233717874}}
{"text":"We test on the more challenging Mixamo dataset to verify our approach's ability in handling meshes with different topologies and complex clothes.","meta":{"url":"http://arxiv.org/abs/2307.13459v1"},"cats":{"new-dataset":0.43069251,"dev-research":0.2218102195,"prompt-eng":0.3814598582,"data-quality":0.1578170522,"ml-security":0.0866684569}}
{"text":"Cross-dataset evaluation further shows the strong generalization ability of our approach.","meta":{"url":"http://arxiv.org/abs/2307.13459v1"},"cats":{"new-dataset":0.3455399611,"dev-research":0.2122094548,"prompt-eng":0.3957579372,"data-quality":0.2432212048,"ml-security":0.1266444831}}
{"text":"In this work we study a well-known and challenging problem of Multi-agent Pathfinding, when a set of agents is confined to a graph, each agent is assigned a unique start and goal vertices and the task is to find a set of collision-free paths (one for each agent) such that each agent reaches its respective goal.","meta":{"url":"http://arxiv.org/abs/2307.13453v1"},"cats":{"new-dataset":0.2005059714,"dev-research":0.1970282188,"prompt-eng":0.3722519622,"data-quality":0.0944319195,"ml-security":0.1266841364}}
{"text":"We investigate how to utilize Monte-Carlo Tree Search (MCTS) to solve the problem.","meta":{"url":"http://arxiv.org/abs/2307.13453v1"},"cats":{"new-dataset":0.0653297829,"dev-research":0.1905164507,"prompt-eng":0.460543427,"data-quality":0.1333225036,"ml-security":0.0791282471}}
{"text":"Although MCTS was shown to demonstrate superior performance in a wide range of problems like playing antagonistic games (e.g. Go, Chess etc.), discovering faster matrix multiplication algorithms etc., its application to the problem at hand was not well studied before.","meta":{"url":"http://arxiv.org/abs/2307.13453v1"},"cats":{"new-dataset":0.0117841441,"dev-research":0.3046550144,"prompt-eng":0.3300110578,"data-quality":0.0948570191,"ml-security":0.1278526319}}
{"text":"To this end we introduce an original variant of MCTS, tailored to multi-agent pathfinding.","meta":{"url":"http://arxiv.org/abs/2307.13453v1"},"cats":{"new-dataset":0.1021382297,"dev-research":0.1895468176,"prompt-eng":0.4637202466,"data-quality":0.0968543508,"ml-security":0.0951120756}}
{"text":"The crux of our approach is how the reward, that guides MCTS, is computed.","meta":{"url":"http://arxiv.org/abs/2307.13453v1"},"cats":{"new-dataset":0.0103345161,"dev-research":0.2499209928,"prompt-eng":0.4256360095,"data-quality":0.1024358065,"ml-security":0.0928851605}}
{"text":"Specifically, we use individual paths to assist the agents with the the goal-reaching behavior, while leaving them freedom to get off the track if it is needed to avoid collisions.","meta":{"url":"http://arxiv.org/abs/2307.13453v1"},"cats":{"new-dataset":0.019900015,"dev-research":0.2718853035,"prompt-eng":0.4161427502,"data-quality":0.0589457935,"ml-security":0.1463525889}}
{"text":"We also use a dedicated decomposition technique to reduce the branching factor of the tree search procedure.","meta":{"url":"http://arxiv.org/abs/2307.13453v1"},"cats":{"new-dataset":0.0468856191,"dev-research":0.2285888219,"prompt-eng":0.4636194198,"data-quality":0.1417220489,"ml-security":0.0534242012}}
{"text":"Empirically we show that the suggested method outperforms the baseline planning algorithm that invokes heuristic search, e.g. A*, at each re-planning step.","meta":{"url":"http://arxiv.org/abs/2307.13453v1"},"cats":{"new-dataset":0.0302413356,"dev-research":0.3246345162,"prompt-eng":0.4626702494,"data-quality":0.0859261705,"ml-security":0.043449408}}
{"text":"A key challenge in human-robot collaboration is the non-stationarity created by humans due to changes in their behaviour.","meta":{"url":"http://arxiv.org/abs/2307.13447v1"},"cats":{"new-dataset":0.1247323668,"dev-research":0.3232203368,"prompt-eng":0.4141275406,"data-quality":0.129914507,"ml-security":0.1000260745}}
{"text":"This alters environmental transitions and hinders human-robot collaboration.","meta":{"url":"http://arxiv.org/abs/2307.13447v1"},"cats":{"new-dataset":0.0218509194,"dev-research":0.3737290406,"prompt-eng":0.3812186717,"data-quality":0.0883094161,"ml-security":0.1485573341}}
{"text":"We propose a principled meta-learning framework to explore how robots could better predict human behaviour, and thereby deal with issues of non-stationarity.","meta":{"url":"http://arxiv.org/abs/2307.13447v1"},"cats":{"new-dataset":0.058588573,"dev-research":0.2057098137,"prompt-eng":0.4355189613,"data-quality":0.1629190958,"ml-security":0.2115158601}}
{"text":"On the basis of this framework, we developed Behaviour-Transform (BeTrans).","meta":{"url":"http://arxiv.org/abs/2307.13447v1"},"cats":{"new-dataset":0.1502170241,"dev-research":0.2850649268,"prompt-eng":0.4478077763,"data-quality":0.1153131886,"ml-security":0.1363265011}}
{"text":"BeTrans is a conditional transformer that enables a robot agent to adapt quickly to new human agents with non-stationary behaviours, due to its notable performance with sequential data.","meta":{"url":"http://arxiv.org/abs/2307.13447v1"},"cats":{"new-dataset":0.1592057623,"dev-research":0.246268057,"prompt-eng":0.4333490488,"data-quality":0.0562777439,"ml-security":0.1037875526}}
{"text":"We trained BeTrans on simulated human agents with different systematic biases in collaborative settings.","meta":{"url":"http://arxiv.org/abs/2307.13447v1"},"cats":{"new-dataset":0.1433609991,"dev-research":0.2902950641,"prompt-eng":0.4340123165,"data-quality":0.1209817093,"ml-security":0.203426342}}
{"text":"We used an original customisable environment to show that BeTrans effectively collaborates with simulated human agents and adapts faster to non-stationary simulated human agents than SOTA techniques.","meta":{"url":"http://arxiv.org/abs/2307.13447v1"},"cats":{"new-dataset":0.1954481548,"dev-research":0.2757916069,"prompt-eng":0.4503308982,"data-quality":0.0677916478,"ml-security":0.1117376926}}
{"text":"Multiple low-Earth orbit satellite constellations, aimed at beaming broadband connectivity from space, are currently under active deployment.","meta":{"url":"http://arxiv.org/abs/2307.13441v1"},"cats":{"new-dataset":0.1057232382,"dev-research":0.2062779531,"prompt-eng":0.3849862997,"data-quality":0.1022646826,"ml-security":0.0582720614}}
{"text":"While such space-based Internet is set to augment, globally, today's terrestrial connectivity, and has managed to generate significant hype, it has been largely difficult for the community to measure, quantify, or understand the nuances of these offerings in the absence of a global measurement infrastructure -- the research community has mostly resorted to simulators, emulators, and limited measurements till now.","meta":{"url":"http://arxiv.org/abs/2307.13441v1"},"cats":{"new-dataset":0.2552830343,"dev-research":0.2573855234,"prompt-eng":0.3789801261,"data-quality":0.1332674131,"ml-security":0.100044386}}
{"text":"In this paper, we identify an opportunity to use the social media `lens' to complement such measurements and mine user-centric insights on the evolving ecosystem at scale.","meta":{"url":"http://arxiv.org/abs/2307.13441v1"},"cats":{"new-dataset":0.2423227915,"dev-research":0.2425736695,"prompt-eng":0.3367126376,"data-quality":0.1502115669,"ml-security":0.0868298917}}
{"text":"Network traffic monitoring using IP flows is used to handle the current challenge of analyzing encrypted network communication.","meta":{"url":"http://arxiv.org/abs/2307.13434v1"},"cats":{"new-dataset":0.0537050758,"dev-research":0.2590456749,"prompt-eng":0.3397669542,"data-quality":0.1416095902,"ml-security":0.3485474329}}
{"text":"Nevertheless, the packet aggregation into flow records naturally causes information loss; therefore, this paper proposes a novel flow extension for traffic features based on the time series analysis of the Single Flow Time series, i.e., a time series created by the number of bytes in each packet and its timestamp.","meta":{"url":"http://arxiv.org/abs/2307.13434v1"},"cats":{"new-dataset":0.1132912902,"dev-research":0.2784655705,"prompt-eng":0.3128175469,"data-quality":0.1406514551,"ml-security":0.1951684687}}
{"text":"We propose 69 universal features based on the statistical analysis of data points, time domain analysis, packet distribution within the flow timespan, time series behavior, and frequency domain analysis.","meta":{"url":"http://arxiv.org/abs/2307.13434v1"},"cats":{"new-dataset":0.208327704,"dev-research":0.2608155722,"prompt-eng":0.3839885993,"data-quality":0.1820331376,"ml-security":0.177691599}}
{"text":"We have demonstrated the usability and universality of the proposed feature vector for various network traffic classification tasks using 15 well-known publicly available datasets.","meta":{"url":"http://arxiv.org/abs/2307.13434v1"},"cats":{"new-dataset":0.1906779931,"dev-research":0.2785787138,"prompt-eng":0.3737902345,"data-quality":0.2684976608,"ml-security":0.2898920595}}
{"text":"Our evaluation shows that the novel feature vector achieves classification performance similar or better than related works on both binary and multiclass classification tasks.","meta":{"url":"http://arxiv.org/abs/2307.13434v1"},"cats":{"new-dataset":0.0952639931,"dev-research":0.3124405556,"prompt-eng":0.4369416285,"data-quality":0.2720313794,"ml-security":0.1572539849}}
{"text":"In more than half of the evaluated tasks, the classification performance increased by up to 5\\%.","meta":{"url":"http://arxiv.org/abs/2307.13434v1"},"cats":{"new-dataset":0.0200086429,"dev-research":0.2767691117,"prompt-eng":0.4207476864,"data-quality":0.2647557298,"ml-security":0.1240736153}}
{"text":"Metaverse aims for building a fully immersive virtual shared space, where the users are able to engage in various activities.","meta":{"url":"http://arxiv.org/abs/2307.13429v1"},"cats":{"new-dataset":0.1616067036,"dev-research":0.3095080953,"prompt-eng":0.3939978753,"data-quality":0.0748856837,"ml-security":0.0785084617}}
{"text":"To successfully deploy the service for each user, the Metaverse service provider and network service provider generally localise the user first and then support the communication between the base station (BS) and the user.","meta":{"url":"http://arxiv.org/abs/2307.13429v1"},"cats":{"new-dataset":0.0532802595,"dev-research":0.2529694384,"prompt-eng":0.4253752473,"data-quality":0.1527464139,"ml-security":0.0597318917}}
{"text":"A reconfigurable intelligent surface (RIS) is capable of creating a reflected link between the BS and the user to enhance line-of-sight.","meta":{"url":"http://arxiv.org/abs/2307.13429v1"},"cats":{"new-dataset":0.0441013944,"dev-research":0.2447156896,"prompt-eng":0.4438973867,"data-quality":0.0796736033,"ml-security":0.0677633441}}
{"text":"Furthermore, the new key performance indicators (KPIs) in Metaverse, such as its energy-consumption-dependent total service cost and transmission latency, are often overlooked in ultra-reliable low latency communication (URLLC) designs, which have to be carefully considered in next-generation URLLC (xURLLC) regimes.","meta":{"url":"http://arxiv.org/abs/2307.13429v1"},"cats":{"new-dataset":0.0296234261,"dev-research":0.2575181455,"prompt-eng":0.4246448923,"data-quality":0.1433611635,"ml-security":0.0672873865}}
{"text":"In this paper, our design objective is to jointly optimise the transmit power, the RIS phase shifts, and the decoding error probability to simultaneously minimise the total service cost and transmission latency and approach the Pareto Front (PF).","meta":{"url":"http://arxiv.org/abs/2307.13429v1"},"cats":{"new-dataset":0.0198527246,"dev-research":0.1881981895,"prompt-eng":0.4597125561,"data-quality":0.1317880466,"ml-security":0.0939472534}}
{"text":"We conceive a twin-stage central controller, which aims for localising the users first and then supports the communication between the BS and users.","meta":{"url":"http://arxiv.org/abs/2307.13429v1"},"cats":{"new-dataset":0.0827666374,"dev-research":0.3113139489,"prompt-eng":0.5210293597,"data-quality":0.101730715,"ml-security":0.1056850658}}
{"text":"In the first stage, we localise the Metaverse users, where the stochastic gradient descent (SGD) algorithm is invoked for accurate user localisation.","meta":{"url":"http://arxiv.org/abs/2307.13429v1"},"cats":{"new-dataset":0.0595944103,"dev-research":0.2741795243,"prompt-eng":0.4616613278,"data-quality":0.2283353333,"ml-security":0.169433027}}
{"text":"In the second stage, a meta-learning-based position-dependent multi-objective soft actor and critic (MO-SAC) algorithm is proposed to approach the PF between the total service cost and transmission latency and to further optimise the latency-dependent reliability.","meta":{"url":"http://arxiv.org/abs/2307.13429v1"},"cats":{"new-dataset":0.0141408404,"dev-research":0.2096588123,"prompt-eng":0.3594678951,"data-quality":0.1818540711,"ml-security":0.1230206661}}
{"text":"Our numerical results demonstrate that ...","meta":{"url":"http://arxiv.org/abs/2307.13429v1"},"cats":{"new-dataset":0.0299833596,"dev-research":0.2198362555,"prompt-eng":0.3748198425,"data-quality":0.1581508168,"ml-security":0.0874903541}}
{"text":"This paper describes an adaptation of the Local Interpretable Model-Agnostic Explanations (LIME) AI method to operate under a biometric verification setting.","meta":{"url":"http://arxiv.org/abs/2307.13428v1"},"cats":{"new-dataset":0.0251804065,"dev-research":0.3295881108,"prompt-eng":0.4519600384,"data-quality":0.2614344112,"ml-security":0.2260764111}}
{"text":"LIME was initially proposed for networks with the same output classes used for training, and it employs the softmax probability to determine which regions of the image contribute the most to classification.","meta":{"url":"http://arxiv.org/abs/2307.13428v1"},"cats":{"new-dataset":0.0620067058,"dev-research":0.2601767005,"prompt-eng":0.3351517446,"data-quality":0.2790687039,"ml-security":0.1961704686}}
{"text":"However, in a verification setting, the classes to be recognized have not been seen during training.","meta":{"url":"http://arxiv.org/abs/2307.13428v1"},"cats":{"new-dataset":0.0554013422,"dev-research":0.187256265,"prompt-eng":0.3440634279,"data-quality":0.3469045301,"ml-security":0.1875619894}}
{"text":"In addition, instead of using the softmax output, face descriptors are usually obtained from a layer before the classification layer.","meta":{"url":"http://arxiv.org/abs/2307.13428v1"},"cats":{"new-dataset":0.0486149395,"dev-research":0.2790102805,"prompt-eng":0.3522397444,"data-quality":0.2213858985,"ml-security":0.2072779877}}
{"text":"The model is adapted to achieve explainability via cosine similarity between feature vectors of perturbated versions of the input image.","meta":{"url":"http://arxiv.org/abs/2307.13428v1"},"cats":{"new-dataset":0.0410447087,"dev-research":0.2633601831,"prompt-eng":0.44682674,"data-quality":0.2322360376,"ml-security":0.1346605451}}
{"text":"The method is showcased for face biometrics with two CNN models based on MobileNetv2 and ResNet50.","meta":{"url":"http://arxiv.org/abs/2307.13428v1"},"cats":{"new-dataset":0.1294000234,"dev-research":0.2220228587,"prompt-eng":0.3584673184,"data-quality":0.1429226448,"ml-security":0.2005295792}}
{"text":"Situation awareness is a crucial cognitive skill that enables individuals to perceive, comprehend, and project the current state of their environment accurately.","meta":{"url":"http://arxiv.org/abs/2307.13427v1"},"cats":{"new-dataset":0.0931953141,"dev-research":0.4261210182,"prompt-eng":0.4044029608,"data-quality":0.1140707518,"ml-security":0.1371150387}}
{"text":"It involves being conscious of relevant information, understanding its meaning, and using that understanding to make well-informed decisions.","meta":{"url":"http://arxiv.org/abs/2307.13427v1"},"cats":{"new-dataset":0.0104246993,"dev-research":0.4010259394,"prompt-eng":0.3452133313,"data-quality":0.1087661356,"ml-security":0.0892586307}}
{"text":"Awareness systems often need to integrate new knowledge and adapt to changing environments.","meta":{"url":"http://arxiv.org/abs/2307.13427v1"},"cats":{"new-dataset":0.0433714737,"dev-research":0.3578527957,"prompt-eng":0.4034192853,"data-quality":0.1308669214,"ml-security":0.1275997843}}
{"text":"Ontology reasoning facilitates knowledge integration and evolution, allowing for seamless updates and expansions of the ontology.","meta":{"url":"http://arxiv.org/abs/2307.13427v1"},"cats":{"new-dataset":0.0702039395,"dev-research":0.3913694401,"prompt-eng":0.3890656758,"data-quality":0.1338297681,"ml-security":0.0600498792}}
{"text":"With the consideration of above, we are providing a quick review on semantic information retrieval and ontology engineering to understand the emerging challenges and future research.","meta":{"url":"http://arxiv.org/abs/2307.13427v1"},"cats":{"new-dataset":0.0881213732,"dev-research":0.3155842712,"prompt-eng":0.4162469412,"data-quality":0.2601815648,"ml-security":0.0442591497}}
{"text":"In the review we have found that the ontology reasoning addresses the limitations of traditional systems by providing a formal, flexible, and scalable framework for knowledge representation, reasoning, and inference.","meta":{"url":"http://arxiv.org/abs/2307.13427v1"},"cats":{"new-dataset":0.0979881721,"dev-research":0.3392271304,"prompt-eng":0.4202433568,"data-quality":0.1128818039,"ml-security":0.0663914609}}
{"text":"In this short paper, we consider a form of higher-order rewriting with a call-by-value evaluation strategy so as to model call-by-value programs.","meta":{"url":"http://arxiv.org/abs/2307.13426v1"},"cats":{"new-dataset":0.0275733142,"dev-research":0.4027200932,"prompt-eng":0.4584873452,"data-quality":0.132292241,"ml-security":0.1214474107}}
{"text":"We briefly present a cost-size semantics to call-by-value rewriting: a class of algebraic interpretations that map terms to tuples that bound both the reductions' cost and the size of normal forms.","meta":{"url":"http://arxiv.org/abs/2307.13426v1"},"cats":{"new-dataset":0.0206734783,"dev-research":0.4075355876,"prompt-eng":0.412792912,"data-quality":0.1796055516,"ml-security":0.1244928146}}
{"text":"Encoding-decoding CNNs play a central role in data-driven noise reduction and can be found within numerous deep-learning algorithms.","meta":{"url":"http://arxiv.org/abs/2307.13425v1"},"cats":{"new-dataset":0.100387208,"dev-research":0.2511730672,"prompt-eng":0.334262563,"data-quality":0.3118474678,"ml-security":0.2710619964}}
{"text":"However, the development of these CNN architectures is often done in ad-hoc fashion and theoretical underpinnings for important design choices is generally lacking.","meta":{"url":"http://arxiv.org/abs/2307.13425v1"},"cats":{"new-dataset":0.0600754295,"dev-research":0.267981433,"prompt-eng":0.3545303076,"data-quality":0.1224763209,"ml-security":0.1514658199}}
{"text":"Up to this moment there are different existing relevant works that strive to explain the internal operation of these CNNs.","meta":{"url":"http://arxiv.org/abs/2307.13425v1"},"cats":{"new-dataset":0.0683089848,"dev-research":0.2568103383,"prompt-eng":0.3136312316,"data-quality":0.2241078461,"ml-security":0.1515810399}}
{"text":"Still, these ideas are either scattered and/or may require significant expertise to be accessible for a bigger audience.","meta":{"url":"http://arxiv.org/abs/2307.13425v1"},"cats":{"new-dataset":0.0392224529,"dev-research":0.2929390827,"prompt-eng":0.3421928291,"data-quality":0.137309702,"ml-security":0.1026366988}}
{"text":"In order to open up this exciting field, this article builds intuition on the theory of deep convolutional framelets and explains diverse ED CNN architectures in a unified theoretical framework.","meta":{"url":"http://arxiv.org/abs/2307.13425v1"},"cats":{"new-dataset":0.2066534573,"dev-research":0.2014210958,"prompt-eng":0.3356148864,"data-quality":0.211866492,"ml-security":0.1833471295}}
{"text":"By connecting basic principles from signal processing to the field of deep learning, this self-contained material offers significant guidance for designing robust and efficient novel CNN architectures.","meta":{"url":"http://arxiv.org/abs/2307.13425v1"},"cats":{"new-dataset":0.1082647234,"dev-research":0.2335713763,"prompt-eng":0.3485349587,"data-quality":0.1908975024,"ml-security":0.2326447979}}
{"text":"In this paper, we conduct a holistic exploration of the Universal Decompositional Semantic (UDS) Parsing.","meta":{"url":"http://arxiv.org/abs/2307.13424v1"},"cats":{"new-dataset":0.1854084342,"dev-research":0.2982605242,"prompt-eng":0.4002255221,"data-quality":0.2768575985,"ml-security":0.0855714525}}
{"text":"We first introduce a cascade model for UDS parsing that decomposes the complex parsing task into semantically appropriate subtasks.","meta":{"url":"http://arxiv.org/abs/2307.13424v1"},"cats":{"new-dataset":0.1365399961,"dev-research":0.3489820712,"prompt-eng":0.456830691,"data-quality":0.1941741796,"ml-security":0.1045224143}}
{"text":"Our approach outperforms the prior models, while significantly reducing inference time.","meta":{"url":"http://arxiv.org/abs/2307.13424v1"},"cats":{"new-dataset":0.0217821992,"dev-research":0.2071134626,"prompt-eng":0.4421379255,"data-quality":0.1934916763,"ml-security":0.1138379617}}
{"text":"We also incorporate syntactic information and further optimized the architecture.","meta":{"url":"http://arxiv.org/abs/2307.13424v1"},"cats":{"new-dataset":0.1127360007,"dev-research":0.3352422233,"prompt-eng":0.449560826,"data-quality":0.2185806669,"ml-security":0.0633023993}}
{"text":"Besides, different ways for data augmentation are explored, which further improve the UDS Parsing.","meta":{"url":"http://arxiv.org/abs/2307.13424v1"},"cats":{"new-dataset":0.2003935828,"dev-research":0.3523189221,"prompt-eng":0.4368900795,"data-quality":0.2980587592,"ml-security":0.1118856785}}
{"text":"Lastly, we conduct experiments to investigate the efficacy of ChatGPT in handling the UDS task, revealing that it excels in attribute parsing but struggles in relation parsing, and using ChatGPT for data augmentation yields suboptimal results.","meta":{"url":"http://arxiv.org/abs/2307.13424v1"},"cats":{"new-dataset":0.3041673966,"dev-research":0.3763403413,"prompt-eng":0.4573387608,"data-quality":0.3131888382,"ml-security":0.118614301}}
{"text":"Our code is available at https://github.com/hexuandeng/HExp4UDS.","meta":{"url":"http://arxiv.org/abs/2307.13424v1"},"cats":{"new-dataset":0.4974868954,"dev-research":0.2742245336,"prompt-eng":0.4738995122,"data-quality":0.1439914524,"ml-security":0.0615882371}}
{"text":"Self-supervised speech representations (SSSRs) have been successfully applied to a number of speech-processing tasks, e.g. as feature extractor for speech quality (SQ) prediction, which is, in turn, relevant for assessment and training speech enhancement systems for users with normal or impaired hearing.","meta":{"url":"http://arxiv.org/abs/2307.13423v1"},"cats":{"new-dataset":0.0616090204,"dev-research":0.274772798,"prompt-eng":0.3963638162,"data-quality":0.2986835215,"ml-security":0.142945662}}
{"text":"However, exact knowledge of why and how quality-related information is encoded well in such representations remains poorly understood.","meta":{"url":"http://arxiv.org/abs/2307.13423v1"},"cats":{"new-dataset":0.026679285,"dev-research":0.3522898373,"prompt-eng":0.3508907373,"data-quality":0.4805043612,"ml-security":0.1846330926}}
{"text":"In this work, techniques for non-intrusive prediction of SQ ratings are extended to the prediction of intelligibility for hearing-impaired users.","meta":{"url":"http://arxiv.org/abs/2307.13423v1"},"cats":{"new-dataset":0.025758764,"dev-research":0.2900023809,"prompt-eng":0.4285813842,"data-quality":0.2533831429,"ml-security":0.2313455045}}
{"text":"It is found that self-supervised representations are useful as input features to non-intrusive prediction models, achieving competitive performance to more complex systems.","meta":{"url":"http://arxiv.org/abs/2307.13423v1"},"cats":{"new-dataset":0.01487698,"dev-research":0.2960109258,"prompt-eng":0.4399685359,"data-quality":0.1836628953,"ml-security":0.40496978}}
{"text":"A detailed analysis of the performance depending on Clarity Prediction Challenge 1 listeners and enhancement systems indicates that more data might be needed to allow generalisation to unknown systems and","meta":{"url":"http://arxiv.org/abs/2307.13423v1"},"cats":{"new-dataset":0.0886770275,"dev-research":0.3884425413,"prompt-eng":0.46398364,"data-quality":0.3852764248,"ml-security":0.2275867687}}
{"text":"(hearing-impaired) individuals","meta":{"url":"http://arxiv.org/abs/2307.13423v1"},"cats":{"new-dataset":0.032120677,"dev-research":0.2678044955,"prompt-eng":0.3449745233,"data-quality":0.1933165025,"ml-security":0.1042843192}}
{"text":"Attention models are typically learned by optimizing one of three standard loss functions that are variously called -- soft attention, hard attention, and latent variable marginal likelihood (LVML) attention.","meta":{"url":"http://arxiv.org/abs/2307.13421v1"},"cats":{"new-dataset":0.0182110417,"dev-research":0.1932092379,"prompt-eng":0.3954924861,"data-quality":0.2009844895,"ml-security":0.152890603}}
{"text":"All three paradigms are motivated by the same goal of finding two models -- a `focus' model that `selects' the right \\textit{segment} of the input and a `classification' model that processes the selected segment into the target label.","meta":{"url":"http://arxiv.org/abs/2307.13421v1"},"cats":{"new-dataset":0.0154534679,"dev-research":0.2533465042,"prompt-eng":0.3829615925,"data-quality":0.1733158089,"ml-security":0.0762771561}}
{"text":"However, they differ significantly in the way the selected segments are aggregated, resulting in distinct dynamics and final results.","meta":{"url":"http://arxiv.org/abs/2307.13421v1"},"cats":{"new-dataset":0.0264433529,"dev-research":0.1998906385,"prompt-eng":0.351308071,"data-quality":0.1286235636,"ml-security":0.0311892603}}
{"text":"We observe a unique signature of models learned using these paradigms and explain this as a consequence of the evolution of the classification model under gradient descent when the focus model is fixed.","meta":{"url":"http://arxiv.org/abs/2307.13421v1"},"cats":{"new-dataset":0.0280778508,"dev-research":0.1902404088,"prompt-eng":0.3826177512,"data-quality":0.2791899481,"ml-security":0.3087819485}}
{"text":"We also analyze these paradigms in a simple setting and derive closed-form expressions for the parameter trajectory under gradient flow.","meta":{"url":"http://arxiv.org/abs/2307.13421v1"},"cats":{"new-dataset":0.0271869145,"dev-research":0.2242554848,"prompt-eng":0.3832509407,"data-quality":0.1262830865,"ml-security":0.1447705011}}
{"text":"With the soft attention loss, the focus model improves quickly at initialization and splutters later on.","meta":{"url":"http://arxiv.org/abs/2307.13421v1"},"cats":{"new-dataset":0.0198953005,"dev-research":0.2780667237,"prompt-eng":0.4294810538,"data-quality":0.1463702087,"ml-security":0.0989070525}}
{"text":"On the other hand, hard attention loss behaves in the opposite fashion.","meta":{"url":"http://arxiv.org/abs/2307.13421v1"},"cats":{"new-dataset":0.0014023116,"dev-research":0.2679050487,"prompt-eng":0.3415101162,"data-quality":0.2081989518,"ml-security":0.2103963835}}
{"text":"Based on our observations, we propose a simple hybrid approach that combines the advantages of the different loss functions and demonstrates it on a collection of semi-synthetic and real-world datasets","meta":{"url":"http://arxiv.org/abs/2307.13421v1"},"cats":{"new-dataset":0.5226405999,"dev-research":0.174788751,"prompt-eng":0.3788214832,"data-quality":0.2191650906,"ml-security":0.148836902}}
{"text":"Learning enabled components (LECs), while critical for decision making in autonomous vehicles (AVs), are likely to make incorrect decisions when presented with samples outside of their training distributions.","meta":{"url":"http://arxiv.org/abs/2307.13419v1"},"cats":{"new-dataset":0.0251671443,"dev-research":0.2856221712,"prompt-eng":0.3818815133,"data-quality":0.299445063,"ml-security":0.2723516659}}
{"text":"Out-of-distribution (OOD) detectors have been proposed to detect such samples, thereby acting as a safety monitor, however, both OOD detectors and LECs require heavy utilization of embedded hardware typically found in AVs.","meta":{"url":"http://arxiv.org/abs/2307.13419v1"},"cats":{"new-dataset":0.2125200424,"dev-research":0.2502770333,"prompt-eng":0.4207441499,"data-quality":0.2681864624,"ml-security":0.274057364}}
{"text":"For both components, there is a tradeoff between non-functional and functional performance, and both impact a vehicle's safety.","meta":{"url":"http://arxiv.org/abs/2307.13419v1"},"cats":{"new-dataset":0.0036392763,"dev-research":0.3355299451,"prompt-eng":0.315695268,"data-quality":0.1032489711,"ml-security":0.1026122432}}
{"text":"For instance, giving an OOD detector a longer response time can increase its accuracy at the expense of the LEC.","meta":{"url":"http://arxiv.org/abs/2307.13419v1"},"cats":{"new-dataset":0.0021982529,"dev-research":0.2752653399,"prompt-eng":0.4030153774,"data-quality":0.1750671305,"ml-security":0.1891601971}}
{"text":"We consider an LEC with binary output like an autonomous emergency braking system (AEBS) and use risk, the combination of severity and occurrence of a failure, to model the effect of both components' design parameters on each other's functional and non-functional performance, as well as their impact on system safety.","meta":{"url":"http://arxiv.org/abs/2307.13419v1"},"cats":{"new-dataset":0.0352469115,"dev-research":0.2884413716,"prompt-eng":0.480016709,"data-quality":0.1647588969,"ml-security":0.2998730931}}
{"text":"We formulate a co-design methodology that uses this risk model to find the design parameters for an OOD detector and LEC that decrease risk below that of the baseline system and demonstrate it on a vision based AEBS.","meta":{"url":"http://arxiv.org/abs/2307.13419v1"},"cats":{"new-dataset":0.0679386552,"dev-research":0.2624774277,"prompt-eng":0.4743698128,"data-quality":0.1859247577,"ml-security":0.2569616069}}
{"text":"Using our methodology, we achieve a 42.3% risk reduction while maintaining equivalent resource utilization.","meta":{"url":"http://arxiv.org/abs/2307.13419v1"},"cats":{"new-dataset":0.0545111359,"dev-research":0.2790316925,"prompt-eng":0.4169417102,"data-quality":0.1112411086,"ml-security":0.1497236109}}
{"text":"Ambiguity is ubiquitous in natural language.","meta":{"url":"http://arxiv.org/abs/2307.13417v1"},"cats":{"new-dataset":0.0083035343,"dev-research":0.389264676,"prompt-eng":0.3502804423,"data-quality":0.4081472679,"ml-security":0.1015703494}}
{"text":"Resolving ambiguous meanings is especially important in information retrieval tasks.","meta":{"url":"http://arxiv.org/abs/2307.13417v1"},"cats":{"new-dataset":0.0096758736,"dev-research":0.3452482514,"prompt-eng":0.4061656329,"data-quality":0.3527807696,"ml-security":0.0492910253}}
{"text":"While word embeddings carry semantic information, they fail to handle ambiguity well.","meta":{"url":"http://arxiv.org/abs/2307.13417v1"},"cats":{"new-dataset":0.012295666,"dev-research":0.2924972525,"prompt-eng":0.3425972053,"data-quality":0.5894921634,"ml-security":0.1060938723}}
{"text":"Transformer models have been shown to handle word ambiguity for complex queries, but they cannot be used to identify ambiguous words, e.g. for a 1-word query.","meta":{"url":"http://arxiv.org/abs/2307.13417v1"},"cats":{"new-dataset":0.014249284,"dev-research":0.1899223625,"prompt-eng":0.4269421691,"data-quality":0.3175046139,"ml-security":0.0870119353}}
{"text":"Furthermore, training these models is costly in terms of time, hardware resources, and training data, prohibiting their use in specialized environments with sensitive data.","meta":{"url":"http://arxiv.org/abs/2307.13417v1"},"cats":{"new-dataset":0.0436731425,"dev-research":0.2472934994,"prompt-eng":0.3447333809,"data-quality":0.1443420396,"ml-security":0.4798706616}}
{"text":"Word embeddings can be trained using moderate hardware resources.","meta":{"url":"http://arxiv.org/abs/2307.13417v1"},"cats":{"new-dataset":0.0882909133,"dev-research":0.2963687203,"prompt-eng":0.4148094823,"data-quality":0.2632666809,"ml-security":0.1149654101}}
{"text":"This paper shows that applying DBSCAN clustering to the latent space can identify ambiguous words and evaluate their level of ambiguity.","meta":{"url":"http://arxiv.org/abs/2307.13417v1"},"cats":{"new-dataset":0.0609695515,"dev-research":0.2842847069,"prompt-eng":0.4206401008,"data-quality":0.5083436764,"ml-security":0.0753948206}}
{"text":"An automatic DBSCAN parameter selection leads to high-quality clusters, which are semantically coherent and correspond well to the perceived meanings of a given word.","meta":{"url":"http://arxiv.org/abs/2307.13417v1"},"cats":{"new-dataset":0.0479845701,"dev-research":0.3520128145,"prompt-eng":0.4791215966,"data-quality":0.3365212726,"ml-security":0.0620232721}}
{"text":"The unprecedented accuracy of convolutional neural networks (CNNs) across a broad range of AI tasks has led to their widespread deployment in mobile and embedded settings.","meta":{"url":"http://arxiv.org/abs/2307.13412v1"},"cats":{"new-dataset":0.1899611194,"dev-research":0.2460353419,"prompt-eng":0.3478606921,"data-quality":0.161692118,"ml-security":0.2286133308}}
{"text":"In a pursuit for high-performance and energy-efficient inference, significant research effort has been invested in the design of FPGA-based CNN accelerators.","meta":{"url":"http://arxiv.org/abs/2307.13412v1"},"cats":{"new-dataset":0.0705254692,"dev-research":0.2308130356,"prompt-eng":0.3561420537,"data-quality":0.0732814227,"ml-security":0.1145468042}}
{"text":"In this context, single computation engines constitute a popular approach to support diverse CNN modes without the overhead of fabric reconfiguration.","meta":{"url":"http://arxiv.org/abs/2307.13412v1"},"cats":{"new-dataset":0.0627223763,"dev-research":0.2470713854,"prompt-eng":0.3459844267,"data-quality":0.1100733114,"ml-security":0.1120583249}}
{"text":"Nevertheless, this flexibility often comes with significantly degraded performance on memory-bound layers and resource underutilisation due to the suboptimal mapping of certain layers on the engine's fixed configuration.","meta":{"url":"http://arxiv.org/abs/2307.13412v1"},"cats":{"new-dataset":0.0220970534,"dev-research":0.338417817,"prompt-eng":0.3890466886,"data-quality":0.1801000592,"ml-security":0.1748246291}}
{"text":"In this work, we investigate the implications in terms of CNN engine design for a class of models that introduce a pre-convolution stage to decompress the weights at run time.","meta":{"url":"http://arxiv.org/abs/2307.13412v1"},"cats":{"new-dataset":0.0873280454,"dev-research":0.1765109008,"prompt-eng":0.3930164606,"data-quality":0.1487790064,"ml-security":0.2992455534}}
{"text":"We refer to these approaches as on-the-fly.","meta":{"url":"http://arxiv.org/abs/2307.13412v1"},"cats":{"new-dataset":0.0159919172,"dev-research":0.3312427408,"prompt-eng":0.3507566936,"data-quality":0.1047453941,"ml-security":0.139549606}}
{"text":"This paper presents unzipFPGA, a novel CNN inference system that counteracts the limitations of existing CNN engines.","meta":{"url":"http://arxiv.org/abs/2307.13412v1"},"cats":{"new-dataset":0.2530725102,"dev-research":0.2099894296,"prompt-eng":0.3555774653,"data-quality":0.1782069223,"ml-security":0.22892252}}
{"text":"The proposed framework comprises a novel CNN hardware architecture that introduces a weights generator module that enables the on-chip on-the-fly generation of weights, alleviating the negative impact of limited bandwidth on memory-bound layers.","meta":{"url":"http://arxiv.org/abs/2307.13412v1"},"cats":{"new-dataset":0.1930395588,"dev-research":0.2369961532,"prompt-eng":0.3627457625,"data-quality":0.1292594152,"ml-security":0.2025545104}}
{"text":"We further enhance unzipFPGA with an automated hardware-aware methodology that tailors the weights generation mechanism to the target CNN-device pair, leading to an improved accuracy-performance balance.","meta":{"url":"http://arxiv.org/abs/2307.13412v1"},"cats":{"new-dataset":0.1889967369,"dev-research":0.2623983204,"prompt-eng":0.3965433367,"data-quality":0.138915888,"ml-security":0.1253016745}}
{"text":"Finally, we introduce an input selective processing element (PE) design that balances the load between PEs in suboptimally mapped layers.","meta":{"url":"http://arxiv.org/abs/2307.13412v1"},"cats":{"new-dataset":0.0340895261,"dev-research":0.2607739148,"prompt-eng":0.4676193587,"data-quality":0.1203402806,"ml-security":0.100310843}}
{"text":"The proposed framework yields hardware designs that achieve an average of 2.57x performance efficiency gain over highly optimised GPU designs for the same power constraints and up to 3.94x higher performance density over a diverse range of state-of-the-art FPGA-based CNN accelerators.","meta":{"url":"http://arxiv.org/abs/2307.13412v1"},"cats":{"new-dataset":0.1088429722,"dev-research":0.238931097,"prompt-eng":0.35213275,"data-quality":0.078240209,"ml-security":0.1387416797}}
{"text":"This research article analyses and demonstrates the hidden implications for fairness of seemingly neutral data coupled with powerful technology, such as machine learning (ML), using Open Banking as an example.","meta":{"url":"http://arxiv.org/abs/2307.13408v1"},"cats":{"new-dataset":0.0534628268,"dev-research":0.2506202107,"prompt-eng":0.3098800944,"data-quality":0.2557693324,"ml-security":0.5552744272}}
{"text":"Open Banking has ignited a revolution in financial services, opening new opportunities for customer acquisition, management, retention, and risk assessment.","meta":{"url":"http://arxiv.org/abs/2307.13408v1"},"cats":{"new-dataset":0.0242308119,"dev-research":0.2291618736,"prompt-eng":0.3212799744,"data-quality":0.0947439154,"ml-security":0.1439932447}}
{"text":"However, the granularity of transaction data holds potential for harm where unnoticed proxies for sensitive and prohibited characteristics may lead to indirect discrimination.","meta":{"url":"http://arxiv.org/abs/2307.13408v1"},"cats":{"new-dataset":0.0192915666,"dev-research":0.2634206946,"prompt-eng":0.3661475553,"data-quality":0.2939411968,"ml-security":0.5852775866}}
{"text":"Against this backdrop, we investigate the dimensions of financial vulnerability (FV), a global concern resulting from COVID-19 and rising inflation.","meta":{"url":"http://arxiv.org/abs/2307.13408v1"},"cats":{"new-dataset":0.3113919285,"dev-research":0.2365642328,"prompt-eng":0.36405459,"data-quality":0.1416348339,"ml-security":0.3744405552}}
{"text":"Specifically, we look to understand the behavioral elements leading up to FV and its impact on at-risk, disadvantaged groups through the lens of fair interpretation.","meta":{"url":"http://arxiv.org/abs/2307.13408v1"},"cats":{"new-dataset":0.0683229395,"dev-research":0.2914143621,"prompt-eng":0.4229363013,"data-quality":0.1375578038,"ml-security":0.217725335}}
{"text":"Using a unique dataset from a UK FinTech lender, we demonstrate the power of fine-grained transaction data while simultaneously cautioning its safe usage.","meta":{"url":"http://arxiv.org/abs/2307.13408v1"},"cats":{"new-dataset":0.5597623396,"dev-research":0.2289162092,"prompt-eng":0.377556837,"data-quality":0.2309899662,"ml-security":0.3000574119}}
{"text":"Three ML classifiers are compared in predicting the likelihood of FV, and groups exhibiting different magnitudes and forms of FV are identified via clustering to highlight the effects of feature combination.","meta":{"url":"http://arxiv.org/abs/2307.13408v1"},"cats":{"new-dataset":0.0428586663,"dev-research":0.2745264135,"prompt-eng":0.4196526666,"data-quality":0.3015578843,"ml-security":0.2518085025}}
{"text":"Our results indicate that engineered features of financial behavior can be predictive of omitted personal information, particularly sensitive or protected characteristics, shedding light on the hidden dangers of Open Banking data.","meta":{"url":"http://arxiv.org/abs/2307.13408v1"},"cats":{"new-dataset":0.0720834095,"dev-research":0.2912216826,"prompt-eng":0.4025622529,"data-quality":0.2231497597,"ml-security":0.5747245627}}
{"text":"We discuss the implications and conclude fairness via unawareness is ineffective in this new technological environment.","meta":{"url":"http://arxiv.org/abs/2307.13408v1"},"cats":{"new-dataset":0.0168644621,"dev-research":0.3014965476,"prompt-eng":0.3593489007,"data-quality":0.3331125976,"ml-security":0.4550832395}}
{"text":"It is a well-known fact that current AI-based language technology -- language models, machine translation systems, multilingual dictionaries and corpora -- focuses on the world's 2-3% most widely spoken languages.","meta":{"url":"http://arxiv.org/abs/2307.13405v1"},"cats":{"new-dataset":0.2767369647,"dev-research":0.2665500174,"prompt-eng":0.3264322679,"data-quality":0.2378510008,"ml-security":0.1201860321}}
{"text":"Recent research efforts have attempted to expand the coverage of AI technology to `under-resourced languages.'","meta":{"url":"http://arxiv.org/abs/2307.13405v1"},"cats":{"new-dataset":0.0480418209,"dev-research":0.32542084,"prompt-eng":0.3314000142,"data-quality":0.1946043898,"ml-security":0.1477937987}}
{"text":"The goal of our paper is to bring attention to a phenomenon that we call linguistic bias: multilingual language processing systems often exhibit a hardwired, yet usually involuntary and hidden representational preference towards certain languages.","meta":{"url":"http://arxiv.org/abs/2307.13405v1"},"cats":{"new-dataset":0.0224260419,"dev-research":0.2698226912,"prompt-eng":0.4144261285,"data-quality":0.3976169719,"ml-security":0.1720649632}}
{"text":"Linguistic bias is manifested in uneven per-language performance even in the case of similar test conditions.","meta":{"url":"http://arxiv.org/abs/2307.13405v1"},"cats":{"new-dataset":0.0038892884,"dev-research":0.2962675486,"prompt-eng":0.3783959171,"data-quality":0.4411283436,"ml-security":0.1082531044}}
{"text":"We show that biased technology is often the result of research and development methodologies that do not do justice to the complexity of the languages being represented, and that can even become ethically problematic as they disregard valuable aspects of diversity as well as the needs of the language communities themselves.","meta":{"url":"http://arxiv.org/abs/2307.13405v1"},"cats":{"new-dataset":0.073109267,"dev-research":0.3691095822,"prompt-eng":0.3023057183,"data-quality":0.3739075701,"ml-security":0.2411275433}}
{"text":"As our attempt at building diversity-aware language resources, we present a new initiative that aims at reducing linguistic bias through both technological design and methodology, based on an eye-level collaboration with local communities.","meta":{"url":"http://arxiv.org/abs/2307.13405v1"},"cats":{"new-dataset":0.2128475658,"dev-research":0.343532951,"prompt-eng":0.3444150417,"data-quality":0.2594424431,"ml-security":0.0620129371}}
{"text":"DAG (directed acyclic graph) tasks are widely used to model parallel real-time workload.","meta":{"url":"http://arxiv.org/abs/2307.13401v1"},"cats":{"new-dataset":0.0708238435,"dev-research":0.3284954001,"prompt-eng":0.3808627603,"data-quality":0.0691529556,"ml-security":0.0772773291}}
{"text":"The real-time performance of a DAG task not only depends on its total workload, but also its graph structure.","meta":{"url":"http://arxiv.org/abs/2307.13401v1"},"cats":{"new-dataset":0.0277168116,"dev-research":0.3763257432,"prompt-eng":0.3654212209,"data-quality":0.0706242807,"ml-security":0.0538571209}}
{"text":"Intuitively, with the same total workload, a DAG task with looser precedence constraints tends to have better real-time performance in terms of worst-case response time.","meta":{"url":"http://arxiv.org/abs/2307.13401v1"},"cats":{"new-dataset":0.0040074281,"dev-research":0.3984029668,"prompt-eng":0.3987651549,"data-quality":0.0924347284,"ml-security":0.1258509076}}
{"text":"However, this paper shows that actually we can shorten the worst-case response time of a DAG task by carefully adding new edges and constructing longer paths.","meta":{"url":"http://arxiv.org/abs/2307.13401v1"},"cats":{"new-dataset":0.0285098017,"dev-research":0.3580126584,"prompt-eng":0.4754621213,"data-quality":0.1375433345,"ml-security":0.1656632392}}
{"text":"We develop techniques based on the state-of-the-art DAG response time analysis techniques to properly add new edges so that the worst-case response time bound guaranteed by formal analysis can be significantly reduced.","meta":{"url":"http://arxiv.org/abs/2307.13401v1"},"cats":{"new-dataset":0.0527223532,"dev-research":0.3113302254,"prompt-eng":0.4603027404,"data-quality":0.1645150006,"ml-security":0.1863449382}}
{"text":"Experiments under different parameter settings demonstrate the effectiveness of the proposed techniques.","meta":{"url":"http://arxiv.org/abs/2307.13401v1"},"cats":{"new-dataset":0.0031889052,"dev-research":0.2487200848,"prompt-eng":0.4957293761,"data-quality":0.1503773863,"ml-security":0.1019641323}}
{"text":"Today, many cities seek to transition to more sustainable transportation systems.","meta":{"url":"http://arxiv.org/abs/2307.13397v1"},"cats":{"new-dataset":0.024878612,"dev-research":0.2245070914,"prompt-eng":0.31478703,"data-quality":0.0510667768,"ml-security":0.0688488876}}
{"text":"Cycling is critical in this transition for shorter trips, including first-and-last-mile links to transit.","meta":{"url":"http://arxiv.org/abs/2307.13397v1"},"cats":{"new-dataset":0.010980154,"dev-research":0.2618903479,"prompt-eng":0.3479865325,"data-quality":0.0771565184,"ml-security":0.0512299156}}
{"text":"Yet, if individuals perceive cycling as unsafe, they will not cycle and choose other transportation modes.","meta":{"url":"http://arxiv.org/abs/2307.13397v1"},"cats":{"new-dataset":0.0062719841,"dev-research":0.2791526507,"prompt-eng":0.3466526917,"data-quality":0.1059651499,"ml-security":0.3412253369}}
{"text":"This study presents a novel approach to identifying how the perception of cycling safety can be analyzed and understood and the impact of the built environment and cycling contexts on such perceptions.","meta":{"url":"http://arxiv.org/abs/2307.13397v1"},"cats":{"new-dataset":0.1691227025,"dev-research":0.4612409255,"prompt-eng":0.3630382787,"data-quality":0.1511710266,"ml-security":0.1954666815}}
{"text":"We base our work on other perception studies and pairwise comparisons, using real-world images to survey respondents.","meta":{"url":"http://arxiv.org/abs/2307.13397v1"},"cats":{"new-dataset":0.1074825111,"dev-research":0.2135081584,"prompt-eng":0.4186312324,"data-quality":0.1641400157,"ml-security":0.0645592271}}
{"text":"We repeatedly show respondents two road environments and ask them to select the one they perceive as safer for cycling.","meta":{"url":"http://arxiv.org/abs/2307.13397v1"},"cats":{"new-dataset":0.0532602882,"dev-research":0.294411062,"prompt-eng":0.4423676195,"data-quality":0.1008564685,"ml-security":0.1978782042}}
{"text":"We compare several methods capable of rating cycling environments from pairwise comparisons and classify cycling environments perceived as safe or unsafe.","meta":{"url":"http://arxiv.org/abs/2307.13397v1"},"cats":{"new-dataset":0.0637084142,"dev-research":0.3341858573,"prompt-eng":0.4468118148,"data-quality":0.1968475099,"ml-security":0.3140894043}}
{"text":"Urban planning can use this score to improve interventions' effectiveness and improve cycling promotion campaigns.","meta":{"url":"http://arxiv.org/abs/2307.13397v1"},"cats":{"new-dataset":0.0251445146,"dev-research":0.3394607282,"prompt-eng":0.3856438516,"data-quality":0.0841707579,"ml-security":0.0819103858}}
{"text":"Furthermore, this approach facilitates the continuous assessment of changing cycling environments, allows for a short-term evaluation of measures, and is efficiently deployed in different locations or contexts.","meta":{"url":"http://arxiv.org/abs/2307.13397v1"},"cats":{"new-dataset":0.0891057936,"dev-research":0.3747208205,"prompt-eng":0.4491005974,"data-quality":0.1107777195,"ml-security":0.0547535207}}
{"text":"This paper discusses the problem of efficiently solving parity games where player Odd has to obey an additional 'strong transition fairness constraint' on its vertices -- given that a player Odd vertex $v$ is visited infinitely often, a particular subset of the outgoing edges (called live edges) of $v$ has to be taken infinitely often.","meta":{"url":"http://arxiv.org/abs/2307.13396v1"},"cats":{"new-dataset":0.0348058289,"dev-research":0.2075500756,"prompt-eng":0.3689361198,"data-quality":0.1318249518,"ml-security":0.2855318803}}
{"text":"Such games, which we call 'Odd-fair parity games', naturally arise from abstractions of cyber-physical systems for planning and control.   ","meta":{"url":"http://arxiv.org/abs/2307.13396v1"},"cats":{"new-dataset":0.1156213213,"dev-research":0.2802255922,"prompt-eng":0.4039618303,"data-quality":0.0709437906,"ml-security":0.2900108772}}
{"text":"In this paper, we present a new Zielonka-type algorithm for solving Odd-fair parity games.","meta":{"url":"http://arxiv.org/abs/2307.13396v1"},"cats":{"new-dataset":0.1453613822,"dev-research":0.1618777156,"prompt-eng":0.3596913055,"data-quality":0.1411278361,"ml-security":0.1633626748}}
{"text":"This algorithm not only shares 'the same worst-case time complexity' as Zielonka's algorithm for (normal) parity games but also preserves the algorithmic advantage Zielonka's algorithm possesses over other parity solvers with exponential time complexity.   ","meta":{"url":"http://arxiv.org/abs/2307.13396v1"},"cats":{"new-dataset":0.0137414922,"dev-research":0.2237660083,"prompt-eng":0.301992654,"data-quality":0.0843844118,"ml-security":0.1784789529}}
{"text":"We additionally introduce a formalization of Odd player winning strategies in such games, which were unexplored previous to this work.","meta":{"url":"http://arxiv.org/abs/2307.13396v1"},"cats":{"new-dataset":0.0602667214,"dev-research":0.2280509644,"prompt-eng":0.4301959515,"data-quality":0.1554015735,"ml-security":0.2229748271}}
{"text":"This formalization serves dual purposes: firstly, it enables us to prove our Zielonka-type algorithm; secondly, it stands as a noteworthy contribution in its own right, augmenting our understanding of additional fairness assumptions in two-player games.","meta":{"url":"http://arxiv.org/abs/2307.13396v1"},"cats":{"new-dataset":0.0339078386,"dev-research":0.2367551524,"prompt-eng":0.3801083476,"data-quality":0.1825680548,"ml-security":0.2111310125}}
{"text":"The consumption of podcast media has been increasing rapidly.","meta":{"url":"http://arxiv.org/abs/2307.13394v1"},"cats":{"new-dataset":0.0735744324,"dev-research":0.2246801481,"prompt-eng":0.2905789288,"data-quality":0.1320029315,"ml-security":0.1083145128}}
{"text":"Due to the lengthy nature of podcast episodes, users often carefully select which ones to listen to.","meta":{"url":"http://arxiv.org/abs/2307.13394v1"},"cats":{"new-dataset":0.0182810181,"dev-research":0.2477817742,"prompt-eng":0.3869557057,"data-quality":0.136502743,"ml-security":0.1034602532}}
{"text":"Although episode descriptions aid users by providing a summary of the entire podcast, they do not provide a topic-by-topic breakdown.","meta":{"url":"http://arxiv.org/abs/2307.13394v1"},"cats":{"new-dataset":0.0646893647,"dev-research":0.2161887815,"prompt-eng":0.3777151233,"data-quality":0.1912130856,"ml-security":0.0624304697}}
{"text":"This study explores the combined application of topic segmentation and text summarisation methods to investigate how podcast episode comprehension can be improved.","meta":{"url":"http://arxiv.org/abs/2307.13394v1"},"cats":{"new-dataset":0.0493556084,"dev-research":0.3053226894,"prompt-eng":0.3728197516,"data-quality":0.2801593095,"ml-security":0.0575407776}}
{"text":"We have sampled 10 episodes from Spotify's English-Language Podcast Dataset and employed TextTiling and TextSplit to segment them.","meta":{"url":"http://arxiv.org/abs/2307.13394v1"},"cats":{"new-dataset":0.7966862173,"dev-research":0.2529922146,"prompt-eng":0.3614031475,"data-quality":0.33899913,"ml-security":0.0641473322}}
{"text":"Moreover, three text summarisation models, namely T5, BART, and Pegasus, were applied to provide a very short title for each segment.","meta":{"url":"http://arxiv.org/abs/2307.13394v1"},"cats":{"new-dataset":0.1466600442,"dev-research":0.197441404,"prompt-eng":0.3767845871,"data-quality":0.2504492803,"ml-security":0.0506907852}}
{"text":"The segmentation part was evaluated using our annotated sample with the $P_k$ and WindowDiff ($WD$) metrics.","meta":{"url":"http://arxiv.org/abs/2307.13394v1"},"cats":{"new-dataset":0.3521256535,"dev-research":0.1769336585,"prompt-eng":0.4383280296,"data-quality":0.2577556299,"ml-security":0.0508190841}}
{"text":"A survey was also rolled out ($N=25$) to assess the quality of the generated summaries.","meta":{"url":"http://arxiv.org/abs/2307.13394v1"},"cats":{"new-dataset":0.2696096667,"dev-research":0.2747879169,"prompt-eng":0.3901739837,"data-quality":0.2615504114,"ml-security":0.0445816876}}
{"text":"The TextSplit algorithm achieved the lowest mean for both evaluation metrics ($\\bar{P_k}=0.41$ and $\\bar{WD}=0.41$), while the T5 model produced the best summaries, achieving a relevancy score only $8\\%$ less to the one achieved by the human-written titles.","meta":{"url":"http://arxiv.org/abs/2307.13394v1"},"cats":{"new-dataset":0.0287710011,"dev-research":0.2450338292,"prompt-eng":0.3660710766,"data-quality":0.2345054251,"ml-security":0.0425329827}}
{"text":"Counterfactual Explanations (CEs) are an important tool in Algorithmic Recourse for addressing two questions:","meta":{"url":"http://arxiv.org/abs/2307.13390v1"},"cats":{"new-dataset":0.0338962156,"dev-research":0.4766261373,"prompt-eng":0.4348550837,"data-quality":0.3652763012,"ml-security":0.3381701327}}
{"text":"1.","meta":{"url":"http://arxiv.org/abs/2307.13390v1"},"cats":{"new-dataset":0.2998246933,"dev-research":0.1942544764,"prompt-eng":0.4155360939,"data-quality":0.1614195889,"ml-security":0.0895766279}}
{"text":"What are the crucial factors that led to an automated prediction/decision?","meta":{"url":"http://arxiv.org/abs/2307.13390v1"},"cats":{"new-dataset":0.0076647737,"dev-research":0.3744395291,"prompt-eng":0.4094359576,"data-quality":0.1826404302,"ml-security":0.2007787543}}
{"text":"2. How can these factors be changed to achieve a more favorable outcome from a user's perspective?","meta":{"url":"http://arxiv.org/abs/2307.13390v1"},"cats":{"new-dataset":0.0052747764,"dev-research":0.3085127052,"prompt-eng":0.4018140189,"data-quality":0.1025556418,"ml-security":0.1124189732}}
{"text":"Thus, guiding the user's interaction with AI systems by proposing easy-to-understand explanations and easy-to-attain feasible changes is essential for the trustworthy adoption and long-term acceptance of AI systems.","meta":{"url":"http://arxiv.org/abs/2307.13390v1"},"cats":{"new-dataset":0.0196380219,"dev-research":0.4290512339,"prompt-eng":0.4322430415,"data-quality":0.1526014668,"ml-security":0.2508956697}}
{"text":"In the literature, various methods have been proposed to generate CEs, and different quality measures have been suggested to evaluate these methods.","meta":{"url":"http://arxiv.org/abs/2307.13390v1"},"cats":{"new-dataset":0.079986134,"dev-research":0.3587797937,"prompt-eng":0.4809536638,"data-quality":0.2525562638,"ml-security":0.0556111171}}
{"text":"However, the generation of CEs is usually computationally expensive, and the resulting suggestions are unrealistic and thus non-actionable.","meta":{"url":"http://arxiv.org/abs/2307.13390v1"},"cats":{"new-dataset":0.0157159596,"dev-research":0.3552583771,"prompt-eng":0.4054593978,"data-quality":0.1176659696,"ml-security":0.1408079546}}
{"text":"In this paper, we introduce a new method to generate CEs for a pre-trained binary classifier by first shaping the latent space of an autoencoder to be a mixture of Gaussian distributions.","meta":{"url":"http://arxiv.org/abs/2307.13390v1"},"cats":{"new-dataset":0.1146136602,"dev-research":0.2435560978,"prompt-eng":0.4967615769,"data-quality":0.3009416041,"ml-security":0.1881064054}}
{"text":"CEs are then generated in latent space by linear interpolation between the query sample and the centroid of the target class.","meta":{"url":"http://arxiv.org/abs/2307.13390v1"},"cats":{"new-dataset":0.1051134831,"dev-research":0.2186968913,"prompt-eng":0.431369297,"data-quality":0.2057114347,"ml-security":0.0950583021}}
{"text":"We show that our method maintains the characteristics of the input sample during the counterfactual search.","meta":{"url":"http://arxiv.org/abs/2307.13390v1"},"cats":{"new-dataset":0.0732661803,"dev-research":0.2527833225,"prompt-eng":0.5064629669,"data-quality":0.3704174454,"ml-security":0.1741909242}}
{"text":"In various experiments, we show that the proposed method is competitive based on different quality measures on image and tabular datasets -- efficiently returns results that are closer to the original data manifold compared to three state-of-the-art methods, which are essential for realistic high-dimensional machine learning applications.","meta":{"url":"http://arxiv.org/abs/2307.13390v1"},"cats":{"new-dataset":0.1245295869,"dev-research":0.2211179856,"prompt-eng":0.3287793559,"data-quality":0.2188344443,"ml-security":0.1490974254}}
{"text":"Social coding platforms have revolutionized collaboration in software development, leading to using software bots for streamlining operations.","meta":{"url":"http://arxiv.org/abs/2307.13386v1"},"cats":{"new-dataset":0.1059722226,"dev-research":0.5695501326,"prompt-eng":0.3951173539,"data-quality":0.1665924676,"ml-security":0.1743222978}}
{"text":"However, The presence of open-source software (OSS) bots gives rise to problems including impersonation, spamming, bias, and security risks.","meta":{"url":"http://arxiv.org/abs/2307.13386v1"},"cats":{"new-dataset":0.0533476267,"dev-research":0.4003086862,"prompt-eng":0.3676171953,"data-quality":0.2245356882,"ml-security":0.6125677274}}
{"text":"Identifying bot accounts and behavior is a challenging task in the OSS project.","meta":{"url":"http://arxiv.org/abs/2307.13386v1"},"cats":{"new-dataset":0.1002456136,"dev-research":0.3233647506,"prompt-eng":0.4526878093,"data-quality":0.2620721603,"ml-security":0.3763944921}}
{"text":"This research aims to investigate bots' behavior in open-source software projects and identify bot accounts with maximum possible accuracy.","meta":{"url":"http://arxiv.org/abs/2307.13386v1"},"cats":{"new-dataset":0.1216648723,"dev-research":0.412396461,"prompt-eng":0.3975533834,"data-quality":0.2823519463,"ml-security":0.3484606285}}
{"text":"Our team gathered a dataset of 19,779 accounts that meet standardized criteria to enable future research on bots in open-source projects.","meta":{"url":"http://arxiv.org/abs/2307.13386v1"},"cats":{"new-dataset":0.7484996865,"dev-research":0.2685944754,"prompt-eng":0.376213274,"data-quality":0.1346637093,"ml-security":0.1991491224}}
{"text":"We follow a rigorous workflow to ensure that the data we collect is accurate, generalizable, scalable, and up-to-date.","meta":{"url":"http://arxiv.org/abs/2307.13386v1"},"cats":{"new-dataset":0.2834768333,"dev-research":0.3131258632,"prompt-eng":0.4275437172,"data-quality":0.1922496533,"ml-security":0.1116447579}}
{"text":"We've identified four types of bot accounts in open-source software projects by analyzing their behavior across 17 features in 5 dimensions.","meta":{"url":"http://arxiv.org/abs/2307.13386v1"},"cats":{"new-dataset":0.1782557134,"dev-research":0.3753612055,"prompt-eng":0.380776793,"data-quality":0.1908040483,"ml-security":0.2753407121}}
{"text":"Our team created BotHawk, a highly effective model for detecting bots in open-source software projects.","meta":{"url":"http://arxiv.org/abs/2307.13386v1"},"cats":{"new-dataset":0.2855528766,"dev-research":0.4046378878,"prompt-eng":0.4199875634,"data-quality":0.2310969392,"ml-security":0.3417483785}}
{"text":"It outperforms other models, achieving an AUC of 0.947 and an F1-score of 0.89.","meta":{"url":"http://arxiv.org/abs/2307.13386v1"},"cats":{"new-dataset":0.0135356056,"dev-research":0.2120494021,"prompt-eng":0.4104566962,"data-quality":0.2162673123,"ml-security":0.0989624297}}
{"text":"BotHawk can detect a wider variety of bots, including CI/CD and scanning bots.","meta":{"url":"http://arxiv.org/abs/2307.13386v1"},"cats":{"new-dataset":0.0586726905,"dev-research":0.30339635,"prompt-eng":0.403192782,"data-quality":0.2265019412,"ml-security":0.1985997295}}
{"text":"Furthermore, we find that the number of followers, number of repositories, and tags contain the most relevant features to identify the account type.","meta":{"url":"http://arxiv.org/abs/2307.13386v1"},"cats":{"new-dataset":0.1202515665,"dev-research":0.2574298962,"prompt-eng":0.424045855,"data-quality":0.1954539724,"ml-security":0.117976787}}
{"text":"Code coverage is a widely used metric for quantifying the extent to which program elements, such as statements or branches, are executed during testing.","meta":{"url":"http://arxiv.org/abs/2307.13383v1"},"cats":{"new-dataset":0.0814459452,"dev-research":0.5304726472,"prompt-eng":0.4229619656,"data-quality":0.2499301544,"ml-security":0.1725890135}}
{"text":"Calculating code coverage is resource-intensive, requiring code building and execution with additional overhead for the instrumentation.","meta":{"url":"http://arxiv.org/abs/2307.13383v1"},"cats":{"new-dataset":0.0730751396,"dev-research":0.4644674385,"prompt-eng":0.3762049315,"data-quality":0.1206546685,"ml-security":0.1448674201}}
{"text":"Furthermore, computing coverage of any snippet of code requires the whole program context.","meta":{"url":"http://arxiv.org/abs/2307.13383v1"},"cats":{"new-dataset":0.0257725572,"dev-research":0.4705991261,"prompt-eng":0.3933273057,"data-quality":0.2166035332,"ml-security":0.2428891542}}
{"text":"Using Machine Learning to amortize this expensive process could lower the cost of code coverage by requiring only the source code context, and the task of code coverage prediction can be a novel benchmark for judging the ability of models to understand code.","meta":{"url":"http://arxiv.org/abs/2307.13383v1"},"cats":{"new-dataset":0.0316979302,"dev-research":0.4893111886,"prompt-eng":0.3878379654,"data-quality":0.2332214157,"ml-security":0.4513126548}}
{"text":"We propose a novel benchmark task called Code Coverage Prediction for Large Language Models (LLMs).","meta":{"url":"http://arxiv.org/abs/2307.13383v1"},"cats":{"new-dataset":0.2319215925,"dev-research":0.3180298717,"prompt-eng":0.4464812141,"data-quality":0.3500291665,"ml-security":0.2647346436}}
{"text":"We formalize this task to evaluate the capability of LLMs in understanding code execution by determining which lines of a method are executed by a given test case and inputs.","meta":{"url":"http://arxiv.org/abs/2307.13383v1"},"cats":{"new-dataset":0.0326012337,"dev-research":0.4635444494,"prompt-eng":0.5070897665,"data-quality":0.2209664156,"ml-security":0.1797339574}}
{"text":"We curate and release a dataset we call COVERAGEEVAL by executing tests and code from the HumanEval dataset and collecting code coverage information.","meta":{"url":"http://arxiv.org/abs/2307.13383v1"},"cats":{"new-dataset":0.8901379473,"dev-research":0.3365295784,"prompt-eng":0.4492128565,"data-quality":0.2094420671,"ml-security":0.2193999651}}
{"text":"We report the performance of four state-of-the-art LLMs used for code-related tasks, including OpenAI's GPT-4 and GPT-3.5-Turbo, Google's BARD, and Anthropic's Claude, on the Code Coverage Prediction task.","meta":{"url":"http://arxiv.org/abs/2307.13383v1"},"cats":{"new-dataset":0.2858746222,"dev-research":0.3488424172,"prompt-eng":0.4596398603,"data-quality":0.1940793586,"ml-security":0.1464930565}}
{"text":"Finally, we argue that code coverage as a metric and pre-training data source are valuable for overall LLM performance on software engineering tasks.","meta":{"url":"http://arxiv.org/abs/2307.13383v1"},"cats":{"new-dataset":0.1463379206,"dev-research":0.4646163794,"prompt-eng":0.430075863,"data-quality":0.2202963589,"ml-security":0.2094632258}}
{"text":"We present Scaff-PD, a fast and communication-efficient algorithm for distributionally robust federated learning.","meta":{"url":"http://arxiv.org/abs/2307.13381v1"},"cats":{"new-dataset":0.2055523063,"dev-research":0.193349981,"prompt-eng":0.3994697072,"data-quality":0.2418696253,"ml-security":0.2883706762}}
{"text":"Our approach improves fairness by optimizing a family of distributionally robust objectives tailored to heterogeneous clients.","meta":{"url":"http://arxiv.org/abs/2307.13381v1"},"cats":{"new-dataset":0.0238657808,"dev-research":0.2382275913,"prompt-eng":0.4255388984,"data-quality":0.2125550098,"ml-security":0.3410860224}}
{"text":"We leverage the special structure of these objectives, and design an accelerated primal dual (APD) algorithm which uses bias corrected local steps (as in Scaffold) to achieve significant gains in communication efficiency and convergence speed.","meta":{"url":"http://arxiv.org/abs/2307.13381v1"},"cats":{"new-dataset":0.0313544172,"dev-research":0.245736903,"prompt-eng":0.4283273487,"data-quality":0.126072389,"ml-security":0.1274131719}}
{"text":"We evaluate Scaff-PD on several benchmark datasets and demonstrate its effectiveness in improving fairness and robustness while maintaining competitive accuracy.","meta":{"url":"http://arxiv.org/abs/2307.13381v1"},"cats":{"new-dataset":0.4575928371,"dev-research":0.2562981762,"prompt-eng":0.3868188269,"data-quality":0.2858124204,"ml-security":0.2845289013}}
{"text":"Our results suggest that Scaff-PD is a promising approach for federated learning in resource-constrained and heterogeneous settings.","meta":{"url":"http://arxiv.org/abs/2307.13381v1"},"cats":{"new-dataset":0.150937927,"dev-research":0.2055183844,"prompt-eng":0.3716509771,"data-quality":0.1481829558,"ml-security":0.1437141634}}
{"text":"Acknowledgments in scientific papers may give an insight into aspects of the scientific community, such as reward systems, collaboration patterns, and hidden research trends.","meta":{"url":"http://arxiv.org/abs/2307.13377v1"},"cats":{"new-dataset":0.0578213786,"dev-research":0.2857830253,"prompt-eng":0.3534314754,"data-quality":0.2786195815,"ml-security":0.0889145696}}
{"text":"The aim of the paper is to evaluate the performance of different embedding models for the task of automatic extraction and classification of acknowledged entities from the acknowledgment text in scientific papers.","meta":{"url":"http://arxiv.org/abs/2307.13377v1"},"cats":{"new-dataset":0.0765983761,"dev-research":0.2077298008,"prompt-eng":0.4231998307,"data-quality":0.4835103664,"ml-security":0.0583030665}}
{"text":"We trained and implemented a named entity recognition (NER) task using the Flair NLP framework.","meta":{"url":"http://arxiv.org/abs/2307.13377v1"},"cats":{"new-dataset":0.1297740569,"dev-research":0.2970842542,"prompt-eng":0.4534837103,"data-quality":0.3473397227,"ml-security":0.0837216257}}
{"text":"The training was conducted using three default Flair NER models with four differently-sized corpora and different versions of the Flair NLP framework.","meta":{"url":"http://arxiv.org/abs/2307.13377v1"},"cats":{"new-dataset":0.2105933389,"dev-research":0.231905142,"prompt-eng":0.3639674217,"data-quality":0.2458552566,"ml-security":0.066416387}}
{"text":"The Flair Embeddings model trained on the medium corpus with the latest FLAIR version showed the best accuracy of 0.79.","meta":{"url":"http://arxiv.org/abs/2307.13377v1"},"cats":{"new-dataset":0.2996191812,"dev-research":0.2485432854,"prompt-eng":0.4237211621,"data-quality":0.3870715011,"ml-security":0.0750152179}}
{"text":"Expanding the size of a training corpus from very small to medium size massively increased the accuracy of all training algorithms, but further expansion of the training corpus did not bring further improvement.","meta":{"url":"http://arxiv.org/abs/2307.13377v1"},"cats":{"new-dataset":0.011977241,"dev-research":0.2199151915,"prompt-eng":0.2634220218,"data-quality":0.3111525184,"ml-security":0.1237390202}}
{"text":"Moreover, the performance of the model slightly deteriorated.","meta":{"url":"http://arxiv.org/abs/2307.13377v1"},"cats":{"new-dataset":0.0096890999,"dev-research":0.1870310972,"prompt-eng":0.3907921803,"data-quality":0.2428980633,"ml-security":0.0886938113}}
{"text":"Our model is able to recognize six entity types: funding agency, grant number, individuals, university, corporation, and miscellaneous.","meta":{"url":"http://arxiv.org/abs/2307.13377v1"},"cats":{"new-dataset":0.1608610161,"dev-research":0.1458177998,"prompt-eng":0.4072337433,"data-quality":0.1575763391,"ml-security":0.0610975224}}
{"text":"The model works more precisely for some entity types than for others; thus, individuals and grant numbers showed a very good F1-Score over 0.9.","meta":{"url":"http://arxiv.org/abs/2307.13377v1"},"cats":{"new-dataset":0.0206403266,"dev-research":0.2002780269,"prompt-eng":0.3940735933,"data-quality":0.179956714,"ml-security":0.088809512}}
{"text":"Most of the previous works on acknowledgment analysis were limited by the manual evaluation of data and therefore by the amount of processed data.","meta":{"url":"http://arxiv.org/abs/2307.13377v1"},"cats":{"new-dataset":0.3205811633,"dev-research":0.28168685,"prompt-eng":0.3250171298,"data-quality":0.2427396478,"ml-security":0.1088026093}}
{"text":"This model can be applied for the comprehensive analysis of acknowledgment texts and may potentially make a great contribution to the field of automated acknowledgment analysis.","meta":{"url":"http://arxiv.org/abs/2307.13377v1"},"cats":{"new-dataset":0.1415745259,"dev-research":0.2673353566,"prompt-eng":0.4490172502,"data-quality":0.3645843708,"ml-security":0.0922725762}}
{"text":"In reinforcement learning (RL), rewards of states are typically considered additive, and following the Markov assumption, they are $\\textit{independent}$ of states visited previously.","meta":{"url":"http://arxiv.org/abs/2307.13372v1"},"cats":{"new-dataset":0.0142055617,"dev-research":0.1968268816,"prompt-eng":0.3127227919,"data-quality":0.0798699312,"ml-security":0.1420658471}}
{"text":"In many important applications, such as coverage control, experiment design and informative path planning, rewards naturally have diminishing returns, i.e., their value decreases in light of similar states visited previously.","meta":{"url":"http://arxiv.org/abs/2307.13372v1"},"cats":{"new-dataset":0.0053637492,"dev-research":0.2545056001,"prompt-eng":0.4012871859,"data-quality":0.0967521814,"ml-security":0.2096228068}}
{"text":"To tackle this, we propose $\\textit{submodular RL}$ (SubRL), a paradigm which seeks to optimize more general, non-additive (and history-dependent) rewards modelled via submodular set functions which capture diminishing returns.","meta":{"url":"http://arxiv.org/abs/2307.13372v1"},"cats":{"new-dataset":0.0419080543,"dev-research":0.2301688495,"prompt-eng":0.4039324684,"data-quality":0.1177749615,"ml-security":0.1685331182}}
{"text":"Unfortunately, in general, even in tabular settings, we show that the resulting optimization problem is hard to approximate.","meta":{"url":"http://arxiv.org/abs/2307.13372v1"},"cats":{"new-dataset":0.0407243986,"dev-research":0.2483191366,"prompt-eng":0.4087725399,"data-quality":0.1741030436,"ml-security":0.0665759303}}
{"text":"On the other hand, motivated by the success of greedy algorithms in classical submodular optimization, we propose SubPO, a simple policy gradient-based algorithm for SubRL that handles non-additive rewards by greedily maximizing marginal gains.","meta":{"url":"http://arxiv.org/abs/2307.13372v1"},"cats":{"new-dataset":0.0229949654,"dev-research":0.2095372004,"prompt-eng":0.3378825475,"data-quality":0.0892014127,"ml-security":0.1686750115}}
{"text":"Indeed, under some assumptions on the underlying Markov Decision Process (MDP), SubPO recovers optimal constant factor approximations of submodular bandits.","meta":{"url":"http://arxiv.org/abs/2307.13372v1"},"cats":{"new-dataset":0.0148161678,"dev-research":0.1885359168,"prompt-eng":0.3757447111,"data-quality":0.1386700662,"ml-security":0.1694605822}}
{"text":"Moreover, we derive a natural policy gradient approach for locally optimizing SubRL instances even in large state- and action- spaces.","meta":{"url":"http://arxiv.org/abs/2307.13372v1"},"cats":{"new-dataset":0.0308292139,"dev-research":0.1966220952,"prompt-eng":0.3827457115,"data-quality":0.1089666529,"ml-security":0.1887898072}}
{"text":"We showcase the versatility of our approach by applying SubPO to several applications, such as biodiversity monitoring, Bayesian experiment design, informative path planning, and coverage maximization.","meta":{"url":"http://arxiv.org/abs/2307.13372v1"},"cats":{"new-dataset":0.1439549663,"dev-research":0.227449709,"prompt-eng":0.4813286531,"data-quality":0.1084635894,"ml-security":0.0636800732}}
{"text":"Our results demonstrate sample efficiency, as well as scalability to high-dimensional state-action spaces.","meta":{"url":"http://arxiv.org/abs/2307.13372v1"},"cats":{"new-dataset":0.0500503419,"dev-research":0.1833965081,"prompt-eng":0.4253313872,"data-quality":0.0767511519,"ml-security":0.1267325395}}
{"text":"We study Bayesian optimization (BO) in high-dimensional and non-stationary scenarios.","meta":{"url":"http://arxiv.org/abs/2307.13371v1"},"cats":{"new-dataset":0.0209125062,"dev-research":0.1911632466,"prompt-eng":0.4199983232,"data-quality":0.0847611166,"ml-security":0.1736492606}}
{"text":"Existing algorithms for such scenarios typically require extensive hyperparameter tuning, which limits their practical effectiveness.","meta":{"url":"http://arxiv.org/abs/2307.13371v1"},"cats":{"new-dataset":0.0069796442,"dev-research":0.239229352,"prompt-eng":0.5045280854,"data-quality":0.1742757687,"ml-security":0.1651491631}}
{"text":"We propose a framework, called BALLET, which adaptively filters for a high-confidence region of interest (ROI) as a superlevel-set of a nonparametric probabilistic model such as a Gaussian process (GP).","meta":{"url":"http://arxiv.org/abs/2307.13371v1"},"cats":{"new-dataset":0.0329054556,"dev-research":0.1518028237,"prompt-eng":0.4607068405,"data-quality":0.1124327622,"ml-security":0.1228132555}}
{"text":"Our approach is easy to tune, and is able to focus on local region of the optimization space that can be tackled by existing BO methods.","meta":{"url":"http://arxiv.org/abs/2307.13371v1"},"cats":{"new-dataset":0.0380633297,"dev-research":0.2820390373,"prompt-eng":0.4178136178,"data-quality":0.1755935956,"ml-security":0.0851284216}}
{"text":"The key idea is to use two probabilistic models: a coarse GP to identify the ROI, and a localized GP for optimization within the ROI.","meta":{"url":"http://arxiv.org/abs/2307.13371v1"},"cats":{"new-dataset":0.0205188843,"dev-research":0.182286787,"prompt-eng":0.4330274803,"data-quality":0.1219487226,"ml-security":0.0909144151}}
{"text":"We show theoretically that BALLET can efficiently shrink the search space, and can exhibit a tighter regret bound than standard BO without ROI filtering.","meta":{"url":"http://arxiv.org/abs/2307.13371v1"},"cats":{"new-dataset":0.0222523769,"dev-research":0.1804504704,"prompt-eng":0.3881363606,"data-quality":0.0995158871,"ml-security":0.1622488897}}
{"text":"We demonstrate empirically the effectiveness of BALLET on both synthetic and real-world optimization tasks.","meta":{"url":"http://arxiv.org/abs/2307.13371v1"},"cats":{"new-dataset":0.0315890695,"dev-research":0.3458850037,"prompt-eng":0.4076487718,"data-quality":0.0821545639,"ml-security":0.1255148053}}
{"text":"We introduce a novel speaker model \\textsc{Kefa} for navigation instruction generation.","meta":{"url":"http://arxiv.org/abs/2307.13368v1"},"cats":{"new-dataset":0.2680436149,"dev-research":0.2598443853,"prompt-eng":0.4524914315,"data-quality":0.1696992049,"ml-security":0.0613190106}}
{"text":"The existing speaker models in Vision-and-Language Navigation suffer from the large domain gap of vision features between different environments and insufficient temporal grounding capability.","meta":{"url":"http://arxiv.org/abs/2307.13368v1"},"cats":{"new-dataset":0.0973652043,"dev-research":0.1923092318,"prompt-eng":0.391198069,"data-quality":0.1652699026,"ml-security":0.0793183032}}
{"text":"To address the challenges, we propose a Knowledge Refinement Module to enhance the feature representation with external knowledge facts, and an Adaptive Temporal Alignment method to enforce fine-grained alignment between the generated instructions and the observation sequences.","meta":{"url":"http://arxiv.org/abs/2307.13368v1"},"cats":{"new-dataset":0.3327625838,"dev-research":0.4615424953,"prompt-eng":0.4487661334,"data-quality":0.2792317801,"ml-security":0.0783426623}}
{"text":"Moreover, we propose a new metric SPICE-D for navigation instruction evaluation, which is aware of the correctness of direction phrases.","meta":{"url":"http://arxiv.org/abs/2307.13368v1"},"cats":{"new-dataset":0.1160095662,"dev-research":0.3865442337,"prompt-eng":0.4849311149,"data-quality":0.241139585,"ml-security":0.0681267075}}
{"text":"The experimental results on R2R and UrbanWalk datasets show that the proposed KEFA speaker achieves state-of-the-art instruction generation performance for both indoor and outdoor scenes.","meta":{"url":"http://arxiv.org/abs/2307.13368v1"},"cats":{"new-dataset":0.4582996026,"dev-research":0.2967953355,"prompt-eng":0.3934945828,"data-quality":0.1737751292,"ml-security":0.0581698707}}
{"text":"Recently, with the emergence of numerous Large Language Models (LLMs), the implementation of AI has entered a new era.","meta":{"url":"http://arxiv.org/abs/2307.13365v1"},"cats":{"new-dataset":0.0640698665,"dev-research":0.2139857316,"prompt-eng":0.36220526,"data-quality":0.1598224088,"ml-security":0.1215668355}}
{"text":"Irrespective of these models' own capacity and structure, there is a growing demand for LLMs to possess enhanced comprehension of longer and more complex contexts with relatively smaller sizes.","meta":{"url":"http://arxiv.org/abs/2307.13365v1"},"cats":{"new-dataset":0.0194329823,"dev-research":0.1947070776,"prompt-eng":0.3626770797,"data-quality":0.0834719896,"ml-security":0.111823807}}
{"text":"Models often encounter an upper limit when processing sequences of sentences that extend beyond their comprehension capacity and result in off-topic or even chaotic responses.","meta":{"url":"http://arxiv.org/abs/2307.13365v1"},"cats":{"new-dataset":0.0315014531,"dev-research":0.1731611489,"prompt-eng":0.3716620903,"data-quality":0.1557980393,"ml-security":0.1609521792}}
{"text":"While several recent works attempt to address this issue in various ways, they rarely focus on \"why models are unable to compensate or strengthen their capabilities on their own\".","meta":{"url":"http://arxiv.org/abs/2307.13365v1"},"cats":{"new-dataset":0.005385518,"dev-research":0.2332592404,"prompt-eng":0.3754996377,"data-quality":0.1703101088,"ml-security":0.1958504439}}
{"text":"In this paper, we thoroughly investigate the nature of information transfer within LLMs and propose a novel technique called Attention Transition.","meta":{"url":"http://arxiv.org/abs/2307.13365v1"},"cats":{"new-dataset":0.017343442,"dev-research":0.2187106075,"prompt-eng":0.4577236347,"data-quality":0.1537664226,"ml-security":0.130780639}}
{"text":"This technique empowers models to achieve longer and better context comprehension with minimal additional training or impact on generation fluency.","meta":{"url":"http://arxiv.org/abs/2307.13365v1"},"cats":{"new-dataset":0.0184222022,"dev-research":0.3352895401,"prompt-eng":0.3967769513,"data-quality":0.1580200025,"ml-security":0.1167500139}}
{"text":"Our experiments are conducted in XSum and achieve substantial improvement compared with the original generation results.","meta":{"url":"http://arxiv.org/abs/2307.13365v1"},"cats":{"new-dataset":0.1389786611,"dev-research":0.3036109631,"prompt-eng":0.4477562134,"data-quality":0.1968527224,"ml-security":0.0606323782}}
{"text":"3D visual grounding aims to localize the target object in a 3D point cloud by a free-form language description.","meta":{"url":"http://arxiv.org/abs/2307.13363v1"},"cats":{"new-dataset":0.1390567505,"dev-research":0.3397339421,"prompt-eng":0.3881314156,"data-quality":0.1660795586,"ml-security":0.1132901288}}
{"text":"Typically, the sentences describing the target object tend to provide information about its relative relation between other objects and its position within the whole scene.","meta":{"url":"http://arxiv.org/abs/2307.13363v1"},"cats":{"new-dataset":0.0236090566,"dev-research":0.3181493216,"prompt-eng":0.3894039703,"data-quality":0.230203233,"ml-security":0.1003210189}}
{"text":"In this work, we propose a relation-aware one-stage framework, named 3D Relative Position-aware Network (3DRP-Net), which can effectively capture the relative spatial relationships between objects and enhance object attributes.","meta":{"url":"http://arxiv.org/abs/2307.13363v1"},"cats":{"new-dataset":0.1928108792,"dev-research":0.2977575483,"prompt-eng":0.3869063876,"data-quality":0.1527287742,"ml-security":0.0935970505}}
{"text":"Specifically, 1) we propose a 3D Relative Position Multi-head Attention (3DRP-MA) module to analyze relative relations from different directions in the context of object pairs, which helps the model to focus on the specific object relations mentioned in the sentence.","meta":{"url":"http://arxiv.org/abs/2307.13363v1"},"cats":{"new-dataset":0.1792811864,"dev-research":0.2572952329,"prompt-eng":0.4219976908,"data-quality":0.1588740299,"ml-security":0.052451299}}
{"text":"2) We designed a soft-labeling strategy to alleviate the spatial ambiguity caused by redundant points, which further stabilizes and enhances the learning process through a constant and discriminative distribution.","meta":{"url":"http://arxiv.org/abs/2307.13363v1"},"cats":{"new-dataset":0.0769329708,"dev-research":0.2701643541,"prompt-eng":0.4331051206,"data-quality":0.4951754647,"ml-security":0.1293841613}}
{"text":"Extensive experiments conducted on three benchmarks (i.e., ScanRefer and Nr3D/Sr3D) demonstrate that our method outperforms all the state-of-the-art methods in general.","meta":{"url":"http://arxiv.org/abs/2307.13363v1"},"cats":{"new-dataset":0.0411152952,"dev-research":0.2545020769,"prompt-eng":0.3926015131,"data-quality":0.1286375645,"ml-security":0.0520202372}}
{"text":"The source code will be released on GitHub.","meta":{"url":"http://arxiv.org/abs/2307.13363v1"},"cats":{"new-dataset":0.4177597587,"dev-research":0.3380689639,"prompt-eng":0.3689900733,"data-quality":0.2268764136,"ml-security":0.100432781}}
{"text":"Numerous fields, such as ecology, biology, and neuroscience, use animal recordings to track and measure animal behaviour.","meta":{"url":"http://arxiv.org/abs/2307.13361v1"},"cats":{"new-dataset":0.086941615,"dev-research":0.2686669304,"prompt-eng":0.3861690697,"data-quality":0.1565935093,"ml-security":0.076312501}}
{"text":"Over time, a significant volume of such data has been produced, but some computer vision techniques cannot explore it due to the lack of annotations.","meta":{"url":"http://arxiv.org/abs/2307.13361v1"},"cats":{"new-dataset":0.4571893777,"dev-research":0.2121349571,"prompt-eng":0.3595438346,"data-quality":0.2945720354,"ml-security":0.1064398019}}
{"text":"To address this, we propose an approach for estimating 2D mouse body pose from unlabelled images using a synthetically generated empirical pose prior.","meta":{"url":"http://arxiv.org/abs/2307.13361v1"},"cats":{"new-dataset":0.4120184864,"dev-research":0.2508499966,"prompt-eng":0.4058106542,"data-quality":0.1667370196,"ml-security":0.100521137}}
{"text":"Our proposal is based on a recent self-supervised method for estimating 2D human pose that uses single images and a set of unpaired typical 2D poses within a GAN framework.","meta":{"url":"http://arxiv.org/abs/2307.13361v1"},"cats":{"new-dataset":0.3502481802,"dev-research":0.2138410599,"prompt-eng":0.3918333582,"data-quality":0.1481524927,"ml-security":0.1578466367}}
{"text":"We adapt this method to the limb structure of the mouse and generate the empirical prior of 2D poses from a synthetic 3D mouse model, thereby avoiding manual annotation.","meta":{"url":"http://arxiv.org/abs/2307.13361v1"},"cats":{"new-dataset":0.3106671237,"dev-research":0.2851704142,"prompt-eng":0.4278747282,"data-quality":0.1643613863,"ml-security":0.0862162951}}
{"text":"In experiments on a new mouse video dataset, we evaluate the performance of the approach by comparing pose predictions to a manually obtained ground truth.","meta":{"url":"http://arxiv.org/abs/2307.13361v1"},"cats":{"new-dataset":0.3550903529,"dev-research":0.2532166576,"prompt-eng":0.4104059464,"data-quality":0.2230125472,"ml-security":0.1064737579}}
{"text":"We also compare predictions with those from a supervised state-of-the-art method for animal pose estimation.","meta":{"url":"http://arxiv.org/abs/2307.13361v1"},"cats":{"new-dataset":0.2806023051,"dev-research":0.1978573421,"prompt-eng":0.4318719618,"data-quality":0.1744142711,"ml-security":0.1071682689}}
{"text":"The latter evaluation indicates promising results despite the lack of paired training data.","meta":{"url":"http://arxiv.org/abs/2307.13361v1"},"cats":{"new-dataset":0.0826560267,"dev-research":0.2209957402,"prompt-eng":0.3628332302,"data-quality":0.2824753257,"ml-security":0.0985389197}}
{"text":"Finally, qualitative results using a dataset of horse images show the potential of the setting to adapt to other animal species.","meta":{"url":"http://arxiv.org/abs/2307.13361v1"},"cats":{"new-dataset":0.3941768538,"dev-research":0.2004933813,"prompt-eng":0.3807926964,"data-quality":0.10711209,"ml-security":0.0653218867}}
{"text":"Undoing computations of a concurrent system is beneficial in many situations, e.g., in reversible debugging of multi-threaded programs and in recovery from errors due to optimistic execution in parallel discrete event simulation.","meta":{"url":"http://arxiv.org/abs/2307.13360v1"},"cats":{"new-dataset":0.021613324,"dev-research":0.4814581583,"prompt-eng":0.4097892168,"data-quality":0.1473020591,"ml-security":0.2254067834}}
{"text":"A number of approaches have been proposed for how to reverse formal models of concurrent computation including process calculi such as CCS, languages like Erlang, prime event structures and occurrence nets.","meta":{"url":"http://arxiv.org/abs/2307.13360v1"},"cats":{"new-dataset":0.0648524936,"dev-research":0.3871633271,"prompt-eng":0.4662955536,"data-quality":0.1209166568,"ml-security":0.1026795772}}
{"text":"However it has not been settled what properties a reversible system should enjoy, nor how the various properties that have been suggested, such as the parabolic lemma and the causal-consistency property, are related.","meta":{"url":"http://arxiv.org/abs/2307.13360v1"},"cats":{"new-dataset":0.0073870159,"dev-research":0.1646572913,"prompt-eng":0.3779408696,"data-quality":0.1112495663,"ml-security":0.1915646395}}
{"text":"We contribute to a solution to these issues by using a generic labelled transition system equipped with a relation capturing whether transitions are independent to explore the implications between these properties.","meta":{"url":"http://arxiv.org/abs/2307.13360v1"},"cats":{"new-dataset":0.0500868041,"dev-research":0.1837018183,"prompt-eng":0.4791283465,"data-quality":0.226167548,"ml-security":0.0714782257}}
{"text":"In particular, we show how they are derivable from a set of axioms.","meta":{"url":"http://arxiv.org/abs/2307.13360v1"},"cats":{"new-dataset":0.0494658633,"dev-research":0.3523747441,"prompt-eng":0.3437266318,"data-quality":0.1775537758,"ml-security":0.2635403021}}
{"text":"Our intention is that when establishing properties of some formalism it will be easier to verify the axioms rather than proving properties such as the parabolic lemma directly.","meta":{"url":"http://arxiv.org/abs/2307.13360v1"},"cats":{"new-dataset":0.0080805106,"dev-research":0.3194781516,"prompt-eng":0.3583135836,"data-quality":0.1331771024,"ml-security":0.1499579168}}
{"text":"We also introduce two new notions related to causal consistent reversibility, namely causal liveness and causal safety, stating, respectively, that an action can be undone if and only if it is independent from all the following ones.","meta":{"url":"http://arxiv.org/abs/2307.13360v1"},"cats":{"new-dataset":0.0269850723,"dev-research":0.2744777676,"prompt-eng":0.3900233589,"data-quality":0.2102448637,"ml-security":0.2477352662}}
{"text":"We show that both causal liveness and causal safety are derivable from our axioms.","meta":{"url":"http://arxiv.org/abs/2307.13360v1"},"cats":{"new-dataset":0.0467419831,"dev-research":0.3554071325,"prompt-eng":0.3667708719,"data-quality":0.1827170922,"ml-security":0.3449948898}}
{"text":"Motivated by real-life deployments of multi-round federated analytics with secure aggregation, we investigate the fundamental communication-accuracy tradeoffs of the heavy hitter discovery and approximate (open-domain) histogram problems under a linear sketching constraint.","meta":{"url":"http://arxiv.org/abs/2307.13347v1"},"cats":{"new-dataset":0.2660603072,"dev-research":0.2807701912,"prompt-eng":0.3550957846,"data-quality":0.1741851812,"ml-security":0.1651503028}}
{"text":"We propose efficient algorithms based on local subsampling and invertible bloom look-up tables (IBLTs).","meta":{"url":"http://arxiv.org/abs/2307.13347v1"},"cats":{"new-dataset":0.227849761,"dev-research":0.2190659362,"prompt-eng":0.406803695,"data-quality":0.171074283,"ml-security":0.0703730898}}
{"text":"We also show that our algorithms are information-theoretically optimal for a broad class of interactive schemes.","meta":{"url":"http://arxiv.org/abs/2307.13347v1"},"cats":{"new-dataset":0.0596466406,"dev-research":0.236122533,"prompt-eng":0.4430572888,"data-quality":0.114655675,"ml-security":0.178228962}}
{"text":"The results show that the linear sketching constraint does increase the communication cost for both tasks by introducing an extra linear dependence on the number of users in a round.","meta":{"url":"http://arxiv.org/abs/2307.13347v1"},"cats":{"new-dataset":0.0220808164,"dev-research":0.4149070161,"prompt-eng":0.4280316794,"data-quality":0.0848777054,"ml-security":0.1074108921}}
{"text":"Moreover, our results also establish a separation between the communication cost for heavy hitter discovery and approximate histogram in the multi-round setting.","meta":{"url":"http://arxiv.org/abs/2307.13347v1"},"cats":{"new-dataset":0.1771843581,"dev-research":0.222305214,"prompt-eng":0.388576511,"data-quality":0.1720661092,"ml-security":0.113680733}}
{"text":"The dependence on the number of rounds $R$ is at most logarithmic for heavy hitter discovery whereas that of approximate histogram is $\\Theta(\\sqrt{R})$. We also empirically demonstrate our findings.","meta":{"url":"http://arxiv.org/abs/2307.13347v1"},"cats":{"new-dataset":0.0944072611,"dev-research":0.2279336968,"prompt-eng":0.3354902205,"data-quality":0.1677695808,"ml-security":0.1146141648}}
{"text":"Obstructive Sleep Apnea-Hypopnea Syndrome (OSAHS) is a chronic breathing disorder caused by a blockage in the upper airways.","meta":{"url":"http://arxiv.org/abs/2307.13346v1"},"cats":{"new-dataset":0.0524070569,"dev-research":0.2370057726,"prompt-eng":0.2689097329,"data-quality":0.0867338078,"ml-security":0.2067651243}}
{"text":"Snoring is a prominent symptom of OSAHS, and previous studies have attempted to identify the obstruction site of the upper airways by snoring sounds.","meta":{"url":"http://arxiv.org/abs/2307.13346v1"},"cats":{"new-dataset":0.0093300266,"dev-research":0.2102605409,"prompt-eng":0.3285034551,"data-quality":0.1218443265,"ml-security":0.1764771524}}
{"text":"Despite some progress, the classification of the obstruction site remains challenging in real-world clinical settings due to the influence of sleep body position on upper airways.","meta":{"url":"http://arxiv.org/abs/2307.13346v1"},"cats":{"new-dataset":0.0221561641,"dev-research":0.197573671,"prompt-eng":0.3721553223,"data-quality":0.1425801408,"ml-security":0.1387924904}}
{"text":"To address this challenge, this paper proposes a snore-based sleep body position recognition dataset (SSBPR) consisting of 7570 snoring recordings, which comprises six distinct labels for sleep body position: supine, supine but left lateral head, supine but right lateral head, left-side lying, right-side lying and prone.","meta":{"url":"http://arxiv.org/abs/2307.13346v1"},"cats":{"new-dataset":0.5215545243,"dev-research":0.1760886795,"prompt-eng":0.3794838873,"data-quality":0.1801287209,"ml-security":0.1424676414}}
{"text":"Experimental results show that snoring sounds exhibit certain acoustic features that enable their effective utilization for identifying body posture during sleep in real-world scenarios.","meta":{"url":"http://arxiv.org/abs/2307.13346v1"},"cats":{"new-dataset":0.0325847954,"dev-research":0.2204493747,"prompt-eng":0.3664430959,"data-quality":0.1684950135,"ml-security":0.1932585605}}
{"text":"Deep Learning models like Convolutional Neural Networks (CNN) are powerful image classifiers, but what factors determine whether they attend to similar image areas as humans do?","meta":{"url":"http://arxiv.org/abs/2307.13345v1"},"cats":{"new-dataset":0.0115604911,"dev-research":0.2398980069,"prompt-eng":0.3262933128,"data-quality":0.1490866207,"ml-security":0.2019463123}}
{"text":"While previous studies have focused on technological factors, little is known about the role of factors that affect human attention.","meta":{"url":"http://arxiv.org/abs/2307.13345v1"},"cats":{"new-dataset":0.0104579014,"dev-research":0.2929913673,"prompt-eng":0.311079676,"data-quality":0.10566194,"ml-security":0.0785189474}}
{"text":"In the present study, we investigated how the tasks used to elicit human attention maps interact with image characteristics in modulating the similarity between humans and CNN.","meta":{"url":"http://arxiv.org/abs/2307.13345v1"},"cats":{"new-dataset":0.0330334269,"dev-research":0.2655368744,"prompt-eng":0.4622659108,"data-quality":0.1546804031,"ml-security":0.1069040452}}
{"text":"We varied the intentionality of human tasks, ranging from spontaneous gaze during categorization over intentional gaze-pointing up to manual area selection.","meta":{"url":"http://arxiv.org/abs/2307.13345v1"},"cats":{"new-dataset":0.0125064064,"dev-research":0.3052857582,"prompt-eng":0.4693254297,"data-quality":0.1619222479,"ml-security":0.0856845799}}
{"text":"Moreover, we varied the type of image to be categorized, using either singular, salient objects, indoor scenes consisting of object arrangements, or landscapes without distinct objects defining the category.","meta":{"url":"http://arxiv.org/abs/2307.13345v1"},"cats":{"new-dataset":0.3085661784,"dev-research":0.2083014507,"prompt-eng":0.389100693,"data-quality":0.18157407,"ml-security":0.0679408063}}
{"text":"The human attention maps generated in this way were compared to the CNN attention maps revealed by explainable artificial intelligence (Grad-CAM).","meta":{"url":"http://arxiv.org/abs/2307.13345v1"},"cats":{"new-dataset":0.0759747022,"dev-research":0.2912685989,"prompt-eng":0.376421927,"data-quality":0.1743282881,"ml-security":0.1442654599}}
{"text":"The influence of human tasks strongly depended on image type:","meta":{"url":"http://arxiv.org/abs/2307.13345v1"},"cats":{"new-dataset":0.0116307698,"dev-research":0.2493298756,"prompt-eng":0.3738386634,"data-quality":0.0968384257,"ml-security":0.0852056681}}
{"text":"For objects, human manual selection produced maps that were most similar to CNN, while the specific eye movement task has little impact.","meta":{"url":"http://arxiv.org/abs/2307.13345v1"},"cats":{"new-dataset":0.0160812911,"dev-research":0.2924740947,"prompt-eng":0.3667278873,"data-quality":0.1311683193,"ml-security":0.0692727973}}
{"text":"For indoor scenes, spontaneous gaze produced the least similarity, while for landscapes, similarity was equally low across all human tasks.","meta":{"url":"http://arxiv.org/abs/2307.13345v1"},"cats":{"new-dataset":0.0086168204,"dev-research":0.2654820347,"prompt-eng":0.3660211749,"data-quality":0.1576448094,"ml-security":0.0435404824}}
{"text":"To better understand these results, we also compared the different human attention maps to each other.","meta":{"url":"http://arxiv.org/abs/2307.13345v1"},"cats":{"new-dataset":0.0345498882,"dev-research":0.2426377131,"prompt-eng":0.3668272282,"data-quality":0.1957470193,"ml-security":0.0551388043}}
{"text":"Our results highlight the importance of taking human factors into account when comparing the attention of humans and CNN.","meta":{"url":"http://arxiv.org/abs/2307.13345v1"},"cats":{"new-dataset":0.107635322,"dev-research":0.2935317146,"prompt-eng":0.405481445,"data-quality":0.1782388998,"ml-security":0.0787202224}}
{"text":"The local road network information is essential for autonomous navigation.","meta":{"url":"http://arxiv.org/abs/2307.13344v1"},"cats":{"new-dataset":0.059124733,"dev-research":0.2677282117,"prompt-eng":0.368357899,"data-quality":0.1261249086,"ml-security":0.1045978117}}
{"text":"This information is commonly obtained from offline HD-Maps in terms of lane graphs.","meta":{"url":"http://arxiv.org/abs/2307.13344v1"},"cats":{"new-dataset":0.4557429878,"dev-research":0.2144897869,"prompt-eng":0.373701296,"data-quality":0.1318828352,"ml-security":0.0478414718}}
{"text":"However, the local road network at a given moment can be drastically different than the one given in the offline maps; due to construction works, accidents etc.","meta":{"url":"http://arxiv.org/abs/2307.13344v1"},"cats":{"new-dataset":0.0492956329,"dev-research":0.3140812771,"prompt-eng":0.3149356622,"data-quality":0.1486320776,"ml-security":0.0897831046}}
{"text":"Moreover, the autonomous vehicle might be at a location not covered in the offline HD-Map.","meta":{"url":"http://arxiv.org/abs/2307.13344v1"},"cats":{"new-dataset":0.0459462136,"dev-research":0.2183744811,"prompt-eng":0.3446923282,"data-quality":0.1361999539,"ml-security":0.1395240384}}
{"text":"Thus, online estimation of the lane graph is crucial for widespread and reliable autonomous navigation.","meta":{"url":"http://arxiv.org/abs/2307.13344v1"},"cats":{"new-dataset":0.0700086318,"dev-research":0.2479213857,"prompt-eng":0.3637812167,"data-quality":0.1404417419,"ml-security":0.0656909285}}
{"text":"In this work, we tackle online Bird's-Eye-View lane graph extraction from a single onboard camera image.","meta":{"url":"http://arxiv.org/abs/2307.13344v1"},"cats":{"new-dataset":0.4263232729,"dev-research":0.2351877784,"prompt-eng":0.3790457178,"data-quality":0.1424842275,"ml-security":0.0492259605}}
{"text":"We propose to use prior information to increase quality of the estimations.","meta":{"url":"http://arxiv.org/abs/2307.13344v1"},"cats":{"new-dataset":0.0531846911,"dev-research":0.2865142648,"prompt-eng":0.4658877621,"data-quality":0.2664845364,"ml-security":0.0993518966}}
{"text":"The prior is extracted from the dataset through a transformer based Wasserstein Autoencoder.","meta":{"url":"http://arxiv.org/abs/2307.13344v1"},"cats":{"new-dataset":0.2956951475,"dev-research":0.1837828558,"prompt-eng":0.4228279194,"data-quality":0.1837330084,"ml-security":0.0899254307}}
{"text":"The autoencoder is then used to enhance the initial lane graph estimates.","meta":{"url":"http://arxiv.org/abs/2307.13344v1"},"cats":{"new-dataset":0.0582209923,"dev-research":0.3388021759,"prompt-eng":0.4107489954,"data-quality":0.1965016104,"ml-security":0.0615601646}}
{"text":"This is done through optimization of the latent space vector.","meta":{"url":"http://arxiv.org/abs/2307.13344v1"},"cats":{"new-dataset":0.0075852651,"dev-research":0.1770582907,"prompt-eng":0.3722071856,"data-quality":0.1745169186,"ml-security":0.1267340843}}
{"text":"The optimization encourages the lane graph estimation to be logical by discouraging it to diverge from the prior distribution.","meta":{"url":"http://arxiv.org/abs/2307.13344v1"},"cats":{"new-dataset":0.0038970309,"dev-research":0.3233599197,"prompt-eng":0.376834916,"data-quality":0.1891888862,"ml-security":0.1006531982}}
{"text":"We test the method on two benchmark datasets, NuScenes and Argoverse.","meta":{"url":"http://arxiv.org/abs/2307.13344v1"},"cats":{"new-dataset":0.4665407361,"dev-research":0.2137931356,"prompt-eng":0.3806935016,"data-quality":0.2324698968,"ml-security":0.0892915234}}
{"text":"The results show that the proposed method significantly improves the performance compared to state-of-the-art methods.","meta":{"url":"http://arxiv.org/abs/2307.13344v1"},"cats":{"new-dataset":0.0145287523,"dev-research":0.3526191185,"prompt-eng":0.4381560996,"data-quality":0.2386649255,"ml-security":0.0463967499}}
{"text":"Chain-of-thought (CoT) prompting has been shown to empirically improve the accuracy of large language models (LLMs) on various question answering tasks.","meta":{"url":"http://arxiv.org/abs/2307.13339v1"},"cats":{"new-dataset":0.0190506119,"dev-research":0.2411878246,"prompt-eng":0.5063190445,"data-quality":0.1937104701,"ml-security":0.0817647658}}
{"text":"While understanding why CoT prompting is effective is crucial to ensuring that this phenomenon is a consequence of desired model behavior, little work has addressed this; nonetheless, such an understanding is a critical prerequisite for responsible model deployment.","meta":{"url":"http://arxiv.org/abs/2307.13339v1"},"cats":{"new-dataset":0.0055756568,"dev-research":0.4395045121,"prompt-eng":0.5286451846,"data-quality":0.1882343044,"ml-security":0.2498270191}}
{"text":"We address this question by leveraging gradient-based feature attribution methods which produce saliency scores that capture the influence of input tokens on model output.","meta":{"url":"http://arxiv.org/abs/2307.13339v1"},"cats":{"new-dataset":0.0565529748,"dev-research":0.259811082,"prompt-eng":0.4646733411,"data-quality":0.3678676295,"ml-security":0.1751374595}}
{"text":"Specifically, we probe several open-source LLMs to investigate whether CoT prompting affects the relative importances they assign to particular input tokens.","meta":{"url":"http://arxiv.org/abs/2307.13339v1"},"cats":{"new-dataset":0.0173225509,"dev-research":0.3605845999,"prompt-eng":0.5692991842,"data-quality":0.2623968275,"ml-security":0.2486535657}}
{"text":"Our results indicate that while CoT prompting does not increase the magnitude of saliency scores attributed to semantically relevant tokens in the prompt compared to standard few-shot prompting, it increases the robustness of saliency scores to question perturbations and variations in model output.","meta":{"url":"http://arxiv.org/abs/2307.13339v1"},"cats":{"new-dataset":0.0397413748,"dev-research":0.3158047369,"prompt-eng":0.5473926772,"data-quality":0.2923253116,"ml-security":0.1186553639}}
{"text":"Quantization is a promising approach to reduce the high computational complexity of image super-resolution (SR) networks.","meta":{"url":"http://arxiv.org/abs/2307.13337v1"},"cats":{"new-dataset":0.0352420967,"dev-research":0.237400146,"prompt-eng":0.3058436091,"data-quality":0.1189749817,"ml-security":0.0881297752}}
{"text":"However, compared to high-level tasks like image classification, low-bit quantization leads to severe accuracy loss in SR networks.","meta":{"url":"http://arxiv.org/abs/2307.13337v1"},"cats":{"new-dataset":0.0180108875,"dev-research":0.2864521377,"prompt-eng":0.3550664879,"data-quality":0.3145995897,"ml-security":0.2140173957}}
{"text":"This is because feature distributions of SR networks are significantly divergent for each channel or input image, and is thus difficult to determine a quantization range.","meta":{"url":"http://arxiv.org/abs/2307.13337v1"},"cats":{"new-dataset":0.015215567,"dev-research":0.2088466443,"prompt-eng":0.2972982512,"data-quality":0.2190192922,"ml-security":0.1224027618}}
{"text":"Existing SR quantization works approach this distribution mismatch problem by dynamically adapting quantization ranges to the variant distributions during test time.","meta":{"url":"http://arxiv.org/abs/2307.13337v1"},"cats":{"new-dataset":0.0518621297,"dev-research":0.2187287233,"prompt-eng":0.424819779,"data-quality":0.340141724,"ml-security":0.2037437424}}
{"text":"However, such dynamic adaptation incurs additional computational costs that limit the benefits of quantization.","meta":{"url":"http://arxiv.org/abs/2307.13337v1"},"cats":{"new-dataset":0.0056954027,"dev-research":0.2623343797,"prompt-eng":0.3650824889,"data-quality":0.0995803393,"ml-security":0.1584613717}}
{"text":"Instead, we propose a new quantization-aware training framework that effectively Overcomes the Distribution Mismatch problem in SR networks without the need for dynamic adaptation.","meta":{"url":"http://arxiv.org/abs/2307.13337v1"},"cats":{"new-dataset":0.0399627817,"dev-research":0.2335928914,"prompt-eng":0.392927678,"data-quality":0.4372994192,"ml-security":0.34329678}}
{"text":"Intuitively, the mismatch can be reduced by directly regularizing the variance in features during training.","meta":{"url":"http://arxiv.org/abs/2307.13337v1"},"cats":{"new-dataset":0.0026155158,"dev-research":0.3197478649,"prompt-eng":0.4015795147,"data-quality":0.5951285368,"ml-security":0.3076865867}}
{"text":"However, we observe that variance regularization can collide with the reconstruction loss during training and adversely impact SR accuracy.","meta":{"url":"http://arxiv.org/abs/2307.13337v1"},"cats":{"new-dataset":0.0120448537,"dev-research":0.2144471744,"prompt-eng":0.386146273,"data-quality":0.3358942256,"ml-security":0.2894878295}}
{"text":"Thus, we avoid the conflict between two losses by regularizing the variance only when the gradients of variance regularization are cooperative with that of reconstruction.","meta":{"url":"http://arxiv.org/abs/2307.13337v1"},"cats":{"new-dataset":0.00559252,"dev-research":0.1820893582,"prompt-eng":0.3818577112,"data-quality":0.2647131571,"ml-security":0.2854186051}}
{"text":"Additionally, to further reduce the distribution mismatch, we introduce distribution offsets to layers with a significant mismatch, which either scales or shifts channel-wise features.","meta":{"url":"http://arxiv.org/abs/2307.13337v1"},"cats":{"new-dataset":0.1139255281,"dev-research":0.2572031198,"prompt-eng":0.4359747286,"data-quality":0.3348637199,"ml-security":0.1620861599}}
{"text":"Our proposed algorithm, called ODM, effectively reduces the mismatch in distributions with minimal computational overhead.","meta":{"url":"http://arxiv.org/abs/2307.13337v1"},"cats":{"new-dataset":0.0560907146,"dev-research":0.2665243999,"prompt-eng":0.4453574007,"data-quality":0.2895815519,"ml-security":0.1044566286}}
{"text":"Experimental results show that ODM effectively outperforms existing SR quantization approaches with similar or fewer computations, demonstrating the importance of reducing the distribution mismatch problem.","meta":{"url":"http://arxiv.org/abs/2307.13337v1"},"cats":{"new-dataset":0.0260565608,"dev-research":0.3112659989,"prompt-eng":0.3705241236,"data-quality":0.2165297449,"ml-security":0.1181376016}}
{"text":"Our code is available at https://github.com/Cheeun/ODM.","meta":{"url":"http://arxiv.org/abs/2307.13337v1"},"cats":{"new-dataset":0.4002757054,"dev-research":0.2769553467,"prompt-eng":0.4545766462,"data-quality":0.1374789511,"ml-security":0.0607109945}}
{"text":"Random forest is effective for prediction tasks but the randomness of tree generation hinders interpretability in feature importance analysis.","meta":{"url":"http://arxiv.org/abs/2307.13333v1"},"cats":{"new-dataset":0.0131712651,"dev-research":0.3401289133,"prompt-eng":0.4148281458,"data-quality":0.2519081398,"ml-security":0.2271543902}}
{"text":"To address this, we proposed DT-Sampler, a SAT-based method for measuring feature importance in tree-based model.","meta":{"url":"http://arxiv.org/abs/2307.13333v1"},"cats":{"new-dataset":0.0306339041,"dev-research":0.3299107093,"prompt-eng":0.4486493494,"data-quality":0.1906452227,"ml-security":0.0945000629}}
{"text":"Our method has fewer parameters than random forest and provides higher interpretability and stability for the analysis in real-world problems.","meta":{"url":"http://arxiv.org/abs/2307.13333v1"},"cats":{"new-dataset":0.0665361567,"dev-research":0.2377039902,"prompt-eng":0.3662878676,"data-quality":0.2440338526,"ml-security":0.1294156734}}
{"text":"An implementation of DT-Sampler is available at https://github.com/tsudalab/DT-sampler.","meta":{"url":"http://arxiv.org/abs/2307.13333v1"},"cats":{"new-dataset":0.3056540925,"dev-research":0.2383677927,"prompt-eng":0.4714687156,"data-quality":0.1486965375,"ml-security":0.049464636}}
{"text":"Theoretical guarantees in reinforcement learning (RL) are known to suffer multiplicative blow-up factors with respect to the misspecification error of function approximation.","meta":{"url":"http://arxiv.org/abs/2307.13332v1"},"cats":{"new-dataset":0.0085438259,"dev-research":0.185964346,"prompt-eng":0.3554817117,"data-quality":0.1439039896,"ml-security":0.2801256676}}
{"text":"Yet, the nature of such \\emph{approximation factors} -- especially their optimal form in a given learning problem -- is poorly understood.","meta":{"url":"http://arxiv.org/abs/2307.13332v1"},"cats":{"new-dataset":0.0053891054,"dev-research":0.2018883537,"prompt-eng":0.3516162788,"data-quality":0.2100022625,"ml-security":0.1515096628}}
{"text":"In this paper we study this question in linear off-policy value function estimation, where many open questions remain.","meta":{"url":"http://arxiv.org/abs/2307.13332v1"},"cats":{"new-dataset":0.1289396709,"dev-research":0.1641735463,"prompt-eng":0.4019979661,"data-quality":0.1745829387,"ml-security":0.1698991103}}
{"text":"We study the approximation factor in a broad spectrum of settings, such as with the weighted $L_2$-norm (where the weighting is the offline state distribution), the $L_\\infty$ norm, the presence vs. absence of state aliasing, and full vs. partial coverage of the state space.","meta":{"url":"http://arxiv.org/abs/2307.13332v1"},"cats":{"new-dataset":0.0566595358,"dev-research":0.1603068698,"prompt-eng":0.3908549995,"data-quality":0.1469226405,"ml-security":0.1165556641}}
{"text":"We establish the optimal asymptotic approximation factors (up to constants) for all of these settings.","meta":{"url":"http://arxiv.org/abs/2307.13332v1"},"cats":{"new-dataset":0.0521426029,"dev-research":0.1548262687,"prompt-eng":0.3769762279,"data-quality":0.1241622438,"ml-security":0.1188039972}}
{"text":"In particular, our bounds identify two instance-dependent factors for the $L_2(\\mu)$ norm and only one for the $L_\\infty$ norm, which are shown to dictate the hardness of off-policy evaluation under misspecification.","meta":{"url":"http://arxiv.org/abs/2307.13332v1"},"cats":{"new-dataset":0.0484004324,"dev-research":0.1866541873,"prompt-eng":0.3723211317,"data-quality":0.2133224663,"ml-security":0.1835977903}}
{"text":"As medical ultrasound is becoming a prevailing examination approach nowadays, robotic ultrasound systems can facilitate the scanning process and prevent professional sonographers from repetitive and tedious work.","meta":{"url":"http://arxiv.org/abs/2307.13323v1"},"cats":{"new-dataset":0.0118529456,"dev-research":0.2763530517,"prompt-eng":0.4232723137,"data-quality":0.073565178,"ml-security":0.1067554458}}
{"text":"Despite the recent progress, it is still a challenge to enable robots to autonomously accomplish the ultrasound examination, which is largely due to the lack of a proper task representation method, and also an adaptation approach to generalize learned skills across different patients.","meta":{"url":"http://arxiv.org/abs/2307.13323v1"},"cats":{"new-dataset":0.0210883736,"dev-research":0.2486564218,"prompt-eng":0.427683448,"data-quality":0.098850368,"ml-security":0.1473085765}}
{"text":"To solve these problems, we propose the latent task representation and the robotic skills adaptation for autonomous ultrasound in this paper.","meta":{"url":"http://arxiv.org/abs/2307.13323v1"},"cats":{"new-dataset":0.0464754508,"dev-research":0.215201074,"prompt-eng":0.4299991164,"data-quality":0.1113388734,"ml-security":0.0691250869}}
{"text":"During the offline stage, the multimodal ultrasound skills are merged and encapsulated into a low-dimensional probability model through a fully self-supervised framework, which takes clinically demonstrated ultrasound images, probe orientations, and contact forces into account.","meta":{"url":"http://arxiv.org/abs/2307.13323v1"},"cats":{"new-dataset":0.1367044045,"dev-research":0.1957359034,"prompt-eng":0.4407416851,"data-quality":0.0944330525,"ml-security":0.0971037382}}
{"text":"During the online stage, the probability model will select and evaluate the optimal prediction.","meta":{"url":"http://arxiv.org/abs/2307.13323v1"},"cats":{"new-dataset":0.007649488,"dev-research":0.2016360269,"prompt-eng":0.4622768503,"data-quality":0.0941103078,"ml-security":0.0987046363}}
{"text":"For unstable singularities, the adaptive optimizer fine-tunes them to near and stable predictions in high-confidence regions.","meta":{"url":"http://arxiv.org/abs/2307.13323v1"},"cats":{"new-dataset":0.0257125106,"dev-research":0.2532383018,"prompt-eng":0.4407803021,"data-quality":0.2130004914,"ml-security":0.2408674119}}
{"text":"Experimental results show that the proposed approach can generate complex ultrasound strategies for diverse populations and achieve significantly better quantitative results than our previous method.","meta":{"url":"http://arxiv.org/abs/2307.13323v1"},"cats":{"new-dataset":0.1063623879,"dev-research":0.2156687122,"prompt-eng":0.4067718975,"data-quality":0.0828418246,"ml-security":0.0771257983}}
{"text":"For the discrete-time AWGN channel with a power constraint, we give an alternative derivation of Shannon's sphere-packing upper bound on the optimal block error exponent and prove for the first time an analogous lower bound on the optimal correct-decoding exponent.","meta":{"url":"http://arxiv.org/abs/2307.13322v1"},"cats":{"new-dataset":0.0350135951,"dev-research":0.2112315364,"prompt-eng":0.3424741123,"data-quality":0.1855286497,"ml-security":0.1195085831}}
{"text":"The derivations use the method of types with finite alphabets of sizes depending on the block length n and with the number of types sub-exponential in n.","meta":{"url":"http://arxiv.org/abs/2307.13322v1"},"cats":{"new-dataset":0.0322587057,"dev-research":0.2792368169,"prompt-eng":0.3176979853,"data-quality":0.1068854158,"ml-security":0.0990789006}}
{"text":"Invariant generation is the classical problem that aims at automated generation of assertions that over-approximates the set of reachable program states in a program.","meta":{"url":"http://arxiv.org/abs/2307.13318v1"},"cats":{"new-dataset":0.0401617662,"dev-research":0.4080236941,"prompt-eng":0.4709028802,"data-quality":0.2327859688,"ml-security":0.2624377105}}
{"text":"We consider the problem of generating affine invariants over affine while loops (i.e., loops with affine loop guards, conditional branches and assignment statements), and explore the automated generation of disjunctive affine invariants.","meta":{"url":"http://arxiv.org/abs/2307.13318v1"},"cats":{"new-dataset":0.0973220751,"dev-research":0.3946451111,"prompt-eng":0.4516053451,"data-quality":0.1842544714,"ml-security":0.13996333}}
{"text":"Disjunctive invariants are an important class of invariants that capture disjunctive features in programs such as multiple phases, transitions between different modes, etc., and are typically more precise than conjunctive invariants over programs with these features.","meta":{"url":"http://arxiv.org/abs/2307.13318v1"},"cats":{"new-dataset":0.0264917273,"dev-research":0.4081196921,"prompt-eng":0.414598247,"data-quality":0.1697313479,"ml-security":0.195418519}}
{"text":"To generate tight affine invariants, existing constraint-solving approaches have investigated the application of Farkas' Lemma to conjunctive affine invariant generation, but none of them considers disjunctive affine invariants.","meta":{"url":"http://arxiv.org/abs/2307.13318v1"},"cats":{"new-dataset":0.0573173042,"dev-research":0.2774687781,"prompt-eng":0.3877229432,"data-quality":0.1672020215,"ml-security":0.1225778532}}
{"text":"Anomaly segmentation is a critical task for driving applications, and it is approached traditionally as a per-pixel classification problem.","meta":{"url":"http://arxiv.org/abs/2307.13316v1"},"cats":{"new-dataset":0.0501501269,"dev-research":0.3042064481,"prompt-eng":0.3973435975,"data-quality":0.4196265985,"ml-security":0.3845560806}}
{"text":"However, reasoning individually about each pixel without considering their contextual semantics results in high uncertainty around the objects' boundaries and numerous false positives.","meta":{"url":"http://arxiv.org/abs/2307.13316v1"},"cats":{"new-dataset":0.0358426386,"dev-research":0.323188674,"prompt-eng":0.39839414,"data-quality":0.3321589662,"ml-security":0.1377036697}}
{"text":"We propose a paradigm change by shifting from a per-pixel classification to a mask classification.","meta":{"url":"http://arxiv.org/abs/2307.13316v1"},"cats":{"new-dataset":0.1298919253,"dev-research":0.233809791,"prompt-eng":0.3826285693,"data-quality":0.2985280959,"ml-security":0.2375457533}}
{"text":"Our mask-based method, Mask2Anomaly, demonstrates the feasibility of integrating an anomaly detection method in a mask-classification architecture.","meta":{"url":"http://arxiv.org/abs/2307.13316v1"},"cats":{"new-dataset":0.1085404523,"dev-research":0.3308136439,"prompt-eng":0.4028714494,"data-quality":0.4014530258,"ml-security":0.582011764}}
{"text":"Mask2Anomaly includes several technical novelties that are designed to improve the detection of anomalies in masks: i) a global masked attention module to focus individually on the foreground and background regions; ii) a mask contrastive learning that maximizes the margin between an anomaly and known classes; and iii) a mask refinement solution to reduce false positives.","meta":{"url":"http://arxiv.org/abs/2307.13316v1"},"cats":{"new-dataset":0.0960633914,"dev-research":0.3096765086,"prompt-eng":0.3988976329,"data-quality":0.3415894187,"ml-security":0.4185460195}}
{"text":"Mask2Anomaly achieves new state-of-the-art results across a range of benchmarks, both in the per-pixel and component-level evaluations.","meta":{"url":"http://arxiv.org/abs/2307.13316v1"},"cats":{"new-dataset":0.1788034414,"dev-research":0.2818651097,"prompt-eng":0.4071150444,"data-quality":0.1962838148,"ml-security":0.1427159629}}
{"text":"In particular, Mask2Anomaly reduces the average false positives rate by 60% wrt","meta":{"url":"http://arxiv.org/abs/2307.13316v1"},"cats":{"new-dataset":0.0054405927,"dev-research":0.2703220868,"prompt-eng":0.3992607695,"data-quality":0.3078881841,"ml-security":0.3525721706}}
{"text":"the previous state-of-the-art.","meta":{"url":"http://arxiv.org/abs/2307.13316v1"},"cats":{"new-dataset":0.194622414,"dev-research":0.2668627454,"prompt-eng":0.3915583068,"data-quality":0.1740630706,"ml-security":0.0690359799}}
{"text":"Github page: https://github.com/shyam671/Mask2Anomaly-Unmasking-Anomalies-in-Road-Scene-Segmentation.","meta":{"url":"http://arxiv.org/abs/2307.13316v1"},"cats":{"new-dataset":0.4617973899,"dev-research":0.2365652391,"prompt-eng":0.4013810524,"data-quality":0.3059222004,"ml-security":0.1716891715}}
{"text":"Machine learning makes multimedia data (e.g., images) more attractive, however, multimedia data is usually distributed and privacy sensitive.","meta":{"url":"http://arxiv.org/abs/2307.13314v1"},"cats":{"new-dataset":0.0569266539,"dev-research":0.2458859166,"prompt-eng":0.2954647188,"data-quality":0.2100924502,"ml-security":0.4397857635}}
{"text":"Multiple distributed multimedia clients can resort to federated learning (FL) to jointly learn a global shared model without requiring to share their private samples with any third-party entities.","meta":{"url":"http://arxiv.org/abs/2307.13314v1"},"cats":{"new-dataset":0.1146383638,"dev-research":0.1927743942,"prompt-eng":0.3548722013,"data-quality":0.1608549292,"ml-security":0.1685036061}}
{"text":"In this paper, we show that FL suffers from the cross-client generative adversarial networks (GANs)-based (C-GANs) attack, in which a malicious client (i.e., adversary) can reconstruct samples with the same distribution as the training samples from other clients (i.e., victims).","meta":{"url":"http://arxiv.org/abs/2307.13314v1"},"cats":{"new-dataset":0.1044212588,"dev-research":0.2289457851,"prompt-eng":0.3588837773,"data-quality":0.2987561594,"ml-security":0.7679114289}}
{"text":"Since a benign client's data can be leaked to the adversary, this attack brings the risk of local data leakage for clients in many security-critical FL applications.","meta":{"url":"http://arxiv.org/abs/2307.13314v1"},"cats":{"new-dataset":0.0885840127,"dev-research":0.3451063433,"prompt-eng":0.3502373589,"data-quality":0.2783128583,"ml-security":0.7892868783}}
{"text":"Thus, we propose Fed-EDKD (i.e., Federated Ensemble Data-free Knowledge Distillation) technique to improve the current popular FL schemes to resist C-GANs attack.","meta":{"url":"http://arxiv.org/abs/2307.13314v1"},"cats":{"new-dataset":0.1510128469,"dev-research":0.2950383709,"prompt-eng":0.3550396295,"data-quality":0.1978571336,"ml-security":0.4252305948}}
{"text":"In Fed-EDKD, each client submits a local model to the server for obtaining an ensemble global model.","meta":{"url":"http://arxiv.org/abs/2307.13314v1"},"cats":{"new-dataset":0.1088982779,"dev-research":0.225449557,"prompt-eng":0.4415077511,"data-quality":0.1521715768,"ml-security":0.1519926922}}
{"text":"Then, to avoid model expansion, Fed-EDKD adopts data-free knowledge distillation techniques to transfer knowledge from the ensemble global model to a compressed model.","meta":{"url":"http://arxiv.org/abs/2307.13314v1"},"cats":{"new-dataset":0.0800205783,"dev-research":0.2761589386,"prompt-eng":0.3500999068,"data-quality":0.1765010261,"ml-security":0.1749440688}}
{"text":"By this way, Fed-EDKD reduces the adversary's control capability over the global model, so Fed-EDKD can effectively mitigate C-GANs attack.","meta":{"url":"http://arxiv.org/abs/2307.13314v1"},"cats":{"new-dataset":0.0117546537,"dev-research":0.3056024941,"prompt-eng":0.3342449338,"data-quality":0.1601000897,"ml-security":0.4354337695}}
{"text":"Finally, the experimental results demonstrate that Fed-EDKD significantly mitigates C-GANs attack while only incurring a slight accuracy degradation of FL.","meta":{"url":"http://arxiv.org/abs/2307.13314v1"},"cats":{"new-dataset":0.0679142863,"dev-research":0.3359605057,"prompt-eng":0.3716417584,"data-quality":0.3060706981,"ml-security":0.4745014145}}
{"text":"Contour based scene text detection methods have rapidly developed recently, but still suffer from inaccurate frontend contour initialization, multi-stage error accumulation, or deficient local information aggregation.","meta":{"url":"http://arxiv.org/abs/2307.13310v1"},"cats":{"new-dataset":0.1085806468,"dev-research":0.2442594133,"prompt-eng":0.3930575836,"data-quality":0.5696994222,"ml-security":0.0851100217}}
{"text":"To tackle these limitations, we propose a novel arbitrary-shaped scene text detection framework named CT-Net by progressive contour regression with contour transformers.","meta":{"url":"http://arxiv.org/abs/2307.13310v1"},"cats":{"new-dataset":0.1972236458,"dev-research":0.2088542196,"prompt-eng":0.3508168212,"data-quality":0.3283401682,"ml-security":0.0965730641}}
{"text":"Specifically, we first employ a contour initialization module that generates coarse text contours without any post-processing.","meta":{"url":"http://arxiv.org/abs/2307.13310v1"},"cats":{"new-dataset":0.0859711677,"dev-research":0.2727441126,"prompt-eng":0.4372044507,"data-quality":0.2233771544,"ml-security":0.0604150399}}
{"text":"Then, we adopt contour refinement modules to adaptively refine text contours in an iterative manner, which are beneficial for context information capturing and progressive global contour deformation.","meta":{"url":"http://arxiv.org/abs/2307.13310v1"},"cats":{"new-dataset":0.0999162845,"dev-research":0.2976600228,"prompt-eng":0.4255031582,"data-quality":0.264874279,"ml-security":0.0510891671}}
{"text":"Besides, we propose an adaptive training strategy to enable the contour transformers to learn more potential deformation paths, and introduce a re-score mechanism that can effectively suppress false positives.","meta":{"url":"http://arxiv.org/abs/2307.13310v1"},"cats":{"new-dataset":0.0464245002,"dev-research":0.2668034999,"prompt-eng":0.476729215,"data-quality":0.2929966953,"ml-security":0.1966084581}}
{"text":"Extensive experiments are conducted on four challenging datasets, which demonstrate the accuracy and efficiency of our CT-Net over state-of-the-art methods.","meta":{"url":"http://arxiv.org/abs/2307.13310v1"},"cats":{"new-dataset":0.3999455937,"dev-research":0.2618265394,"prompt-eng":0.3731859441,"data-quality":0.2113519473,"ml-security":0.1248528564}}
{"text":"Particularly, CT-Net achieves F-measure of 86.1 at 11.2 frames per second (FPS) and F-measure of 87.8 at 10.1 FPS for CTW1500 and Total-Text datasets, respectively.","meta":{"url":"http://arxiv.org/abs/2307.13310v1"},"cats":{"new-dataset":0.2607585243,"dev-research":0.2470829011,"prompt-eng":0.3791851261,"data-quality":0.163876682,"ml-security":0.0587696961}}
{"text":"This work studies post-training parameter quantization in large language models (LLMs).","meta":{"url":"http://arxiv.org/abs/2307.13304v1"},"cats":{"new-dataset":0.067183017,"dev-research":0.166603081,"prompt-eng":0.4050586901,"data-quality":0.3301395277,"ml-security":0.1783518254}}
{"text":"We introduce quantization with incoherence processing (QuIP), a new method based on the insight that quantization benefits from incoherent weight and Hessian matrices, i.e., from the weights and the directions in which it is important to round them accurately being unaligned with the coordinate axes.","meta":{"url":"http://arxiv.org/abs/2307.13304v1"},"cats":{"new-dataset":0.012565117,"dev-research":0.2263354884,"prompt-eng":0.354423488,"data-quality":0.2281327081,"ml-security":0.1696504808}}
{"text":"QuIP consists of two steps: (1) an adaptive rounding procedure minimizing a quadratic proxy objective; (2) efficient pre- and post-processing that ensures weight and Hessian incoherence via multiplication by random orthogonal matrices.","meta":{"url":"http://arxiv.org/abs/2307.13304v1"},"cats":{"new-dataset":0.0109534439,"dev-research":0.2727985396,"prompt-eng":0.4520571583,"data-quality":0.1404319566,"ml-security":0.0919362103}}
{"text":"We complement QuIP with the first theoretical analysis for an LLM-scale quantization algorithm, and show that our theory also applies to an existing method, OPTQ.","meta":{"url":"http://arxiv.org/abs/2307.13304v1"},"cats":{"new-dataset":0.0433683695,"dev-research":0.1870389463,"prompt-eng":0.3933738236,"data-quality":0.2025258982,"ml-security":0.0872564313}}
{"text":"Empirically, we find that our incoherence preprocessing improves several existing quantization algorithms and yields the first LLM quantization methods that produce viable results using only two bits per weight.","meta":{"url":"http://arxiv.org/abs/2307.13304v1"},"cats":{"new-dataset":0.0502160499,"dev-research":0.3035105391,"prompt-eng":0.3937174208,"data-quality":0.2917207443,"ml-security":0.181353746}}
{"text":"Our code can be found at https://github.com/jerry-chee/QuIP .","meta":{"url":"http://arxiv.org/abs/2307.13304v1"},"cats":{"new-dataset":0.3674274023,"dev-research":0.3122639165,"prompt-eng":0.4757006718,"data-quality":0.1763576921,"ml-security":0.0747904437}}
{"text":"Common deep learning models for 3D environment perception often use pillarization/voxelization methods to convert point cloud data into pillars/voxels and then process it with a 2D/3D convolutional neural network (CNN).","meta":{"url":"http://arxiv.org/abs/2307.13300v1"},"cats":{"new-dataset":0.1059357022,"dev-research":0.2157530167,"prompt-eng":0.3210640024,"data-quality":0.088737636,"ml-security":0.1399720859}}
{"text":"The pioneer work PointNet has been widely applied as a local feature descriptor, a fundamental component in deep learning models for 3D perception, to extract features of a point cloud.","meta":{"url":"http://arxiv.org/abs/2307.13300v1"},"cats":{"new-dataset":0.1322714929,"dev-research":0.2539773222,"prompt-eng":0.3494868679,"data-quality":0.1637534538,"ml-security":0.1369032594}}
{"text":"This is achieved by using a symmetric max-pooling operator which provides unique pillar/voxel features.","meta":{"url":"http://arxiv.org/abs/2307.13300v1"},"cats":{"new-dataset":0.063535352,"dev-research":0.2067797326,"prompt-eng":0.4033147653,"data-quality":0.1146819139,"ml-security":0.0584992738}}
{"text":"However, by ignoring most of the points, the max-pooling operator causes an information loss, which reduces the model performance.","meta":{"url":"http://arxiv.org/abs/2307.13300v1"},"cats":{"new-dataset":0.004886627,"dev-research":0.2133123925,"prompt-eng":0.3619179566,"data-quality":0.2265815594,"ml-security":0.1712145092}}
{"text":"To address this issue, we propose a novel local feature descriptor, mini-PointNetPlus, as an alternative for plug-and-play to PointNet.","meta":{"url":"http://arxiv.org/abs/2307.13300v1"},"cats":{"new-dataset":0.1542687262,"dev-research":0.3526682411,"prompt-eng":0.4425918447,"data-quality":0.2539582431,"ml-security":0.1100480022}}
{"text":"Our basic idea is to separately project the data points to the individual features considered, each leading to a permutation invariant.","meta":{"url":"http://arxiv.org/abs/2307.13300v1"},"cats":{"new-dataset":0.23515849,"dev-research":0.2644891562,"prompt-eng":0.4431762909,"data-quality":0.1826054951,"ml-security":0.100087802}}
{"text":"Thus, the proposed descriptor transforms an unordered point cloud to a stable order.","meta":{"url":"http://arxiv.org/abs/2307.13300v1"},"cats":{"new-dataset":0.108855975,"dev-research":0.2023806478,"prompt-eng":0.4129266918,"data-quality":0.1261430725,"ml-security":0.0688974411}}
{"text":"The vanilla PointNet is proved to be a special case of our mini-PointNetPlus.","meta":{"url":"http://arxiv.org/abs/2307.13300v1"},"cats":{"new-dataset":0.1078350987,"dev-research":0.189248237,"prompt-eng":0.3490565476,"data-quality":0.2254002988,"ml-security":0.1835183834}}
{"text":"Due to fully utilizing the features by the proposed descriptor, we demonstrate in experiment a considerable performance improvement for 3D perception.","meta":{"url":"http://arxiv.org/abs/2307.13300v1"},"cats":{"new-dataset":0.0581133036,"dev-research":0.2751309857,"prompt-eng":0.4310530722,"data-quality":0.1460474182,"ml-security":0.0803977792}}
{"text":"Legal case retrieval is a special Information Retrieval~(IR) task focusing on legal case documents.","meta":{"url":"http://arxiv.org/abs/2307.13298v1"},"cats":{"new-dataset":0.0832913719,"dev-research":0.2589789325,"prompt-eng":0.3698365396,"data-quality":0.1623275409,"ml-security":0.0824244219}}
{"text":"Depending on the downstream tasks of the retrieved case documents, users' information needs in legal case retrieval could be significantly different from those in Web search and traditional ad-hoc retrieval tasks.","meta":{"url":"http://arxiv.org/abs/2307.13298v1"},"cats":{"new-dataset":0.0281478793,"dev-research":0.2316395103,"prompt-eng":0.3872161048,"data-quality":0.1411782155,"ml-security":0.1063656988}}
{"text":"While there are several studies that retrieve legal cases based on text similarity, the underlying search intents of legal retrieval users, as shown in this paper, are more complicated than that yet mostly unexplored.","meta":{"url":"http://arxiv.org/abs/2307.13298v1"},"cats":{"new-dataset":0.0579247087,"dev-research":0.2218630403,"prompt-eng":0.3287061629,"data-quality":0.2025814136,"ml-security":0.1858538144}}
{"text":"To this end, we present a novel hierarchical intent taxonomy of legal case retrieval.","meta":{"url":"http://arxiv.org/abs/2307.13298v1"},"cats":{"new-dataset":0.1163374038,"dev-research":0.2256596706,"prompt-eng":0.3865494259,"data-quality":0.1657561279,"ml-security":0.10614209}}
{"text":"It consists of five intent types categorized by three criteria, i.e., search for Particular Case(s), Characterization, Penalty, Procedure, and Interest.","meta":{"url":"http://arxiv.org/abs/2307.13298v1"},"cats":{"new-dataset":0.0385585102,"dev-research":0.2086582007,"prompt-eng":0.3838859768,"data-quality":0.0927190382,"ml-security":0.075232395}}
{"text":"The taxonomy was constructed transparently and evaluated extensively through interviews, editorial user studies, and query log analysis.","meta":{"url":"http://arxiv.org/abs/2307.13298v1"},"cats":{"new-dataset":0.1399931342,"dev-research":0.2856888077,"prompt-eng":0.3918693798,"data-quality":0.1730443984,"ml-security":0.0808202404}}
{"text":"Through a laboratory user study, we reveal significant differences in user behavior and satisfaction under different search intents in legal case retrieval.","meta":{"url":"http://arxiv.org/abs/2307.13298v1"},"cats":{"new-dataset":0.0156141536,"dev-research":0.2675629858,"prompt-eng":0.4013256448,"data-quality":0.1730051777,"ml-security":0.0972417494}}
{"text":"Furthermore, we apply the proposed taxonomy to various downstream legal retrieval tasks, e.g., result ranking and satisfaction prediction, and demonstrate its effectiveness.","meta":{"url":"http://arxiv.org/abs/2307.13298v1"},"cats":{"new-dataset":0.0850883538,"dev-research":0.2302108634,"prompt-eng":0.4268466963,"data-quality":0.2595572136,"ml-security":0.0705821713}}
{"text":"Our work provides important insights into the understanding of user intents in legal case retrieval and potentially leads to better retrieval techniques in the legal domain, such as intent-aware ranking strategies and evaluation methodologies.","meta":{"url":"http://arxiv.org/abs/2307.13298v1"},"cats":{"new-dataset":0.0467980804,"dev-research":0.2681001669,"prompt-eng":0.3890451834,"data-quality":0.1738074598,"ml-security":0.1099032961}}
{"text":"Recently, speech codecs based on neural networks have proven to perform better than traditional methods.","meta":{"url":"http://arxiv.org/abs/2307.13295v1"},"cats":{"new-dataset":0.0446861028,"dev-research":0.3445929735,"prompt-eng":0.30024683,"data-quality":0.3002881472,"ml-security":0.1457092583}}
{"text":"However, redundancy in traditional parameter quantization is visible within the codec architecture of combining the traditional codec with the neural vocoder.","meta":{"url":"http://arxiv.org/abs/2307.13295v1"},"cats":{"new-dataset":0.0243876668,"dev-research":0.3165328865,"prompt-eng":0.3871592288,"data-quality":0.2739377069,"ml-security":0.1510674806}}
{"text":"In this paper, we propose a novel framework named CQNV, which combines the coarsely quantized parameters of a traditional parametric codec to reduce the bitrate with a neural vocoder to improve the quality of the decoded speech.","meta":{"url":"http://arxiv.org/abs/2307.13295v1"},"cats":{"new-dataset":0.2174396671,"dev-research":0.2598853543,"prompt-eng":0.3662872586,"data-quality":0.1915819237,"ml-security":0.1457650185}}
{"text":"Furthermore, we introduce a parameters processing module into the neural vocoder to enhance the application of the bitstream of traditional speech coding parameters to the neural vocoder, further improving the reconstructed speech's quality.","meta":{"url":"http://arxiv.org/abs/2307.13295v1"},"cats":{"new-dataset":0.1849409691,"dev-research":0.2803413039,"prompt-eng":0.3987702519,"data-quality":0.249856261,"ml-security":0.1369418574}}
{"text":"In the experiments, both subjective and objective evaluations demonstrate the effectiveness of the proposed CQNV framework.","meta":{"url":"http://arxiv.org/abs/2307.13295v1"},"cats":{"new-dataset":0.0173466748,"dev-research":0.301642946,"prompt-eng":0.4133279721,"data-quality":0.1851911437,"ml-security":0.0820574684}}
{"text":"Specifically, our proposed method can achieve higher quality reconstructed speech at 1.1 kbps than Lyra and Encodec at 3 kbps.","meta":{"url":"http://arxiv.org/abs/2307.13295v1"},"cats":{"new-dataset":0.140224595,"dev-research":0.2303814884,"prompt-eng":0.3772232398,"data-quality":0.210564284,"ml-security":0.0668973426}}
{"text":"Although face recognition starts to play an important role in our daily life, we need to pay attention that data-driven face recognition vision systems are vulnerable to adversarial attacks.","meta":{"url":"http://arxiv.org/abs/2307.13294v1"},"cats":{"new-dataset":0.0668360993,"dev-research":0.2922729274,"prompt-eng":0.3718001535,"data-quality":0.2675259231,"ml-security":0.779589266}}
{"text":"However, the current two categories of adversarial attacks, namely digital attacks and physical attacks both have drawbacks, with the former ones impractical and the latter one conspicuous, high-computational and inexecutable.","meta":{"url":"http://arxiv.org/abs/2307.13294v1"},"cats":{"new-dataset":0.0263771129,"dev-research":0.3105502116,"prompt-eng":0.3393356781,"data-quality":0.2105926528,"ml-security":0.8241691497}}
{"text":"To address the issues, we propose a practical, executable, inconspicuous and low computational adversarial attack based on LED illumination modulation.","meta":{"url":"http://arxiv.org/abs/2307.13294v1"},"cats":{"new-dataset":0.0712135095,"dev-research":0.2789325274,"prompt-eng":0.4064710783,"data-quality":0.2229944266,"ml-security":0.7810703638}}
{"text":"To fool the systems, the proposed attack generates imperceptible luminance changes to human eyes through fast intensity modulation of scene LED illumination and uses the rolling shutter effect of CMOS image sensors in face recognition systems to implant luminance information perturbation to the captured face images.","meta":{"url":"http://arxiv.org/abs/2307.13294v1"},"cats":{"new-dataset":0.0623556774,"dev-research":0.2685452192,"prompt-eng":0.434707482,"data-quality":0.1723060128,"ml-security":0.6284006376}}
{"text":"In summary,we present a denial-of-service (DoS) attack for face detection and a dodging attack for face verification.","meta":{"url":"http://arxiv.org/abs/2307.13294v1"},"cats":{"new-dataset":0.1152091513,"dev-research":0.2554660876,"prompt-eng":0.4051739145,"data-quality":0.2078379922,"ml-security":0.6287854838}}
{"text":"We also evaluate their effectiveness against well-known face detection models, Dlib, MTCNN and RetinaFace , and face verification models, Dlib, FaceNet,and ArcFace.","meta":{"url":"http://arxiv.org/abs/2307.13294v1"},"cats":{"new-dataset":0.1312995833,"dev-research":0.307060744,"prompt-eng":0.4128900536,"data-quality":0.1948044762,"ml-security":0.2495294787}}
{"text":"The extensive experiments show that the success rates of DoS attacks against face detection models reach 97.67%, 100%, and 100%, respectively, and the success rates of dodging attacks against all face verification models reach 100%.","meta":{"url":"http://arxiv.org/abs/2307.13294v1"},"cats":{"new-dataset":0.0256893446,"dev-research":0.2607376686,"prompt-eng":0.4167139962,"data-quality":0.2173269243,"ml-security":0.6707979013}}
{"text":"Probabilistic shaping is a pragmatic approach to improve the performance of coherent optical fiber communication systems.","meta":{"url":"http://arxiv.org/abs/2307.13292v1"},"cats":{"new-dataset":0.0069449965,"dev-research":0.3060092352,"prompt-eng":0.408439482,"data-quality":0.1322602203,"ml-security":0.0690002535}}
{"text":"In the nonlinear regime, the advantages offered by probabilistic shaping might increase thanks to the opportunity to obtain an additional nonlinear shaping gain.","meta":{"url":"http://arxiv.org/abs/2307.13292v1"},"cats":{"new-dataset":0.0049863529,"dev-research":0.1859085345,"prompt-eng":0.3968262517,"data-quality":0.0907188288,"ml-security":0.1232640296}}
{"text":"Unfortunately, the optimization of conventional shaping techniques, such as probabilistic amplitude shaping (PAS), yields a relevant nonlinear shaping gain only in scenarios of limited practical interest.","meta":{"url":"http://arxiv.org/abs/2307.13292v1"},"cats":{"new-dataset":0.0121481401,"dev-research":0.2111473059,"prompt-eng":0.4402192323,"data-quality":0.1376673431,"ml-security":0.1146082105}}
{"text":"In this manuscript we use sequence selection to investigate the potential, opportunities, and challenges offered by nonlinear probabilistic shaping.","meta":{"url":"http://arxiv.org/abs/2307.13292v1"},"cats":{"new-dataset":0.0307043615,"dev-research":0.1795405459,"prompt-eng":0.4335743912,"data-quality":0.1262902405,"ml-security":0.1012859811}}
{"text":"First, we show that ideal sequence selection is able to provide up to 0.13 bit/s/Hz gain with respect to PAS with an optimized blocklength.","meta":{"url":"http://arxiv.org/abs/2307.13292v1"},"cats":{"new-dataset":0.130360306,"dev-research":0.1874714082,"prompt-eng":0.4312387564,"data-quality":0.1262890215,"ml-security":0.0912875256}}
{"text":"However, this additional gain is obtained only if the selection metric accounts for the signs of the symbols: they must be known to compute the selection metric, but there is no need to shape them.","meta":{"url":"http://arxiv.org/abs/2307.13292v1"},"cats":{"new-dataset":0.0049386612,"dev-research":0.1980397217,"prompt-eng":0.3998561975,"data-quality":0.1708452158,"ml-security":0.1058882463}}
{"text":"Furthermore, we show that the selection depends in a non-critical way on the symbol rate and link length: the sequences selected for a certain scenario still provide a relevant gain if these are modified.","meta":{"url":"http://arxiv.org/abs/2307.13292v1"},"cats":{"new-dataset":0.0243155243,"dev-research":0.1976472543,"prompt-eng":0.4414167736,"data-quality":0.1645144391,"ml-security":0.0986359189}}
{"text":"Then, we analyze and compare several practical implementations of sequence selection by taking into account interaction with forward error correction (FEC) and complexity.","meta":{"url":"http://arxiv.org/abs/2307.13292v1"},"cats":{"new-dataset":0.0246767385,"dev-research":0.2586305799,"prompt-eng":0.4264210639,"data-quality":0.2098726542,"ml-security":0.0904765252}}
{"text":"Overall, the single block and the multi block FEC-independent bit scrambling are the best options, with a gain up to 0.08 bit/s/Hz.","meta":{"url":"http://arxiv.org/abs/2307.13292v1"},"cats":{"new-dataset":0.0512670843,"dev-research":0.1865632033,"prompt-eng":0.3862624126,"data-quality":0.1028447125,"ml-security":0.1058273896}}
{"text":"The main challenge and limitation to their practical implementation remains the evaluation of the metric, whose complexity is currently too high.","meta":{"url":"http://arxiv.org/abs/2307.13292v1"},"cats":{"new-dataset":0.0667727963,"dev-research":0.2959148493,"prompt-eng":0.3321188348,"data-quality":0.2008900046,"ml-security":0.0931722887}}
{"text":"Finally, we show that the nonlinear shaping gain provided by sequence selection persists when carrier phase recovery is included.","meta":{"url":"http://arxiv.org/abs/2307.13292v1"},"cats":{"new-dataset":0.019968815,"dev-research":0.1652912404,"prompt-eng":0.4398184973,"data-quality":0.1806773236,"ml-security":0.1080932449}}
{"text":"Background: Biomedical data are usually collections of longitudinal data assessed at certain points in time.","meta":{"url":"http://arxiv.org/abs/2307.13288v1"},"cats":{"new-dataset":0.1828130534,"dev-research":0.2133271295,"prompt-eng":0.3572658185,"data-quality":0.1207803062,"ml-security":0.0799638617}}
{"text":"Clinical observations assess the presences and severity of symptoms, which are the basis for description and modeling of disease progression.","meta":{"url":"http://arxiv.org/abs/2307.13288v1"},"cats":{"new-dataset":0.058342416,"dev-research":0.2861300842,"prompt-eng":0.4632952286,"data-quality":0.1202044689,"ml-security":0.0809867607}}
{"text":"Deciphering potential underlying unknowns solely from the distinct observation would substantially improve the understanding of pathological cascades.","meta":{"url":"http://arxiv.org/abs/2307.13288v1"},"cats":{"new-dataset":0.0170458866,"dev-research":0.2643650636,"prompt-eng":0.3981274501,"data-quality":0.2434745967,"ml-security":0.2373686601}}
{"text":"Hidden Markov Models (HMMs) have been successfully applied to the processing of possibly noisy continuous signals.","meta":{"url":"http://arxiv.org/abs/2307.13288v1"},"cats":{"new-dataset":0.0722939393,"dev-research":0.1788488553,"prompt-eng":0.421488013,"data-quality":0.2384783804,"ml-security":0.1709275621}}
{"text":"The aim was to improve the application HMMs to multivariate time-series of categorically distributed data.","meta":{"url":"http://arxiv.org/abs/2307.13288v1"},"cats":{"new-dataset":0.218432627,"dev-research":0.1754419158,"prompt-eng":0.380511123,"data-quality":0.1331071208,"ml-security":0.0987035194}}
{"text":"Here, we used HHMs to study prediction of the loss of free walking ability as one major clinical deterioration in the most common autosomal dominantly inherited ataxia disorder worldwide.","meta":{"url":"http://arxiv.org/abs/2307.13288v1"},"cats":{"new-dataset":0.0677846729,"dev-research":0.2193916368,"prompt-eng":0.4443634578,"data-quality":0.1546002041,"ml-security":0.1233613594}}
{"text":"We used HHMs to investigate the prediction of loss of the ability to walk freely, representing a major clinical deterioration in the most common autosomal-dominant inherited ataxia disorder worldwide.   ","meta":{"url":"http://arxiv.org/abs/2307.13288v1"},"cats":{"new-dataset":0.0453000172,"dev-research":0.2045876438,"prompt-eng":0.4390194514,"data-quality":0.1498885216,"ml-security":0.1262343865}}
{"text":"Results: We present a prediction pipeline which processes data paired with a configuration file, enabling to construct, validate and query a fully parameterized HMM-based model.","meta":{"url":"http://arxiv.org/abs/2307.13288v1"},"cats":{"new-dataset":0.4787950485,"dev-research":0.2354798864,"prompt-eng":0.5110354038,"data-quality":0.1827897836,"ml-security":0.1376225655}}
{"text":"In particular, we provide a theoretical and practical framework for multivariate time-series inference based on HMMs that includes constructing multiple HMMs, each to predict a particular observable variable.","meta":{"url":"http://arxiv.org/abs/2307.13288v1"},"cats":{"new-dataset":0.1081901691,"dev-research":0.1534147504,"prompt-eng":0.4211055238,"data-quality":0.1068011306,"ml-security":0.1547288788}}
{"text":"Our analysis is done on random data, but also on biomedical data based on Spinocerebellar ataxia type 3 disease.   ","meta":{"url":"http://arxiv.org/abs/2307.13288v1"},"cats":{"new-dataset":0.2014093382,"dev-research":0.2078889106,"prompt-eng":0.396495923,"data-quality":0.1503706332,"ml-security":0.1091159919}}
{"text":"Conclusions: HHMs are a promising approach to study biomedical data that naturally are represented as multivariate time-series.","meta":{"url":"http://arxiv.org/abs/2307.13288v1"},"cats":{"new-dataset":0.1041984591,"dev-research":0.1829061339,"prompt-eng":0.3161662595,"data-quality":0.0942995063,"ml-security":0.0780521168}}
{"text":"Our implementation of a HHMs framework is publicly available and can easily be adapted for further applications.","meta":{"url":"http://arxiv.org/abs/2307.13288v1"},"cats":{"new-dataset":0.1863586538,"dev-research":0.2553975768,"prompt-eng":0.4077853986,"data-quality":0.0895134718,"ml-security":0.1113322428}}
{"text":"Spawning duplicate requests, called cloning, is a powerful technique to reduce tail latency by masking service-time variability.","meta":{"url":"http://arxiv.org/abs/2307.13285v1"},"cats":{"new-dataset":0.0193837623,"dev-research":0.3039170643,"prompt-eng":0.4564825966,"data-quality":0.1292613782,"ml-security":0.1846282151}}
{"text":"However, traditional client-based cloning is static and harmful to performance under high load, while a recent coordinator-based approach is slow and not scalable.","meta":{"url":"http://arxiv.org/abs/2307.13285v1"},"cats":{"new-dataset":0.0321531506,"dev-research":0.3982716176,"prompt-eng":0.4379310871,"data-quality":0.1031309857,"ml-security":0.1177476519}}
{"text":"Both approaches are insufficient to serve modern microsecond-scale Remote Procedure Calls (RPCs).","meta":{"url":"http://arxiv.org/abs/2307.13285v1"},"cats":{"new-dataset":0.0290092513,"dev-research":0.2868577822,"prompt-eng":0.3426756148,"data-quality":0.0963111123,"ml-security":0.0884600706}}
{"text":"To this end, we present NetClone, a request cloning system that performs cloning decisions dynamically within nanoseconds at scale.","meta":{"url":"http://arxiv.org/abs/2307.13285v1"},"cats":{"new-dataset":0.160958699,"dev-research":0.296470816,"prompt-eng":0.4934920825,"data-quality":0.0954450321,"ml-security":0.1549148042}}
{"text":"Rather than the client or the coordinator, NetClone performs request cloning in the network switch by leveraging the capability of programmable switch ASICs.","meta":{"url":"http://arxiv.org/abs/2307.13285v1"},"cats":{"new-dataset":0.0090881329,"dev-research":0.333726323,"prompt-eng":0.4288323135,"data-quality":0.100045163,"ml-security":0.1273740039}}
{"text":"Specifically, NetClone replicates requests based on server states and blocks redundant responses using request fingerprints in the switch data plane.","meta":{"url":"http://arxiv.org/abs/2307.13285v1"},"cats":{"new-dataset":0.0317362587,"dev-research":0.2645034633,"prompt-eng":0.4233347213,"data-quality":0.1342069128,"ml-security":0.224807304}}
{"text":"To realize the idea while satisfying the strict hardware constraints, we address several technical challenges when designing a custom switch data plane.","meta":{"url":"http://arxiv.org/abs/2307.13285v1"},"cats":{"new-dataset":0.1489348406,"dev-research":0.3006691168,"prompt-eng":0.4257088065,"data-quality":0.0966712527,"ml-security":0.0932674474}}
{"text":"NetClone can be integrated with emerging in-network request schedulers like RackSched.","meta":{"url":"http://arxiv.org/abs/2307.13285v1"},"cats":{"new-dataset":0.029839142,"dev-research":0.2702825849,"prompt-eng":0.4101538865,"data-quality":0.0857871667,"ml-security":0.123809551}}
{"text":"We implement a NetClone prototype with an Intel Tofino switch and a cluster of commodity servers.","meta":{"url":"http://arxiv.org/abs/2307.13285v1"},"cats":{"new-dataset":0.1281940003,"dev-research":0.278601512,"prompt-eng":0.4174482909,"data-quality":0.0929908643,"ml-security":0.1175550905}}
{"text":"Our experimental results show that NetClone can improve the tail latency of microsecond-scale RPCs for synthetic and real-world application workloads and is robust to various system conditions.","meta":{"url":"http://arxiv.org/abs/2307.13285v1"},"cats":{"new-dataset":0.0608270845,"dev-research":0.3214825852,"prompt-eng":0.3975713651,"data-quality":0.0867673999,"ml-security":0.1318792168}}
{"text":"We present a novel method for reconstructing clothed humans from a sparse set of, e.g., 1 to 6 RGB images.","meta":{"url":"http://arxiv.org/abs/2307.13282v1"},"cats":{"new-dataset":0.5706939063,"dev-research":0.1762331003,"prompt-eng":0.4187296485,"data-quality":0.1244648947,"ml-security":0.0796440329}}
{"text":"Despite impressive results from recent works employing deep implicit representation, we revisit the volumetric approach and demonstrate that better performance can be achieved with proper system design.","meta":{"url":"http://arxiv.org/abs/2307.13282v1"},"cats":{"new-dataset":0.0662288217,"dev-research":0.2722957096,"prompt-eng":0.3944905598,"data-quality":0.1634070568,"ml-security":0.1248786994}}
{"text":"The volumetric representation offers significant advantages in leveraging 3D spatial context through 3D convolutions, and the notorious quantization error is largely negligible with a reasonably large yet affordable volume resolution, e.g., 512.","meta":{"url":"http://arxiv.org/abs/2307.13282v1"},"cats":{"new-dataset":0.1271928502,"dev-research":0.2335522424,"prompt-eng":0.3261440441,"data-quality":0.1520863589,"ml-security":0.0771782422}}
{"text":"To handle memory and computation costs, we propose a sophisticated coarse-to-fine strategy with voxel culling and subspace sparse convolution.","meta":{"url":"http://arxiv.org/abs/2307.13282v1"},"cats":{"new-dataset":0.1337935635,"dev-research":0.2386241715,"prompt-eng":0.3578496827,"data-quality":0.1454751771,"ml-security":0.1190126194}}
{"text":"Our method starts with a discretized visual hull to compute a coarse shape and then focuses on a narrow band nearby the coarse shape for refinement.","meta":{"url":"http://arxiv.org/abs/2307.13282v1"},"cats":{"new-dataset":0.090677612,"dev-research":0.2219685567,"prompt-eng":0.3953571231,"data-quality":0.1234316086,"ml-security":0.0477781653}}
{"text":"Once the shape is reconstructed, we adopt an image-based rendering approach, which computes the colors of surface points by blending input images with learned weights.","meta":{"url":"http://arxiv.org/abs/2307.13282v1"},"cats":{"new-dataset":0.1177462498,"dev-research":0.2038949948,"prompt-eng":0.3999666683,"data-quality":0.1132710327,"ml-security":0.0869954899}}
{"text":"Extensive experimental results show that our method significantly reduces the mean point-to-surface (P2S) precision of state-of-the-art methods by more than 50% to achieve approximately 2mm accuracy with a 512 volume resolution.","meta":{"url":"http://arxiv.org/abs/2307.13282v1"},"cats":{"new-dataset":0.0249300709,"dev-research":0.2333422364,"prompt-eng":0.398430553,"data-quality":0.1941852042,"ml-security":0.0442514082}}
{"text":"Additionally, images rendered from our textured model achieve a higher peak signal-to-noise ratio (PSNR) compared to state-of-the-art methods.","meta":{"url":"http://arxiv.org/abs/2307.13282v1"},"cats":{"new-dataset":0.1210237727,"dev-research":0.2133617356,"prompt-eng":0.4451296526,"data-quality":0.192254947,"ml-security":0.0731530514}}
{"text":"The prediction of molecular properties is one of the most important and challenging tasks in the field of artificial intelligence-based drug design.","meta":{"url":"http://arxiv.org/abs/2307.13275v1"},"cats":{"new-dataset":0.0371115197,"dev-research":0.1977313167,"prompt-eng":0.3699362949,"data-quality":0.1084982244,"ml-security":0.161328054}}
{"text":"Among the current mainstream methods, the most commonly used feature representation for training DNN models is based on SMILES and molecular graphs, although these methods are concise and effective, they also limit the ability to capture spatial information.","meta":{"url":"http://arxiv.org/abs/2307.13275v1"},"cats":{"new-dataset":0.104242027,"dev-research":0.3199126426,"prompt-eng":0.3408997511,"data-quality":0.2036761252,"ml-security":0.1575427408}}
{"text":"In this work, we propose Curvature-based Transformer to improve the ability of Graph Transformer neural network models to extract structural information on molecular graph data by introducing Discretization of Ricci Curvature.","meta":{"url":"http://arxiv.org/abs/2307.13275v1"},"cats":{"new-dataset":0.0560593293,"dev-research":0.1606982357,"prompt-eng":0.3033277238,"data-quality":0.1416914747,"ml-security":0.0950822423}}
{"text":"To embed the curvature in the model, we add the curvature information of the graph as positional Encoding to the node features during the attention-score calculation.","meta":{"url":"http://arxiv.org/abs/2307.13275v1"},"cats":{"new-dataset":0.0517761433,"dev-research":0.1903313961,"prompt-eng":0.4216148762,"data-quality":0.1868773957,"ml-security":0.0743426362}}
{"text":"This method can introduce curvature information from graph data without changing the original network architecture, and it has the potential to be extended to other models.","meta":{"url":"http://arxiv.org/abs/2307.13275v1"},"cats":{"new-dataset":0.0221056541,"dev-research":0.1850903753,"prompt-eng":0.3477807944,"data-quality":0.1565763986,"ml-security":0.1031236123}}
{"text":"We performed experiments on chemical molecular datasets including PCQM4M-LST, MoleculeNet and compared with models such as Uni-Mol, Graphormer, and the results show that this method can achieve the state-of-the-art results.","meta":{"url":"http://arxiv.org/abs/2307.13275v1"},"cats":{"new-dataset":0.1865830724,"dev-research":0.2112203711,"prompt-eng":0.3558386652,"data-quality":0.2047513805,"ml-security":0.0953051975}}
{"text":"It is proved that the discretized Ricci curvature also reflects the structural and functional relationship while describing the local geometry of the graph molecular data.","meta":{"url":"http://arxiv.org/abs/2307.13275v1"},"cats":{"new-dataset":0.0712815705,"dev-research":0.181635802,"prompt-eng":0.3275347566,"data-quality":0.1364710094,"ml-security":0.0499661262}}
{"text":"The engineering community currently encounters significant challenges in the development of intelligent transportation algorithms that can be transferred from simulation to reality with minimal effort.","meta":{"url":"http://arxiv.org/abs/2307.13272v1"},"cats":{"new-dataset":0.0548622123,"dev-research":0.2843889048,"prompt-eng":0.3578613487,"data-quality":0.1166573706,"ml-security":0.1242337907}}
{"text":"This can be achieved by robustifying the algorithms using domain adaptation methods and/or by adopting cutting-edge tools that help support this objective seamlessly.","meta":{"url":"http://arxiv.org/abs/2307.13272v1"},"cats":{"new-dataset":0.0393798495,"dev-research":0.2702945978,"prompt-eng":0.4523097827,"data-quality":0.3144802371,"ml-security":0.1470781985}}
{"text":"This work presents AutoDRIVE, an openly accessible digital twin ecosystem designed to facilitate synergistic development, simulation and deployment of cyber-physical solutions pertaining to autonomous driving technology; and focuses on bridging the autonomy-oriented simulation-to-reality (sim2real) gap using the proposed ecosystem.","meta":{"url":"http://arxiv.org/abs/2307.13272v1"},"cats":{"new-dataset":0.2116143434,"dev-research":0.3330272864,"prompt-eng":0.3986375497,"data-quality":0.0951581953,"ml-security":0.1210776771}}
{"text":"In this paper, we extensively explore the modeling and simulation aspects of the ecosystem and substantiate its efficacy by demonstrating the successful transition of two candidate autonomy algorithms from simulation to reality to help support our claims: (i) autonomous parking using probabilistic robotics approach; (ii) behavioral cloning using deep imitation learning.","meta":{"url":"http://arxiv.org/abs/2307.13272v1"},"cats":{"new-dataset":0.1111342036,"dev-research":0.2133114656,"prompt-eng":0.3886692415,"data-quality":0.1180354253,"ml-security":0.1295831113}}
{"text":"The outcomes of these case studies further strengthen the credibility of AutoDRIVE as an invaluable tool for advancing the state-of-the-art in autonomous driving technology.","meta":{"url":"http://arxiv.org/abs/2307.13272v1"},"cats":{"new-dataset":0.0332651422,"dev-research":0.3308997843,"prompt-eng":0.4007799715,"data-quality":0.20335353,"ml-security":0.1715670093}}
{"text":"A biologically plausible method for training an Artificial Neural Network (ANN) involves treating each unit as a stochastic Reinforcement Learning (RL) agent, thereby considering the network as a team of agents.","meta":{"url":"http://arxiv.org/abs/2307.13270v1"},"cats":{"new-dataset":0.0502820727,"dev-research":0.2334399367,"prompt-eng":0.3553128363,"data-quality":0.1865270995,"ml-security":0.259525697}}
{"text":"Consequently, all units can learn via REINFORCE, a local learning rule modulated by a global reward signal, which aligns more closely with biologically observed forms of synaptic plasticity.","meta":{"url":"http://arxiv.org/abs/2307.13270v1"},"cats":{"new-dataset":0.0333668657,"dev-research":0.2426370011,"prompt-eng":0.3892420966,"data-quality":0.1724570041,"ml-security":0.198581264}}
{"text":"Nevertheless, this learning method is often slow and scales poorly with network size due to inefficient structural credit assignment, since a single reward signal is broadcast to all units without considering individual contributions.","meta":{"url":"http://arxiv.org/abs/2307.13270v1"},"cats":{"new-dataset":0.0172889667,"dev-research":0.1949152358,"prompt-eng":0.3647181722,"data-quality":0.2120298619,"ml-security":0.1868824606}}
{"text":"Weight Maximization, a proposed solution, replaces a unit's reward signal with the norm of its outgoing weight, thereby allowing each hidden unit to maximize the norm of the outgoing weight instead of the global reward signal.","meta":{"url":"http://arxiv.org/abs/2307.13270v1"},"cats":{"new-dataset":0.0113712712,"dev-research":0.2129400756,"prompt-eng":0.4383241731,"data-quality":0.1396986799,"ml-security":0.2782407551}}
{"text":"In this research report, we analyze the theoretical properties of Weight Maximization and propose a variant, Unbiased Weight Maximization.","meta":{"url":"http://arxiv.org/abs/2307.13270v1"},"cats":{"new-dataset":0.0230642078,"dev-research":0.1407078074,"prompt-eng":0.3972694258,"data-quality":0.145022899,"ml-security":0.1941807776}}
{"text":"This new approach provides an unbiased learning rule that increases learning speed and improves asymptotic performance.","meta":{"url":"http://arxiv.org/abs/2307.13270v1"},"cats":{"new-dataset":0.0347535989,"dev-research":0.1938702875,"prompt-eng":0.3619041678,"data-quality":0.2122384422,"ml-security":0.2167379065}}
{"text":"Notably, to our knowledge, this is the first learning rule for a network of Bernoulli-logistic units that is unbiased and scales well with the number of network's units in terms of learning speed.","meta":{"url":"http://arxiv.org/abs/2307.13270v1"},"cats":{"new-dataset":0.0398200806,"dev-research":0.1920895095,"prompt-eng":0.3245559239,"data-quality":0.1889077358,"ml-security":0.1988464749}}
{"text":"Low-rank adaptations (LoRA) are often employed to fine-tune large language models (LLMs) for new tasks.","meta":{"url":"http://arxiv.org/abs/2307.13269v1"},"cats":{"new-dataset":0.0659349691,"dev-research":0.1744682579,"prompt-eng":0.4033348836,"data-quality":0.1888236317,"ml-security":0.1091356405}}
{"text":"This paper investigates LoRA composability for cross-task generalization and introduces LoraHub, a strategic framework devised for the purposive assembly of LoRA modules trained on diverse given tasks, with the objective of achieving adaptable performance on unseen tasks.","meta":{"url":"http://arxiv.org/abs/2307.13269v1"},"cats":{"new-dataset":0.0740709161,"dev-research":0.2691582072,"prompt-eng":0.4294447089,"data-quality":0.1256950892,"ml-security":0.1219773228}}
{"text":"With just a few examples from a novel task, LoraHub enables the fluid combination of multiple LoRA modules, eradicating the need for human expertise.","meta":{"url":"http://arxiv.org/abs/2307.13269v1"},"cats":{"new-dataset":0.0742164225,"dev-research":0.2607126028,"prompt-eng":0.380338583,"data-quality":0.1001242934,"ml-security":0.0912083806}}
{"text":"Notably, the composition requires neither additional model parameters nor gradients.","meta":{"url":"http://arxiv.org/abs/2307.13269v1"},"cats":{"new-dataset":0.0030477084,"dev-research":0.1433011673,"prompt-eng":0.3005883871,"data-quality":0.1551049768,"ml-security":0.1014825968}}
{"text":"Our empirical results, derived from the Big-Bench Hard (BBH) benchmark, suggest that LoraHub can effectively mimic the performance of in-context learning in few-shot scenarios, excluding the necessity of in-context examples alongside each inference input.","meta":{"url":"http://arxiv.org/abs/2307.13269v1"},"cats":{"new-dataset":0.1163771263,"dev-research":0.2383373791,"prompt-eng":0.3682967527,"data-quality":0.2107501874,"ml-security":0.1488520088}}
{"text":"A significant contribution of our research is the fostering of a community for LoRA, where users can share their trained LoRA modules, thereby facilitating their application to new tasks.","meta":{"url":"http://arxiv.org/abs/2307.13269v1"},"cats":{"new-dataset":0.2151720573,"dev-research":0.3347742121,"prompt-eng":0.4126712037,"data-quality":0.1254310433,"ml-security":0.1346365961}}
{"text":"We anticipate this resource will widen access to and spur advancements in general intelligence as well as LLMs in production.","meta":{"url":"http://arxiv.org/abs/2307.13269v1"},"cats":{"new-dataset":0.110323691,"dev-research":0.241598213,"prompt-eng":0.3921979784,"data-quality":0.1306717218,"ml-security":0.1469760446}}
{"text":"Code will be available at https://github.com/sail-sg/lorahub.","meta":{"url":"http://arxiv.org/abs/2307.13269v1"},"cats":{"new-dataset":0.3992378441,"dev-research":0.3067609451,"prompt-eng":0.43313634,"data-quality":0.1345844281,"ml-security":0.0737690796}}
{"text":"Distributed collaborative machine learning (DCML) is a promising method in the Internet of Things (IoT) domain for training deep learning models, as data is distributed across multiple devices.","meta":{"url":"http://arxiv.org/abs/2307.13266v1"},"cats":{"new-dataset":0.108783903,"dev-research":0.2406998333,"prompt-eng":0.3422914048,"data-quality":0.1156090961,"ml-security":0.1726842472}}
{"text":"A key advantage of this approach is that it improves data privacy by removing the necessity for the centralized aggregation of raw data but also empowers IoT devices with low computational power.","meta":{"url":"http://arxiv.org/abs/2307.13266v1"},"cats":{"new-dataset":0.0139738874,"dev-research":0.329565019,"prompt-eng":0.3005406457,"data-quality":0.0676460973,"ml-security":0.2108434777}}
{"text":"Among various techniques in a DCML framework, federated split learning, known as splitfed learning (SFL), is the most suitable for efficient training and testing when devices have limited computational capabilities.","meta":{"url":"http://arxiv.org/abs/2307.13266v1"},"cats":{"new-dataset":0.0689131112,"dev-research":0.2918702823,"prompt-eng":0.3985051904,"data-quality":0.1592892504,"ml-security":0.1596444361}}
{"text":"Nevertheless, when resource-constrained IoT devices have only positive labeled data, multiclass classification deep learning models in SFL fail to converge or provide suboptimal results.","meta":{"url":"http://arxiv.org/abs/2307.13266v1"},"cats":{"new-dataset":0.0814802772,"dev-research":0.1837249636,"prompt-eng":0.3364916829,"data-quality":0.2804066946,"ml-security":0.3248887395}}
{"text":"To overcome these challenges, we propose splitfed learning with positive labels (SFPL).","meta":{"url":"http://arxiv.org/abs/2307.13266v1"},"cats":{"new-dataset":0.0963357829,"dev-research":0.2523150103,"prompt-eng":0.4197678097,"data-quality":0.4264532958,"ml-security":0.1230564284}}
{"text":"SFPL applies a random shuffling function to the smashed data received from clients before supplying it to the server for model training.","meta":{"url":"http://arxiv.org/abs/2307.13266v1"},"cats":{"new-dataset":0.1614377349,"dev-research":0.2711187741,"prompt-eng":0.403369961,"data-quality":0.1926415044,"ml-security":0.2255652412}}
{"text":"Additionally, SFPL incorporates the local batch normalization for the client-side model portion during the inference phase.","meta":{"url":"http://arxiv.org/abs/2307.13266v1"},"cats":{"new-dataset":0.088466416,"dev-research":0.2600219955,"prompt-eng":0.4662445016,"data-quality":0.1609691068,"ml-security":0.0824367007}}
{"text":"Our results demonstrate that SFPL outperforms SFL: (i) by factors of 51.54 and 32.57 for ResNet-56 and ResNet-32, respectively, with the CIFAR-100 dataset, and (ii) by factors of 9.23 and 8.52 for ResNet-32 and ResNet-8, respectively, with CIFAR-10 dataset.","meta":{"url":"http://arxiv.org/abs/2307.13266v1"},"cats":{"new-dataset":0.3981686724,"dev-research":0.2990492943,"prompt-eng":0.3812154366,"data-quality":0.1564736056,"ml-security":0.0831264587}}
{"text":"Overall, this investigation underscores the efficacy of the proposed SFPL framework in DCML.","meta":{"url":"http://arxiv.org/abs/2307.13266v1"},"cats":{"new-dataset":0.1407967936,"dev-research":0.3779177198,"prompt-eng":0.4009717929,"data-quality":0.1328380153,"ml-security":0.0658039337}}
{"text":"The maximum independent set problem is a classical NP-hard problem in theoretical computer science.","meta":{"url":"http://arxiv.org/abs/2307.13261v1"},"cats":{"new-dataset":0.1700256097,"dev-research":0.2455645068,"prompt-eng":0.3129857493,"data-quality":0.1355545088,"ml-security":0.1907985572}}
{"text":"In this work, we study a special case where the family of graphs considered is restricted to intersection graphs of sets of axis-aligned hyperrectangles and the input is provided in an online fashion.","meta":{"url":"http://arxiv.org/abs/2307.13261v1"},"cats":{"new-dataset":0.3760921756,"dev-research":0.2434902445,"prompt-eng":0.3453103196,"data-quality":0.2038599457,"ml-security":0.0814343573}}
{"text":"We prove bounds on the competitive ratio of an optimal online algorithm under the adaptive offline, adaptive online, and oblivious adversary models, for several classes of hyperrectangles and restrictions on the order of the input.   ","meta":{"url":"http://arxiv.org/abs/2307.13261v1"},"cats":{"new-dataset":0.0374676463,"dev-research":0.1633063703,"prompt-eng":0.3273676157,"data-quality":0.1423518031,"ml-security":0.3597502319}}
{"text":"We are the first to present results on this problem under the oblivious adversary model.","meta":{"url":"http://arxiv.org/abs/2307.13261v1"},"cats":{"new-dataset":0.0753594141,"dev-research":0.1520633497,"prompt-eng":0.379670463,"data-quality":0.167489346,"ml-security":0.4809460177}}
{"text":"We prove bounds on the competitive ratio for unit hypercubes, $\\sigma$-bounded hypercubes, unit-volume hypercubes, arbitrary hypercubes, and arbitrary hyperrectangles, in both arbitrary and non-dominated order.","meta":{"url":"http://arxiv.org/abs/2307.13261v1"},"cats":{"new-dataset":0.0936204599,"dev-research":0.1628890458,"prompt-eng":0.3301471493,"data-quality":0.1463829515,"ml-security":0.144052953}}
{"text":"We are also the first to present results under the adaptive offline and adaptive online adversary models with input in non-dominated order, proving bounds on the competitive ratio for the same classes of hyperrectangles; for input in arbitrary order, we present the first results on $\\sigma$-bounded hypercubes, unit-volume hyperrectangles, arbitrary hypercubes, and arbitrary hyperrectangles.","meta":{"url":"http://arxiv.org/abs/2307.13261v1"},"cats":{"new-dataset":0.0488413879,"dev-research":0.1705172036,"prompt-eng":0.362564165,"data-quality":0.1871513217,"ml-security":0.4495432251}}
{"text":"For input in dominating order, we show that the performance of the naive greedy algorithm matches the performance of an optimal offline algorithm in all cases.","meta":{"url":"http://arxiv.org/abs/2307.13261v1"},"cats":{"new-dataset":0.0382192543,"dev-research":0.2156013402,"prompt-eng":0.3825877582,"data-quality":0.1512882194,"ml-security":0.2632702406}}
{"text":"We also give lower bounds on the competitive ratio of a probabilistic greedy algorithm under the oblivious adversary model.","meta":{"url":"http://arxiv.org/abs/2307.13261v1"},"cats":{"new-dataset":0.042765516,"dev-research":0.171552372,"prompt-eng":0.3606952041,"data-quality":0.1186517535,"ml-security":0.4579426344}}
{"text":"We conclude by discussing several promising directions for future work.","meta":{"url":"http://arxiv.org/abs/2307.13261v1"},"cats":{"new-dataset":0.03925313,"dev-research":0.2412026472,"prompt-eng":0.4260885946,"data-quality":0.1177180791,"ml-security":0.084627092}}
{"text":"Gait recognition aims to distinguish different walking patterns by analyzing video-level human silhouettes, rather than relying on appearance information.","meta":{"url":"http://arxiv.org/abs/2307.13259v1"},"cats":{"new-dataset":0.0562935922,"dev-research":0.233263957,"prompt-eng":0.3893943063,"data-quality":0.1718538523,"ml-security":0.0825195834}}
{"text":"Previous research on gait recognition has primarily focused on extracting local or global spatial-temporal representations, while overlooking the intrinsic periodic features of gait sequences, which, when fully utilized, can significantly enhance performance.","meta":{"url":"http://arxiv.org/abs/2307.13259v1"},"cats":{"new-dataset":0.092809292,"dev-research":0.2315360188,"prompt-eng":0.3739648943,"data-quality":0.1154349892,"ml-security":0.0913845155}}
{"text":"In this work, we propose a plug-and-play strategy, called Temporal Periodic Alignment (TPA), which leverages the periodic nature and fine-grained temporal dependencies of gait patterns.","meta":{"url":"http://arxiv.org/abs/2307.13259v1"},"cats":{"new-dataset":0.2173225295,"dev-research":0.2570421677,"prompt-eng":0.4371780886,"data-quality":0.0935121342,"ml-security":0.0745843093}}
{"text":"The TPA strategy comprises two key components.","meta":{"url":"http://arxiv.org/abs/2307.13259v1"},"cats":{"new-dataset":0.0256838742,"dev-research":0.2051530155,"prompt-eng":0.3514715015,"data-quality":0.0529227914,"ml-security":0.0925148097}}
{"text":"The first component is Adaptive Fourier-transform Position Encoding (AFPE), which adaptively converts features and discrete-time signals into embeddings that are sensitive to periodic walking patterns.","meta":{"url":"http://arxiv.org/abs/2307.13259v1"},"cats":{"new-dataset":0.0711509679,"dev-research":0.2510622634,"prompt-eng":0.3883023326,"data-quality":0.1322281422,"ml-security":0.0631016643}}
{"text":"The second component is the Temporal Aggregation Module (TAM), which separates embeddings into trend and seasonal components, and extracts meaningful temporal correlations to identify primary components, while filtering out random noise.","meta":{"url":"http://arxiv.org/abs/2307.13259v1"},"cats":{"new-dataset":0.179109628,"dev-research":0.2584888425,"prompt-eng":0.3922352381,"data-quality":0.1949963422,"ml-security":0.076954836}}
{"text":"We present a simple and effective baseline method for gait recognition, based on the TPA strategy.","meta":{"url":"http://arxiv.org/abs/2307.13259v1"},"cats":{"new-dataset":0.0967820358,"dev-research":0.2032487617,"prompt-eng":0.3993075801,"data-quality":0.1190176107,"ml-security":0.0764212366}}
{"text":"Extensive experiments conducted on three popular public datasets (CASIA-B, OU-MVLP, and GREW) demonstrate that our proposed method achieves state-of-the-art performance on multiple benchmark tests.","meta":{"url":"http://arxiv.org/abs/2307.13259v1"},"cats":{"new-dataset":0.4019895904,"dev-research":0.2484115769,"prompt-eng":0.4172872951,"data-quality":0.2416949319,"ml-security":0.1199171857}}
{"text":"A biologically plausible method for training an Artificial Neural Network (ANN) involves treating each unit as a stochastic Reinforcement Learning (RL) agent, thereby considering the network as a team of agents.","meta":{"url":"http://arxiv.org/abs/2307.13256v1"},"cats":{"new-dataset":0.0502820727,"dev-research":0.2334399367,"prompt-eng":0.3553128363,"data-quality":0.1865270995,"ml-security":0.259525697}}
{"text":"Consequently, all units can learn via REINFORCE, a local learning rule modulated by a global reward signal, which aligns more closely with biologically observed forms of synaptic plasticity.","meta":{"url":"http://arxiv.org/abs/2307.13256v1"},"cats":{"new-dataset":0.0333668657,"dev-research":0.2426370011,"prompt-eng":0.3892420966,"data-quality":0.1724570041,"ml-security":0.198581264}}
{"text":"However, this learning method tends to be slow and does not scale well with the size of the network.","meta":{"url":"http://arxiv.org/abs/2307.13256v1"},"cats":{"new-dataset":0.0105254705,"dev-research":0.2214787122,"prompt-eng":0.3191250972,"data-quality":0.1648288984,"ml-security":0.1470092051}}
{"text":"This inefficiency arises from two factors impeding effective structural credit assignment: (i) all units independently explore the network, and (ii) a single reward is used to evaluate the actions of all units.","meta":{"url":"http://arxiv.org/abs/2307.13256v1"},"cats":{"new-dataset":0.0063200648,"dev-research":0.2254765832,"prompt-eng":0.3450077072,"data-quality":0.1512811334,"ml-security":0.0965760795}}
{"text":"Accordingly, methods aimed at improving structural credit assignment can generally be classified into two categories.","meta":{"url":"http://arxiv.org/abs/2307.13256v1"},"cats":{"new-dataset":0.0086787698,"dev-research":0.2785253765,"prompt-eng":0.3877001175,"data-quality":0.2341721043,"ml-security":0.1132286033}}
{"text":"The first category includes algorithms that enable coordinated exploration among units, such as MAP propagation.","meta":{"url":"http://arxiv.org/abs/2307.13256v1"},"cats":{"new-dataset":0.0700133414,"dev-research":0.2880583601,"prompt-eng":0.3830279916,"data-quality":0.0979952796,"ml-security":0.0664930185}}
{"text":"The second category encompasses algorithms that compute a more specific reward signal for each unit within the network, like Weight Maximization and its variants.","meta":{"url":"http://arxiv.org/abs/2307.13256v1"},"cats":{"new-dataset":0.0166960835,"dev-research":0.2453071111,"prompt-eng":0.3614016316,"data-quality":0.1305310925,"ml-security":0.1409908989}}
{"text":"In this research report, our focus is on the first category.","meta":{"url":"http://arxiv.org/abs/2307.13256v1"},"cats":{"new-dataset":0.2107603823,"dev-research":0.2359876475,"prompt-eng":0.3802379476,"data-quality":0.122339812,"ml-security":0.0478802448}}
{"text":"We propose the use of Boltzmann machines or a recurrent network for coordinated exploration.","meta":{"url":"http://arxiv.org/abs/2307.13256v1"},"cats":{"new-dataset":0.1141582804,"dev-research":0.2036304336,"prompt-eng":0.4326182389,"data-quality":0.0947106772,"ml-security":0.1053900348}}
{"text":"We show that the negative phase, which is typically necessary to train Boltzmann machines, can be removed.","meta":{"url":"http://arxiv.org/abs/2307.13256v1"},"cats":{"new-dataset":0.0215902466,"dev-research":0.1601240916,"prompt-eng":0.3883699516,"data-quality":0.1586468349,"ml-security":0.2030858672}}
{"text":"The resulting learning rules are similar to the reward-modulated Hebbian learning rule.","meta":{"url":"http://arxiv.org/abs/2307.13256v1"},"cats":{"new-dataset":0.0116259791,"dev-research":0.1834586635,"prompt-eng":0.3899608406,"data-quality":0.1523931717,"ml-security":0.1673263259}}
{"text":"Experimental results demonstrate that coordinated exploration significantly exceeds independent exploration in training speed for multiple stochastic and discrete units based on REINFORCE, even surpassing straight-through estimator (STE) backpropagation.","meta":{"url":"http://arxiv.org/abs/2307.13256v1"},"cats":{"new-dataset":0.0241124607,"dev-research":0.209626149,"prompt-eng":0.4093863184,"data-quality":0.1495230761,"ml-security":0.1719479852}}
{"text":"Many studies in vision tasks have aimed to create effective embedding spaces for single-label object prediction within an image.","meta":{"url":"http://arxiv.org/abs/2307.13254v1"},"cats":{"new-dataset":0.0366220709,"dev-research":0.2018856815,"prompt-eng":0.4243870774,"data-quality":0.3703652356,"ml-security":0.11129235}}
{"text":"However, in reality, most objects possess multiple specific attributes, such as shape, color, and length, with each attribute composed of various classes.","meta":{"url":"http://arxiv.org/abs/2307.13254v1"},"cats":{"new-dataset":0.0228204681,"dev-research":0.2460428505,"prompt-eng":0.3419543949,"data-quality":0.1264524772,"ml-security":0.1244959799}}
{"text":"To apply models in real-world scenarios, it is essential to be able to distinguish between the granular components of an object.","meta":{"url":"http://arxiv.org/abs/2307.13254v1"},"cats":{"new-dataset":0.0098566914,"dev-research":0.2561552192,"prompt-eng":0.4151155172,"data-quality":0.1258258486,"ml-security":0.1196510105}}
{"text":"Conventional approaches to embedding multiple specific attributes into a single network often result in entanglement, where fine-grained features of each attribute cannot be identified separately.","meta":{"url":"http://arxiv.org/abs/2307.13254v1"},"cats":{"new-dataset":0.0257178266,"dev-research":0.2293057467,"prompt-eng":0.3706270907,"data-quality":0.2455474178,"ml-security":0.1812866957}}
{"text":"To address this problem, we propose a Conditional Cross-Attention Network that induces disentangled multi-space embeddings for various specific attributes with only a single backbone.","meta":{"url":"http://arxiv.org/abs/2307.13254v1"},"cats":{"new-dataset":0.1018249207,"dev-research":0.1679429847,"prompt-eng":0.4089923099,"data-quality":0.1821926912,"ml-security":0.1477930358}}
{"text":"Firstly, we employ a cross-attention mechanism to fuse and switch the information of conditions (specific attributes), and we demonstrate its effectiveness through a diverse visualization example.","meta":{"url":"http://arxiv.org/abs/2307.13254v1"},"cats":{"new-dataset":0.105002388,"dev-research":0.3718527874,"prompt-eng":0.5003784065,"data-quality":0.1481079025,"ml-security":0.1061891277}}
{"text":"Secondly, we leverage the vision transformer for the first time to a fine-grained image retrieval task and present a simple yet effective framework compared to existing methods.","meta":{"url":"http://arxiv.org/abs/2307.13254v1"},"cats":{"new-dataset":0.2257205924,"dev-research":0.2039949525,"prompt-eng":0.4181829704,"data-quality":0.234345019,"ml-security":0.0625762767}}
{"text":"Unlike previous studies where performance varied depending on the benchmark dataset, our proposed method achieved consistent state-of-the-art performance on the FashionAI, DARN, DeepFashion, and Zappos50K benchmark datasets.","meta":{"url":"http://arxiv.org/abs/2307.13254v1"},"cats":{"new-dataset":0.3954561045,"dev-research":0.2143443913,"prompt-eng":0.4016192124,"data-quality":0.2495252122,"ml-security":0.088616936}}
{"text":"Instance segmentation on 3D point clouds (3DIS) is a longstanding challenge in computer vision, where state-of-the-art methods are mainly based on full supervision.","meta":{"url":"http://arxiv.org/abs/2307.13251v1"},"cats":{"new-dataset":0.1594839811,"dev-research":0.2475993318,"prompt-eng":0.3898528163,"data-quality":0.2009436661,"ml-security":0.115823381}}
{"text":"As annotating ground truth dense instance masks is tedious and expensive, solving 3DIS with weak supervision has become more practical.","meta":{"url":"http://arxiv.org/abs/2307.13251v1"},"cats":{"new-dataset":0.1146852009,"dev-research":0.3158285988,"prompt-eng":0.4203962173,"data-quality":0.2542045858,"ml-security":0.1800077327}}
{"text":"In this paper, we propose GaPro, a new instance segmentation for 3D point clouds using axis-aligned 3D bounding box supervision.","meta":{"url":"http://arxiv.org/abs/2307.13251v1"},"cats":{"new-dataset":0.1849803764,"dev-research":0.2523944806,"prompt-eng":0.3747164433,"data-quality":0.1461420592,"ml-security":0.0971494477}}
{"text":"Our two-step approach involves generating pseudo labels from box annotations and training a 3DIS network with the resulting labels.","meta":{"url":"http://arxiv.org/abs/2307.13251v1"},"cats":{"new-dataset":0.2993091763,"dev-research":0.285541203,"prompt-eng":0.4504702921,"data-quality":0.3685842901,"ml-security":0.1076844571}}
{"text":"Additionally, we employ the self-training strategy to improve the performance of our method further.","meta":{"url":"http://arxiv.org/abs/2307.13251v1"},"cats":{"new-dataset":0.0184626098,"dev-research":0.3020673129,"prompt-eng":0.4028713915,"data-quality":0.1316927478,"ml-security":0.1126224003}}
{"text":"We devise an effective Gaussian Process to generate pseudo instance masks from the bounding boxes and resolve ambiguities when they overlap, resulting in pseudo instance masks with their uncertainty values.","meta":{"url":"http://arxiv.org/abs/2307.13251v1"},"cats":{"new-dataset":0.1128878055,"dev-research":0.2373745327,"prompt-eng":0.4598346796,"data-quality":0.2178055791,"ml-security":0.165198982}}
{"text":"Our experiments show that GaPro outperforms previous weakly supervised 3D instance segmentation methods and has competitive performance compared to state-of-the-art fully supervised ones.","meta":{"url":"http://arxiv.org/abs/2307.13251v1"},"cats":{"new-dataset":0.0894836107,"dev-research":0.2127374412,"prompt-eng":0.3945273327,"data-quality":0.3018910081,"ml-security":0.1065420634}}
{"text":"Furthermore, we demonstrate the robustness of our approach, where we can adapt various state-of-the-art fully supervised methods to the weak supervision task by using our pseudo labels for training.","meta":{"url":"http://arxiv.org/abs/2307.13251v1"},"cats":{"new-dataset":0.1416366517,"dev-research":0.2391675113,"prompt-eng":0.5078384522,"data-quality":0.5781656055,"ml-security":0.2387347897}}
{"text":"The source code and trained models are available at https://github.com/VinAIResearch/GaPro.","meta":{"url":"http://arxiv.org/abs/2307.13251v1"},"cats":{"new-dataset":0.5066274092,"dev-research":0.2347907893,"prompt-eng":0.4087741628,"data-quality":0.1487479824,"ml-security":0.0943569847}}
{"text":"The main challenge in video question answering (VideoQA) is to capture and understand the complex spatial and temporal relations between objects based on given questions.","meta":{"url":"http://arxiv.org/abs/2307.13250v1"},"cats":{"new-dataset":0.3388639416,"dev-research":0.2427019247,"prompt-eng":0.385624362,"data-quality":0.1599690025,"ml-security":0.0632059299}}
{"text":"Existing graph-based methods for VideoQA usually ignore keywords in questions and employ a simple graph to aggregate features without considering relative relations between objects, which may lead to inferior performance.","meta":{"url":"http://arxiv.org/abs/2307.13250v1"},"cats":{"new-dataset":0.0965845994,"dev-research":0.3324453705,"prompt-eng":0.3684282583,"data-quality":0.2850923995,"ml-security":0.0547840653}}
{"text":"In this paper, we propose a Keyword-aware Relative Spatio-Temporal (KRST) graph network for VideoQA.","meta":{"url":"http://arxiv.org/abs/2307.13250v1"},"cats":{"new-dataset":0.335570248,"dev-research":0.2725235922,"prompt-eng":0.3107194226,"data-quality":0.2421035269,"ml-security":0.0584256162}}
{"text":"First, to make question features aware of keywords, we employ an attention mechanism to assign high weights to keywords during question encoding.","meta":{"url":"http://arxiv.org/abs/2307.13250v1"},"cats":{"new-dataset":0.0401640226,"dev-research":0.3385518816,"prompt-eng":0.510852278,"data-quality":0.3411808858,"ml-security":0.1569335126}}
{"text":"The keyword-aware question features are then used to guide video graph construction.","meta":{"url":"http://arxiv.org/abs/2307.13250v1"},"cats":{"new-dataset":0.1077571152,"dev-research":0.4027710596,"prompt-eng":0.3901917971,"data-quality":0.2628352012,"ml-security":0.052547889}}
{"text":"Second, because relations are relative, we integrate the relative relation modeling to better capture the spatio-temporal dynamics among object nodes.","meta":{"url":"http://arxiv.org/abs/2307.13250v1"},"cats":{"new-dataset":0.133614691,"dev-research":0.2515417759,"prompt-eng":0.3972140692,"data-quality":0.1458924645,"ml-security":0.0628653882}}
{"text":"Moreover, we disentangle the spatio-temporal reasoning into an object-level spatial graph and a frame-level temporal graph, which reduces the impact of spatial and temporal relation reasoning on each other.","meta":{"url":"http://arxiv.org/abs/2307.13250v1"},"cats":{"new-dataset":0.1661539258,"dev-research":0.3575023676,"prompt-eng":0.377379527,"data-quality":0.1190008413,"ml-security":0.0928397044}}
{"text":"Extensive experiments on the TGIF-QA, MSVD-QA and MSRVTT-QA datasets demonstrate the superiority of our KRST over multiple state-of-the-art methods.","meta":{"url":"http://arxiv.org/abs/2307.13250v1"},"cats":{"new-dataset":0.1770067591,"dev-research":0.2264828947,"prompt-eng":0.415823415,"data-quality":0.1959227874,"ml-security":0.0694929895}}
{"text":"We introduce a new paradigm for game theory -- Bayesian satisfaction.","meta":{"url":"http://arxiv.org/abs/2307.13247v1"},"cats":{"new-dataset":0.0248407466,"dev-research":0.2373317469,"prompt-eng":0.4486996739,"data-quality":0.1268699458,"ml-security":0.1056650281}}
{"text":"This novel approach is a synthesis of the idea of Bayesian rationality introduced by Aumann, and satisfaction games.","meta":{"url":"http://arxiv.org/abs/2307.13247v1"},"cats":{"new-dataset":0.0464405181,"dev-research":0.241254693,"prompt-eng":0.4573711875,"data-quality":0.1507374022,"ml-security":0.1383778979}}
{"text":"The concept of Bayesian rationality for which, in part, Robert Aumann was awarded the Nobel Prize in 2005, is concerned with players in a game acting in their own best interest given a subjective knowledge of the other players' behaviours as represented by a probability distribution.","meta":{"url":"http://arxiv.org/abs/2307.13247v1"},"cats":{"new-dataset":0.0341302059,"dev-research":0.2494000153,"prompt-eng":0.3816556498,"data-quality":0.1287965917,"ml-security":0.1945340186}}
{"text":"Satisfaction games have emerged in the engineering literature as a way of modelling competitive interactions in resource allocation problems where players seek to attain a specified level of utility, rather than trying to maximise utility.","meta":{"url":"http://arxiv.org/abs/2307.13247v1"},"cats":{"new-dataset":0.0201557961,"dev-research":0.3148925416,"prompt-eng":0.4088409849,"data-quality":0.0882633105,"ml-security":0.1202807232}}
{"text":"In this paper, we explore the relationship between optimality in Aumann's sense (correlated equilibria), and satisfaction in games.","meta":{"url":"http://arxiv.org/abs/2307.13247v1"},"cats":{"new-dataset":0.0138039646,"dev-research":0.2236693725,"prompt-eng":0.3711696917,"data-quality":0.0826797741,"ml-security":0.1108193347}}
{"text":"We show that correlated equilibria in a satisfaction game represent stable outcomes in which no player can increase their probability of satisfaction by unilateral deviation from the specified behaviour.","meta":{"url":"http://arxiv.org/abs/2307.13247v1"},"cats":{"new-dataset":0.0206432477,"dev-research":0.2014208971,"prompt-eng":0.4062745524,"data-quality":0.1339928863,"ml-security":0.1398893752}}
{"text":"Thus, we propose a whole new class of equilibrium outcomes in satisfaction games which include existing notions of equilibria in such games.","meta":{"url":"http://arxiv.org/abs/2307.13247v1"},"cats":{"new-dataset":0.0386645377,"dev-research":0.1760332423,"prompt-eng":0.4066229513,"data-quality":0.1145363353,"ml-security":0.1483817078}}
{"text":"Iterative algorithms for computing such equilibria based on the existing ideas of regret matching are presented and interpreted within the satisfaction framework.","meta":{"url":"http://arxiv.org/abs/2307.13247v1"},"cats":{"new-dataset":0.0195349368,"dev-research":0.2085609869,"prompt-eng":0.428567397,"data-quality":0.1025515026,"ml-security":0.0978394766}}
{"text":"Numerical examples of resource allocation are presented to illustrate the behaviour of these algorithms.","meta":{"url":"http://arxiv.org/abs/2307.13247v1"},"cats":{"new-dataset":0.0261079847,"dev-research":0.2707655859,"prompt-eng":0.3607148264,"data-quality":0.1259251427,"ml-security":0.1274032341}}
{"text":"A notable feature of these algorithms is that they almost always find equilibrium outcomes whereas existing approaches in satisfaction games may not.","meta":{"url":"http://arxiv.org/abs/2307.13247v1"},"cats":{"new-dataset":0.0056561011,"dev-research":0.2295574507,"prompt-eng":0.3451106978,"data-quality":0.1360039459,"ml-security":0.1459979819}}
{"text":"Due to the enormous technical challenges and wide range of applications, scene text recognition (STR) has been an active research topic in computer vision for years.","meta":{"url":"http://arxiv.org/abs/2307.13244v1"},"cats":{"new-dataset":0.1051797018,"dev-research":0.212657081,"prompt-eng":0.3634436372,"data-quality":0.3150120963,"ml-security":0.0860585775}}
{"text":"To tackle this tough problem, numerous innovative methods have been successively proposed, and incorporating linguistic knowledge into STR models has recently become a prominent trend.","meta":{"url":"http://arxiv.org/abs/2307.13244v1"},"cats":{"new-dataset":0.0230044995,"dev-research":0.2437894464,"prompt-eng":0.4173093904,"data-quality":0.2548997064,"ml-security":0.0872658157}}
{"text":"In this work, we first draw inspiration from the recent progress in Vision Transformer (ViT) to construct a conceptually simple yet functionally powerful vision STR model, which is built upon ViT and a tailored Adaptive Addressing and Aggregation (A$^3$) module.","meta":{"url":"http://arxiv.org/abs/2307.13244v1"},"cats":{"new-dataset":0.0887676298,"dev-research":0.2096509921,"prompt-eng":0.4282895696,"data-quality":0.1166300407,"ml-security":0.0684941392}}
{"text":"It already outperforms most previous state-of-the-art models for scene text recognition, including both pure vision models and language-augmented methods.","meta":{"url":"http://arxiv.org/abs/2307.13244v1"},"cats":{"new-dataset":0.0797614811,"dev-research":0.1990108827,"prompt-eng":0.3581588542,"data-quality":0.3079678454,"ml-security":0.0890270865}}
{"text":"To integrate linguistic knowledge, we further propose a Multi-Granularity Prediction strategy to inject information from the language modality into the model in an implicit way, \\ie, subword representations (BPE and WordPiece) widely used in NLP are introduced into the output space, in addition to the conventional character level representation, while no independent language model (LM) is adopted.","meta":{"url":"http://arxiv.org/abs/2307.13244v1"},"cats":{"new-dataset":0.0516365717,"dev-research":0.2603252295,"prompt-eng":0.3897611475,"data-quality":0.2906301229,"ml-security":0.0996430517}}
{"text":"To produce the final recognition results, two strategies for effectively fusing the multi-granularity predictions are devised.","meta":{"url":"http://arxiv.org/abs/2307.13244v1"},"cats":{"new-dataset":0.1106396735,"dev-research":0.240003308,"prompt-eng":0.4645815963,"data-quality":0.277863567,"ml-security":0.1022103294}}
{"text":"The resultant algorithm (termed MGP-STR) is able to push the performance envelope of STR to an even higher level.","meta":{"url":"http://arxiv.org/abs/2307.13244v1"},"cats":{"new-dataset":0.0123651303,"dev-research":0.2155901175,"prompt-eng":0.4788756128,"data-quality":0.1060359844,"ml-security":0.0636917246}}
{"text":"Specifically, MGP-STR achieves an average recognition accuracy of $94\\%$ on standard benchmarks for scene text recognition.","meta":{"url":"http://arxiv.org/abs/2307.13244v1"},"cats":{"new-dataset":0.0688645306,"dev-research":0.1954830297,"prompt-eng":0.4168793653,"data-quality":0.3237067229,"ml-security":0.0579719413}}
{"text":"Moreover, it also achieves state-of-the-art results on widely-used handwritten benchmarks as well as more challenging scene text datasets, demonstrating the generality of the proposed MGP-STR algorithm.","meta":{"url":"http://arxiv.org/abs/2307.13244v1"},"cats":{"new-dataset":0.2853380476,"dev-research":0.1872171273,"prompt-eng":0.3866570515,"data-quality":0.2143156378,"ml-security":0.0575882508}}
{"text":"The source code and models will be available at: \\url{https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/OCR/MGP-STR}.","meta":{"url":"http://arxiv.org/abs/2307.13244v1"},"cats":{"new-dataset":0.4724175988,"dev-research":0.2035260807,"prompt-eng":0.4224894597,"data-quality":0.1507797419,"ml-security":0.073172734}}
{"text":"The robust balancing capability of humanoid robots against disturbances has been considered as one of the crucial requirements for their practical mobility in real-world environments.","meta":{"url":"http://arxiv.org/abs/2307.13243v1"},"cats":{"new-dataset":0.0315836766,"dev-research":0.1974719489,"prompt-eng":0.380012327,"data-quality":0.0768338215,"ml-security":0.1841675848}}
{"text":"In particular, many studies have been devoted to the efficient implementation of the three balance strategies, inspired by human balance strategies involving ankle, hip, and stepping strategies, to endow humanoid robots with human-level balancing capability.","meta":{"url":"http://arxiv.org/abs/2307.13243v1"},"cats":{"new-dataset":0.0313745664,"dev-research":0.1960364858,"prompt-eng":0.3689836242,"data-quality":0.0396340424,"ml-security":0.0888810584}}
{"text":"In this paper, a robust balance control framework for humanoid robots is proposed.","meta":{"url":"http://arxiv.org/abs/2307.13243v1"},"cats":{"new-dataset":0.0735412599,"dev-research":0.1794492831,"prompt-eng":0.361779127,"data-quality":0.0817733491,"ml-security":0.1111635894}}
{"text":"Firstly, a novel Model Predictive Control (MPC) framework is proposed for Capture Point (CP) tracking control, enabling the integration of ankle, hip, and stepping strategies within a single framework.","meta":{"url":"http://arxiv.org/abs/2307.13243v1"},"cats":{"new-dataset":0.0411761636,"dev-research":0.2633433205,"prompt-eng":0.4420722845,"data-quality":0.0720704937,"ml-security":0.1185810718}}
{"text":"Additionally, a variable weighting method is introduced that adjusts the weighting parameters of the Centroidal Angular Momentum (CAM) damping control over the time horizon of MPC to improve the balancing performance.","meta":{"url":"http://arxiv.org/abs/2307.13243v1"},"cats":{"new-dataset":0.0119896503,"dev-research":0.2487241338,"prompt-eng":0.4371886131,"data-quality":0.0941654953,"ml-security":0.0823183996}}
{"text":"Secondly, a hierarchical structure of the MPC and a stepping controller was proposed, allowing for the step time optimization.","meta":{"url":"http://arxiv.org/abs/2307.13243v1"},"cats":{"new-dataset":0.0148322683,"dev-research":0.2730657582,"prompt-eng":0.4560366633,"data-quality":0.0541248675,"ml-security":0.0410355788}}
{"text":"The robust balancing performance of the proposed method is validated through extensive simulations and real robot experiments.","meta":{"url":"http://arxiv.org/abs/2307.13243v1"},"cats":{"new-dataset":0.0056793838,"dev-research":0.2069805266,"prompt-eng":0.4022809567,"data-quality":0.1223106073,"ml-security":0.1393730273}}
{"text":"Furthermore, a superior balancing performance is demonstrated, particularly in the presence of disturbances, compared to a state-of-the-art Quadratic Programming (QP)-based CP controller that employs the ankle, hip, and stepping strategies.","meta":{"url":"http://arxiv.org/abs/2307.13243v1"},"cats":{"new-dataset":0.009425608,"dev-research":0.3284951113,"prompt-eng":0.42303388,"data-quality":0.0550018082,"ml-security":0.1034362038}}
{"text":"The supplementary video is available at https://youtu.be/CrD75UbYzdc","meta":{"url":"http://arxiv.org/abs/2307.13243v1"},"cats":{"new-dataset":0.2792994323,"dev-research":0.2046963142,"prompt-eng":0.3657248453,"data-quality":0.1412605816,"ml-security":0.0810591345}}
{"text":"We study the open question of how players learn to play a social optimum pure-strategy Nash equilibrium (PSNE) through repeated interactions in general-sum coordination games.","meta":{"url":"http://arxiv.org/abs/2307.13242v1"},"cats":{"new-dataset":0.0311111356,"dev-research":0.2213857625,"prompt-eng":0.319905697,"data-quality":0.0599897629,"ml-security":0.2029766748}}
{"text":"A social optimum of a game is the stable Pareto-optimal state that provides a maximum return in the sum of all players' payoffs (social welfare) and always exists.","meta":{"url":"http://arxiv.org/abs/2307.13242v1"},"cats":{"new-dataset":0.0369437701,"dev-research":0.2039516627,"prompt-eng":0.3424359192,"data-quality":0.0693354522,"ml-security":0.1480191352}}
{"text":"We consider finite repeated games where each player only has access to its own utility (or payoff) function but is able to exchange information with other players.","meta":{"url":"http://arxiv.org/abs/2307.13242v1"},"cats":{"new-dataset":0.0541577281,"dev-research":0.2215295141,"prompt-eng":0.3499868246,"data-quality":0.053362991,"ml-security":0.2589686355}}
{"text":"We develop a novel regret matching (RM) based algorithm for computing an efficient PSNE solution that could approach a desired Pareto-optimal outcome yielding the highest social welfare among all the attainable equilibria in the long run.","meta":{"url":"http://arxiv.org/abs/2307.13242v1"},"cats":{"new-dataset":0.0133559303,"dev-research":0.1835674588,"prompt-eng":0.4160956243,"data-quality":0.0746474627,"ml-security":0.1675588168}}
{"text":"Our proposed learning procedure follows the regret minimization framework but extends it in three major ways: (1) agents use global, instead of local, utility for calculating regrets, (2) each agent maintains a small and diminishing exploration probability in order to explore various PSNEs, and (3) agents stay with the actions that achieve the best global utility thus far, regardless of regrets.","meta":{"url":"http://arxiv.org/abs/2307.13242v1"},"cats":{"new-dataset":0.0259162405,"dev-research":0.2354718238,"prompt-eng":0.4457256122,"data-quality":0.1322018392,"ml-security":0.2498879692}}
{"text":"We prove that these three extensions enable the algorithm to select the stable social optimum equilibrium instead of converging to an arbitrary or cyclic equilibrium as in the conventional RM approach.","meta":{"url":"http://arxiv.org/abs/2307.13242v1"},"cats":{"new-dataset":0.026465929,"dev-research":0.1805316541,"prompt-eng":0.3519044787,"data-quality":0.091027224,"ml-security":0.118361086}}
{"text":"We demonstrate the effectiveness of our approach through a set of applications in multi-agent distributed control, including a large-scale resource allocation game and a hard combinatorial task assignment problem for which no efficient (polynomial) solution exists.","meta":{"url":"http://arxiv.org/abs/2307.13242v1"},"cats":{"new-dataset":0.1035394714,"dev-research":0.2386303246,"prompt-eng":0.4110248262,"data-quality":0.0888959749,"ml-security":0.1575555267}}
{"text":"The utilization of Large Language Models (LLMs) for the construction of AI systems has garnered significant attention across diverse fields.","meta":{"url":"http://arxiv.org/abs/2307.13240v1"},"cats":{"new-dataset":0.0676330298,"dev-research":0.2007580503,"prompt-eng":0.3877180021,"data-quality":0.1506757246,"ml-security":0.1485314387}}
{"text":"The extension of LLMs to the domain of fashion holds substantial commercial potential but also inherent challenges due to the intricate semantic interactions in fashion-related generation.","meta":{"url":"http://arxiv.org/abs/2307.13240v1"},"cats":{"new-dataset":0.0545528915,"dev-research":0.2265811403,"prompt-eng":0.3729502289,"data-quality":0.2152401164,"ml-security":0.0717855303}}
{"text":"To address this issue, we developed a hierarchical AI system called Fashion Matrix dedicated to editing photos by just talking.","meta":{"url":"http://arxiv.org/abs/2307.13240v1"},"cats":{"new-dataset":0.1246624539,"dev-research":0.2649452644,"prompt-eng":0.4108753995,"data-quality":0.1671507486,"ml-security":0.0728604224}}
{"text":"This system facilitates diverse prompt-driven tasks, encompassing garment or accessory replacement, recoloring, addition, and removal.","meta":{"url":"http://arxiv.org/abs/2307.13240v1"},"cats":{"new-dataset":0.0581801323,"dev-research":0.2794404661,"prompt-eng":0.5090427594,"data-quality":0.1020756982,"ml-security":0.0524130826}}
{"text":"Specifically, Fashion Matrix employs LLM as its foundational support and engages in iterative interactions with users.","meta":{"url":"http://arxiv.org/abs/2307.13240v1"},"cats":{"new-dataset":0.0532510885,"dev-research":0.2443395898,"prompt-eng":0.3951887359,"data-quality":0.070694869,"ml-security":0.0756833412}}
{"text":"It employs a range of Semantic Segmentation Models (e.g., Grounded-SAM, MattingAnything, etc.) to delineate the specific editing masks based on user instructions.","meta":{"url":"http://arxiv.org/abs/2307.13240v1"},"cats":{"new-dataset":0.0878353469,"dev-research":0.3438277371,"prompt-eng":0.4438750666,"data-quality":0.1694655897,"ml-security":0.11276333}}
{"text":"Subsequently, Visual Foundation Models (e.g., Stable Diffusion, ControlNet, etc.) are leveraged to generate edited images from text prompts and masks, thereby facilitating the automation of fashion editing processes.","meta":{"url":"http://arxiv.org/abs/2307.13240v1"},"cats":{"new-dataset":0.0657702676,"dev-research":0.3163056537,"prompt-eng":0.4547643814,"data-quality":0.1685043175,"ml-security":0.1383172258}}
{"text":"Experiments demonstrate the outstanding ability of Fashion Matrix to explores the collaborative potential of functionally diverse pre-trained models in the domain of fashion editing.","meta":{"url":"http://arxiv.org/abs/2307.13240v1"},"cats":{"new-dataset":0.0398883818,"dev-research":0.2299508805,"prompt-eng":0.3969584384,"data-quality":0.1462341302,"ml-security":0.1028097786}}
{"text":"Semi-supervised anomaly detection methods leverage a few anomaly examples to yield drastically improved performance compared to unsupervised models.","meta":{"url":"http://arxiv.org/abs/2307.13239v1"},"cats":{"new-dataset":0.046433868,"dev-research":0.2459572227,"prompt-eng":0.4266965791,"data-quality":0.3855793414,"ml-security":0.4684635069}}
{"text":"However, they still suffer from two limitations: 1) unlabeled anomalies (i.e., anomaly contamination) may mislead the learning process when all the unlabeled data are employed as inliers for model training; 2) only discrete supervision information (such as binary or ordinal data labels) is exploited, which leads to suboptimal learning of anomaly scores that essentially take on a continuous distribution.","meta":{"url":"http://arxiv.org/abs/2307.13239v1"},"cats":{"new-dataset":0.02058079,"dev-research":0.2126712887,"prompt-eng":0.3330317632,"data-quality":0.4909763796,"ml-security":0.4417274362}}
{"text":"Therefore, this paper proposes a novel semi-supervised anomaly detection method, which devises \\textit{contamination-resilient continuous supervisory signals}.","meta":{"url":"http://arxiv.org/abs/2307.13239v1"},"cats":{"new-dataset":0.1087295836,"dev-research":0.312384721,"prompt-eng":0.4526690765,"data-quality":0.4131643573,"ml-security":0.5462024883}}
{"text":"Specifically, we propose a mass interpolation method to diffuse the abnormality of labeled anomalies, thereby creating new data samples labeled with continuous abnormal degrees.","meta":{"url":"http://arxiv.org/abs/2307.13239v1"},"cats":{"new-dataset":0.229080562,"dev-research":0.2204740064,"prompt-eng":0.4275075169,"data-quality":0.4418656021,"ml-security":0.1411721363}}
{"text":"Meanwhile, the contaminated area can be covered by new data samples generated via combinations of data with correct labels.","meta":{"url":"http://arxiv.org/abs/2307.13239v1"},"cats":{"new-dataset":0.3608791006,"dev-research":0.2881261155,"prompt-eng":0.3905400686,"data-quality":0.4505240102,"ml-security":0.3237127857}}
{"text":"A feature learning-based objective is added to serve as an optimization constraint to regularize the network and further enhance the robustness w.r.t.","meta":{"url":"http://arxiv.org/abs/2307.13239v1"},"cats":{"new-dataset":0.0097723647,"dev-research":0.3051692097,"prompt-eng":0.4025626953,"data-quality":0.3238989804,"ml-security":0.324190681}}
{"text":"anomaly contamination.","meta":{"url":"http://arxiv.org/abs/2307.13239v1"},"cats":{"new-dataset":0.129099329,"dev-research":0.2945455536,"prompt-eng":0.3609535721,"data-quality":0.3873389741,"ml-security":0.4711555594}}
{"text":"Extensive experiments on 11 real-world datasets show that our approach significantly outperforms state-of-the-art competitors by 20%-30% in AUC-PR and obtains more robust and superior performance in settings with different anomaly contamination levels and varying numbers of labeled anomalies.","meta":{"url":"http://arxiv.org/abs/2307.13239v1"},"cats":{"new-dataset":0.4304193257,"dev-research":0.3204797866,"prompt-eng":0.4475617201,"data-quality":0.4394273039,"ml-security":0.4190474646}}
{"text":"The source code is available at https://github.com/xuhongzuo/rosas/.","meta":{"url":"http://arxiv.org/abs/2307.13239v1"},"cats":{"new-dataset":0.4812623125,"dev-research":0.2744279769,"prompt-eng":0.4125773984,"data-quality":0.1746991725,"ml-security":0.0762650365}}
{"text":"Reconfigurable intelligent surface (RIS) is a promising technology that can reshape the electromagnetic environment in wireless networks, offering various possibilities for enhancing wireless channels.","meta":{"url":"http://arxiv.org/abs/2307.13237v1"},"cats":{"new-dataset":0.0264408767,"dev-research":0.2307509853,"prompt-eng":0.3925253766,"data-quality":0.0822139249,"ml-security":0.0865159733}}
{"text":"Motivated by this, we investigate the channel optimization for multiple-input multiple-output (MIMO) systems assisted by RIS.","meta":{"url":"http://arxiv.org/abs/2307.13237v1"},"cats":{"new-dataset":0.0111583386,"dev-research":0.2063672392,"prompt-eng":0.4008208038,"data-quality":0.1081271248,"ml-security":0.0793513678}}
{"text":"In this paper, an efficient RIS optimization method is proposed to enhance the effective rank of the MIMO channel for achievable rate improvement.","meta":{"url":"http://arxiv.org/abs/2307.13237v1"},"cats":{"new-dataset":0.0067654354,"dev-research":0.2332051298,"prompt-eng":0.389630652,"data-quality":0.1282520572,"ml-security":0.0623027551}}
{"text":"Numerical results are presented to verify the effectiveness of RIS in improving MIMO channels.","meta":{"url":"http://arxiv.org/abs/2307.13237v1"},"cats":{"new-dataset":0.0145417688,"dev-research":0.2119272936,"prompt-eng":0.3920334014,"data-quality":0.1584959146,"ml-security":0.072597026}}
{"text":"Additionally, we construct a 2$\\times$2 RIS-assisted MIMO prototype to perform experimental measurements and validate the performance of our proposed algorithm.","meta":{"url":"http://arxiv.org/abs/2307.13237v1"},"cats":{"new-dataset":0.0599577246,"dev-research":0.1943754722,"prompt-eng":0.4546356483,"data-quality":0.1287671219,"ml-security":0.0817478679}}
{"text":"The results reveal a significant increase in effective rank and achievable rate for the RIS-assisted MIMO channel compared to the MIMO channel without RIS.","meta":{"url":"http://arxiv.org/abs/2307.13237v1"},"cats":{"new-dataset":0.0196140809,"dev-research":0.1894552985,"prompt-eng":0.4042463644,"data-quality":0.1309785184,"ml-security":0.0737296508}}
{"text":"The goal of the audio-visual segmentation (AVS) task is to segment the sounding objects in the video frames using audio cues.","meta":{"url":"http://arxiv.org/abs/2307.13236v1"},"cats":{"new-dataset":0.0564214754,"dev-research":0.2832331193,"prompt-eng":0.3833480528,"data-quality":0.2064510747,"ml-security":0.0625367784}}
{"text":"However, current fusion-based methods have the performance limitations due to the small receptive field of convolution and inadequate fusion of audio-visual features.","meta":{"url":"http://arxiv.org/abs/2307.13236v1"},"cats":{"new-dataset":0.0257002239,"dev-research":0.2657236484,"prompt-eng":0.3333012945,"data-quality":0.1967720815,"ml-security":0.086687878}}
{"text":"To overcome these issues, we propose a novel \\textbf{Au}dio-aware query-enhanced \\textbf{TR}ansformer (AuTR) to tackle the task.","meta":{"url":"http://arxiv.org/abs/2307.13236v1"},"cats":{"new-dataset":0.0630076084,"dev-research":0.2954359194,"prompt-eng":0.5002458081,"data-quality":0.2581537513,"ml-security":0.0954738741}}
{"text":"Unlike existing methods, our approach introduces a multimodal transformer architecture that enables deep fusion and aggregation of audio-visual features.","meta":{"url":"http://arxiv.org/abs/2307.13236v1"},"cats":{"new-dataset":0.1511353208,"dev-research":0.253502899,"prompt-eng":0.3497584612,"data-quality":0.1724770826,"ml-security":0.0605665246}}
{"text":"Furthermore, we devise an audio-aware query-enhanced transformer decoder that explicitly helps the model focus on the segmentation of the pinpointed sounding objects based on audio signals, while disregarding silent yet salient objects.","meta":{"url":"http://arxiv.org/abs/2307.13236v1"},"cats":{"new-dataset":0.1068250908,"dev-research":0.2261829042,"prompt-eng":0.4298896082,"data-quality":0.2188491242,"ml-security":0.0956123885}}
{"text":"Experimental results show that our method outperforms previous methods and demonstrates better generalization ability in multi-sound and open-set scenarios.","meta":{"url":"http://arxiv.org/abs/2307.13236v1"},"cats":{"new-dataset":0.06932208,"dev-research":0.2670273878,"prompt-eng":0.4063132849,"data-quality":0.2514088703,"ml-security":0.0944414597}}
{"text":"Differential privacy is a widely accepted measure of privacy in the context of deep learning algorithms, and achieving it relies on a noisy training approach known as differentially private stochastic gradient descent (DP-SGD).","meta":{"url":"http://arxiv.org/abs/2307.13231v1"},"cats":{"new-dataset":0.0762066843,"dev-research":0.2068672157,"prompt-eng":0.2887728609,"data-quality":0.2179299405,"ml-security":0.6823163014}}
{"text":"DP-SGD requires direct noise addition to every gradient in a dense neural network, the privacy is achieved at a significant utility cost.","meta":{"url":"http://arxiv.org/abs/2307.13231v1"},"cats":{"new-dataset":0.0186268533,"dev-research":0.2416931186,"prompt-eng":0.3471689447,"data-quality":0.1690171227,"ml-security":0.5286952866}}
{"text":"In this work, we present Spectral-DP, a new differentially private learning approach which combines gradient perturbation in the spectral domain with spectral filtering to achieve a desired privacy guarantee with a lower noise scale and thus better utility.","meta":{"url":"http://arxiv.org/abs/2307.13231v1"},"cats":{"new-dataset":0.0800253574,"dev-research":0.1974168226,"prompt-eng":0.3200978985,"data-quality":0.1958717078,"ml-security":0.6363218404}}
{"text":"We develop differentially private deep learning methods based on Spectral-DP for architectures that contain both convolution and fully connected layers.","meta":{"url":"http://arxiv.org/abs/2307.13231v1"},"cats":{"new-dataset":0.0897080492,"dev-research":0.2146920555,"prompt-eng":0.2722954917,"data-quality":0.1500442985,"ml-security":0.6168409286}}
{"text":"In particular, for fully connected layers, we combine a block-circulant based spatial restructuring with Spectral-DP to achieve better utility.","meta":{"url":"http://arxiv.org/abs/2307.13231v1"},"cats":{"new-dataset":0.045648501,"dev-research":0.2903461651,"prompt-eng":0.4156349838,"data-quality":0.1211792982,"ml-security":0.0518373193}}
{"text":"Through comprehensive experiments, we study and provide guidelines to implement Spectral-DP deep learning on benchmark datasets.","meta":{"url":"http://arxiv.org/abs/2307.13231v1"},"cats":{"new-dataset":0.3114449995,"dev-research":0.2486031443,"prompt-eng":0.3507760599,"data-quality":0.2536825195,"ml-security":0.1453978877}}
{"text":"In comparison with state-of-the-art DP-SGD based approaches, Spectral-DP is shown to have uniformly better utility performance in both training from scratch and transfer learning settings.","meta":{"url":"http://arxiv.org/abs/2307.13231v1"},"cats":{"new-dataset":0.0459625235,"dev-research":0.2601777701,"prompt-eng":0.4058390593,"data-quality":0.2057813458,"ml-security":0.138017299}}
{"text":"We propose Strivec, a novel neural representation that models a 3D scene as a radiance field with sparsely distributed and compactly factorized local tensor feature grids.","meta":{"url":"http://arxiv.org/abs/2307.13226v1"},"cats":{"new-dataset":0.253671852,"dev-research":0.1813912738,"prompt-eng":0.3645264197,"data-quality":0.109722371,"ml-security":0.0971780746}}
{"text":"Our approach leverages tensor decomposition, following the recent work TensoRF, to model the tensor grids.","meta":{"url":"http://arxiv.org/abs/2307.13226v1"},"cats":{"new-dataset":0.114870552,"dev-research":0.1487434295,"prompt-eng":0.4012505008,"data-quality":0.1365073699,"ml-security":0.1103787605}}
{"text":"In contrast to TensoRF which uses a global tensor and focuses on their vector-matrix decomposition, we propose to utilize a cloud of local tensors and apply the classic CANDECOMP/PARAFAC (CP) decomposition to factorize each tensor into triple vectors that express local feature distributions along spatial axes and compactly encode a local neural field.","meta":{"url":"http://arxiv.org/abs/2307.13226v1"},"cats":{"new-dataset":0.2592600304,"dev-research":0.2138648023,"prompt-eng":0.3821468502,"data-quality":0.1368899333,"ml-security":0.1103208497}}
{"text":"We also apply multi-scale tensor grids to discover the geometry and appearance commonalities and exploit spatial coherence with the tri-vector factorization at multiple local scales.","meta":{"url":"http://arxiv.org/abs/2307.13226v1"},"cats":{"new-dataset":0.1898684135,"dev-research":0.1935698638,"prompt-eng":0.3630066304,"data-quality":0.1180784954,"ml-security":0.0780004701}}
{"text":"The final radiance field properties are regressed by aggregating neural features from multiple local tensors across all scales.","meta":{"url":"http://arxiv.org/abs/2307.13226v1"},"cats":{"new-dataset":0.1622231827,"dev-research":0.1601397173,"prompt-eng":0.3716226143,"data-quality":0.1438869569,"ml-security":0.116994514}}
{"text":"Our tri-vector tensors are sparsely distributed around the actual scene surface, discovered by a fast coarse reconstruction, leveraging the sparsity of a 3D scene.","meta":{"url":"http://arxiv.org/abs/2307.13226v1"},"cats":{"new-dataset":0.1342205984,"dev-research":0.1986181703,"prompt-eng":0.3528401435,"data-quality":0.1979420315,"ml-security":0.118809487}}
{"text":"We demonstrate that our model can achieve better rendering quality while using significantly fewer parameters than previous methods, including TensoRF and Instant-NGP.","meta":{"url":"http://arxiv.org/abs/2307.13226v1"},"cats":{"new-dataset":0.0999763161,"dev-research":0.2169681166,"prompt-eng":0.4242417099,"data-quality":0.1435667408,"ml-security":0.0759326866}}
{"text":"With the popularity of smartphones and tablets, users have become accustomed to using different devices for different tasks, such as using their phones to play games and tablets to watch movies.","meta":{"url":"http://arxiv.org/abs/2307.13225v1"},"cats":{"new-dataset":0.0244956539,"dev-research":0.3643381044,"prompt-eng":0.3512167122,"data-quality":0.0660318915,"ml-security":0.1378720273}}
{"text":"To conquer the market, one app is often available on both smartphones and tablets.","meta":{"url":"http://arxiv.org/abs/2307.13225v1"},"cats":{"new-dataset":0.0362468009,"dev-research":0.285613049,"prompt-eng":0.310814763,"data-quality":0.0678684158,"ml-security":0.1399831889}}
{"text":"However, although one app has similar graphic user interfaces (GUIs) and functionalities on phone and tablet, current app developers typically start from scratch when developing a tablet-compatible version of their app, which drives up development costs and wastes existing design resources.","meta":{"url":"http://arxiv.org/abs/2307.13225v1"},"cats":{"new-dataset":0.0532438935,"dev-research":0.5388760292,"prompt-eng":0.3294975664,"data-quality":0.0856936834,"ml-security":0.1089869934}}
{"text":"Researchers are attempting to employ deep learning in automated GUIs development to enhance developers' productivity.","meta":{"url":"http://arxiv.org/abs/2307.13225v1"},"cats":{"new-dataset":0.0779885642,"dev-research":0.5279364886,"prompt-eng":0.377149136,"data-quality":0.1431694724,"ml-security":0.2247080169}}
{"text":"Deep learning models rely heavily on high-quality datasets.","meta":{"url":"http://arxiv.org/abs/2307.13225v1"},"cats":{"new-dataset":0.1226551041,"dev-research":0.2163365576,"prompt-eng":0.3015697434,"data-quality":0.2525108409,"ml-security":0.3678501638}}
{"text":"There are currently several publicly accessible GUI page datasets for phones, but none for pairwise GUIs between phones and tablets.","meta":{"url":"http://arxiv.org/abs/2307.13225v1"},"cats":{"new-dataset":0.5877098194,"dev-research":0.205193048,"prompt-eng":0.3444982844,"data-quality":0.1041604504,"ml-security":0.1041978653}}
{"text":"This poses a significant barrier to the employment of deep learning in automated GUI development.","meta":{"url":"http://arxiv.org/abs/2307.13225v1"},"cats":{"new-dataset":0.0345452747,"dev-research":0.3858048742,"prompt-eng":0.4183188388,"data-quality":0.1571884682,"ml-security":0.310322009}}
{"text":"In this paper, we collect and make public the Papt dataset, which is a pairwise dataset for GUI conversion and retrieval between Android phones and tablets.","meta":{"url":"http://arxiv.org/abs/2307.13225v1"},"cats":{"new-dataset":0.7593161514,"dev-research":0.2291950288,"prompt-eng":0.4237310845,"data-quality":0.147027332,"ml-security":0.1182690953}}
{"text":"The dataset contains 10,035 phone-tablet GUI page pairs from 5,593 phone-tablet app pairs.","meta":{"url":"http://arxiv.org/abs/2307.13225v1"},"cats":{"new-dataset":0.907989448,"dev-research":0.2027754315,"prompt-eng":0.3328335145,"data-quality":0.1511419212,"ml-security":0.0991034275}}
{"text":"We illustrate the approaches of collecting pairwise data and statistical analysis of this dataset.","meta":{"url":"http://arxiv.org/abs/2307.13225v1"},"cats":{"new-dataset":0.8062510026,"dev-research":0.2495033644,"prompt-eng":0.3854234247,"data-quality":0.1681609678,"ml-security":0.0644970352}}
{"text":"We also illustrate the advantages of our dataset compared to other current datasets.","meta":{"url":"http://arxiv.org/abs/2307.13225v1"},"cats":{"new-dataset":0.7826527907,"dev-research":0.2848447552,"prompt-eng":0.353627651,"data-quality":0.169887859,"ml-security":0.0989210436}}
{"text":"Through preliminary experiments on this dataset, we analyse the present challenges of utilising deep learning in automated GUI development and find that our dataset can assist the application of some deep learning models to tasks involving automatic GUI development.","meta":{"url":"http://arxiv.org/abs/2307.13225v1"},"cats":{"new-dataset":0.2851849405,"dev-research":0.3534198395,"prompt-eng":0.4361149489,"data-quality":0.2275422853,"ml-security":0.201773046}}
{"text":"Large language models have made significant progress in the past few years.","meta":{"url":"http://arxiv.org/abs/2307.13221v1"},"cats":{"new-dataset":0.0837743847,"dev-research":0.1977814888,"prompt-eng":0.3318770585,"data-quality":0.1456689477,"ml-security":0.0744953191}}
{"text":"However, they are either generic {\\it or} field specific, splitting the community into different groups.","meta":{"url":"http://arxiv.org/abs/2307.13221v1"},"cats":{"new-dataset":0.0745804619,"dev-research":0.2655419894,"prompt-eng":0.398230596,"data-quality":0.1796309085,"ml-security":0.0699732536}}
{"text":"In this paper, we unify these large language models into a larger map, where the generic {\\it and} specific models are linked together and can improve each other, based on the user personal input and information from the internet.","meta":{"url":"http://arxiv.org/abs/2307.13221v1"},"cats":{"new-dataset":0.2307231021,"dev-research":0.2750190748,"prompt-eng":0.4555458196,"data-quality":0.2714370603,"ml-security":0.1307018528}}
{"text":"The idea of linking several large language models together is inspired by the functionality of human brain.","meta":{"url":"http://arxiv.org/abs/2307.13221v1"},"cats":{"new-dataset":0.02283515,"dev-research":0.2541788657,"prompt-eng":0.3807308968,"data-quality":0.1403327332,"ml-security":0.0751777756}}
{"text":"The specific regions on the brain cortex are specific for certain low level functionality.","meta":{"url":"http://arxiv.org/abs/2307.13221v1"},"cats":{"new-dataset":0.0089501636,"dev-research":0.2314435386,"prompt-eng":0.3569236504,"data-quality":0.1263873287,"ml-security":0.0918378478}}
{"text":"And these regions can jointly work together to achieve more complex high level functionality.","meta":{"url":"http://arxiv.org/abs/2307.13221v1"},"cats":{"new-dataset":0.0445612097,"dev-research":0.3511506638,"prompt-eng":0.3901871706,"data-quality":0.0838727957,"ml-security":0.0499426335}}
{"text":"Such behavior on human brain cortex sheds the light to design the multilevel large language models that contain global level, field level and user level models.","meta":{"url":"http://arxiv.org/abs/2307.13221v1"},"cats":{"new-dataset":0.0476868937,"dev-research":0.2060138702,"prompt-eng":0.415216841,"data-quality":0.1472269749,"ml-security":0.1402659151}}
{"text":"The user level models run on local machines to achieve efficient response and protect the user's privacy.","meta":{"url":"http://arxiv.org/abs/2307.13221v1"},"cats":{"new-dataset":0.0476179604,"dev-research":0.2428301487,"prompt-eng":0.4631403096,"data-quality":0.0654432328,"ml-security":0.351749887}}
{"text":"Such multilevel models reduce some redundancy and perform better than the single level models.","meta":{"url":"http://arxiv.org/abs/2307.13221v1"},"cats":{"new-dataset":0.0024716994,"dev-research":0.2015732713,"prompt-eng":0.3960697913,"data-quality":0.1202248392,"ml-security":0.1105564564}}
{"text":"The proposed multilevel idea can be applied in various applications, such as natural language processing, computer vision tasks, professional assistant, business and healthcare.","meta":{"url":"http://arxiv.org/abs/2307.13221v1"},"cats":{"new-dataset":0.0144815316,"dev-research":0.2623601555,"prompt-eng":0.4053899021,"data-quality":0.1222702082,"ml-security":0.0809154206}}
{"text":"The availability of both structured and unstructured databases, such as electronic health data, social media data, patent data, and surveys that are often updated in real time, among others, has grown rapidly over the past decade.","meta":{"url":"http://arxiv.org/abs/2307.13219v1"},"cats":{"new-dataset":0.3590393664,"dev-research":0.2348827513,"prompt-eng":0.291526747,"data-quality":0.1024842404,"ml-security":0.1014646842}}
{"text":"With this expansion, the statistical and methodological questions around data integration, or rather merging multiple data sources, has also grown.","meta":{"url":"http://arxiv.org/abs/2307.13219v1"},"cats":{"new-dataset":0.196862748,"dev-research":0.267589935,"prompt-eng":0.3611590889,"data-quality":0.1980391548,"ml-security":0.067706348}}
{"text":"Specifically, the science of the ``data cleaning pipeline'' contains four stages that allow an analyst to perform downstream tasks, predictive analyses, or statistical analyses on ``cleaned data.''","meta":{"url":"http://arxiv.org/abs/2307.13219v1"},"cats":{"new-dataset":0.1877134072,"dev-research":0.3334698358,"prompt-eng":0.3801936425,"data-quality":0.1650187655,"ml-security":0.0967942133}}
{"text":"This article provides a review of this emerging field, introducing technical terminology and commonly used methods.","meta":{"url":"http://arxiv.org/abs/2307.13219v1"},"cats":{"new-dataset":0.0106949932,"dev-research":0.3088409457,"prompt-eng":0.4343350126,"data-quality":0.1796251065,"ml-security":0.0607430988}}
{"text":"Semantic segmentation plays a vital role in computer vision tasks, enabling precise pixel-level understanding of images.","meta":{"url":"http://arxiv.org/abs/2307.13215v1"},"cats":{"new-dataset":0.0574870367,"dev-research":0.279611878,"prompt-eng":0.4018525779,"data-quality":0.2691633147,"ml-security":0.0987113279}}
{"text":"In this paper, we present a comprehensive library for semantic segmentation, which contains implementations of popular segmentation models like SegNet, FCN, UNet, and PSPNet.","meta":{"url":"http://arxiv.org/abs/2307.13215v1"},"cats":{"new-dataset":0.4268686163,"dev-research":0.2363233434,"prompt-eng":0.4203264621,"data-quality":0.2264893847,"ml-security":0.1067485633}}
{"text":"We also evaluate and compare these models on several datasets, offering researchers and practitioners a powerful toolset for tackling diverse segmentation challenges.","meta":{"url":"http://arxiv.org/abs/2307.13215v1"},"cats":{"new-dataset":0.5747726469,"dev-research":0.2005787371,"prompt-eng":0.3848995541,"data-quality":0.2588413038,"ml-security":0.0972296707}}
{"text":"Federated learning (FL) enables a decentralized machine learning paradigm for multiple clients to collaboratively train a generalized global model without sharing their private data.","meta":{"url":"http://arxiv.org/abs/2307.13214v1"},"cats":{"new-dataset":0.0965303348,"dev-research":0.2055667453,"prompt-eng":0.342373877,"data-quality":0.1141769463,"ml-security":0.3373201379}}
{"text":"Most existing works simply propose typical FL systems for single-modal data, thus limiting its potential on exploiting valuable multimodal data for future personalized applications.","meta":{"url":"http://arxiv.org/abs/2307.13214v1"},"cats":{"new-dataset":0.2260865565,"dev-research":0.2244523054,"prompt-eng":0.4334708824,"data-quality":0.1357881391,"ml-security":0.1006849502}}
{"text":"Furthermore, the majority of FL approaches still rely on the labeled data at the client side, which is limited in real-world applications due to the inability of self-annotation from users.","meta":{"url":"http://arxiv.org/abs/2307.13214v1"},"cats":{"new-dataset":0.0678935536,"dev-research":0.3390061095,"prompt-eng":0.4121907881,"data-quality":0.3175632439,"ml-security":0.2087066313}}
{"text":"In light of these limitations, we propose a novel multimodal FL framework that employs a semi-supervised learning approach to leverage the representations from different modalities.","meta":{"url":"http://arxiv.org/abs/2307.13214v1"},"cats":{"new-dataset":0.2549843774,"dev-research":0.1939295927,"prompt-eng":0.3997946216,"data-quality":0.1929955798,"ml-security":0.0764054179}}
{"text":"Bringing this concept into a system, we develop a distillation-based multimodal embedding knowledge transfer mechanism, namely FedMEKT, which allows the server and clients to exchange the joint knowledge of their learning models extracted from a small multimodal proxy dataset.","meta":{"url":"http://arxiv.org/abs/2307.13214v1"},"cats":{"new-dataset":0.2253495439,"dev-research":0.2880730631,"prompt-eng":0.3985546962,"data-quality":0.2067655501,"ml-security":0.1561785893}}
{"text":"Our FedMEKT iteratively updates the generalized global encoders with the joint embedding knowledge from the participating clients.","meta":{"url":"http://arxiv.org/abs/2307.13214v1"},"cats":{"new-dataset":0.1862230642,"dev-research":0.2673682289,"prompt-eng":0.3882584543,"data-quality":0.211503782,"ml-security":0.1443242018}}
{"text":"Thereby, to address the modality discrepancy and labeled data constraint in existing FL systems, our proposed FedMEKT comprises local multimodal autoencoder learning, generalized multimodal autoencoder construction, and generalized classifier learning.","meta":{"url":"http://arxiv.org/abs/2307.13214v1"},"cats":{"new-dataset":0.2709541923,"dev-research":0.2898560915,"prompt-eng":0.4107764912,"data-quality":0.3277597043,"ml-security":0.1230113113}}
{"text":"Through extensive experiments on three multimodal human activity recognition datasets, we demonstrate that FedMEKT achieves superior global encoder performance on linear evaluation and guarantees user privacy for personal data and model parameters while demanding less communication cost than other baselines.","meta":{"url":"http://arxiv.org/abs/2307.13214v1"},"cats":{"new-dataset":0.4871922944,"dev-research":0.2851857176,"prompt-eng":0.3865064434,"data-quality":0.1555607676,"ml-security":0.2947276951}}
{"text":"Predicting lower limb motion intent is vital for controlling exoskeleton robots and prosthetic limbs.","meta":{"url":"http://arxiv.org/abs/2307.13209v1"},"cats":{"new-dataset":0.0153762538,"dev-research":0.2501879439,"prompt-eng":0.3862478568,"data-quality":0.0841183102,"ml-security":0.147857328}}
{"text":"Surface electromyography (sEMG) attracts increasing attention in recent years as it enables ahead-of-time prediction of motion intentions before actual movement.","meta":{"url":"http://arxiv.org/abs/2307.13209v1"},"cats":{"new-dataset":0.0420460992,"dev-research":0.2504823349,"prompt-eng":0.3846256162,"data-quality":0.0757129949,"ml-security":0.0926411214}}
{"text":"However, the estimation performance of human joint trajectory remains a challenging problem due to the inter- and intra-subject variations.","meta":{"url":"http://arxiv.org/abs/2307.13209v1"},"cats":{"new-dataset":0.0324851983,"dev-research":0.1820879263,"prompt-eng":0.3962449831,"data-quality":0.1148131664,"ml-security":0.0484457291}}
{"text":"The former is related to physiological differences (such as height and weight) and preferred walking patterns of individuals, while the latter is mainly caused by irregular and gait-irrelevant muscle activity.","meta":{"url":"http://arxiv.org/abs/2307.13209v1"},"cats":{"new-dataset":0.0025178761,"dev-research":0.3107999195,"prompt-eng":0.3023203526,"data-quality":0.1001350659,"ml-security":0.0709162381}}
{"text":"This paper proposes a model integrating two gait cycle-inspired learning strategies to mitigate the challenge for predicting human knee joint trajectory.","meta":{"url":"http://arxiv.org/abs/2307.13209v1"},"cats":{"new-dataset":0.0485441803,"dev-research":0.2165510612,"prompt-eng":0.3695733603,"data-quality":0.0834085756,"ml-security":0.1796503424}}
{"text":"The first strategy is to decouple knee joint angles into motion patterns and amplitudes former exhibit low variability while latter show high variability among individuals.","meta":{"url":"http://arxiv.org/abs/2307.13209v1"},"cats":{"new-dataset":0.0365311921,"dev-research":0.2476742246,"prompt-eng":0.4172215919,"data-quality":0.0908345702,"ml-security":0.0585629273}}
{"text":"By learning through separate network entities, the model manages to capture both the common and personalized gait features.","meta":{"url":"http://arxiv.org/abs/2307.13209v1"},"cats":{"new-dataset":0.038779215,"dev-research":0.2412554629,"prompt-eng":0.4237944529,"data-quality":0.1117434629,"ml-security":0.131633339}}
{"text":"In the second, muscle principal activation masks are extracted from gait cycles in a prolonged walk.","meta":{"url":"http://arxiv.org/abs/2307.13209v1"},"cats":{"new-dataset":0.0412903812,"dev-research":0.1881795142,"prompt-eng":0.3632246372,"data-quality":0.118977196,"ml-security":0.1365554873}}
{"text":"These masks are used to filter out components unrelated to walking from raw sEMG and provide auxiliary guidance to capture more gait-related features.","meta":{"url":"http://arxiv.org/abs/2307.13209v1"},"cats":{"new-dataset":0.0473403342,"dev-research":0.256597257,"prompt-eng":0.3563101435,"data-quality":0.1230627909,"ml-security":0.1583628366}}
{"text":"Experimental results indicate that our model could predict knee angles with the average root mean square error (RMSE) of 3.03(0.49) degrees and 50ms ahead of time.","meta":{"url":"http://arxiv.org/abs/2307.13209v1"},"cats":{"new-dataset":0.0351573028,"dev-research":0.2188406828,"prompt-eng":0.3657684627,"data-quality":0.0675135452,"ml-security":0.0996653664}}
{"text":"To our knowledge this is the best performance in relevant literatures that has been reported, with reduced RMSE by at least 9.5%.","meta":{"url":"http://arxiv.org/abs/2307.13209v1"},"cats":{"new-dataset":0.0280711906,"dev-research":0.2165374746,"prompt-eng":0.4126885835,"data-quality":0.2405260428,"ml-security":0.0691307571}}
{"text":"Graph neural networks (GNNs) have become powerful tools for processing graph-based information in various domains.","meta":{"url":"http://arxiv.org/abs/2307.13206v1"},"cats":{"new-dataset":0.0998066493,"dev-research":0.2838613491,"prompt-eng":0.3128569138,"data-quality":0.216575409,"ml-security":0.1322071857}}
{"text":"A desirable property of GNNs is transferability, where a trained network can swap in information from a different graph without retraining and retain its accuracy.","meta":{"url":"http://arxiv.org/abs/2307.13206v1"},"cats":{"new-dataset":0.0188268509,"dev-research":0.252235332,"prompt-eng":0.2956668014,"data-quality":0.2049371509,"ml-security":0.2345745649}}
{"text":"A recent method of capturing transferability of GNNs is through the use of graphons, which are symmetric, measurable functions representing the limit of large dense graphs.","meta":{"url":"http://arxiv.org/abs/2307.13206v1"},"cats":{"new-dataset":0.0892375096,"dev-research":0.2333828276,"prompt-eng":0.32431432,"data-quality":0.1828809895,"ml-security":0.2183127835}}
{"text":"In this work, we contribute to the application of graphons to GNNs by presenting an explicit two-layer graphon neural network (WNN) architecture.","meta":{"url":"http://arxiv.org/abs/2307.13206v1"},"cats":{"new-dataset":0.1924433577,"dev-research":0.2232539244,"prompt-eng":0.3330739295,"data-quality":0.2127005457,"ml-security":0.2049555001}}
{"text":"We prove its ability to approximate bandlimited signals within a specified error tolerance using a minimal number of network weights.","meta":{"url":"http://arxiv.org/abs/2307.13206v1"},"cats":{"new-dataset":0.0257659881,"dev-research":0.1911895341,"prompt-eng":0.3744583373,"data-quality":0.3042200335,"ml-security":0.2238308714}}
{"text":"We then leverage this result, to establish the transferability of an explicit two-layer GNN over all sufficiently large graphs in a sequence converging to a graphon.","meta":{"url":"http://arxiv.org/abs/2307.13206v1"},"cats":{"new-dataset":0.1821811175,"dev-research":0.202828381,"prompt-eng":0.3234226182,"data-quality":0.1992281955,"ml-security":0.1982940732}}
{"text":"Our work addresses transferability between both deterministic weighted graphs and simple random graphs and overcomes issues related to the curse of dimensionality that arise in other GNN results.","meta":{"url":"http://arxiv.org/abs/2307.13206v1"},"cats":{"new-dataset":0.0864607611,"dev-research":0.1976640194,"prompt-eng":0.3447851512,"data-quality":0.2638441625,"ml-security":0.264284436}}
{"text":"The proposed WNN and GNN architectures offer practical solutions for handling graph data of varying sizes while maintaining performance guarantees without extensive retraining.","meta":{"url":"http://arxiv.org/abs/2307.13206v1"},"cats":{"new-dataset":0.1921123726,"dev-research":0.3262189092,"prompt-eng":0.3196831469,"data-quality":0.2022015959,"ml-security":0.1414912729}}
{"text":"Multimodal Sentiment Analysis (MSA) aims to mine sentiment information from text, visual, and acoustic modalities.","meta":{"url":"http://arxiv.org/abs/2307.13205v1"},"cats":{"new-dataset":0.0953813095,"dev-research":0.2915782303,"prompt-eng":0.3374185587,"data-quality":0.1914251584,"ml-security":0.0704197795}}
{"text":"Previous works have focused on representation learning and feature fusion strategies.","meta":{"url":"http://arxiv.org/abs/2307.13205v1"},"cats":{"new-dataset":0.0599242974,"dev-research":0.2572645057,"prompt-eng":0.3510742275,"data-quality":0.1712969438,"ml-security":0.1051837537}}
{"text":"However, most of these efforts ignored the disparity in the semantic richness of different modalities and treated each modality in the same manner.","meta":{"url":"http://arxiv.org/abs/2307.13205v1"},"cats":{"new-dataset":0.0058536569,"dev-research":0.2361370148,"prompt-eng":0.3103134469,"data-quality":0.3166105808,"ml-security":0.0683262302}}
{"text":"That may lead to strong modalities being neglected and weak modalities being overvalued.","meta":{"url":"http://arxiv.org/abs/2307.13205v1"},"cats":{"new-dataset":0.0055551105,"dev-research":0.2558037922,"prompt-eng":0.3618311976,"data-quality":0.3133582258,"ml-security":0.19118401}}
{"text":"Motivated by these observations, we propose a Text-oriented Modality Reinforcement Network (TMRN), which focuses on the dominance of the text modality in MSA.","meta":{"url":"http://arxiv.org/abs/2307.13205v1"},"cats":{"new-dataset":0.0757002252,"dev-research":0.2349826836,"prompt-eng":0.3965863107,"data-quality":0.2014204496,"ml-security":0.1002940101}}
{"text":"More specifically, we design a Text-Centered Cross-modal Attention (TCCA) module to make full interaction for text/acoustic and text/visual pairs, and a Text-Gated Self-Attention (TGSA) module to guide the self-reinforcement of the other two modalities.","meta":{"url":"http://arxiv.org/abs/2307.13205v1"},"cats":{"new-dataset":0.0974017342,"dev-research":0.2640168889,"prompt-eng":0.4661288696,"data-quality":0.1539989642,"ml-security":0.0787028399}}
{"text":"Furthermore, we present an adaptive fusion mechanism to decide the proportion of different modalities involved in the fusion process.","meta":{"url":"http://arxiv.org/abs/2307.13205v1"},"cats":{"new-dataset":0.0085774204,"dev-research":0.216102411,"prompt-eng":0.4490812365,"data-quality":0.0981032486,"ml-security":0.0621139761}}
{"text":"Finally, we combine the feature matrices into vectors to get the final representation for the downstream tasks.","meta":{"url":"http://arxiv.org/abs/2307.13205v1"},"cats":{"new-dataset":0.1767171122,"dev-research":0.2411477282,"prompt-eng":0.4495171795,"data-quality":0.1437424615,"ml-security":0.0649339352}}
{"text":"Experimental results show that our TMRN outperforms the state-of-the-art methods on two MSA benchmarks.","meta":{"url":"http://arxiv.org/abs/2307.13205v1"},"cats":{"new-dataset":0.07390205,"dev-research":0.2838086194,"prompt-eng":0.3998699541,"data-quality":0.1849156207,"ml-security":0.0734733043}}
{"text":"Task-oriented grasping (TOG) refers to the problem of predicting grasps on an object that enable subsequent manipulation tasks.","meta":{"url":"http://arxiv.org/abs/2307.13204v1"},"cats":{"new-dataset":0.0658167731,"dev-research":0.2934813256,"prompt-eng":0.4340867711,"data-quality":0.0739366291,"ml-security":0.0745614009}}
{"text":"To model the complex relationships between objects, tasks, and grasps, existing methods incorporate semantic knowledge as priors into TOG pipelines.","meta":{"url":"http://arxiv.org/abs/2307.13204v1"},"cats":{"new-dataset":0.1417010018,"dev-research":0.3315657527,"prompt-eng":0.4436803558,"data-quality":0.1182888418,"ml-security":0.0474751827}}
{"text":"However, the existing semantic knowledge is typically constructed based on closed-world concept sets, restraining the generalization to novel concepts out of the pre-defined sets.","meta":{"url":"http://arxiv.org/abs/2307.13204v1"},"cats":{"new-dataset":0.1037808535,"dev-research":0.3506501222,"prompt-eng":0.3451713924,"data-quality":0.2772854283,"ml-security":0.1349559235}}
{"text":"To address this issue, we propose GraspGPT, a large language model (LLM) based TOG framework that leverages the open-end semantic knowledge from an LLM to achieve zero-shot generalization to novel concepts.","meta":{"url":"http://arxiv.org/abs/2307.13204v1"},"cats":{"new-dataset":0.3774573,"dev-research":0.2632799178,"prompt-eng":0.3776895916,"data-quality":0.2054560482,"ml-security":0.1028073461}}
{"text":"We conduct experiments on Language Augmented TaskGrasp (LA-TaskGrasp) dataset and demonstrate that GraspGPT outperforms existing TOG methods on different held-out settings when generalizing to novel concepts out of the training set.","meta":{"url":"http://arxiv.org/abs/2307.13204v1"},"cats":{"new-dataset":0.4062467767,"dev-research":0.2826298867,"prompt-eng":0.4102634193,"data-quality":0.2173386708,"ml-security":0.0843405944}}
{"text":"The effectiveness of GraspGPT is further validated in real-robot experiments.","meta":{"url":"http://arxiv.org/abs/2307.13204v1"},"cats":{"new-dataset":0.0507284554,"dev-research":0.2682656323,"prompt-eng":0.4189737413,"data-quality":0.0951230569,"ml-security":0.0917760396}}
{"text":"Our code, data, appendix, and video are publicly available at https://sites.google.com/view/graspgpt/.","meta":{"url":"http://arxiv.org/abs/2307.13204v1"},"cats":{"new-dataset":0.8177176775,"dev-research":0.2562865926,"prompt-eng":0.3674441852,"data-quality":0.1304586869,"ml-security":0.1153955588}}
{"text":"A useful capability is that of classifying some agent's behavior using data from a sequence, or trace, of sensor measurements.","meta":{"url":"http://arxiv.org/abs/2307.13203v1"},"cats":{"new-dataset":0.122324414,"dev-research":0.2860235334,"prompt-eng":0.4437409341,"data-quality":0.1402002832,"ml-security":0.1657519665}}
{"text":"The sensor selection problem involves choosing a subset of available sensors to ensure that, when generated, observation traces will contain enough information to determine whether the agent's activities match some pattern.","meta":{"url":"http://arxiv.org/abs/2307.13203v1"},"cats":{"new-dataset":0.0348910792,"dev-research":0.2718019538,"prompt-eng":0.4609357721,"data-quality":0.1137099855,"ml-security":0.1489420958}}
{"text":"In generalizing prior work, this paper studies a formulation in which multiple behavioral itineraries may be supplied, with sensors selected to distinguish between behaviors.","meta":{"url":"http://arxiv.org/abs/2307.13203v1"},"cats":{"new-dataset":0.0457859852,"dev-research":0.222999232,"prompt-eng":0.5064609255,"data-quality":0.1213553669,"ml-security":0.1264695985}}
{"text":"This allows one to pose fine grained questions, e.g., to position the agent's activity on a spectrum.","meta":{"url":"http://arxiv.org/abs/2307.13203v1"},"cats":{"new-dataset":0.0560924467,"dev-research":0.2948712064,"prompt-eng":0.4890253724,"data-quality":0.1171283851,"ml-security":0.0845955622}}
{"text":"In addition, with multiple itineraries, one can also ask about choices of sensors where some behavior is always plausibly concealed by (or mistaken for, or conflated with) another.","meta":{"url":"http://arxiv.org/abs/2307.13203v1"},"cats":{"new-dataset":0.0721499819,"dev-research":0.2378707471,"prompt-eng":0.4858025546,"data-quality":0.166514449,"ml-security":0.2352334597}}
{"text":"Using sensor ambiguity to limit the acquisition of knowledge is a strong privacy guarantee, and one which some earlier work has examined.","meta":{"url":"http://arxiv.org/abs/2307.13203v1"},"cats":{"new-dataset":0.0370899144,"dev-research":0.2669272505,"prompt-eng":0.3741476164,"data-quality":0.205632183,"ml-security":0.3711812402}}
{"text":"By concretely formulating privacy requirements for sensor selection, this paper connects both lines of work: privacy -- where there is a bound from above, and behavior verification -- where sensors are bounded from below.","meta":{"url":"http://arxiv.org/abs/2307.13203v1"},"cats":{"new-dataset":0.0993682309,"dev-research":0.2479949243,"prompt-eng":0.4423994159,"data-quality":0.1366354353,"ml-security":0.4118716656}}
{"text":"We examine the worst case computational complexity that results from both types of bounds, proving that upper bounds are more challenging under standard computational complexity assumptions.","meta":{"url":"http://arxiv.org/abs/2307.13203v1"},"cats":{"new-dataset":0.0211295826,"dev-research":0.2832112034,"prompt-eng":0.3026402411,"data-quality":0.1216388002,"ml-security":0.1986100749}}
{"text":"The problem is intractable in general, but we give a novel approach to solving this problem that can exploit interrelationships between constraints, and we see opportunities for a few optimizations.","meta":{"url":"http://arxiv.org/abs/2307.13203v1"},"cats":{"new-dataset":0.0608973387,"dev-research":0.2188071452,"prompt-eng":0.4164268553,"data-quality":0.16505356,"ml-security":0.1477916073}}
{"text":"Case studies are presented to demonstrate the usefulness and scalability of our proposed solution, and to assess the impact of the optimizations.","meta":{"url":"http://arxiv.org/abs/2307.13203v1"},"cats":{"new-dataset":0.0202892677,"dev-research":0.254563618,"prompt-eng":0.4012943708,"data-quality":0.1220982253,"ml-security":0.0571998318}}
{"text":"As Reinforcement Learning (RL) agents are increasingly employed in diverse decision-making problems using reward preferences, it becomes important to ensure that policies learned by these frameworks in mapping observations to a probability distribution of the possible actions are explainable.","meta":{"url":"http://arxiv.org/abs/2307.13192v1"},"cats":{"new-dataset":0.0762266737,"dev-research":0.206439362,"prompt-eng":0.43180611,"data-quality":0.1668717017,"ml-security":0.2540865952}}
{"text":"However, there is little to no work in the systematic understanding of these complex policies in a contrastive manner, i.e., what minimal changes to the policy would improve/worsen its performance to a desired level.","meta":{"url":"http://arxiv.org/abs/2307.13192v1"},"cats":{"new-dataset":0.0120245683,"dev-research":0.2597387545,"prompt-eng":0.3638616233,"data-quality":0.1479068297,"ml-security":0.1270705301}}
{"text":"In this work, we present COUNTERPOL, the first framework to analyze RL policies using counterfactual explanations in the form of minimal changes to the policy that lead to the desired outcome.","meta":{"url":"http://arxiv.org/abs/2307.13192v1"},"cats":{"new-dataset":0.1637535276,"dev-research":0.2686641788,"prompt-eng":0.45626256,"data-quality":0.2612971969,"ml-security":0.2630005858}}
{"text":"We do so by incorporating counterfactuals in supervised learning in RL with the target outcome regulated using desired return.","meta":{"url":"http://arxiv.org/abs/2307.13192v1"},"cats":{"new-dataset":0.0581493026,"dev-research":0.2382479734,"prompt-eng":0.4992328297,"data-quality":0.3179274905,"ml-security":0.1633879908}}
{"text":"We establish a theoretical connection between Counterpol and widely used trust region-based policy optimization methods in RL.","meta":{"url":"http://arxiv.org/abs/2307.13192v1"},"cats":{"new-dataset":0.0311323317,"dev-research":0.2278128595,"prompt-eng":0.3842664386,"data-quality":0.1225667589,"ml-security":0.2764743489}}
{"text":"Extensive empirical analysis shows the efficacy of COUNTERPOL in generating explanations for (un)learning skills while keeping close to the original policy.","meta":{"url":"http://arxiv.org/abs/2307.13192v1"},"cats":{"new-dataset":0.0379299147,"dev-research":0.3327771388,"prompt-eng":0.3680162903,"data-quality":0.2300971692,"ml-security":0.4040930211}}
{"text":"Our results on five different RL environments with diverse state and action spaces demonstrate the utility of counterfactual explanations, paving the way for new frontiers in designing and developing counterfactual policies.","meta":{"url":"http://arxiv.org/abs/2307.13192v1"},"cats":{"new-dataset":0.2030672136,"dev-research":0.3369058208,"prompt-eng":0.4388847288,"data-quality":0.2068132644,"ml-security":0.2355306783}}
{"text":"Emotion regulation is the process of consciously altering one's affective state, that is the underlying emotional state such as happiness, confidence, guilt, anger etc.","meta":{"url":"http://arxiv.org/abs/2307.13187v1"},"cats":{"new-dataset":0.0364037941,"dev-research":0.2905749704,"prompt-eng":0.3553861445,"data-quality":0.1078331254,"ml-security":0.1133966466}}
{"text":"The ability to effectively regulate emotions is necessary for functioning efficiently in everyday life.","meta":{"url":"http://arxiv.org/abs/2307.13187v1"},"cats":{"new-dataset":0.0102560177,"dev-research":0.3621409148,"prompt-eng":0.3204216922,"data-quality":0.066218459,"ml-security":0.0825921581}}
{"text":"Today, the pervasiveness of digital technology is being purposefully employed to modify our affective states, a process known as digital emotion regulation.","meta":{"url":"http://arxiv.org/abs/2307.13187v1"},"cats":{"new-dataset":0.0260243562,"dev-research":0.4083127189,"prompt-eng":0.3681171898,"data-quality":0.0837753365,"ml-security":0.1478665602}}
{"text":"Understanding digital emotion regulation can help support the rise of ethical technology design, development, and deployment.","meta":{"url":"http://arxiv.org/abs/2307.13187v1"},"cats":{"new-dataset":0.0633666149,"dev-research":0.4248308648,"prompt-eng":0.3262402187,"data-quality":0.1722766872,"ml-security":0.2107935346}}
{"text":"This article presents an overview of digital emotion regulation in social media applications, as well as a synthesis of recent research on emotion regulation interventions for social media.","meta":{"url":"http://arxiv.org/abs/2307.13187v1"},"cats":{"new-dataset":0.0618055976,"dev-research":0.3024547875,"prompt-eng":0.3595007468,"data-quality":0.137239054,"ml-security":0.1499646228}}
{"text":"We share our findings from analysing state-of-the-art literature on how different social media applications are utilised at different stages in the process of emotion regulation.","meta":{"url":"http://arxiv.org/abs/2307.13187v1"},"cats":{"new-dataset":0.0292710821,"dev-research":0.2768375433,"prompt-eng":0.3357752241,"data-quality":0.1498666805,"ml-security":0.1018882789}}
{"text":"IoT is changing the way Internet is used due to the availability of a large amount of data timely collected from every-day life objects.","meta":{"url":"http://arxiv.org/abs/2307.13186v1"},"cats":{"new-dataset":0.0730777499,"dev-research":0.2675810497,"prompt-eng":0.3133117789,"data-quality":0.0913299664,"ml-security":0.1526370187}}
{"text":"Designing applications in this new scenario poses new challenges.","meta":{"url":"http://arxiv.org/abs/2307.13186v1"},"cats":{"new-dataset":0.0465422589,"dev-research":0.404335915,"prompt-eng":0.4204155056,"data-quality":0.1371934039,"ml-security":0.1655279738}}
{"text":"This extended abstract discusses them and presents the objective of the BeT project whose main aim is to introduce a reference architecture, a conceptual framework, and related techniques to design behavior-enabled IoT systems and applications.","meta":{"url":"http://arxiv.org/abs/2307.13186v1"},"cats":{"new-dataset":0.2038538748,"dev-research":0.2994526462,"prompt-eng":0.442064718,"data-quality":0.0852831818,"ml-security":0.1539080113}}
{"text":"In this paper, we introduce curve-lifted codes over fields of arbitrary characteristic, inspired by Hermitian-lifted codes over $\\mathbb{F}_{2^r}$. These codes are designed for locality and availability, and their particular parameters depend on the choice of curve and its properties.","meta":{"url":"http://arxiv.org/abs/2307.13183v1"},"cats":{"new-dataset":0.1087419547,"dev-research":0.260461819,"prompt-eng":0.4387645752,"data-quality":0.1387734016,"ml-security":0.1077539079}}
{"text":"Due to the construction, the numbers of rational points of intersection between curves and lines play a key role.","meta":{"url":"http://arxiv.org/abs/2307.13183v1"},"cats":{"new-dataset":0.0738890136,"dev-research":0.3045430315,"prompt-eng":0.3394686928,"data-quality":0.1236937464,"ml-security":0.0980832561}}
{"text":"To demonstrate that and generate new families of locally recoverable codes (LRCs) with high availabilty, we focus on norm-trace-lifted codes.","meta":{"url":"http://arxiv.org/abs/2307.13183v1"},"cats":{"new-dataset":0.2492584201,"dev-research":0.2902693267,"prompt-eng":0.4252744338,"data-quality":0.2380087804,"ml-security":0.136275756}}
{"text":"In some cases, they are easier to define than their Hermitian counterparts and consequently have a better asymptotic bound on the code rate.","meta":{"url":"http://arxiv.org/abs/2307.13183v1"},"cats":{"new-dataset":0.0042478339,"dev-research":0.3549509765,"prompt-eng":0.3036802456,"data-quality":0.1181696538,"ml-security":0.13095933}}
{"text":"We describe a method for the neural decoding of memory from EEG data.","meta":{"url":"http://arxiv.org/abs/2307.13181v1"},"cats":{"new-dataset":0.0752509825,"dev-research":0.2277019646,"prompt-eng":0.4356132258,"data-quality":0.2075435698,"ml-security":0.1382700408}}
{"text":"Using this method, a concept being recalled can be identified from an EEG trace with an average top-1 accuracy of about 78.4% (chance 4%).","meta":{"url":"http://arxiv.org/abs/2307.13181v1"},"cats":{"new-dataset":0.015640501,"dev-research":0.269706882,"prompt-eng":0.4956044598,"data-quality":0.3420294243,"ml-security":0.1066110426}}
{"text":"The method employs deep representation learning with supervised contrastive loss to map an EEG recording of brain activity to a low-dimensional space.","meta":{"url":"http://arxiv.org/abs/2307.13181v1"},"cats":{"new-dataset":0.0663713273,"dev-research":0.2599461711,"prompt-eng":0.3611557087,"data-quality":0.1902882556,"ml-security":0.0916636441}}
{"text":"Because representation learning is used, concepts can be identified even if they do not appear in the training data set.","meta":{"url":"http://arxiv.org/abs/2307.13181v1"},"cats":{"new-dataset":0.0219701259,"dev-research":0.30569986,"prompt-eng":0.3125279238,"data-quality":0.3729069117,"ml-security":0.2408872077}}
{"text":"However, reference EEG data must exist for each such concept.","meta":{"url":"http://arxiv.org/abs/2307.13181v1"},"cats":{"new-dataset":0.1254996161,"dev-research":0.1813934542,"prompt-eng":0.3821309125,"data-quality":0.1764147654,"ml-security":0.0810621445}}
{"text":"We also show an application of the method to the problem of information retrieval.","meta":{"url":"http://arxiv.org/abs/2307.13181v1"},"cats":{"new-dataset":0.0331086255,"dev-research":0.2705443227,"prompt-eng":0.4296731487,"data-quality":0.3070468856,"ml-security":0.0841321478}}
{"text":"In neural information retrieval, EEG data is captured while a user recalls the contents of a document, and a list of links to predicted documents is produced.","meta":{"url":"http://arxiv.org/abs/2307.13181v1"},"cats":{"new-dataset":0.0486394445,"dev-research":0.2742693352,"prompt-eng":0.4134991313,"data-quality":0.2160542847,"ml-security":0.1201418617}}
{"text":"In the field of medical imaging, 3D deep learning models play a crucial role in building powerful predictive models of disease progression.","meta":{"url":"http://arxiv.org/abs/2307.13865v1"},"cats":{"new-dataset":0.0727428906,"dev-research":0.1963710765,"prompt-eng":0.3126004287,"data-quality":0.0627907459,"ml-security":0.1912287661}}
{"text":"However, the size of these models presents significant challenges, both in terms of computational resources and data requirements.","meta":{"url":"http://arxiv.org/abs/2307.13865v1"},"cats":{"new-dataset":0.1288629629,"dev-research":0.1955996592,"prompt-eng":0.3519030537,"data-quality":0.1075255125,"ml-security":0.0836121491}}
{"text":"Moreover, achieving high-quality pretraining of 3D models proves to be even more challenging.","meta":{"url":"http://arxiv.org/abs/2307.13865v1"},"cats":{"new-dataset":0.0389927779,"dev-research":0.3436419734,"prompt-eng":0.4134354168,"data-quality":0.1175684635,"ml-security":0.1150436293}}
{"text":"To address these issues, hybrid 2.5D approaches provide an effective solution for utilizing 3D volumetric data efficiently using 2D models.","meta":{"url":"http://arxiv.org/abs/2307.13865v1"},"cats":{"new-dataset":0.121990615,"dev-research":0.2168405238,"prompt-eng":0.3432357763,"data-quality":0.077862548,"ml-security":0.0578261734}}
{"text":"Combining 2D and 3D techniques offers a promising avenue for optimizing performance while minimizing memory requirements.","meta":{"url":"http://arxiv.org/abs/2307.13865v1"},"cats":{"new-dataset":0.0110806437,"dev-research":0.4072484957,"prompt-eng":0.3517391838,"data-quality":0.0660197105,"ml-security":0.0746711372}}
{"text":"In this paper, we explore 2.5D architectures based on a combination of convolutional neural networks (CNNs), long short-term memory (LSTM), and Transformers.","meta":{"url":"http://arxiv.org/abs/2307.13865v1"},"cats":{"new-dataset":0.2456640076,"dev-research":0.1981712859,"prompt-eng":0.3236571785,"data-quality":0.0855547571,"ml-security":0.1879617937}}
{"text":"In addition, leveraging the benefits of recent non-contrastive pretraining approaches in 2D, we enhanced the performance and data efficiency of 2.5D techniques even further.","meta":{"url":"http://arxiv.org/abs/2307.13865v1"},"cats":{"new-dataset":0.0830594221,"dev-research":0.3144864891,"prompt-eng":0.3711188423,"data-quality":0.1146023768,"ml-security":0.069112262}}
{"text":"We demonstrate the effectiveness of architectures and associated pretraining on a task of predicting progression to wet age-related macular degeneration (AMD) within a six-month period on two large longitudinal OCT datasets.","meta":{"url":"http://arxiv.org/abs/2307.13865v1"},"cats":{"new-dataset":0.1040600127,"dev-research":0.3291537919,"prompt-eng":0.3726475844,"data-quality":0.1237260032,"ml-security":0.1220291191}}
{"text":"Automated design of analog and radio-frequency circuits using supervised or reinforcement learning from simulation data has recently been studied as an alternative to manual expert design.","meta":{"url":"http://arxiv.org/abs/2307.13861v1"},"cats":{"new-dataset":0.0426869626,"dev-research":0.3218463928,"prompt-eng":0.4770809227,"data-quality":0.1937963059,"ml-security":0.1491389522}}
{"text":"It is straightforward for a design agent to learn an inverse function from desired performance metrics to circuit parameters.","meta":{"url":"http://arxiv.org/abs/2307.13861v1"},"cats":{"new-dataset":0.0247995161,"dev-research":0.3382088309,"prompt-eng":0.4851899914,"data-quality":0.1177983476,"ml-security":0.1512253024}}
{"text":"However, it is more common for a user to have threshold performance criteria rather than an exact target vector of feasible performance measures.","meta":{"url":"http://arxiv.org/abs/2307.13861v1"},"cats":{"new-dataset":0.0009132214,"dev-research":0.3495553552,"prompt-eng":0.4628985991,"data-quality":0.1318847904,"ml-security":0.1987096291}}
{"text":"In this work, we propose a method for generating from simulation data a dataset on which a system can be trained via supervised learning to design circuits to meet threshold specifications.","meta":{"url":"http://arxiv.org/abs/2307.13861v1"},"cats":{"new-dataset":0.3333426494,"dev-research":0.2606078093,"prompt-eng":0.4723517418,"data-quality":0.2209051733,"ml-security":0.2608433949}}
{"text":"We moreover perform the to-date most extensive evaluation of automated analog circuit design, including experimenting in a significantly more diverse set of circuits than in prior work, covering linear, nonlinear, and autonomous circuit configurations, and show that our method consistently reaches success rate better than 90% at 5% error margin, while also improving data efficiency by upward of an order of magnitude.","meta":{"url":"http://arxiv.org/abs/2307.13861v1"},"cats":{"new-dataset":0.0508519191,"dev-research":0.3863469537,"prompt-eng":0.4560414827,"data-quality":0.2782033777,"ml-security":0.1500426762}}
{"text":"A demo of this system is available at circuits.streamlit.app","meta":{"url":"http://arxiv.org/abs/2307.13861v1"},"cats":{"new-dataset":0.3235930376,"dev-research":0.3127430672,"prompt-eng":0.4433516735,"data-quality":0.1263735299,"ml-security":0.0914578527}}
{"text":"The 2021 Canadian census is notable for using a unique form of privacy, random rounding, which independently and probabilistically rounds discrete numerical attribute values.","meta":{"url":"http://arxiv.org/abs/2307.13859v1"},"cats":{"new-dataset":0.2415649619,"dev-research":0.2371252135,"prompt-eng":0.4136885176,"data-quality":0.1750364088,"ml-security":0.1773969627}}
{"text":"In this work, we explore how hierarchical summative correlation between discrete variables allows for both probabilistic and exact solutions to attribute values in the 2021 Canadian Census disclosure.","meta":{"url":"http://arxiv.org/abs/2307.13859v1"},"cats":{"new-dataset":0.2885444758,"dev-research":0.1977406728,"prompt-eng":0.4168275947,"data-quality":0.1779485774,"ml-security":0.0606951777}}
{"text":"We demonstrate that, in some cases, it is possible to \"unround\" and extract the original private values before rounding, both in the presence and absence of provided population invariants.","meta":{"url":"http://arxiv.org/abs/2307.13859v1"},"cats":{"new-dataset":0.1263830663,"dev-research":0.1927552321,"prompt-eng":0.4352312876,"data-quality":0.1995909925,"ml-security":0.3318432271}}
{"text":"Using these methods, we expose the exact value of 624 previously private attributes in the 2021 Canadian census disclosure.","meta":{"url":"http://arxiv.org/abs/2307.13859v1"},"cats":{"new-dataset":0.5332516507,"dev-research":0.201671483,"prompt-eng":0.3865528936,"data-quality":0.17120859,"ml-security":0.1271213641}}
{"text":"We also infer the potential values of more than 1000 private attributes with a high probability of correctness.","meta":{"url":"http://arxiv.org/abs/2307.13859v1"},"cats":{"new-dataset":0.0824791349,"dev-research":0.1939206258,"prompt-eng":0.4346304319,"data-quality":0.1931859609,"ml-security":0.3811958207}}
{"text":"Finally, we propose how a simple solution based on unbounded discrete noise can effectively negate exact unrounding while maintaining high utility in the final product.","meta":{"url":"http://arxiv.org/abs/2307.13859v1"},"cats":{"new-dataset":0.0236056103,"dev-research":0.2323948927,"prompt-eng":0.4418355166,"data-quality":0.2948189589,"ml-security":0.2730175842}}
{"text":"Recent work has shown that when both the chart and caption emphasize the same aspects of the data, readers tend to remember the doubly-emphasized features as takeaways; when there is a mismatch, readers rely on the chart to form takeaways and can miss information in the caption text.","meta":{"url":"http://arxiv.org/abs/2307.13858v1"},"cats":{"new-dataset":0.0654516789,"dev-research":0.3306974087,"prompt-eng":0.3485838934,"data-quality":0.3562744353,"ml-security":0.1190475408}}
{"text":"Through a survey of 280 chart-caption pairs in real-world sources (e.g., news media, poll reports, government reports, academic articles, and Tableau Public), we find that captions often do not emphasize the same information in practice, which could limit how effectively readers take away the authors' intended messages.","meta":{"url":"http://arxiv.org/abs/2307.13858v1"},"cats":{"new-dataset":0.2263273895,"dev-research":0.2843397841,"prompt-eng":0.3280458472,"data-quality":0.3410291958,"ml-security":0.1057496033}}
{"text":"Motivated by the survey findings, we present EmphasisChecker, an interactive tool that highlights visually prominent chart features as well as the features emphasized by the caption text along with any mismatches in the emphasis.","meta":{"url":"http://arxiv.org/abs/2307.13858v1"},"cats":{"new-dataset":0.3096450797,"dev-research":0.3540518179,"prompt-eng":0.4264051923,"data-quality":0.3575831116,"ml-security":0.0677313401}}
{"text":"The tool implements a time-series prominent feature detector based on the Ramer-Douglas-Peucker algorithm and a text reference extractor that identifies time references and data descriptions in the caption and matches them with chart data.","meta":{"url":"http://arxiv.org/abs/2307.13858v1"},"cats":{"new-dataset":0.4024650641,"dev-research":0.3155277067,"prompt-eng":0.3782483577,"data-quality":0.2247530514,"ml-security":0.0908064498}}
{"text":"This information enables authors to compare features emphasized by these two modalities, quickly see mismatches, and make necessary revisions.","meta":{"url":"http://arxiv.org/abs/2307.13858v1"},"cats":{"new-dataset":0.0634778142,"dev-research":0.3881392436,"prompt-eng":0.4449675452,"data-quality":0.3376523146,"ml-security":0.0756950416}}
{"text":"A user study confirms that our tool is both useful and easy to use when authoring charts and captions.","meta":{"url":"http://arxiv.org/abs/2307.13858v1"},"cats":{"new-dataset":0.0638458268,"dev-research":0.429134694,"prompt-eng":0.3915010418,"data-quality":0.1987053582,"ml-security":0.056771595}}
{"text":"Following their success in visual recognition tasks, Vision Transformers(ViTs) are being increasingly employed for image restoration.","meta":{"url":"http://arxiv.org/abs/2307.13856v1"},"cats":{"new-dataset":0.0267452528,"dev-research":0.2446604178,"prompt-eng":0.3895188592,"data-quality":0.1426262873,"ml-security":0.0806833031}}
{"text":"As a few recent works claim that ViTs for image classification also have better robustness properties, we investigate whether the improved adversarial robustness of ViTs extends to image restoration.","meta":{"url":"http://arxiv.org/abs/2307.13856v1"},"cats":{"new-dataset":0.0755491159,"dev-research":0.2056612815,"prompt-eng":0.3788852216,"data-quality":0.4556113216,"ml-security":0.5290989797}}
{"text":"We consider the recently proposed Restormer model, as well as NAFNet and the \"Baseline network\" which are both simplified versions of a Restormer.","meta":{"url":"http://arxiv.org/abs/2307.13856v1"},"cats":{"new-dataset":0.0736516155,"dev-research":0.1869715329,"prompt-eng":0.4365823717,"data-quality":0.1715152815,"ml-security":0.0940542484}}
{"text":"We use Projected Gradient Descent (PGD) and CosPGD, a recently proposed adversarial attack tailored to pixel-wise prediction tasks for our robustness evaluation.","meta":{"url":"http://arxiv.org/abs/2307.13856v1"},"cats":{"new-dataset":0.122255218,"dev-research":0.2550713581,"prompt-eng":0.4144786598,"data-quality":0.3033351797,"ml-security":0.8042741961}}
{"text":"Our experiments are performed on real-world images from the GoPro dataset for image deblurring.","meta":{"url":"http://arxiv.org/abs/2307.13856v1"},"cats":{"new-dataset":0.4524533775,"dev-research":0.2188023357,"prompt-eng":0.37531384,"data-quality":0.247329926,"ml-security":0.1374782009}}
{"text":"Our analysis indicates that contrary to as advocated by ViTs in image classification works, these models are highly susceptible to adversarial attacks.","meta":{"url":"http://arxiv.org/abs/2307.13856v1"},"cats":{"new-dataset":0.0415830815,"dev-research":0.2181814903,"prompt-eng":0.380549024,"data-quality":0.4182177781,"ml-security":0.7462463041}}
{"text":"We attempt to improve their robustness through adversarial training.","meta":{"url":"http://arxiv.org/abs/2307.13856v1"},"cats":{"new-dataset":0.0525066954,"dev-research":0.2637292619,"prompt-eng":0.4148847622,"data-quality":0.5204976335,"ml-security":0.7295448741}}
{"text":"While this yields a significant increase in robustness for Restormer, results on other networks are less promising.","meta":{"url":"http://arxiv.org/abs/2307.13856v1"},"cats":{"new-dataset":0.0149693085,"dev-research":0.2313093546,"prompt-eng":0.4413064457,"data-quality":0.3129736617,"ml-security":0.194097698}}
{"text":"Interestingly, the design choices in NAFNet and Baselines, which were based on iid performance, and not on robust generalization, seem to be at odds with the model robustness.","meta":{"url":"http://arxiv.org/abs/2307.13856v1"},"cats":{"new-dataset":0.0404017172,"dev-research":0.2423597439,"prompt-eng":0.4114360969,"data-quality":0.2678499068,"ml-security":0.2484405143}}
{"text":"Thus, we investigate this further and find a fix.","meta":{"url":"http://arxiv.org/abs/2307.13856v1"},"cats":{"new-dataset":0.1381415501,"dev-research":0.2931877983,"prompt-eng":0.380468315,"data-quality":0.4114118255,"ml-security":0.0923983783}}
{"text":"Convolutional layers have long served as the primary workhorse for image classification.","meta":{"url":"http://arxiv.org/abs/2307.13855v1"},"cats":{"new-dataset":0.1021590111,"dev-research":0.1971998466,"prompt-eng":0.3187364711,"data-quality":0.1775869675,"ml-security":0.1822824808}}
{"text":"Recently, an alternative to convolution was proposed using the Sharpened Cosine Similarity (SCS), which in theory may serve as a better feature detector.","meta":{"url":"http://arxiv.org/abs/2307.13855v1"},"cats":{"new-dataset":0.0405310675,"dev-research":0.2677673829,"prompt-eng":0.3971093376,"data-quality":0.2723807547,"ml-security":0.1196013225}}
{"text":"While multiple sources report promising results, there has not been to date a full-scale empirical analysis of neural network performance using these new layers.","meta":{"url":"http://arxiv.org/abs/2307.13855v1"},"cats":{"new-dataset":0.0415522385,"dev-research":0.2263942857,"prompt-eng":0.3318144952,"data-quality":0.2086357155,"ml-security":0.2228029956}}
{"text":"In our work, we explore SCS's parameter behavior and potential as a drop-in replacement for convolutions in multiple CNN architectures benchmarked on CIFAR-10.","meta":{"url":"http://arxiv.org/abs/2307.13855v1"},"cats":{"new-dataset":0.0950617834,"dev-research":0.1885971149,"prompt-eng":0.382315225,"data-quality":0.2077304581,"ml-security":0.2044962838}}
{"text":"We find that while SCS may not yield significant increases in accuracy, it may learn more interpretable representations.","meta":{"url":"http://arxiv.org/abs/2307.13855v1"},"cats":{"new-dataset":0.0118052308,"dev-research":0.3007215556,"prompt-eng":0.4000337192,"data-quality":0.2776991458,"ml-security":0.2152345685}}
{"text":"We also find that, in some circumstances, SCS may confer a slight increase in adversarial robustness.","meta":{"url":"http://arxiv.org/abs/2307.13855v1"},"cats":{"new-dataset":0.0076123702,"dev-research":0.2562878771,"prompt-eng":0.3952769701,"data-quality":0.3225479214,"ml-security":0.7128017933}}
{"text":"With generative AI advances, the exciting potential for autonomous agents to manage daily tasks via natural language commands has emerged.","meta":{"url":"http://arxiv.org/abs/2307.13854v1"},"cats":{"new-dataset":0.131016438,"dev-research":0.2612450512,"prompt-eng":0.4264036837,"data-quality":0.1319397857,"ml-security":0.0843445173}}
{"text":"However, cur rent agents are primarily created and tested in simplified synthetic environments, substantially limiting real-world scenario representation.","meta":{"url":"http://arxiv.org/abs/2307.13854v1"},"cats":{"new-dataset":0.1085040397,"dev-research":0.3029585226,"prompt-eng":0.3713490096,"data-quality":0.1000587457,"ml-security":0.2084796945}}
{"text":"In this paper, we build an environment for agent command and control that is highly realistic and reproducible.","meta":{"url":"http://arxiv.org/abs/2307.13854v1"},"cats":{"new-dataset":0.3531125171,"dev-research":0.2966945902,"prompt-eng":0.5251365636,"data-quality":0.0975923654,"ml-security":0.1833456539}}
{"text":"Specifically, we focus on agents that perform tasks on websites, and we create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management.","meta":{"url":"http://arxiv.org/abs/2307.13854v1"},"cats":{"new-dataset":0.1650949804,"dev-research":0.2876158071,"prompt-eng":0.4285053769,"data-quality":0.0902043317,"ml-security":0.0723782793}}
{"text":"Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving.","meta":{"url":"http://arxiv.org/abs/2307.13854v1"},"cats":{"new-dataset":0.1836194235,"dev-research":0.4698712229,"prompt-eng":0.4034211927,"data-quality":0.1040662713,"ml-security":0.1059822672}}
{"text":"Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions.","meta":{"url":"http://arxiv.org/abs/2307.13854v1"},"cats":{"new-dataset":0.1712146158,"dev-research":0.3833006958,"prompt-eng":0.5141762691,"data-quality":0.2719160095,"ml-security":0.071883407}}
{"text":"The tasks in our benchmark are diverse, long-horizon, and are designed to emulate tasks that humans routinely perform on the internet.","meta":{"url":"http://arxiv.org/abs/2307.13854v1"},"cats":{"new-dataset":0.1365516438,"dev-research":0.2737436439,"prompt-eng":0.4091044186,"data-quality":0.1015192295,"ml-security":0.1128254576}}
{"text":"We design and implement several autonomous agents, integrating recent techniques such as reasoning before acting.","meta":{"url":"http://arxiv.org/abs/2307.13854v1"},"cats":{"new-dataset":0.0997293487,"dev-research":0.3108439491,"prompt-eng":0.4802066819,"data-quality":0.0824747626,"ml-security":0.1120908756}}
{"text":"The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 10.59%.","meta":{"url":"http://arxiv.org/abs/2307.13854v1"},"cats":{"new-dataset":0.0865598114,"dev-research":0.2898249744,"prompt-eng":0.4450727718,"data-quality":0.0921484152,"ml-security":0.0752117648}}
{"text":"These results highlight the need for further development of robust agents, that current state-of-the-art LMs are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress.","meta":{"url":"http://arxiv.org/abs/2307.13854v1"},"cats":{"new-dataset":0.1527497447,"dev-research":0.2790343629,"prompt-eng":0.480422713,"data-quality":0.1996035828,"ml-security":0.1803792411}}
{"text":"Our code, data, environment reproduction resources, and video demonstrations are publicly available at https://webarena.dev/.","meta":{"url":"http://arxiv.org/abs/2307.13854v1"},"cats":{"new-dataset":0.7768239693,"dev-research":0.3468165737,"prompt-eng":0.41617347,"data-quality":0.1354453793,"ml-security":0.1178511765}}
{"text":"Decentralized machine learning has broadened its scope recently with the invention of Federated Learning (FL), Split Learning (SL), and their hybrids like Split Federated Learning (SplitFed or SFL).","meta":{"url":"http://arxiv.org/abs/2307.13851v1"},"cats":{"new-dataset":0.0445038125,"dev-research":0.2194889143,"prompt-eng":0.3209551632,"data-quality":0.1147002029,"ml-security":0.2960042536}}
{"text":"The goal of SFL is to reduce the computational power required by each client in FL and parallelize SL while maintaining privacy.","meta":{"url":"http://arxiv.org/abs/2307.13851v1"},"cats":{"new-dataset":0.0364790494,"dev-research":0.3220020058,"prompt-eng":0.3642324091,"data-quality":0.0572476093,"ml-security":0.1531055411}}
{"text":"This paper investigates the robustness of SFL against packet loss on communication links.","meta":{"url":"http://arxiv.org/abs/2307.13851v1"},"cats":{"new-dataset":0.0294582668,"dev-research":0.2892459876,"prompt-eng":0.3682965982,"data-quality":0.2649177807,"ml-security":0.2961777244}}
{"text":"The performance of various SFL aggregation strategies is examined by splitting the model at two points -- shallow split and deep split -- and testing whether the split point makes a statistically significant difference to the accuracy of the final model.","meta":{"url":"http://arxiv.org/abs/2307.13851v1"},"cats":{"new-dataset":0.0224154919,"dev-research":0.2116256114,"prompt-eng":0.4119375787,"data-quality":0.1562016775,"ml-security":0.0657336518}}
{"text":"Experiments are carried out on a segmentation model for human embryo images and indicate the statistically significant advantage of a deeper split point.","meta":{"url":"http://arxiv.org/abs/2307.13851v1"},"cats":{"new-dataset":0.0652844055,"dev-research":0.1749648403,"prompt-eng":0.4055532067,"data-quality":0.1737562915,"ml-security":0.1088530141}}
{"text":"Understanding multimodal perception for embodied AI is an open question because such inputs may contain highly complementary as well as redundant information for the task.","meta":{"url":"http://arxiv.org/abs/2307.13850v1"},"cats":{"new-dataset":0.0135174091,"dev-research":0.2578646449,"prompt-eng":0.3844942691,"data-quality":0.1271583864,"ml-security":0.1262893698}}
{"text":"A relevant direction for multimodal policies is understanding the global trends of each modality at the fusion layer.","meta":{"url":"http://arxiv.org/abs/2307.13850v1"},"cats":{"new-dataset":0.1519361426,"dev-research":0.2462590742,"prompt-eng":0.3970674331,"data-quality":0.1229663693,"ml-security":0.1147848078}}
{"text":"To this end, we disentangle the attributions for visual, language, and previous action inputs across different policies trained on the ALFRED dataset.","meta":{"url":"http://arxiv.org/abs/2307.13850v1"},"cats":{"new-dataset":0.2383653641,"dev-research":0.254900754,"prompt-eng":0.4138631763,"data-quality":0.2838698306,"ml-security":0.1935012637}}
{"text":"Attribution analysis can be utilized to rank and group the failure scenarios, investigate modeling and dataset biases, and critically analyze multimodal EAI policies for robustness and user trust before deployment.","meta":{"url":"http://arxiv.org/abs/2307.13850v1"},"cats":{"new-dataset":0.166683609,"dev-research":0.3868337503,"prompt-eng":0.4766570553,"data-quality":0.3636431317,"ml-security":0.2948133019}}
{"text":"We present MAEA, a framework to compute global attributions per modality of any differentiable policy.","meta":{"url":"http://arxiv.org/abs/2307.13850v1"},"cats":{"new-dataset":0.1505437326,"dev-research":0.1641423044,"prompt-eng":0.3958309944,"data-quality":0.2184610368,"ml-security":0.2409758586}}
{"text":"In addition, we show how attributions enable lower-level behavior analysis in EAI policies for language and visual attributions.","meta":{"url":"http://arxiv.org/abs/2307.13850v1"},"cats":{"new-dataset":0.0879860716,"dev-research":0.3847898448,"prompt-eng":0.4655614556,"data-quality":0.3540457663,"ml-security":0.2847582879}}
{"text":"This paper introduces TeleBTC, a fully decentralized protocol designed to wrap Bitcoin (BTC) on programmable blockchains.","meta":{"url":"http://arxiv.org/abs/2307.13848v1"},"cats":{"new-dataset":0.2045079473,"dev-research":0.2771408017,"prompt-eng":0.3831803497,"data-quality":0.0945738246,"ml-security":0.1877185838}}
{"text":"The creation of a decentralized wrapped BTC presents challenges due to the non-programmable nature of Bitcoin, making it difficult to custody BTCs in a decentralized way.","meta":{"url":"http://arxiv.org/abs/2307.13848v1"},"cats":{"new-dataset":0.0602007777,"dev-research":0.2859097925,"prompt-eng":0.3840939172,"data-quality":0.1088936031,"ml-security":0.2106033134}}
{"text":"Existing solutions have addressed this challenge by introducing an external layer of validators who take custody of users' BTCs.","meta":{"url":"http://arxiv.org/abs/2307.13848v1"},"cats":{"new-dataset":0.0634702422,"dev-research":0.2669721422,"prompt-eng":0.4658118757,"data-quality":0.2062521933,"ml-security":0.3409672411}}
{"text":"However, the security and decentralization of this layer are inferior to the underlying blockchains on which wrapped BTC is built.","meta":{"url":"http://arxiv.org/abs/2307.13848v1"},"cats":{"new-dataset":0.0131360459,"dev-research":0.238554332,"prompt-eng":0.3505186675,"data-quality":0.1161348092,"ml-security":0.3095235656}}
{"text":"Moreover, the process of joining or leaving for a validator has become overly complex and expensive.","meta":{"url":"http://arxiv.org/abs/2307.13848v1"},"cats":{"new-dataset":0.0037206245,"dev-research":0.3394193242,"prompt-eng":0.3534283312,"data-quality":0.1680076312,"ml-security":0.1872820026}}
{"text":"To overcome these limitations, we propose a novel approach that eliminates the need for such an external layer by leveraging the light client bridge protocol.","meta":{"url":"http://arxiv.org/abs/2307.13848v1"},"cats":{"new-dataset":0.0637131717,"dev-research":0.2778773147,"prompt-eng":0.4165642843,"data-quality":0.102761171,"ml-security":0.1420031182}}
{"text":"Additionally, we employ economic mechanisms such as incentivization and slashing, resulting in a secure and trust-minimized wrapped BTC solution.","meta":{"url":"http://arxiv.org/abs/2307.13848v1"},"cats":{"new-dataset":0.0171608847,"dev-research":0.2360387262,"prompt-eng":0.3906009367,"data-quality":0.101033868,"ml-security":0.3187745446}}
{"text":"With TeleBTC, users can seamlessly transfer their BTC to other blockchains and utilize it within decentralized applications.","meta":{"url":"http://arxiv.org/abs/2307.13848v1"},"cats":{"new-dataset":0.0448489711,"dev-research":0.3024408713,"prompt-eng":0.397079607,"data-quality":0.0673783188,"ml-security":0.134770019}}
{"text":"Furthermore, they can unwrap their TeleBTC and reclaim the native BTC.","meta":{"url":"http://arxiv.org/abs/2307.13848v1"},"cats":{"new-dataset":0.0552693478,"dev-research":0.2251284483,"prompt-eng":0.3727364954,"data-quality":0.1340534607,"ml-security":0.1552800174}}
{"text":"To address the high costs associated with light client bridges, we present an optimistic approach that minimizes the cost.","meta":{"url":"http://arxiv.org/abs/2307.13848v1"},"cats":{"new-dataset":0.0423553054,"dev-research":0.2988408573,"prompt-eng":0.4418562543,"data-quality":0.1183854321,"ml-security":0.0993041601}}
{"text":"This approach significantly reduces the operational expenses of running the protocol.","meta":{"url":"http://arxiv.org/abs/2307.13848v1"},"cats":{"new-dataset":0.0091311021,"dev-research":0.296131279,"prompt-eng":0.4174015492,"data-quality":0.0884194623,"ml-security":0.1839302223}}
{"text":"Reachability types are a recent proposal that has shown promise in scaling to higher-order but monomorphic settings, tracking aliasing and separation on top of a substrate inspired by separation logic.","meta":{"url":"http://arxiv.org/abs/2307.13844v1"},"cats":{"new-dataset":0.0236260816,"dev-research":0.265118097,"prompt-eng":0.4161719424,"data-quality":0.1015366956,"ml-security":0.1126190318}}
{"text":"The prior $\\lambda^*$ reachability type system qualifies types with sets of reachable variables and guarantees separation if two terms have disjoint qualifiers.","meta":{"url":"http://arxiv.org/abs/2307.13844v1"},"cats":{"new-dataset":0.0111371733,"dev-research":0.1886426621,"prompt-eng":0.3821369074,"data-quality":0.1225476431,"ml-security":0.1334331445}}
{"text":"However, naive extensions with type polymorphism and/or precise reachability polymorphism are unsound, making $\\lambda^*$ unsuitable for adoption in real languages.","meta":{"url":"http://arxiv.org/abs/2307.13844v1"},"cats":{"new-dataset":0.0060421351,"dev-research":0.3072724046,"prompt-eng":0.2992534217,"data-quality":0.2248235511,"ml-security":0.2244117632}}
{"text":"Combining reachability and type polymorphism that is precise, sound, and parametric remains an open challenge.   ","meta":{"url":"http://arxiv.org/abs/2307.13844v1"},"cats":{"new-dataset":0.0315600574,"dev-research":0.2689573745,"prompt-eng":0.4250694356,"data-quality":0.1496578191,"ml-security":0.1022876394}}
{"text":"This paper presents a rethinking of the design of reachability tracking and proposes a solution to the key challenge of reachability polymorphism.","meta":{"url":"http://arxiv.org/abs/2307.13844v1"},"cats":{"new-dataset":0.0525542184,"dev-research":0.3357830186,"prompt-eng":0.472982502,"data-quality":0.1420580437,"ml-security":0.1413437502}}
{"text":"Instead of always tracking the transitive closure of reachable variables as in the original design, we only track variables reachable in a single step and compute transitive closures only when necessary, thus preserving chains of reachability over known variables that can be refined using substitution.","meta":{"url":"http://arxiv.org/abs/2307.13844v1"},"cats":{"new-dataset":0.0383590418,"dev-research":0.2849994014,"prompt-eng":0.4739141408,"data-quality":0.138961454,"ml-security":0.1175320194}}
{"text":"To enable this property, we introduce a new freshness qualifier, which indicates variables whose reachability sets may grow during evaluation steps.","meta":{"url":"http://arxiv.org/abs/2307.13844v1"},"cats":{"new-dataset":0.0814190315,"dev-research":0.319302337,"prompt-eng":0.4597461536,"data-quality":0.2535414654,"ml-security":0.1331143117}}
{"text":"These ideas yield the simply-typed $\\lambda^\\diamond$-calculus with precise lightweight, i.e., quantifier-free, reachability polymorphism, and the $\\mathsf{F}_{<:}^\\diamond$-calculus with bounded parametric polymorphism over types and reachability qualifiers.","meta":{"url":"http://arxiv.org/abs/2307.13844v1"},"cats":{"new-dataset":0.0302230845,"dev-research":0.288330848,"prompt-eng":0.3742742646,"data-quality":0.1018159648,"ml-security":0.1795016414}}
{"text":"We prove type soundness and a preservation of separation property in Coq.","meta":{"url":"http://arxiv.org/abs/2307.13844v1"},"cats":{"new-dataset":0.0289265278,"dev-research":0.242660035,"prompt-eng":0.3536190299,"data-quality":0.245701572,"ml-security":0.1229889978}}
{"text":"Crafting effective deep learning models for medical image analysis is a complex task, particularly in cases where the medical image dataset lacks significant inter-class variation.","meta":{"url":"http://arxiv.org/abs/2307.13842v1"},"cats":{"new-dataset":0.0669239666,"dev-research":0.2083336231,"prompt-eng":0.3378184038,"data-quality":0.1632624912,"ml-security":0.1876749758}}
{"text":"This challenge is further aggravated when employing such datasets to generate synthetic images using generative adversarial networks (GANs), as the output of GANs heavily relies on the input data.","meta":{"url":"http://arxiv.org/abs/2307.13842v1"},"cats":{"new-dataset":0.6850357908,"dev-research":0.1984792084,"prompt-eng":0.3702834125,"data-quality":0.2747364761,"ml-security":0.2784421509}}
{"text":"In this research, we propose a novel filtering algorithm called Cosine Similarity-based Image Filtering (CosSIF).","meta":{"url":"http://arxiv.org/abs/2307.13842v1"},"cats":{"new-dataset":0.0390383323,"dev-research":0.2473219663,"prompt-eng":0.3563853832,"data-quality":0.1643119501,"ml-security":0.0602580358}}
{"text":"We leverage CosSIF to develop two distinct filtering methods: Filtering Before GAN Training (FBGT) and Filtering After GAN Training (FAGT).","meta":{"url":"http://arxiv.org/abs/2307.13842v1"},"cats":{"new-dataset":0.0685520963,"dev-research":0.2061358549,"prompt-eng":0.3871001654,"data-quality":0.2326417405,"ml-security":0.1239773497}}
{"text":"FBGT involves the removal of real images that exhibit similarities to images of other classes before utilizing them as the training dataset for a GAN.","meta":{"url":"http://arxiv.org/abs/2307.13842v1"},"cats":{"new-dataset":0.0800673041,"dev-research":0.2608032614,"prompt-eng":0.3295956074,"data-quality":0.2478080846,"ml-security":0.134904717}}
{"text":"On the other hand, FAGT focuses on eliminating synthetic images with less discriminative features compared to real images used for training the GAN.","meta":{"url":"http://arxiv.org/abs/2307.13842v1"},"cats":{"new-dataset":0.0155268557,"dev-research":0.249429517,"prompt-eng":0.3362714423,"data-quality":0.2071107112,"ml-security":0.1188657708}}
{"text":"Experimental results reveal that employing either the FAGT or FBGT method with modern transformer and convolutional-based networks leads to substantial performance gains in various evaluation metrics.","meta":{"url":"http://arxiv.org/abs/2307.13842v1"},"cats":{"new-dataset":0.0215124009,"dev-research":0.2530614789,"prompt-eng":0.3897101057,"data-quality":0.179499023,"ml-security":0.0646315467}}
{"text":"FAGT implementation on the ISIC-2016 dataset surpasses the baseline method in terms of sensitivity by 1.59\\% and AUC by 1.88\\%.","meta":{"url":"http://arxiv.org/abs/2307.13842v1"},"cats":{"new-dataset":0.3007382504,"dev-research":0.2335621365,"prompt-eng":0.451943032,"data-quality":0.2209445668,"ml-security":0.0599169967}}
{"text":"Furthermore, for the HAM10000 dataset, applying FABT outperforms the baseline approach in terms of recall by 13.75\\%, and with the sole implementation of FAGT, achieves a maximum accuracy of 94.44\\%.","meta":{"url":"http://arxiv.org/abs/2307.13842v1"},"cats":{"new-dataset":0.3360873408,"dev-research":0.2180371122,"prompt-eng":0.4210108853,"data-quality":0.2751561997,"ml-security":0.0740108904}}
{"text":"Distributions on integers are ubiquitous in probabilistic modeling but remain challenging for many of today's probabilistic programming languages (PPLs).","meta":{"url":"http://arxiv.org/abs/2307.13837v1"},"cats":{"new-dataset":0.0930309891,"dev-research":0.2648411057,"prompt-eng":0.4166782077,"data-quality":0.1421992459,"ml-security":0.1797410265}}
{"text":"The core challenge comes from discrete structure: many of today's PPL inference strategies rely on enumeration, sampling, or differentiation in order to scale, which fail for high-dimensional complex discrete distributions involving integers.","meta":{"url":"http://arxiv.org/abs/2307.13837v1"},"cats":{"new-dataset":0.0853474517,"dev-research":0.1997135838,"prompt-eng":0.3798134674,"data-quality":0.1292285214,"ml-security":0.1606588958}}
{"text":"Our insight is that there is structure in arithmetic that these approaches are not using.","meta":{"url":"http://arxiv.org/abs/2307.13837v1"},"cats":{"new-dataset":0.004446359,"dev-research":0.3014827133,"prompt-eng":0.2666210548,"data-quality":0.1143183933,"ml-security":0.109430196}}
{"text":"We present a binary encoding strategy for discrete distributions that exploits the rich logical structure of integer operations like summation and comparison.","meta":{"url":"http://arxiv.org/abs/2307.13837v1"},"cats":{"new-dataset":0.0581044891,"dev-research":0.2589317141,"prompt-eng":0.4341505107,"data-quality":0.1353552644,"ml-security":0.1441578781}}
{"text":"We leverage this structured encoding with knowledge compilation to perform exact probabilistic inference, and show that this approach scales to much larger integer distributions with arithmetic.","meta":{"url":"http://arxiv.org/abs/2307.13837v1"},"cats":{"new-dataset":0.1810235101,"dev-research":0.266829819,"prompt-eng":0.4367992079,"data-quality":0.1652497786,"ml-security":0.1465046017}}
{"text":"Side-channel attacks pose significant challenges to the security of embedded systems, often allowing attackers to circumvent encryption algorithms in minutes compared to the trillions of years required for brute-force attacks.","meta":{"url":"http://arxiv.org/abs/2307.13834v1"},"cats":{"new-dataset":0.0581203584,"dev-research":0.3241491606,"prompt-eng":0.3854912491,"data-quality":0.1594250865,"ml-security":0.6297491775}}
{"text":"To mitigate these vulnerabilities, various countermeasures have been developed.","meta":{"url":"http://arxiv.org/abs/2307.13834v1"},"cats":{"new-dataset":0.1200360583,"dev-research":0.4011669227,"prompt-eng":0.3886844343,"data-quality":0.1867096548,"ml-security":0.6072234572}}
{"text":"This study focuses on two specific countermeasures: randomization of the encryption algorithm's clock and the incorporation of a dummy core to disguise power traces.   ","meta":{"url":"http://arxiv.org/abs/2307.13834v1"},"cats":{"new-dataset":0.0402622482,"dev-research":0.2598089697,"prompt-eng":0.4131429175,"data-quality":0.1389706387,"ml-security":0.5355481692}}
{"text":"The objective of this research is to identify the optimal frequencies that yield the highest level of randomness when these two countermeasures are combined.","meta":{"url":"http://arxiv.org/abs/2307.13834v1"},"cats":{"new-dataset":0.034152153,"dev-research":0.1649945153,"prompt-eng":0.4540777145,"data-quality":0.1942390871,"ml-security":0.175642661}}
{"text":"By investigating the interplay between clock randomization and the presence of dummy cores, we aim to enhance the overall security of embedded systems.","meta":{"url":"http://arxiv.org/abs/2307.13834v1"},"cats":{"new-dataset":0.0326340671,"dev-research":0.3455563022,"prompt-eng":0.44622354,"data-quality":0.1481822855,"ml-security":0.5669364789}}
{"text":"The insights gained from this study will contribute to the development of more robust countermeasures against side-channel attacks, bolstering the protection of sensitive information and systems.   ","meta":{"url":"http://arxiv.org/abs/2307.13834v1"},"cats":{"new-dataset":0.0869890098,"dev-research":0.377482888,"prompt-eng":0.3931594697,"data-quality":0.2288341338,"ml-security":0.7976749278}}
{"text":"To achieve this, we conduct simulations and perform side-channel attacks on an FPGA to establish the relationship between frequencies and the resulting protection.","meta":{"url":"http://arxiv.org/abs/2307.13834v1"},"cats":{"new-dataset":0.1151399173,"dev-research":0.2588058271,"prompt-eng":0.4391681759,"data-quality":0.1200184772,"ml-security":0.4820257914}}
{"text":"We break the encryption on a non-duplicated circuit and note the least amount of measured power traces necessary and the timing overhead.","meta":{"url":"http://arxiv.org/abs/2307.13834v1"},"cats":{"new-dataset":0.0941583926,"dev-research":0.2372831179,"prompt-eng":0.4533021623,"data-quality":0.1270166863,"ml-security":0.2652675272}}
{"text":"We do this for all sets of frequencies considered which gives a good indication of which sets of frequencies give good protection.","meta":{"url":"http://arxiv.org/abs/2307.13834v1"},"cats":{"new-dataset":0.037934678,"dev-research":0.2914147154,"prompt-eng":0.4438580877,"data-quality":0.1843661691,"ml-security":0.35542687}}
{"text":"By comparing the frequencies generated with those from the duplicated circuit we use similar conclusions to prove whether a frequency set is secure or not.   ","meta":{"url":"http://arxiv.org/abs/2307.13834v1"},"cats":{"new-dataset":0.037329876,"dev-research":0.2518184825,"prompt-eng":0.4289031233,"data-quality":0.2890432331,"ml-security":0.3067683949}}
{"text":"Based on our results we argue that having one frequency lower than half of the base frequency and the other frequencies being close but not higher than the base gives the highest security compared to the timing overhead measured.","meta":{"url":"http://arxiv.org/abs/2307.13834v1"},"cats":{"new-dataset":0.0362970666,"dev-research":0.2197684752,"prompt-eng":0.4421092041,"data-quality":0.133580527,"ml-security":0.2865071984}}
{"text":"Cardinality estimation methods based on probability distribution estimation have achieved high-precision estimation results compared to traditional methods.","meta":{"url":"http://arxiv.org/abs/2307.13494v1"},"cats":{"new-dataset":0.0411148721,"dev-research":0.2151934028,"prompt-eng":0.4213097332,"data-quality":0.2271561173,"ml-security":0.0807952334}}
{"text":"However, the most advanced methods suffer from high estimation costs due to the sampling method they use when dealing with range queries.","meta":{"url":"http://arxiv.org/abs/2307.13494v1"},"cats":{"new-dataset":0.0270133712,"dev-research":0.2727330442,"prompt-eng":0.3595880306,"data-quality":0.1302581703,"ml-security":0.1198530858}}
{"text":"Also, such a sampling method makes them difficult to differentiate, so the supervision signal from the query workload is difficult to train the model to improve the accuracy of cardinality estimation.","meta":{"url":"http://arxiv.org/abs/2307.13494v1"},"cats":{"new-dataset":0.0054881151,"dev-research":0.2690513015,"prompt-eng":0.4039540236,"data-quality":0.2758812505,"ml-security":0.110586811}}
{"text":"In this paper, we propose a new hybrid and deterministic modeling approach (Duet) for the cardinality estimation problem which has better efficiency and scalability compared to previous approaches.","meta":{"url":"http://arxiv.org/abs/2307.13494v1"},"cats":{"new-dataset":0.0679840675,"dev-research":0.2166162556,"prompt-eng":0.4374461259,"data-quality":0.1969081141,"ml-security":0.1065790147}}
{"text":"Duet allows for direct cardinality estimation of range queries with significantly lower time and memory costs, as well as in a differentiable form.","meta":{"url":"http://arxiv.org/abs/2307.13494v1"},"cats":{"new-dataset":0.0271782373,"dev-research":0.288085439,"prompt-eng":0.3937201235,"data-quality":0.0856889086,"ml-security":0.0773881461}}
{"text":"As the prediction process of this approach is differentiable, we can incorporate queries with larger model estimation errors into the training process to address the long-tail distribution problem of model estimation errors on high dimensional tables.","meta":{"url":"http://arxiv.org/abs/2307.13494v1"},"cats":{"new-dataset":0.1485882357,"dev-research":0.1898833082,"prompt-eng":0.4208875792,"data-quality":0.1849399397,"ml-security":0.22864693}}
{"text":"We evaluate Duet on classical datasets and benchmarks, and the results prove the effectiveness of Duet.","meta":{"url":"http://arxiv.org/abs/2307.13494v1"},"cats":{"new-dataset":0.5810614983,"dev-research":0.2307665741,"prompt-eng":0.382629035,"data-quality":0.2143812448,"ml-security":0.0843901997}}
{"text":"The proliferation of misinformation and propaganda is a global challenge, with profound effects during major crises such as the COVID-19 pandemic and the Russian invasion of Ukraine.","meta":{"url":"http://arxiv.org/abs/2307.13180v1"},"cats":{"new-dataset":0.1053738234,"dev-research":0.2877228605,"prompt-eng":0.3379072202,"data-quality":0.2450147973,"ml-security":0.348337529}}
{"text":"Understanding the spread of misinformation and its social impacts requires identifying the news sources spreading false information.","meta":{"url":"http://arxiv.org/abs/2307.13180v1"},"cats":{"new-dataset":0.0298681384,"dev-research":0.3160094305,"prompt-eng":0.3229993777,"data-quality":0.4222092199,"ml-security":0.370179779}}
{"text":"While machine learning (ML) techniques have been proposed to address this issue, ML models have failed to provide an efficient implementation scenario that yields useful results.","meta":{"url":"http://arxiv.org/abs/2307.13180v1"},"cats":{"new-dataset":0.0315829865,"dev-research":0.2449095282,"prompt-eng":0.4065919551,"data-quality":0.3084338992,"ml-security":0.2895562582}}
{"text":"In prior research, the precision of deployment in real traffic deteriorates significantly, experiencing a decrement up to ten times compared to the results derived from benchmark data sets.","meta":{"url":"http://arxiv.org/abs/2307.13180v1"},"cats":{"new-dataset":0.0827907924,"dev-research":0.3366240762,"prompt-eng":0.3838278481,"data-quality":0.2169347798,"ml-security":0.1933684777}}
{"text":"Our research addresses this gap by proposing a graph-based approach to capture navigational patterns and generate traffic-based features which are used to train a classification model.","meta":{"url":"http://arxiv.org/abs/2307.13180v1"},"cats":{"new-dataset":0.2709911696,"dev-research":0.2876577601,"prompt-eng":0.3937323078,"data-quality":0.2198614693,"ml-security":0.1418456902}}
{"text":"These navigational and traffic-based features result in classifiers that present outstanding performance when evaluated against real traffic.","meta":{"url":"http://arxiv.org/abs/2307.13180v1"},"cats":{"new-dataset":0.0810248353,"dev-research":0.2931807537,"prompt-eng":0.3936460481,"data-quality":0.2651441481,"ml-security":0.2022813075}}
{"text":"Moreover, we also propose graph-based filtering techniques to filter out models to be classified by our framework.","meta":{"url":"http://arxiv.org/abs/2307.13180v1"},"cats":{"new-dataset":0.0377541595,"dev-research":0.2197836392,"prompt-eng":0.3689377014,"data-quality":0.2214472362,"ml-security":0.1804740275}}
{"text":"These filtering techniques increase the signal-to-noise ratio of the models to be classified, greatly reducing false positives and the computational cost of deploying the model.","meta":{"url":"http://arxiv.org/abs/2307.13180v1"},"cats":{"new-dataset":0.0248797946,"dev-research":0.2504509157,"prompt-eng":0.4128760161,"data-quality":0.2668103113,"ml-security":0.2257253975}}
{"text":"Our proposed framework for the detection of misinformation domains achieves a precision of 0.78 when evaluated in real traffic.","meta":{"url":"http://arxiv.org/abs/2307.13180v1"},"cats":{"new-dataset":0.0697970349,"dev-research":0.2975757738,"prompt-eng":0.4187864177,"data-quality":0.5773762904,"ml-security":0.3302790719}}
{"text":"This outcome represents an improvement factor of over ten times over those achieved in previous studies.","meta":{"url":"http://arxiv.org/abs/2307.13180v1"},"cats":{"new-dataset":0.0147020299,"dev-research":0.2943664447,"prompt-eng":0.3610904907,"data-quality":0.1349314821,"ml-security":0.0614195028}}
{"text":"In recent decades, social network anonymization has become a crucial research field due to its pivotal role in preserving users' privacy.","meta":{"url":"http://arxiv.org/abs/2307.13179v1"},"cats":{"new-dataset":0.0371629986,"dev-research":0.2054940375,"prompt-eng":0.302888817,"data-quality":0.1283028742,"ml-security":0.348421743}}
{"text":"However, the high diversity of approaches introduced in relevant studies poses a challenge to gaining a profound understanding of the field.","meta":{"url":"http://arxiv.org/abs/2307.13179v1"},"cats":{"new-dataset":0.0223689488,"dev-research":0.2665424333,"prompt-eng":0.2615442325,"data-quality":0.1036218557,"ml-security":0.0926330605}}
{"text":"In response to this, the current study presents an exhaustive and well-structured bibliometric analysis of the social network anonymization field.","meta":{"url":"http://arxiv.org/abs/2307.13179v1"},"cats":{"new-dataset":0.2019209393,"dev-research":0.2330141501,"prompt-eng":0.3015905341,"data-quality":0.1930715528,"ml-security":0.1082099547}}
{"text":"To begin our research, related studies from the period of 2007-2022 were collected from the Scopus Database then pre-processed.","meta":{"url":"http://arxiv.org/abs/2307.13179v1"},"cats":{"new-dataset":0.3060743975,"dev-research":0.1895962464,"prompt-eng":0.3444704247,"data-quality":0.1116103843,"ml-security":0.0486037642}}
{"text":"Following this, the VOSviewer was used to visualize the network of authors' keywords.","meta":{"url":"http://arxiv.org/abs/2307.13179v1"},"cats":{"new-dataset":0.1390222704,"dev-research":0.3166507551,"prompt-eng":0.3414020544,"data-quality":0.2274109919,"ml-security":0.077150778}}
{"text":"Subsequently, extensive statistical and network analyses were performed to identify the most prominent keywords and trending topics.","meta":{"url":"http://arxiv.org/abs/2307.13179v1"},"cats":{"new-dataset":0.0631660563,"dev-research":0.3037124513,"prompt-eng":0.3468919085,"data-quality":0.2381742873,"ml-security":0.0970482163}}
{"text":"Additionally, the application of co-word analysis through SciMAT and the Alluvial diagram allowed us to explore the themes of social network anonymization and scrutinize their evolution over time.","meta":{"url":"http://arxiv.org/abs/2307.13179v1"},"cats":{"new-dataset":0.2178100383,"dev-research":0.291530111,"prompt-eng":0.3289685836,"data-quality":0.2062082469,"ml-security":0.1275432351}}
{"text":"These analyses culminated in an innovative taxonomy of the existing approaches and anticipation of potential trends in this domain.","meta":{"url":"http://arxiv.org/abs/2307.13179v1"},"cats":{"new-dataset":0.0772885698,"dev-research":0.3000881169,"prompt-eng":0.3363517266,"data-quality":0.1035336354,"ml-security":0.0820001524}}
{"text":"To the best of our knowledge, this is the first bibliometric analysis in the social network anonymization field, which offers a deeper understanding of the current state and an insightful roadmap for future research in this domain.","meta":{"url":"http://arxiv.org/abs/2307.13179v1"},"cats":{"new-dataset":0.3551398137,"dev-research":0.2188180775,"prompt-eng":0.3011345239,"data-quality":0.2020265088,"ml-security":0.1159142376}}
{"text":"Vulnerable road users (VRUs), such as pedestrians and bicyclists, are at a higher risk of being involved in crashes with motor vehicles, and crashes involving VRUs also are more likely to result in severe injuries or fatalities.","meta":{"url":"http://arxiv.org/abs/2307.13178v1"},"cats":{"new-dataset":0.0278273316,"dev-research":0.3787617059,"prompt-eng":0.346968867,"data-quality":0.1170358785,"ml-security":0.4834247191}}
{"text":"Signalized intersections are a major safety concern for VRUs due to their complex and dynamic nature, highlighting the need to understand how these road users interact with motor vehicles and deploy evidence-based countermeasures to improve safety performance.","meta":{"url":"http://arxiv.org/abs/2307.13178v1"},"cats":{"new-dataset":0.0804857213,"dev-research":0.3741471892,"prompt-eng":0.4048610624,"data-quality":0.1590800908,"ml-security":0.3184078578}}
{"text":"Crashes involving VRUs are relatively infrequent, making it difficult to understand the underlying contributing factors.","meta":{"url":"http://arxiv.org/abs/2307.13178v1"},"cats":{"new-dataset":0.032899911,"dev-research":0.4145597312,"prompt-eng":0.3547732454,"data-quality":0.247585095,"ml-security":0.2262109289}}
{"text":"An alternative is to identify and use conflicts between VRUs and motorized vehicles as a surrogate for safety performance.","meta":{"url":"http://arxiv.org/abs/2307.13178v1"},"cats":{"new-dataset":0.0500690902,"dev-research":0.3506299056,"prompt-eng":0.4270569305,"data-quality":0.137740398,"ml-security":0.2202500427}}
{"text":"Automatically detecting these conflicts using a video-based systems is a crucial step in developing smart infrastructure to enhance VRU safety.","meta":{"url":"http://arxiv.org/abs/2307.13178v1"},"cats":{"new-dataset":0.1796157102,"dev-research":0.4347963713,"prompt-eng":0.4547305438,"data-quality":0.2504065664,"ml-security":0.2951032773}}
{"text":"The Pennsylvania Department of Transportation conducted a study using video-based event monitoring system to assess VRU and motor vehicle interactions at fifteen signalized intersections across Pennsylvania to improve VRU safety performance.","meta":{"url":"http://arxiv.org/abs/2307.13178v1"},"cats":{"new-dataset":0.1489629149,"dev-research":0.3133863601,"prompt-eng":0.4067296356,"data-quality":0.1151714706,"ml-security":0.102848093}}
{"text":"This research builds on that study to assess the reliability of automatically generated surrogates in predicting confirmed conflicts using advanced data-driven models.","meta":{"url":"http://arxiv.org/abs/2307.13178v1"},"cats":{"new-dataset":0.1342529185,"dev-research":0.3681760688,"prompt-eng":0.4959431099,"data-quality":0.2822293089,"ml-security":0.3220478643}}
{"text":"The surrogate data used for analysis include automatically collectable variables such as vehicular and VRU speeds, movements, post-encroachment time, in addition to manually collected variables like signal states, lighting, and weather conditions.","meta":{"url":"http://arxiv.org/abs/2307.13178v1"},"cats":{"new-dataset":0.2022382465,"dev-research":0.2941005813,"prompt-eng":0.3996775679,"data-quality":0.082912561,"ml-security":0.1164854436}}
{"text":"The findings highlight the varying importance of specific surrogates in predicting true conflicts, some being more informative than others.","meta":{"url":"http://arxiv.org/abs/2307.13178v1"},"cats":{"new-dataset":0.0780717062,"dev-research":0.3154673649,"prompt-eng":0.4074539019,"data-quality":0.2040849188,"ml-security":0.2821212954}}
{"text":"The findings can assist transportation agencies to collect the right types of data to help prioritize infrastructure investments, such as bike lanes and crosswalks, and evaluate their effectiveness.","meta":{"url":"http://arxiv.org/abs/2307.13178v1"},"cats":{"new-dataset":0.1173176648,"dev-research":0.3119963585,"prompt-eng":0.3988970062,"data-quality":0.1214189823,"ml-security":0.0907527068}}
{"text":"In natural language generation (NLG), insight mining is seen as a data-to-text task, where data is mined for interesting patterns and verbalised into 'insight' statements.","meta":{"url":"http://arxiv.org/abs/2307.13176v1"},"cats":{"new-dataset":0.1711770023,"dev-research":0.4035223552,"prompt-eng":0.3499684089,"data-quality":0.2107943834,"ml-security":0.0714755276}}
{"text":"An 'over-generate and rank' paradigm is intuitively used to generate such insights.","meta":{"url":"http://arxiv.org/abs/2307.13176v1"},"cats":{"new-dataset":0.025381531,"dev-research":0.3461461018,"prompt-eng":0.4204577187,"data-quality":0.1431305274,"ml-security":0.1391054584}}
{"text":"The multidimensionality and subjectivity of this process make it challenging.","meta":{"url":"http://arxiv.org/abs/2307.13176v1"},"cats":{"new-dataset":0.014582195,"dev-research":0.2835317652,"prompt-eng":0.3848340902,"data-quality":0.0859291458,"ml-security":0.0745945455}}
{"text":"This paper introduces a schema-driven method to generate actionable insights from data to drive growth and change.","meta":{"url":"http://arxiv.org/abs/2307.13176v1"},"cats":{"new-dataset":0.2483496643,"dev-research":0.4098037424,"prompt-eng":0.3684458415,"data-quality":0.1204112874,"ml-security":0.0944117465}}
{"text":"It also introduces a technique to rank the insights to align with user interests based on their feedback.","meta":{"url":"http://arxiv.org/abs/2307.13176v1"},"cats":{"new-dataset":0.0116154701,"dev-research":0.4078411012,"prompt-eng":0.4081744971,"data-quality":0.1543572453,"ml-security":0.0768990424}}
{"text":"We show preliminary qualitative results of the insights generated using our technique and demonstrate its ability to adapt to feedback.","meta":{"url":"http://arxiv.org/abs/2307.13176v1"},"cats":{"new-dataset":0.0712329725,"dev-research":0.3674723825,"prompt-eng":0.4330033601,"data-quality":0.2055077443,"ml-security":0.091217742}}
{"text":"We present a novel method for mining opinions from text collections using generative language models trained on data collected from different populations.","meta":{"url":"http://arxiv.org/abs/2307.13173v1"},"cats":{"new-dataset":0.1991654895,"dev-research":0.2637744166,"prompt-eng":0.36301239,"data-quality":0.3294730473,"ml-security":0.0759438276}}
{"text":"We describe the basic definitions, methodology and a generic algorithm for opinion insight mining.","meta":{"url":"http://arxiv.org/abs/2307.13173v1"},"cats":{"new-dataset":0.0609460683,"dev-research":0.3713358352,"prompt-eng":0.3427932064,"data-quality":0.2640019993,"ml-security":0.0731474627}}
{"text":"We demonstrate the performance of our method in an experiment where a pre-trained generative model is fine-tuned using specifically tailored content with unnatural and fully annotated opinions.","meta":{"url":"http://arxiv.org/abs/2307.13173v1"},"cats":{"new-dataset":0.0743701898,"dev-research":0.2787934053,"prompt-eng":0.4627780939,"data-quality":0.4002062776,"ml-security":0.0991239279}}
{"text":"We show that our approach can learn and transfer the opinions to the semantic classes while maintaining the proportion of polarisation.","meta":{"url":"http://arxiv.org/abs/2307.13173v1"},"cats":{"new-dataset":0.0983220233,"dev-research":0.2987726977,"prompt-eng":0.3920360402,"data-quality":0.4224425952,"ml-security":0.1058852666}}
{"text":"Finally, we demonstrate the usage of an insight mining system to scale up the discovery of opinion insights from a real text corpus.","meta":{"url":"http://arxiv.org/abs/2307.13173v1"},"cats":{"new-dataset":0.171676382,"dev-research":0.3134900603,"prompt-eng":0.3324947573,"data-quality":0.2654870244,"ml-security":0.0762337295}}
{"text":"Trusted Execution Environments (TEEs) are hardware-enforced memory isolation units, emerging as a pivotal security solution for security-critical applications.","meta":{"url":"http://arxiv.org/abs/2307.13172v1"},"cats":{"new-dataset":0.1006158928,"dev-research":0.3454149209,"prompt-eng":0.4201748023,"data-quality":0.1743362905,"ml-security":0.5183989465}}
{"text":"TEEs, like Intel SGX and ARM TrustZone, allow the isolation of confidential code and data within an untrusted host environment, such as the cloud and IoT.","meta":{"url":"http://arxiv.org/abs/2307.13172v1"},"cats":{"new-dataset":0.068467254,"dev-research":0.3125255879,"prompt-eng":0.3337966902,"data-quality":0.1430937166,"ml-security":0.4022304236}}
{"text":"Despite strong security guarantees, TEE adoption has been hindered by an awkward programming model.","meta":{"url":"http://arxiv.org/abs/2307.13172v1"},"cats":{"new-dataset":0.0044538435,"dev-research":0.3246495425,"prompt-eng":0.3760067148,"data-quality":0.2245536597,"ml-security":0.4208634636}}
{"text":"This model requires manual application partitioning and the use of error-prone, memory-unsafe, and potentially information-leaking low-level C/C++ libraries.   ","meta":{"url":"http://arxiv.org/abs/2307.13172v1"},"cats":{"new-dataset":0.0781688185,"dev-research":0.3391087573,"prompt-eng":0.4489023717,"data-quality":0.1866521864,"ml-security":0.2440043292}}
{"text":"We address the above with \\textit{HasTEE}, a domain-specific language (DSL) embedded in Haskell for programming TEE applications.","meta":{"url":"http://arxiv.org/abs/2307.13172v1"},"cats":{"new-dataset":0.1265825542,"dev-research":0.372877407,"prompt-eng":0.4481431416,"data-quality":0.1558562789,"ml-security":0.1432618658}}
{"text":"HasTEE includes a port of the GHC runtime for the Intel-SGX TEE.","meta":{"url":"http://arxiv.org/abs/2307.13172v1"},"cats":{"new-dataset":0.1286294758,"dev-research":0.2757019065,"prompt-eng":0.4116772224,"data-quality":0.0810661392,"ml-security":0.076174466}}
{"text":"HasTEE uses Haskell's type system to automatically partition an application and to enforce \\textit{Information Flow Control} on confidential data.","meta":{"url":"http://arxiv.org/abs/2307.13172v1"},"cats":{"new-dataset":0.0399170448,"dev-research":0.3488540591,"prompt-eng":0.4261513948,"data-quality":0.1355406576,"ml-security":0.2575998787}}
{"text":"The DSL, being embedded in Haskell, allows for the usage of higher-order functions, monads, and a restricted set of I/O operations to write any standard Haskell application.","meta":{"url":"http://arxiv.org/abs/2307.13172v1"},"cats":{"new-dataset":0.0407990216,"dev-research":0.3538780317,"prompt-eng":0.3522531354,"data-quality":0.0590061965,"ml-security":0.1460268941}}
{"text":"Contrary to previous work, HasTEE is lightweight, simple, and is provided as a \\emph{simple security library}; thus avoiding any GHC modifications.","meta":{"url":"http://arxiv.org/abs/2307.13172v1"},"cats":{"new-dataset":0.0395886326,"dev-research":0.2584714173,"prompt-eng":0.461297655,"data-quality":0.114061272,"ml-security":0.192434612}}
{"text":"We show the applicability of HasTEE by implementing case studies on federated learning, an encrypted password wallet, and a differentially-private data clean room.","meta":{"url":"http://arxiv.org/abs/2307.13172v1"},"cats":{"new-dataset":0.1840723014,"dev-research":0.2714012769,"prompt-eng":0.4072874946,"data-quality":0.0964340638,"ml-security":0.434305369}}
{"text":"Recently, there has been growing interest in extending the context length of instruction-following models in order to effectively process single-turn long input (e.g. summarizing a paper) and conversations with more extensive histories.","meta":{"url":"http://arxiv.org/abs/2307.11088v1"},"cats":{"new-dataset":0.0859017232,"dev-research":0.2994409407,"prompt-eng":0.4392561626,"data-quality":0.120231592,"ml-security":0.0960148775}}
{"text":"While proprietary models such as GPT-4 and Claude have demonstrated considerable advancements in handling tens of thousands of tokens of context, open-sourced models are still in the early stages of experimentation.","meta":{"url":"http://arxiv.org/abs/2307.11088v1"},"cats":{"new-dataset":0.1644824543,"dev-research":0.2576024636,"prompt-eng":0.4145852236,"data-quality":0.1496490329,"ml-security":0.1455054953}}
{"text":"It also remains unclear whether developing these long context models can offer substantial gains on practical downstream tasks over retrieval-based methods or models simply trained on chunked contexts.","meta":{"url":"http://arxiv.org/abs/2307.11088v1"},"cats":{"new-dataset":0.0209805978,"dev-research":0.2240053774,"prompt-eng":0.396669686,"data-quality":0.2016930963,"ml-security":0.1024814835}}
{"text":"To address this challenge, we propose to institute standardized evaluation for long context language models.","meta":{"url":"http://arxiv.org/abs/2307.11088v1"},"cats":{"new-dataset":0.0864527626,"dev-research":0.2154360552,"prompt-eng":0.4208090207,"data-quality":0.2529003638,"ml-security":0.073992204}}
{"text":"Concretely, we develop L-Eval which contains 411 long documents and over 2,000 query-response pairs manually annotated and checked by the authors encompassing areas such as law, finance, school lectures, lengthy conversations, news, long-form novels, and meetings.","meta":{"url":"http://arxiv.org/abs/2307.11088v1"},"cats":{"new-dataset":0.5846619389,"dev-research":0.2375962667,"prompt-eng":0.4114488603,"data-quality":0.2268655731,"ml-security":0.1200325706}}
{"text":"L-Eval also adopts diverse evaluation methods and instruction styles, enabling a more reliable assessment of Long Context Language Models (LCLMs).","meta":{"url":"http://arxiv.org/abs/2307.11088v1"},"cats":{"new-dataset":0.0556775815,"dev-research":0.3027682392,"prompt-eng":0.4108867751,"data-quality":0.1989652428,"ml-security":0.1152852802}}
{"text":"Our findings indicate that while open-source models typically lag behind their commercial counterparts, they still exhibit impressive performance.","meta":{"url":"http://arxiv.org/abs/2307.11088v1"},"cats":{"new-dataset":0.0460299498,"dev-research":0.2909394587,"prompt-eng":0.3298913597,"data-quality":0.1598520207,"ml-security":0.1643402753}}
{"text":"LLaMA2 achieves the best results (win 45\\% vs turbo-16k) on open-ended tasks with only 4k context length and ChatGLM2 achieves the best results on closed-ended tasks with 8k input tokens.","meta":{"url":"http://arxiv.org/abs/2307.11088v1"},"cats":{"new-dataset":0.1067867804,"dev-research":0.3075844037,"prompt-eng":0.412616983,"data-quality":0.1396043048,"ml-security":0.1233674836}}
{"text":"We release our new evaluation suite, code, and all generation results including predictions from all open-sourced LCLMs, GPT4-32k, Cluade-100k at {\\url{https://github.com/OpenLMLab/LEval}}.","meta":{"url":"http://arxiv.org/abs/2307.11088v1"},"cats":{"new-dataset":0.6419935316,"dev-research":0.2403820947,"prompt-eng":0.4371837815,"data-quality":0.198505203,"ml-security":0.1196554024}}
{"text":"Learning accurate and parsimonious point cloud representations of scene surfaces from scratch remains a challenge in 3D representation learning.","meta":{"url":"http://arxiv.org/abs/2307.11086v1"},"cats":{"new-dataset":0.1103460874,"dev-research":0.2091971461,"prompt-eng":0.3481068367,"data-quality":0.1642405487,"ml-security":0.1357840976}}
{"text":"Existing point-based methods often suffer from the vanishing gradient problem or require a large number of points to accurately model scene geometry and texture.","meta":{"url":"http://arxiv.org/abs/2307.11086v1"},"cats":{"new-dataset":0.025166723,"dev-research":0.2191366859,"prompt-eng":0.3703271374,"data-quality":0.2119845933,"ml-security":0.1124050111}}
{"text":"To address these limitations, we propose Proximity Attention Point Rendering (PAPR), a novel method that consists of a point-based scene representation and a differentiable renderer.","meta":{"url":"http://arxiv.org/abs/2307.11086v1"},"cats":{"new-dataset":0.1192595065,"dev-research":0.2438152129,"prompt-eng":0.4336738283,"data-quality":0.1240623257,"ml-security":0.0554783216}}
{"text":"Our scene representation uses a point cloud where each point is characterized by its spatial position, foreground score, and view-independent feature vector.","meta":{"url":"http://arxiv.org/abs/2307.11086v1"},"cats":{"new-dataset":0.1863747415,"dev-research":0.2388995971,"prompt-eng":0.3956076982,"data-quality":0.1489674011,"ml-security":0.0722567043}}
{"text":"The renderer selects the relevant points for each ray and produces accurate colours using their associated features.","meta":{"url":"http://arxiv.org/abs/2307.11086v1"},"cats":{"new-dataset":0.046610437,"dev-research":0.283432479,"prompt-eng":0.4411376246,"data-quality":0.1581589465,"ml-security":0.0361968998}}
{"text":"PAPR effectively learns point cloud positions to represent the correct scene geometry, even when the initialization drastically differs from the target geometry.","meta":{"url":"http://arxiv.org/abs/2307.11086v1"},"cats":{"new-dataset":0.0331101471,"dev-research":0.2213297433,"prompt-eng":0.4199439677,"data-quality":0.1713540392,"ml-security":0.0852277619}}
{"text":"Notably, our method captures fine texture details while using only a parsimonious set of points.","meta":{"url":"http://arxiv.org/abs/2307.11086v1"},"cats":{"new-dataset":0.0928711106,"dev-research":0.2752470146,"prompt-eng":0.4185548957,"data-quality":0.2702595883,"ml-security":0.0736739702}}
{"text":"We also demonstrate four practical applications of our method: geometry editing, object manipulation, texture transfer, and exposure control.","meta":{"url":"http://arxiv.org/abs/2307.11086v1"},"cats":{"new-dataset":0.0399497525,"dev-research":0.2918091624,"prompt-eng":0.433170664,"data-quality":0.103873709,"ml-security":0.0811440543}}
{"text":"More results and code are available on our project website at https://zvict.github.io/papr/.","meta":{"url":"http://arxiv.org/abs/2307.11086v1"},"cats":{"new-dataset":0.401669754,"dev-research":0.2391638659,"prompt-eng":0.4430238601,"data-quality":0.1562930296,"ml-security":0.0479751709}}
{"text":"In this perspective paper, we argue that the dominant paradigm in anomaly detection cannot scale indefinitely and will eventually hit fundamental limits.","meta":{"url":"http://arxiv.org/abs/2307.11085v1"},"cats":{"new-dataset":0.0413768849,"dev-research":0.2147118417,"prompt-eng":0.3360663297,"data-quality":0.2414138335,"ml-security":0.5745028194}}
{"text":"This is due to the a no free lunch principle for anomaly detection.","meta":{"url":"http://arxiv.org/abs/2307.11085v1"},"cats":{"new-dataset":0.0208624444,"dev-research":0.2751519402,"prompt-eng":0.3343413624,"data-quality":0.2071844304,"ml-security":0.4328217552}}
{"text":"These limitations can be overcome when there are strong tasks priors, as is the case for many industrial tasks.","meta":{"url":"http://arxiv.org/abs/2307.11085v1"},"cats":{"new-dataset":0.0059351891,"dev-research":0.3000782977,"prompt-eng":0.3847048943,"data-quality":0.0913488799,"ml-security":0.1635256541}}
{"text":"When such priors do not exists, the task is much harder for anomaly detection.","meta":{"url":"http://arxiv.org/abs/2307.11085v1"},"cats":{"new-dataset":0.0074142799,"dev-research":0.2524693033,"prompt-eng":0.3733501741,"data-quality":0.2494918033,"ml-security":0.367007748}}
{"text":"We pose two such tasks as grand challenges for anomaly detection: i) scientific discovery by anomaly detection ii) a \"mini-grand\" challenge of detecting the most anomalous image in the ImageNet dataset.","meta":{"url":"http://arxiv.org/abs/2307.11085v1"},"cats":{"new-dataset":0.4476918553,"dev-research":0.2029529868,"prompt-eng":0.3753800636,"data-quality":0.3603252937,"ml-security":0.3722262098}}
{"text":"We believe new anomaly detection tools and ideas would need to be developed to overcome these challenges.","meta":{"url":"http://arxiv.org/abs/2307.11085v1"},"cats":{"new-dataset":0.0878746118,"dev-research":0.3761762261,"prompt-eng":0.3859179297,"data-quality":0.2205015667,"ml-security":0.4013952483}}
{"text":"Automated surgical step recognition is an important task that can significantly improve patient safety and decision-making during surgeries.","meta":{"url":"http://arxiv.org/abs/2307.11081v1"},"cats":{"new-dataset":0.0231945049,"dev-research":0.3338528415,"prompt-eng":0.4376132206,"data-quality":0.1770884819,"ml-security":0.0866951971}}
{"text":"Existing state-of-the-art methods for surgical step recognition either rely on separate, multi-stage modeling of spatial and temporal information or operate on short-range temporal resolution when learned jointly.","meta":{"url":"http://arxiv.org/abs/2307.11081v1"},"cats":{"new-dataset":0.0569512021,"dev-research":0.2174595702,"prompt-eng":0.3879307699,"data-quality":0.1152093497,"ml-security":0.0502261963}}
{"text":"However, the benefits of joint modeling of spatio-temporal features and long-range information are not taken in account.","meta":{"url":"http://arxiv.org/abs/2307.11081v1"},"cats":{"new-dataset":0.0320429728,"dev-research":0.2224502723,"prompt-eng":0.2981194571,"data-quality":0.1163168589,"ml-security":0.0836942233}}
{"text":"In this paper, we propose a vision transformer-based approach to jointly learn spatio-temporal features directly from sequence of frame-level patches.","meta":{"url":"http://arxiv.org/abs/2307.11081v1"},"cats":{"new-dataset":0.3443521999,"dev-research":0.2740038096,"prompt-eng":0.3661400944,"data-quality":0.1712530315,"ml-security":0.1090391898}}
{"text":"Our method incorporates a gated-temporal attention mechanism that intelligently combines short-term and long-term spatio-temporal feature representations.","meta":{"url":"http://arxiv.org/abs/2307.11081v1"},"cats":{"new-dataset":0.0821084939,"dev-research":0.2493328811,"prompt-eng":0.4263179973,"data-quality":0.1454659829,"ml-security":0.0835884164}}
{"text":"We extensively evaluate our approach on two cataract surgery video datasets, namely Cataract-101 and D99, and demonstrate superior performance compared to various state-of-the-art methods.","meta":{"url":"http://arxiv.org/abs/2307.11081v1"},"cats":{"new-dataset":0.4289402559,"dev-research":0.2303429642,"prompt-eng":0.3780584172,"data-quality":0.1545769941,"ml-security":0.0635475053}}
{"text":"These results validate the suitability of our proposed approach for automated surgical step recognition.","meta":{"url":"http://arxiv.org/abs/2307.11081v1"},"cats":{"new-dataset":0.0278988803,"dev-research":0.2578132147,"prompt-eng":0.4536488707,"data-quality":0.1896515978,"ml-security":0.0577844864}}
{"text":"Our code is released at: https://github.com/nisargshah1999/GLSFormer","meta":{"url":"http://arxiv.org/abs/2307.11081v1"},"cats":{"new-dataset":0.3773631593,"dev-research":0.2478632816,"prompt-eng":0.4618890391,"data-quality":0.1963378306,"ml-security":0.0711653498}}
{"text":"The paradigm of large-scale pre-training followed by downstream fine-tuning has been widely employed in various object detection algorithms.","meta":{"url":"http://arxiv.org/abs/2307.11077v1"},"cats":{"new-dataset":0.0486584802,"dev-research":0.2368981811,"prompt-eng":0.4076391863,"data-quality":0.2495255135,"ml-security":0.1813419288}}
{"text":"In this paper, we reveal discrepancies in data, model, and task between the pre-training and fine-tuning procedure in existing practices, which implicitly limit the detector's performance, generalization ability, and convergence speed.","meta":{"url":"http://arxiv.org/abs/2307.11077v1"},"cats":{"new-dataset":0.072915869,"dev-research":0.2831070757,"prompt-eng":0.4519661419,"data-quality":0.3824488609,"ml-security":0.1986207779}}
{"text":"To this end, we propose AlignDet, a unified pre-training framework that can be adapted to various existing detectors to alleviate the discrepancies.","meta":{"url":"http://arxiv.org/abs/2307.11077v1"},"cats":{"new-dataset":0.3578418636,"dev-research":0.3082041232,"prompt-eng":0.4855728267,"data-quality":0.4459800951,"ml-security":0.1383397861}}
{"text":"AlignDet decouples the pre-training process into two stages, i.e., image-domain and box-domain pre-training.","meta":{"url":"http://arxiv.org/abs/2307.11077v1"},"cats":{"new-dataset":0.0701367414,"dev-research":0.2964995676,"prompt-eng":0.4335341863,"data-quality":0.1764981168,"ml-security":0.0823716886}}
{"text":"The image-domain pre-training optimizes the detection backbone to capture holistic visual abstraction, and box-domain pre-training learns instance-level semantics and task-aware concepts to initialize the parts out of the backbone.","meta":{"url":"http://arxiv.org/abs/2307.11077v1"},"cats":{"new-dataset":0.0827535582,"dev-research":0.3224317751,"prompt-eng":0.4397737229,"data-quality":0.1732851437,"ml-security":0.1617159982}}
{"text":"By incorporating the self-supervised pre-trained backbones, we can pre-train all modules for various detectors in an unsupervised paradigm.","meta":{"url":"http://arxiv.org/abs/2307.11077v1"},"cats":{"new-dataset":0.1437126318,"dev-research":0.2197426937,"prompt-eng":0.520870524,"data-quality":0.2190299312,"ml-security":0.1778884238}}
{"text":"As depicted in Figure 1, extensive experiments demonstrate that AlignDet can achieve significant improvements across diverse protocols, such as detection algorithm, model backbone, data setting, and training schedule.","meta":{"url":"http://arxiv.org/abs/2307.11077v1"},"cats":{"new-dataset":0.1300407151,"dev-research":0.3128434704,"prompt-eng":0.4360017798,"data-quality":0.1986675121,"ml-security":0.1119271077}}
{"text":"For example, AlignDet improves FCOS by 5.3 mAP, RetinaNet by 2.1 mAP, Faster R-CNN by 3.3 mAP, and DETR by 2.3 mAP under fewer epochs.","meta":{"url":"http://arxiv.org/abs/2307.11077v1"},"cats":{"new-dataset":0.0730412958,"dev-research":0.3340772619,"prompt-eng":0.3360511146,"data-quality":0.166979988,"ml-security":0.1054541391}}
{"text":"Human mesh reconstruction from a single image is challenging in the presence of occlusion, which can be caused by self, objects, or other humans.","meta":{"url":"http://arxiv.org/abs/2307.11074v1"},"cats":{"new-dataset":0.070193016,"dev-research":0.2172916782,"prompt-eng":0.3827860361,"data-quality":0.0966274579,"ml-security":0.1223225909}}
{"text":"Existing methods either fail to separate human features accurately or lack proper supervision for feature completion.","meta":{"url":"http://arxiv.org/abs/2307.11074v1"},"cats":{"new-dataset":0.0147937342,"dev-research":0.3776151195,"prompt-eng":0.4351105855,"data-quality":0.4529644102,"ml-security":0.1172239163}}
{"text":"In this paper, we propose Dense Inpainting Human Mesh Recovery (DIMR), a two-stage method that leverages dense correspondence maps to handle occlusion.","meta":{"url":"http://arxiv.org/abs/2307.11074v1"},"cats":{"new-dataset":0.0813582075,"dev-research":0.2507590829,"prompt-eng":0.3762067266,"data-quality":0.0993337011,"ml-security":0.0622065834}}
{"text":"Our method utilizes a dense correspondence map to separate visible human features and completes human features on a structured UV map dense human with an attention-based feature completion module.","meta":{"url":"http://arxiv.org/abs/2307.11074v1"},"cats":{"new-dataset":0.1234064169,"dev-research":0.2659309284,"prompt-eng":0.4339511088,"data-quality":0.1313625317,"ml-security":0.0593205123}}
{"text":"We also design a feature inpainting training procedure that guides the network to learn from unoccluded features.","meta":{"url":"http://arxiv.org/abs/2307.11074v1"},"cats":{"new-dataset":0.0829067996,"dev-research":0.3533355837,"prompt-eng":0.4411984211,"data-quality":0.2394710889,"ml-security":0.1516790087}}
{"text":"We evaluate our method on several datasets and demonstrate its superior performance under heavily occluded scenarios compared to other methods.","meta":{"url":"http://arxiv.org/abs/2307.11074v1"},"cats":{"new-dataset":0.4795858561,"dev-research":0.2857116173,"prompt-eng":0.399955973,"data-quality":0.2883063929,"ml-security":0.1206289571}}
{"text":"Extensive experiments show that our method obviously outperforms prior SOTA methods on heavily occluded images and achieves comparable results on the standard benchmarks (3DPW).","meta":{"url":"http://arxiv.org/abs/2307.11074v1"},"cats":{"new-dataset":0.0542057006,"dev-research":0.2235588264,"prompt-eng":0.3795081922,"data-quality":0.1728506271,"ml-security":0.0736829672}}
{"text":"Existing image editing tools, while powerful, typically disregard the underlying 3D geometry from which the image is projected.","meta":{"url":"http://arxiv.org/abs/2307.11073v1"},"cats":{"new-dataset":0.0164992441,"dev-research":0.2983359205,"prompt-eng":0.3701827373,"data-quality":0.1064122427,"ml-security":0.0557285615}}
{"text":"As a result, edits made using these tools may become detached from the geometry and lighting conditions that are at the foundation of the image formation process.","meta":{"url":"http://arxiv.org/abs/2307.11073v1"},"cats":{"new-dataset":0.0290855041,"dev-research":0.3802618254,"prompt-eng":0.3928256784,"data-quality":0.1614618289,"ml-security":0.095618274}}
{"text":"In this work, we formulate the newt ask of language-guided 3D-aware editing, where objects in an image should be edited according to a language instruction in context of the underlying 3D scene.","meta":{"url":"http://arxiv.org/abs/2307.11073v1"},"cats":{"new-dataset":0.196284896,"dev-research":0.3078169489,"prompt-eng":0.4550206723,"data-quality":0.2159144155,"ml-security":0.0591795205}}
{"text":"To promote progress towards this goal, we release OBJECT: a dataset consisting of 400K editing examples created from procedurally generated 3D scenes.","meta":{"url":"http://arxiv.org/abs/2307.11073v1"},"cats":{"new-dataset":0.8764795181,"dev-research":0.3000174426,"prompt-eng":0.3941232047,"data-quality":0.1489220006,"ml-security":0.1050564739}}
{"text":"Each example consists of an input image, editing instruction in language, and the edited image.","meta":{"url":"http://arxiv.org/abs/2307.11073v1"},"cats":{"new-dataset":0.2073070024,"dev-research":0.3359612489,"prompt-eng":0.4217849694,"data-quality":0.2020781311,"ml-security":0.0561060568}}
{"text":"We also introduce 3DIT : single and multi-task models for four editing tasks.","meta":{"url":"http://arxiv.org/abs/2307.11073v1"},"cats":{"new-dataset":0.1381354412,"dev-research":0.303611639,"prompt-eng":0.4703250071,"data-quality":0.124670285,"ml-security":0.0461131428}}
{"text":"Our models show impressive abilities to understand the 3D composition of entire scenes, factoring in surrounding objects, surfaces, lighting conditions, shadows, and physically-plausible object configurations.","meta":{"url":"http://arxiv.org/abs/2307.11073v1"},"cats":{"new-dataset":0.2832906709,"dev-research":0.2199236848,"prompt-eng":0.4015057831,"data-quality":0.1067561807,"ml-security":0.0864858894}}
{"text":"Surprisingly, training on only synthetic scenes from OBJECT, editing capabilities of 3DIT generalize to real-world images.","meta":{"url":"http://arxiv.org/abs/2307.11073v1"},"cats":{"new-dataset":0.1379551625,"dev-research":0.2375015864,"prompt-eng":0.3507167417,"data-quality":0.1452440761,"ml-security":0.1536387882}}
{"text":"Large scientific collaborations often have multiple scientists accessing the same set of files while doing different analyses, which create repeated accesses to the large amounts of shared data located far away.","meta":{"url":"http://arxiv.org/abs/2307.11069v1"},"cats":{"new-dataset":0.2422109867,"dev-research":0.3378904712,"prompt-eng":0.3302698005,"data-quality":0.1394012092,"ml-security":0.1185027642}}
{"text":"These data accesses have long latency due to distance and occupy the limited bandwidth available over the wide-area network.","meta":{"url":"http://arxiv.org/abs/2307.11069v1"},"cats":{"new-dataset":0.0592033932,"dev-research":0.2331437294,"prompt-eng":0.2554320099,"data-quality":0.1139400946,"ml-security":0.1431555435}}
{"text":"To reduce the wide-area network traffic and the data access latency, regional data storage caches have been installed as a new networking service.","meta":{"url":"http://arxiv.org/abs/2307.11069v1"},"cats":{"new-dataset":0.2269402025,"dev-research":0.3136257097,"prompt-eng":0.2882988732,"data-quality":0.178776339,"ml-security":0.1044018189}}
{"text":"To study the effectiveness of such a cache system in scientific applications, we examine the Southern California Petabyte Scale Cache for a high-energy physics experiment.","meta":{"url":"http://arxiv.org/abs/2307.11069v1"},"cats":{"new-dataset":0.1135943632,"dev-research":0.2224492484,"prompt-eng":0.4144597694,"data-quality":0.1152099242,"ml-security":0.0736879095}}
{"text":"By examining about 3TB of operational logs, we show that this cache removed 67.6% of file requests from the wide-area network and reduced the traffic volume on wide-area network by 12.3TB (or 35.4%) an average day.","meta":{"url":"http://arxiv.org/abs/2307.11069v1"},"cats":{"new-dataset":0.1965424117,"dev-research":0.2353863674,"prompt-eng":0.3031461463,"data-quality":0.1757475756,"ml-security":0.1648405035}}
{"text":"The reduction in the traffic volume (35.4%) is less than the reduction in file counts (67.6%) because the larger files are less likely to be reused.","meta":{"url":"http://arxiv.org/abs/2307.11069v1"},"cats":{"new-dataset":0.0271887491,"dev-research":0.2859826303,"prompt-eng":0.2799828876,"data-quality":0.1569460242,"ml-security":0.0986432269}}
{"text":"Due to this difference in data access patterns, the cache system has implemented a policy to avoid evicting smaller files when processing larger files.","meta":{"url":"http://arxiv.org/abs/2307.11069v1"},"cats":{"new-dataset":0.0391424772,"dev-research":0.3217283869,"prompt-eng":0.2953718262,"data-quality":0.1430429133,"ml-security":0.1804091363}}
{"text":"We also build a machine learning model to study the predictability of the cache behavior.","meta":{"url":"http://arxiv.org/abs/2307.11069v1"},"cats":{"new-dataset":0.0504796651,"dev-research":0.318792944,"prompt-eng":0.448013293,"data-quality":0.2132702291,"ml-security":0.2960797397}}
{"text":"Tests show that this model is able to accurately predict the cache accesses, cache misses, and network throughput, making the model useful for future studies on resource provisioning and planning.","meta":{"url":"http://arxiv.org/abs/2307.11069v1"},"cats":{"new-dataset":0.0418968716,"dev-research":0.2937956025,"prompt-eng":0.4343959346,"data-quality":0.1207585479,"ml-security":0.1330266835}}
{"text":"We propose a simple three-stage approach to segment unseen objects in RGB images using their CAD models.","meta":{"url":"http://arxiv.org/abs/2307.11067v1"},"cats":{"new-dataset":0.1628420014,"dev-research":0.180272897,"prompt-eng":0.387349191,"data-quality":0.1908934324,"ml-security":0.0679849339}}
{"text":"Leveraging recent powerful foundation models, DINOv2 and Segment Anything, we create descriptors and generate proposals, including binary masks for a given input RGB image.","meta":{"url":"http://arxiv.org/abs/2307.11067v1"},"cats":{"new-dataset":0.5526210946,"dev-research":0.2253157339,"prompt-eng":0.4517357857,"data-quality":0.171565306,"ml-security":0.1139650956}}
{"text":"By matching proposals with reference descriptors created from CAD models, we achieve precise object ID assignment along with modal masks.","meta":{"url":"http://arxiv.org/abs/2307.11067v1"},"cats":{"new-dataset":0.2057440014,"dev-research":0.265403337,"prompt-eng":0.4678402331,"data-quality":0.1868882885,"ml-security":0.0943367345}}
{"text":"We experimentally demonstrate that our method achieves state-of-the-art results in CAD-based novel object segmentation, surpassing existing approaches on the seven core datasets of the BOP challenge by 19.8\\% AP using the same BOP evaluation protocol.","meta":{"url":"http://arxiv.org/abs/2307.11067v1"},"cats":{"new-dataset":0.5248324349,"dev-research":0.2397714934,"prompt-eng":0.4226867923,"data-quality":0.2475929285,"ml-security":0.0828216287}}
{"text":"Our source code is available at https://github.com/nv-nguyen/cnos.","meta":{"url":"http://arxiv.org/abs/2307.11067v1"},"cats":{"new-dataset":0.4865756266,"dev-research":0.2813894671,"prompt-eng":0.4154501551,"data-quality":0.1634483729,"ml-security":0.0732281587}}
{"text":"In this paper, we focus on decentralized agricultural supply chains consisting of multiple non-competing distributors satisfying the demand of their respective markets.","meta":{"url":"http://arxiv.org/abs/2307.11065v1"},"cats":{"new-dataset":0.1574537337,"dev-research":0.1820724067,"prompt-eng":0.327907113,"data-quality":0.0819617389,"ml-security":0.0678498418}}
{"text":"These distributors source a single product from a farmer through an agricultural cooperative, operating in a single period.","meta":{"url":"http://arxiv.org/abs/2307.11065v1"},"cats":{"new-dataset":0.1007069012,"dev-research":0.2430721824,"prompt-eng":0.3268568096,"data-quality":0.1284775811,"ml-security":0.0800740783}}
{"text":"The agents have the ability to coordinate their actions to maximize their profits, and we use cooperative game theory to analyze cooperation among them.","meta":{"url":"http://arxiv.org/abs/2307.11065v1"},"cats":{"new-dataset":0.0699275711,"dev-research":0.2557564361,"prompt-eng":0.3795853537,"data-quality":0.0629772606,"ml-security":0.1571966051}}
{"text":"The distributors can engage in joint ordering, increasing their order size, which leads to a decrease in the price per kilogram.","meta":{"url":"http://arxiv.org/abs/2307.11065v1"},"cats":{"new-dataset":0.0174009647,"dev-research":0.2942495128,"prompt-eng":0.3196295154,"data-quality":0.1394719669,"ml-security":0.0825318681}}
{"text":"Additionally, distributors have the opportunity to cooperate with the farmer, securing a reduced price per kilogram at the cost price, while compensating the farmer for any kilograms not acquired in the cooperation agreement.","meta":{"url":"http://arxiv.org/abs/2307.11065v1"},"cats":{"new-dataset":0.0267819656,"dev-research":0.2448796282,"prompt-eng":0.3644679422,"data-quality":0.1046497744,"ml-security":0.0937388735}}
{"text":"We introduce multidistributor-farmer games and we prove that all the agents have incentives to cooperate.","meta":{"url":"http://arxiv.org/abs/2307.11065v1"},"cats":{"new-dataset":0.2038226559,"dev-research":0.2267681425,"prompt-eng":0.3834500765,"data-quality":0.0892470968,"ml-security":0.1577129703}}
{"text":"We demonstrate the existence of stable allocations, where no subgroup of agents can be better off by separating.","meta":{"url":"http://arxiv.org/abs/2307.11065v1"},"cats":{"new-dataset":0.032399673,"dev-research":0.1674182705,"prompt-eng":0.3864310764,"data-quality":0.1328249738,"ml-security":0.2514181721}}
{"text":"Moreover, we propose and characterize a distribution of the total profit that justly compensates the contribution of the farmer in any group of distributors.","meta":{"url":"http://arxiv.org/abs/2307.11065v1"},"cats":{"new-dataset":0.0994188098,"dev-research":0.2042780727,"prompt-eng":0.3875138646,"data-quality":0.1343404141,"ml-security":0.121129029}}
{"text":"Finally, we explore the conditions under which the farmer can be compensated in order to maximize their revenues when cooperating with all players.","meta":{"url":"http://arxiv.org/abs/2307.11065v1"},"cats":{"new-dataset":0.1952064734,"dev-research":0.2445864827,"prompt-eng":0.3856758357,"data-quality":0.0934242105,"ml-security":0.0993636782}}
{"text":"This position paper describes the Parsl open source research software project and its various phases over seven years.","meta":{"url":"http://arxiv.org/abs/2307.11060v1"},"cats":{"new-dataset":0.4784884903,"dev-research":0.2952825085,"prompt-eng":0.3967129482,"data-quality":0.1041562652,"ml-security":0.082519201}}
{"text":"It defines four types of research software engineers (RSEs) who have been important to the project in those phases; we believe this is also applicable to other research software projects.","meta":{"url":"http://arxiv.org/abs/2307.11060v1"},"cats":{"new-dataset":0.0913863941,"dev-research":0.3354364322,"prompt-eng":0.3653091876,"data-quality":0.0829080062,"ml-security":0.0595919685}}
{"text":"In this project, we implemented an end-to-end system that takes in combined visual features of video frames from a normal camera and depth information from a cloud points scanner, and predicts driving policies (vehicle speed and steering angle).","meta":{"url":"http://arxiv.org/abs/2307.11058v1"},"cats":{"new-dataset":0.4230987027,"dev-research":0.2729183031,"prompt-eng":0.4094214027,"data-quality":0.1042983551,"ml-security":0.0963043974}}
{"text":"We verified the safety of our system by comparing the predicted results with standard behaviors by real-world experienced drivers.","meta":{"url":"http://arxiv.org/abs/2307.11058v1"},"cats":{"new-dataset":0.0864662041,"dev-research":0.3454907237,"prompt-eng":0.466378301,"data-quality":0.1778396592,"ml-security":0.3959811573}}
{"text":"Our test results show that the predictions can be considered as accurate in at lease half of the testing cases (50% 80%, depending on the model), and using combined features improved the performance in most cases than using video frames only.","meta":{"url":"http://arxiv.org/abs/2307.11058v1"},"cats":{"new-dataset":0.0487746262,"dev-research":0.28687687,"prompt-eng":0.4525237566,"data-quality":0.23818523,"ml-security":0.130535151}}
{"text":"We consider a notion of planarity for two-way finite automata and transducers, inspired by Temperley-Lieb monoids of planar diagrams.","meta":{"url":"http://arxiv.org/abs/2307.11057v1"},"cats":{"new-dataset":0.0751636299,"dev-research":0.2554690718,"prompt-eng":0.4117909368,"data-quality":0.1037510822,"ml-security":0.0805968627}}
{"text":"We show that this restriction captures star-free languages and first-order transductions.","meta":{"url":"http://arxiv.org/abs/2307.11057v1"},"cats":{"new-dataset":0.1027407022,"dev-research":0.2181738796,"prompt-eng":0.3752057642,"data-quality":0.1989277569,"ml-security":0.1720498535}}
{"text":"This article presents DataXploreFines, an innovative Shiny application that revolutionizes data exploration, analysis, and visualization.","meta":{"url":"http://arxiv.org/abs/2307.11056v1"},"cats":{"new-dataset":0.5591969136,"dev-research":0.3549254275,"prompt-eng":0.3691087548,"data-quality":0.1235014467,"ml-security":0.0890304348}}
{"text":"The application offers functionalities for data loading, management, summarization, basic graphs, advanced analysis, and contact.","meta":{"url":"http://arxiv.org/abs/2307.11056v1"},"cats":{"new-dataset":0.3459488503,"dev-research":0.3284173863,"prompt-eng":0.387455814,"data-quality":0.1057794891,"ml-security":0.055328015}}
{"text":"Users can upload their datasets in popular formats like CSV or Excel, explore the data structure, perform manipulations, and obtain statistical summaries.","meta":{"url":"http://arxiv.org/abs/2307.11056v1"},"cats":{"new-dataset":0.6256183062,"dev-research":0.3418735592,"prompt-eng":0.3836522652,"data-quality":0.1091373583,"ml-security":0.112000194}}
{"text":"DataXploreFines provides a wide range of interactive visualizations, including histograms, scatter plots, bar charts, and line graphs, enabling users to identify patterns and trends.","meta":{"url":"http://arxiv.org/abs/2307.11056v1"},"cats":{"new-dataset":0.484524857,"dev-research":0.3742970377,"prompt-eng":0.367447884,"data-quality":0.1169688163,"ml-security":0.0856359801}}
{"text":"Additionally, the application offers statistical tools such as time series analysis using ARIMA and SARIMA models, forecasting, and Ljung-Box statistic.","meta":{"url":"http://arxiv.org/abs/2307.11056v1"},"cats":{"new-dataset":0.1982930135,"dev-research":0.2638598589,"prompt-eng":0.365167421,"data-quality":0.0585349712,"ml-security":0.0614913545}}
{"text":"Its user-friendly interface empowers individuals from various domains, including beginners in statistics, to make informed decisions.","meta":{"url":"http://arxiv.org/abs/2307.11056v1"},"cats":{"new-dataset":0.040049146,"dev-research":0.4092571062,"prompt-eng":0.4382976687,"data-quality":0.1058751951,"ml-security":0.1270112686}}
{"text":"Existing high-resolution satellite image forgery localization methods rely on patch-based or downsampling-based training.","meta":{"url":"http://arxiv.org/abs/2307.11052v1"},"cats":{"new-dataset":0.1322772273,"dev-research":0.2482658352,"prompt-eng":0.3850375021,"data-quality":0.3434313666,"ml-security":0.2118184003}}
{"text":"Both of these training methods have major drawbacks, such as inaccurate boundaries between pristine and forged regions, the generation of unwanted artifacts, etc.","meta":{"url":"http://arxiv.org/abs/2307.11052v1"},"cats":{"new-dataset":0.0108012014,"dev-research":0.3399183608,"prompt-eng":0.2991985006,"data-quality":0.2348109718,"ml-security":0.2424669244}}
{"text":"To tackle the aforementioned challenges, inspired by the high-resolution image segmentation literature, we propose a novel model called HRFNet to enable satellite image forgery localization effectively.","meta":{"url":"http://arxiv.org/abs/2307.11052v1"},"cats":{"new-dataset":0.2859532348,"dev-research":0.216325735,"prompt-eng":0.3953967954,"data-quality":0.2963833904,"ml-security":0.1898284459}}
{"text":"Specifically, equipped with shallow and deep branches, our model can successfully integrate RGB and resampling features in both global and local manners to localize forgery more accurately.","meta":{"url":"http://arxiv.org/abs/2307.11052v1"},"cats":{"new-dataset":0.1365590707,"dev-research":0.2401933332,"prompt-eng":0.4562786907,"data-quality":0.2866639976,"ml-security":0.1331464389}}
{"text":"We perform various experiments to demonstrate that our method achieves the best performance, while the memory requirement and processing speed are not compromised compared to existing methods.","meta":{"url":"http://arxiv.org/abs/2307.11052v1"},"cats":{"new-dataset":0.037499056,"dev-research":0.3034872886,"prompt-eng":0.4610660757,"data-quality":0.1894449964,"ml-security":0.1273896475}}
{"text":"Exploration and reward specification are fundamental and intertwined challenges for reinforcement learning.","meta":{"url":"http://arxiv.org/abs/2307.11049v1"},"cats":{"new-dataset":0.0244996728,"dev-research":0.2429788614,"prompt-eng":0.4087135071,"data-quality":0.0909322031,"ml-security":0.1382492007}}
{"text":"Solving sequential decision-making tasks requiring expansive exploration requires either careful design of reward functions or the use of novelty-seeking exploration bonuses.","meta":{"url":"http://arxiv.org/abs/2307.11049v1"},"cats":{"new-dataset":0.0059059353,"dev-research":0.2571756637,"prompt-eng":0.3812321427,"data-quality":0.0765607383,"ml-security":0.1357626063}}
{"text":"Human supervisors can provide effective guidance in the loop to direct the exploration process, but prior methods to leverage this guidance require constant synchronous high-quality human feedback, which is expensive and impractical to obtain.","meta":{"url":"http://arxiv.org/abs/2307.11049v1"},"cats":{"new-dataset":0.0372121989,"dev-research":0.3671678319,"prompt-eng":0.5167536786,"data-quality":0.1127822129,"ml-security":0.0793144372}}
{"text":"In this work, we present a technique called Human Guided Exploration (HuGE), which uses low-quality feedback from non-expert users that may be sporadic, asynchronous, and noisy.","meta":{"url":"http://arxiv.org/abs/2307.11049v1"},"cats":{"new-dataset":0.1144089944,"dev-research":0.3321050448,"prompt-eng":0.5413043809,"data-quality":0.2184568979,"ml-security":0.1037310709}}
{"text":"HuGE guides exploration for reinforcement learning not only in simulation but also in the real world, all without meticulous reward specification.","meta":{"url":"http://arxiv.org/abs/2307.11049v1"},"cats":{"new-dataset":0.1207237135,"dev-research":0.2442803355,"prompt-eng":0.3868454462,"data-quality":0.0849074041,"ml-security":0.1397768287}}
{"text":"The key concept involves bifurcating human feedback and policy learning: human feedback steers exploration, while self-supervised learning from the exploration data yields unbiased policies.","meta":{"url":"http://arxiv.org/abs/2307.11049v1"},"cats":{"new-dataset":0.0491679542,"dev-research":0.2232590509,"prompt-eng":0.398320558,"data-quality":0.181424731,"ml-security":0.1335648954}}
{"text":"This procedure can leverage noisy, asynchronous human feedback to learn policies with no hand-crafted reward design or exploration bonuses.","meta":{"url":"http://arxiv.org/abs/2307.11049v1"},"cats":{"new-dataset":0.063695257,"dev-research":0.2820504513,"prompt-eng":0.4779769006,"data-quality":0.185557725,"ml-security":0.2197044295}}
{"text":"HuGE is able to learn a variety of challenging multi-stage robotic navigation and manipulation tasks in simulation using crowdsourced feedback from non-expert users.","meta":{"url":"http://arxiv.org/abs/2307.11049v1"},"cats":{"new-dataset":0.2546741408,"dev-research":0.2623855373,"prompt-eng":0.4239283989,"data-quality":0.1046759715,"ml-security":0.0936947457}}
{"text":"Moreover, this paradigm can be scaled to learning directly on real-world robots, using occasional, asynchronous feedback from human supervisors.","meta":{"url":"http://arxiv.org/abs/2307.11049v1"},"cats":{"new-dataset":0.0401061046,"dev-research":0.2350713755,"prompt-eng":0.4354713482,"data-quality":0.1145380871,"ml-security":0.1762572272}}
{"text":"When has an agent converged?","meta":{"url":"http://arxiv.org/abs/2307.11044v1"},"cats":{"new-dataset":0.0150094448,"dev-research":0.1578265098,"prompt-eng":0.341932172,"data-quality":0.0637128658,"ml-security":0.1480808366}}
{"text":"Standard models of the reinforcement learning problem give rise to a straightforward definition of convergence: An agent converges when its behavior or performance in each environment state stops changing.","meta":{"url":"http://arxiv.org/abs/2307.11044v1"},"cats":{"new-dataset":0.0218020288,"dev-research":0.1910211893,"prompt-eng":0.3482483865,"data-quality":0.0837727302,"ml-security":0.1877251473}}
{"text":"However, as we shift the focus of our learning problem from the environment's state to the agent's state, the concept of an agent's convergence becomes significantly less clear.","meta":{"url":"http://arxiv.org/abs/2307.11044v1"},"cats":{"new-dataset":0.0382955755,"dev-research":0.2048375774,"prompt-eng":0.3581518856,"data-quality":0.1319154908,"ml-security":0.2314834837}}
{"text":"In this paper, we propose two complementary accounts of agent convergence in a framing of the reinforcement learning problem that centers around bounded agents.","meta":{"url":"http://arxiv.org/abs/2307.11044v1"},"cats":{"new-dataset":0.0373546317,"dev-research":0.1504322445,"prompt-eng":0.3220272562,"data-quality":0.1098632661,"ml-security":0.2413910682}}
{"text":"The first view says that a bounded agent has converged when the minimal number of states needed to describe the agent's future behavior cannot decrease.","meta":{"url":"http://arxiv.org/abs/2307.11044v1"},"cats":{"new-dataset":0.0271055225,"dev-research":0.1670466135,"prompt-eng":0.3012588305,"data-quality":0.0638655307,"ml-security":0.1363961746}}
{"text":"The second view says that a bounded agent has converged just when the agent's performance only changes if the agent's internal state changes.","meta":{"url":"http://arxiv.org/abs/2307.11044v1"},"cats":{"new-dataset":0.0148168113,"dev-research":0.1970687067,"prompt-eng":0.3018931973,"data-quality":0.0775220226,"ml-security":0.1160868453}}
{"text":"We establish basic properties of these two definitions, show that they accommodate typical views of convergence in standard settings, and prove several facts about their nature and relationship.","meta":{"url":"http://arxiv.org/abs/2307.11044v1"},"cats":{"new-dataset":0.0135150134,"dev-research":0.2527716758,"prompt-eng":0.3310430073,"data-quality":0.163170885,"ml-security":0.1071018067}}
{"text":"We take these perspectives, definitions, and analysis to bring clarity to a central idea of the field.","meta":{"url":"http://arxiv.org/abs/2307.11044v1"},"cats":{"new-dataset":0.0925288555,"dev-research":0.3335603717,"prompt-eng":0.3117148853,"data-quality":0.1658334411,"ml-security":0.0579002459}}
{"text":"The development of simple and fast hypergraph spectral methods has been hindered by the lack of numerical algorithms for simulating heat diffusions and computing fundamental objects, such as Personalized PageRank vectors, over hypergraphs.","meta":{"url":"http://arxiv.org/abs/2307.11042v1"},"cats":{"new-dataset":0.0299416333,"dev-research":0.2241894807,"prompt-eng":0.3398698483,"data-quality":0.1550177362,"ml-security":0.0868582513}}
{"text":"In this paper, we overcome this challenge by designing two novel algorithmic primitives.","meta":{"url":"http://arxiv.org/abs/2307.11042v1"},"cats":{"new-dataset":0.1321511415,"dev-research":0.2448070135,"prompt-eng":0.3943818452,"data-quality":0.137715546,"ml-security":0.0977614967}}
{"text":"The first is a simple, easy-to-compute discrete-time heat diffusion that enjoys the same favorable properties as the discrete-time heat diffusion over graphs.","meta":{"url":"http://arxiv.org/abs/2307.11042v1"},"cats":{"new-dataset":0.0188798806,"dev-research":0.210127687,"prompt-eng":0.3454062622,"data-quality":0.0892915792,"ml-security":0.1009067005}}
{"text":"This diffusion can be directly applied to speed up existing hypergraph partitioning algorithms.   ","meta":{"url":"http://arxiv.org/abs/2307.11042v1"},"cats":{"new-dataset":0.0298989776,"dev-research":0.2343347073,"prompt-eng":0.3295330196,"data-quality":0.1644902259,"ml-security":0.0909693657}}
{"text":"Our second contribution is the novel application of mirror descent to compute resolvents of non-differentiable squared norms, which we believe to be of independent interest beyond hypergraph problems.","meta":{"url":"http://arxiv.org/abs/2307.11042v1"},"cats":{"new-dataset":0.0454068375,"dev-research":0.1908511069,"prompt-eng":0.3295276271,"data-quality":0.183266096,"ml-security":0.1830609407}}
{"text":"Based on this new primitive, we derive the first nearly-linear-time algorithm that simulates the discrete-time heat diffusion to approximately compute resolvents of the hypergraph Laplacian operator, which include Personalized PageRank vectors and solutions to the hypergraph analogue of Laplacian systems.","meta":{"url":"http://arxiv.org/abs/2307.11042v1"},"cats":{"new-dataset":0.036040562,"dev-research":0.1915872582,"prompt-eng":0.3376365506,"data-quality":0.111679933,"ml-security":0.1100650707}}
{"text":"Our algorithm runs in time that is linear in the size of the hypergraph and inversely proportional to the hypergraph spectral gap $\\lambda_G$, matching the complexity of analogous diffusion-based algorithms for the graph version of the problem.","meta":{"url":"http://arxiv.org/abs/2307.11042v1"},"cats":{"new-dataset":0.0736485283,"dev-research":0.1421679135,"prompt-eng":0.3308697573,"data-quality":0.1440644068,"ml-security":0.0836620741}}
{"text":"Cyber threats, such as advanced persistent threats (APTs), ransomware, and zero-day exploits, are rapidly evolving and demand improved security measures.","meta":{"url":"http://arxiv.org/abs/2307.11038v1"},"cats":{"new-dataset":0.0635766459,"dev-research":0.313244206,"prompt-eng":0.3529891646,"data-quality":0.0862156061,"ml-security":0.6763005246}}
{"text":"Honeypots and honeynets, as deceptive systems, offer valuable insights into attacker behavior, helping researchers and practitioners develop innovative defense strategies and enhance detection mechanisms.","meta":{"url":"http://arxiv.org/abs/2307.11038v1"},"cats":{"new-dataset":0.0349745981,"dev-research":0.3357911892,"prompt-eng":0.3296343258,"data-quality":0.1907518098,"ml-security":0.7738970273}}
{"text":"However, their deployment involves significant maintenance and overhead expenses.","meta":{"url":"http://arxiv.org/abs/2307.11038v1"},"cats":{"new-dataset":0.0327217169,"dev-research":0.3757715374,"prompt-eng":0.3560959229,"data-quality":0.1234215811,"ml-security":0.0807879452}}
{"text":"At the same time, the complexity of modern computing has prompted the rise of autonomic computing, aiming for systems that can operate without human intervention.","meta":{"url":"http://arxiv.org/abs/2307.11038v1"},"cats":{"new-dataset":0.0178420583,"dev-research":0.3399418442,"prompt-eng":0.4061456759,"data-quality":0.0767023565,"ml-security":0.1240667515}}
{"text":"Recent honeypot and honeynet research claims to incorporate autonomic computing principles, often using terms like adaptive, dynamic, intelligent, and learning.","meta":{"url":"http://arxiv.org/abs/2307.11038v1"},"cats":{"new-dataset":0.0204823737,"dev-research":0.288710138,"prompt-eng":0.3863463825,"data-quality":0.1550478518,"ml-security":0.2983481566}}
{"text":"This study investigates such claims by measuring the extent to which autonomic principles principles are expressed in honeypot and honeynet literature.","meta":{"url":"http://arxiv.org/abs/2307.11038v1"},"cats":{"new-dataset":0.0254854152,"dev-research":0.2631772867,"prompt-eng":0.3652276683,"data-quality":0.1495080542,"ml-security":0.2508203125}}
{"text":"The findings reveal that autonomic computing keywords are present in the literature sample, suggesting an evolution from self-adaptation to autonomic computing implementations.","meta":{"url":"http://arxiv.org/abs/2307.11038v1"},"cats":{"new-dataset":0.0165631675,"dev-research":0.3176155034,"prompt-eng":0.4372799664,"data-quality":0.1852235652,"ml-security":0.1184996102}}
{"text":"Yet, despite these findings, the analysis also shows low frequencies of self-configuration, self-healing, and self-protection keywords.","meta":{"url":"http://arxiv.org/abs/2307.11038v1"},"cats":{"new-dataset":0.0891119916,"dev-research":0.2411529878,"prompt-eng":0.3981057986,"data-quality":0.2418826093,"ml-security":0.1834205192}}
{"text":"Interestingly, self-optimization appeared prominently in the literature.","meta":{"url":"http://arxiv.org/abs/2307.11038v1"},"cats":{"new-dataset":0.0052652322,"dev-research":0.2566088298,"prompt-eng":0.3763877201,"data-quality":0.1170893371,"ml-security":0.1451400789}}
{"text":"While this study presents a foundation for the convergence of autonomic computing and deceptive systems, future research could explore technical implementations in sample articles and test them for autonomic behavior.","meta":{"url":"http://arxiv.org/abs/2307.11038v1"},"cats":{"new-dataset":0.0111195167,"dev-research":0.3125370448,"prompt-eng":0.4488591001,"data-quality":0.192182955,"ml-security":0.3356019794}}
{"text":"Additionally, investigations into the design and implementation of individual autonomic computing principles in honeypots and determining the necessary ratio of these principles for a system to exhibit autonomic behavior could provide valuable insights for both researchers and practitioners.","meta":{"url":"http://arxiv.org/abs/2307.11038v1"},"cats":{"new-dataset":0.0194151712,"dev-research":0.2868160012,"prompt-eng":0.4413097826,"data-quality":0.1045678448,"ml-security":0.2474209461}}
{"text":"Object localization in general environments is a fundamental part of vision systems.","meta":{"url":"http://arxiv.org/abs/2307.11035v1"},"cats":{"new-dataset":0.0198716173,"dev-research":0.2962991106,"prompt-eng":0.400738264,"data-quality":0.2162364657,"ml-security":0.14123235}}
{"text":"While dominating on the COCO benchmark, recent Transformer-based detection methods are not competitive in diverse domains.","meta":{"url":"http://arxiv.org/abs/2307.11035v1"},"cats":{"new-dataset":0.0828454397,"dev-research":0.2264163511,"prompt-eng":0.3978483979,"data-quality":0.2785356677,"ml-security":0.1459707723}}
{"text":"Moreover, these methods still struggle to very accurately estimate the object bounding boxes in complex environments.   ","meta":{"url":"http://arxiv.org/abs/2307.11035v1"},"cats":{"new-dataset":0.0305887341,"dev-research":0.2699450299,"prompt-eng":0.3581792493,"data-quality":0.1881528766,"ml-security":0.1779516794}}
{"text":"We introduce Cascade-DETR for high-quality universal object detection.","meta":{"url":"http://arxiv.org/abs/2307.11035v1"},"cats":{"new-dataset":0.1404998588,"dev-research":0.1858202795,"prompt-eng":0.4302029839,"data-quality":0.2541252999,"ml-security":0.1323104809}}
{"text":"We jointly tackle the generalization to diverse domains and localization accuracy by proposing the Cascade Attention layer, which explicitly integrates object-centric information into the detection decoder by limiting the attention to the previous box prediction.","meta":{"url":"http://arxiv.org/abs/2307.11035v1"},"cats":{"new-dataset":0.1076921794,"dev-research":0.2339844369,"prompt-eng":0.4115458074,"data-quality":0.2984116267,"ml-security":0.1548172199}}
{"text":"To further enhance accuracy, we also revisit the scoring of queries.","meta":{"url":"http://arxiv.org/abs/2307.11035v1"},"cats":{"new-dataset":0.0696767188,"dev-research":0.294672629,"prompt-eng":0.4801596788,"data-quality":0.2635817746,"ml-security":0.0818004169}}
{"text":"Instead of relying on classification scores, we predict the expected IoU of the query, leading to substantially more well-calibrated confidences.","meta":{"url":"http://arxiv.org/abs/2307.11035v1"},"cats":{"new-dataset":0.0360474302,"dev-research":0.212973201,"prompt-eng":0.4971446945,"data-quality":0.2692783229,"ml-security":0.2032290559}}
{"text":"Lastly, we introduce a universal object detection benchmark, UDB10, that contains 10 datasets from diverse domains.","meta":{"url":"http://arxiv.org/abs/2307.11035v1"},"cats":{"new-dataset":0.6273138642,"dev-research":0.2000499429,"prompt-eng":0.3874379091,"data-quality":0.2189596996,"ml-security":0.1779590569}}
{"text":"While also advancing the state-of-the-art on COCO, Cascade-DETR substantially improves DETR-based detectors on all datasets in UDB10, even by over 10 mAP in some cases.","meta":{"url":"http://arxiv.org/abs/2307.11035v1"},"cats":{"new-dataset":0.4784326396,"dev-research":0.2897396197,"prompt-eng":0.4143485645,"data-quality":0.307677824,"ml-security":0.1523954293}}
{"text":"The improvements under stringent quality requirements are even more pronounced.","meta":{"url":"http://arxiv.org/abs/2307.11035v1"},"cats":{"new-dataset":0.0158473317,"dev-research":0.3456111132,"prompt-eng":0.3616878328,"data-quality":0.4064839094,"ml-security":0.0660546001}}
{"text":"Our code and models will be released at https://github.com/SysCV/cascade-detr.","meta":{"url":"http://arxiv.org/abs/2307.11035v1"},"cats":{"new-dataset":0.3429669677,"dev-research":0.2534107562,"prompt-eng":0.4774711361,"data-quality":0.1594309025,"ml-security":0.1144653963}}
{"text":"Recent work has shown that language models' (LMs) prompt-based learning capabilities make them well suited for automating data labeling in domains where manual annotation is expensive.","meta":{"url":"http://arxiv.org/abs/2307.11031v1"},"cats":{"new-dataset":0.2129069697,"dev-research":0.2998823998,"prompt-eng":0.5546863132,"data-quality":0.4923265634,"ml-security":0.1952053141}}
{"text":"The challenge is that while writing an initial prompt is cheap, improving a prompt is costly -- practitioners often require significant labeled data in order to evaluate the impact of prompt modifications.","meta":{"url":"http://arxiv.org/abs/2307.11031v1"},"cats":{"new-dataset":0.0690099652,"dev-research":0.4538135351,"prompt-eng":0.6069832186,"data-quality":0.3329682729,"ml-security":0.2004821725}}
{"text":"Our work asks whether it is possible to improve prompt-based learning without additional labeled data.","meta":{"url":"http://arxiv.org/abs/2307.11031v1"},"cats":{"new-dataset":0.0662008464,"dev-research":0.2664365006,"prompt-eng":0.5176736013,"data-quality":0.2210992219,"ml-security":0.2067696111}}
{"text":"We approach this problem by attempting to modify the predictions of a prompt, rather than the prompt itself.","meta":{"url":"http://arxiv.org/abs/2307.11031v1"},"cats":{"new-dataset":0.1017105121,"dev-research":0.2461163058,"prompt-eng":0.7048469121,"data-quality":0.2900804488,"ml-security":0.2274414065}}
{"text":"Our intuition is that accurate predictions should also be consistent: samples which are similar under some feature representation should receive the same prompt prediction.","meta":{"url":"http://arxiv.org/abs/2307.11031v1"},"cats":{"new-dataset":0.0096033628,"dev-research":0.2893766088,"prompt-eng":0.5160683286,"data-quality":0.3735444125,"ml-security":0.1970332832}}
{"text":"We propose Embroid, a method which computes multiple representations of a dataset under different embedding functions, and uses the consistency between the LM predictions for neighboring samples to identify mispredictions.","meta":{"url":"http://arxiv.org/abs/2307.11031v1"},"cats":{"new-dataset":0.1877815774,"dev-research":0.2511672973,"prompt-eng":0.4012013715,"data-quality":0.4159434619,"ml-security":0.1326273466}}
{"text":"Embroid then uses these neighborhoods to create additional predictions for each sample, and combines these predictions with a simple latent variable graphical model in order to generate a final corrected prediction.","meta":{"url":"http://arxiv.org/abs/2307.11031v1"},"cats":{"new-dataset":0.1262949451,"dev-research":0.2698519175,"prompt-eng":0.4564953253,"data-quality":0.2033712617,"ml-security":0.0679753604}}
{"text":"In addition to providing a theoretical analysis of Embroid, we conduct a rigorous empirical evaluation across six different LMs and up to 95 different tasks.","meta":{"url":"http://arxiv.org/abs/2307.11031v1"},"cats":{"new-dataset":0.0225699417,"dev-research":0.2215863159,"prompt-eng":0.4062387594,"data-quality":0.1759665032,"ml-security":0.0503133763}}
{"text":"We find that (1) Embroid substantially improves performance over original prompts (e.g., by an average of 7.3 points on GPT-JT), (2) also realizes improvements for more sophisticated prompting strategies (e.g., chain-of-thought), and (3) can be specialized to domains like law through the embedding functions.","meta":{"url":"http://arxiv.org/abs/2307.11031v1"},"cats":{"new-dataset":0.0382055496,"dev-research":0.3371226492,"prompt-eng":0.5627342739,"data-quality":0.1594926677,"ml-security":0.1221057525}}
{"text":"Prewriting is the process of discovering and developing ideas before a first draft, which requires divergent thinking and often implies unstructured strategies such as diagramming, outlining, free-writing, etc.","meta":{"url":"http://arxiv.org/abs/2307.10811v1"},"cats":{"new-dataset":0.0262230269,"dev-research":0.4549998076,"prompt-eng":0.3684854203,"data-quality":0.1031769819,"ml-security":0.0647636268}}
{"text":"Although large language models (LLMs) have been demonstrated to be useful for a variety of tasks including creative writing, little is known about how users would collaborate with LLMs to support prewriting.","meta":{"url":"http://arxiv.org/abs/2307.10811v1"},"cats":{"new-dataset":0.0697711112,"dev-research":0.3155840097,"prompt-eng":0.4623204852,"data-quality":0.1409503225,"ml-security":0.0798361934}}
{"text":"The preferred collaborative role and initiative of LLMs during such a creativity process is also unclear.","meta":{"url":"http://arxiv.org/abs/2307.10811v1"},"cats":{"new-dataset":0.0138191995,"dev-research":0.3117017084,"prompt-eng":0.3992686549,"data-quality":0.1768713379,"ml-security":0.0563062205}}
{"text":"To investigate human-LLM collaboration patterns and dynamics during prewriting, we conducted a three-session qualitative study with 15 participants in two creative tasks: story writing and slogan writing.","meta":{"url":"http://arxiv.org/abs/2307.10811v1"},"cats":{"new-dataset":0.0601769169,"dev-research":0.4004161906,"prompt-eng":0.3859825458,"data-quality":0.0859138576,"ml-security":0.0408309018}}
{"text":"The findings indicated that during collaborative prewriting, there appears to be a three-stage iterative Human-AI Co-creativity process that includes Ideation, Illumination, and Implementation stages.","meta":{"url":"http://arxiv.org/abs/2307.10811v1"},"cats":{"new-dataset":0.0402402838,"dev-research":0.4869603061,"prompt-eng":0.3940089956,"data-quality":0.1139711994,"ml-security":0.0557753454}}
{"text":"This collaborative process champions the human in a dominant role, in addition to mixed and shifting levels of initiative that exist between humans and LLMs.","meta":{"url":"http://arxiv.org/abs/2307.10811v1"},"cats":{"new-dataset":0.0512974642,"dev-research":0.314269905,"prompt-eng":0.441850841,"data-quality":0.0605145071,"ml-security":0.0651157765}}
{"text":"This research also reports on collaboration breakdowns that occur during this process, user perceptions of using existing LLMs during Human-AI Co-creativity, and discusses design implications to support this co-creativity process.","meta":{"url":"http://arxiv.org/abs/2307.10811v1"},"cats":{"new-dataset":0.0491843555,"dev-research":0.4342982381,"prompt-eng":0.4178937741,"data-quality":0.135292837,"ml-security":0.1151444012}}
{"text":"VTubers, or Virtual YouTubers, are live streamers who create streaming content using animated 2D or 3D virtual avatars.","meta":{"url":"http://arxiv.org/abs/2307.11025v1"},"cats":{"new-dataset":0.0799892877,"dev-research":0.2518619058,"prompt-eng":0.3895197371,"data-quality":0.1143504794,"ml-security":0.1191126616}}
{"text":"In recent years, there has been a significant increase in the number of VTuber creators and viewers across the globe.","meta":{"url":"http://arxiv.org/abs/2307.11025v1"},"cats":{"new-dataset":0.0769288455,"dev-research":0.2818732441,"prompt-eng":0.3634747229,"data-quality":0.1004084252,"ml-security":0.0758023026}}
{"text":"This practise has drawn research attention into topics such as viewers' engagement behaviors and perceptions, however, as animated avatars offer more identity and performance flexibility than traditional live streaming where one uses their own body, little research has focused on how this flexibility influences how creators present themselves.","meta":{"url":"http://arxiv.org/abs/2307.11025v1"},"cats":{"new-dataset":0.0239617912,"dev-research":0.3149537259,"prompt-eng":0.328665164,"data-quality":0.0856914788,"ml-security":0.0850540675}}
{"text":"This research thus seeks to fill this gap by presenting results from a qualitative study of 16 Chinese-speaking VTubers' streaming practices.","meta":{"url":"http://arxiv.org/abs/2307.11025v1"},"cats":{"new-dataset":0.2171017327,"dev-research":0.3018287649,"prompt-eng":0.2884063012,"data-quality":0.1537068652,"ml-security":0.0666393974}}
{"text":"The data revealed that the virtual avatars that were used while live streaming afforded creators opportunities to present themselves using inflated presentations and resulted in inclusive interactions with viewers.","meta":{"url":"http://arxiv.org/abs/2307.11025v1"},"cats":{"new-dataset":0.150202808,"dev-research":0.2826712621,"prompt-eng":0.3551706501,"data-quality":0.1237608917,"ml-security":0.1324196713}}
{"text":"The results also unveiled the inflated, and often sexualized, gender expressions of VTubers while they were situated in misogynistic environments.","meta":{"url":"http://arxiv.org/abs/2307.11025v1"},"cats":{"new-dataset":0.09586347,"dev-research":0.18709679,"prompt-eng":0.3510838444,"data-quality":0.2003476105,"ml-security":0.1429526008}}
{"text":"The socio-technical facets of VTubing were found to potentially reduce sexual harassment and sexism, whilst also raising self-objectification concerns.","meta":{"url":"http://arxiv.org/abs/2307.11025v1"},"cats":{"new-dataset":0.0100082656,"dev-research":0.3315193103,"prompt-eng":0.3607488748,"data-quality":0.2055956738,"ml-security":0.2754836774}}
{"text":"There are many techniques and tools for termination of C programs, but up to now they were not very powerful for termination proofs of programs whose termination depends on recursive data structures like lists.","meta":{"url":"http://arxiv.org/abs/2307.11024v1"},"cats":{"new-dataset":0.0455548665,"dev-research":0.44775532,"prompt-eng":0.3834883509,"data-quality":0.1772336802,"ml-security":0.2123972667}}
{"text":"We present the first approach that extends powerful techniques for termination analysis of C programs (with memory allocation and explicit pointer arithmetic) to lists.","meta":{"url":"http://arxiv.org/abs/2307.11024v1"},"cats":{"new-dataset":0.0709409054,"dev-research":0.4576415612,"prompt-eng":0.4227299679,"data-quality":0.1910504726,"ml-security":0.2672450434}}
{"text":"Over the last half century, the main application of Brain Computer Interfaces, BCIs has been controlling wheelchairs and neural prostheses or generating text or commands for people with restricted mobility.","meta":{"url":"http://arxiv.org/abs/2307.11023v1"},"cats":{"new-dataset":0.0603485079,"dev-research":0.3142190742,"prompt-eng":0.4090900055,"data-quality":0.0881272659,"ml-security":0.1135881929}}
{"text":"There has been very limited attention in the field to applications for computer aided design, despite the potential of BCIs to provide a new form of environmental interaction.","meta":{"url":"http://arxiv.org/abs/2307.11023v1"},"cats":{"new-dataset":0.0419256384,"dev-research":0.3624327409,"prompt-eng":0.4210013954,"data-quality":0.0655686253,"ml-security":0.0990599651}}
{"text":"In this paper we introduce the development and application of Neuron, a novel BCI tool that enables designers with little experience in neuroscience or computer programming to gain access to neurological data, along with established metrics relevant to design, create BCI interaction prototypes, both with digital onscreen objects and physical devices, and evaluate designs based on neurological information and record measurements for further analysis.","meta":{"url":"http://arxiv.org/abs/2307.11023v1"},"cats":{"new-dataset":0.250707938,"dev-research":0.4398650993,"prompt-eng":0.4350775562,"data-quality":0.1491608798,"ml-security":0.1261150458}}
{"text":"After discussing the BCI tool development, the article presents its capabilities through two case studies, along with a brief evaluation of the tool performance and a discussion of implications, limitations, and future improvement.","meta":{"url":"http://arxiv.org/abs/2307.11023v1"},"cats":{"new-dataset":0.082294626,"dev-research":0.3594496208,"prompt-eng":0.4157842286,"data-quality":0.1219659674,"ml-security":0.0819480476}}
{"text":"Knowledge-intensive tasks (e.g., open-domain question answering (QA)) require a substantial amount of factual knowledge and often rely on external information for assistance.","meta":{"url":"http://arxiv.org/abs/2307.11019v1"},"cats":{"new-dataset":0.0446578932,"dev-research":0.3031441426,"prompt-eng":0.3565075604,"data-quality":0.0986641507,"ml-security":0.1046456403}}
{"text":"Recently, large language models (LLMs) (e.g., ChatGPT), have demonstrated impressive prowess in solving a wide range of tasks with world knowledge, including knowledge-intensive tasks.","meta":{"url":"http://arxiv.org/abs/2307.11019v1"},"cats":{"new-dataset":0.1845657944,"dev-research":0.2055046652,"prompt-eng":0.399773313,"data-quality":0.1235828254,"ml-security":0.0936732851}}
{"text":"However, it remains unclear how well LLMs are able to perceive their factual knowledge boundaries, particularly how they behave when incorporating retrieval augmentation.","meta":{"url":"http://arxiv.org/abs/2307.11019v1"},"cats":{"new-dataset":0.0135227152,"dev-research":0.2381292519,"prompt-eng":0.43966666,"data-quality":0.2707885786,"ml-security":0.0916079328}}
{"text":"In this study, we present an initial analysis of the factual knowledge boundaries of LLMs and how retrieval augmentation affects LLMs on open-domain QA.","meta":{"url":"http://arxiv.org/abs/2307.11019v1"},"cats":{"new-dataset":0.058953106,"dev-research":0.2660756077,"prompt-eng":0.4142087032,"data-quality":0.2275630826,"ml-security":0.0913407752}}
{"text":"Specially, we focus on three primary research questions and analyze them by examining QA performance, priori judgement and posteriori judgement of LLMs.","meta":{"url":"http://arxiv.org/abs/2307.11019v1"},"cats":{"new-dataset":0.0372719643,"dev-research":0.2236091051,"prompt-eng":0.4122872304,"data-quality":0.1161013743,"ml-security":0.0551671701}}
{"text":"We show evidence that LLMs possess unwavering confidence in their capabilities to respond to questions and the accuracy of their responses.","meta":{"url":"http://arxiv.org/abs/2307.11019v1"},"cats":{"new-dataset":0.0272183393,"dev-research":0.2191169624,"prompt-eng":0.4890190855,"data-quality":0.1543705167,"ml-security":0.1985224011}}
{"text":"Furthermore, retrieval augmentation proves to be an effective approach in enhancing LLMs' awareness of knowledge boundaries, thereby improving their judgemental abilities.","meta":{"url":"http://arxiv.org/abs/2307.11019v1"},"cats":{"new-dataset":0.0149298082,"dev-research":0.2688847647,"prompt-eng":0.459620761,"data-quality":0.2009943269,"ml-security":0.092332449}}
{"text":"Additionally, we also find that LLMs have a propensity to rely on the provided retrieval results when formulating answers, while the quality of these results significantly impacts their reliance.","meta":{"url":"http://arxiv.org/abs/2307.11019v1"},"cats":{"new-dataset":0.0094801513,"dev-research":0.2835258587,"prompt-eng":0.460734379,"data-quality":0.2125696556,"ml-security":0.0950359401}}
{"text":"The code to reproduce this work is available at https://github.com/RUCAIBox/LLM-Knowledge-Boundary.","meta":{"url":"http://arxiv.org/abs/2307.11019v1"},"cats":{"new-dataset":0.2241918514,"dev-research":0.1916305409,"prompt-eng":0.4614170699,"data-quality":0.2291238961,"ml-security":0.0675050517}}
{"text":"Myocardial infarction (MI) is one of the most common causes of death in the world.","meta":{"url":"http://arxiv.org/abs/2307.11017v1"},"cats":{"new-dataset":0.0535141715,"dev-research":0.2771120521,"prompt-eng":0.3176415787,"data-quality":0.1840002647,"ml-security":0.1323149774}}
{"text":"Image-based biomarkers commonly used in the clinic, such as ejection fraction, fail to capture more complex patterns in the heart's 3D anatomy and thus limit diagnostic accuracy.","meta":{"url":"http://arxiv.org/abs/2307.11017v1"},"cats":{"new-dataset":0.0153739549,"dev-research":0.2588872511,"prompt-eng":0.3604640907,"data-quality":0.2759063566,"ml-security":0.1404063761}}
{"text":"In this work, we present the multi-objective point cloud autoencoder as a novel geometric deep learning approach for explainable infarction prediction, based on multi-class 3D point cloud representations of cardiac anatomy and function.","meta":{"url":"http://arxiv.org/abs/2307.11017v1"},"cats":{"new-dataset":0.0837549138,"dev-research":0.2539399422,"prompt-eng":0.3290685969,"data-quality":0.1113333152,"ml-security":0.165534231}}
{"text":"Its architecture consists of multiple task-specific branches connected by a low-dimensional latent space to allow for effective multi-objective learning of both reconstruction and MI prediction, while capturing pathology-specific 3D shape information in an interpretable latent space.","meta":{"url":"http://arxiv.org/abs/2307.11017v1"},"cats":{"new-dataset":0.0721646366,"dev-research":0.2066129318,"prompt-eng":0.3921724464,"data-quality":0.1099629441,"ml-security":0.0896766695}}
{"text":"Furthermore, its hierarchical branch design with point cloud-based deep learning operations enables efficient multi-scale feature learning directly on high-resolution anatomy point clouds.","meta":{"url":"http://arxiv.org/abs/2307.11017v1"},"cats":{"new-dataset":0.1393083183,"dev-research":0.2261891686,"prompt-eng":0.3630528872,"data-quality":0.0971296723,"ml-security":0.0808958032}}
{"text":"In our experiments on a large UK Biobank dataset, the multi-objective point cloud autoencoder is able to accurately reconstruct multi-temporal 3D shapes with Chamfer distances between predicted and input anatomies below the underlying images' pixel resolution.","meta":{"url":"http://arxiv.org/abs/2307.11017v1"},"cats":{"new-dataset":0.3666134326,"dev-research":0.2050882294,"prompt-eng":0.3899642277,"data-quality":0.1554002743,"ml-security":0.0812466102}}
{"text":"Our method outperforms multiple machine learning and deep learning benchmarks for the task of incident MI prediction by 19% in terms of Area Under the Receiver Operating Characteristic curve.","meta":{"url":"http://arxiv.org/abs/2307.11017v1"},"cats":{"new-dataset":0.0862250146,"dev-research":0.2332357091,"prompt-eng":0.4365819805,"data-quality":0.2211700533,"ml-security":0.2573971427}}
{"text":"In addition, its task-specific compact latent space exhibits easily separable control and MI clusters with clinically plausible associations between subject encodings and corresponding 3D shapes, thus demonstrating the explainability of the prediction.","meta":{"url":"http://arxiv.org/abs/2307.11017v1"},"cats":{"new-dataset":0.07522117,"dev-research":0.1921674163,"prompt-eng":0.4410880616,"data-quality":0.1115020156,"ml-security":0.1099732964}}
{"text":"Flow map learning (FML), in conjunction with deep neural networks (DNNs), has shown promises for data driven modeling of unknown dynamical systems.","meta":{"url":"http://arxiv.org/abs/2307.11013v1"},"cats":{"new-dataset":0.2372360198,"dev-research":0.2554064991,"prompt-eng":0.3379415711,"data-quality":0.1242424929,"ml-security":0.2829203044}}
{"text":"A remarkable feature of FML is that it is capable of producing accurate predictive models for partially observed systems, even when their exact mathematical models do not exist.","meta":{"url":"http://arxiv.org/abs/2307.11013v1"},"cats":{"new-dataset":0.0350321755,"dev-research":0.2675443928,"prompt-eng":0.4388207626,"data-quality":0.1701626312,"ml-security":0.155977936}}
{"text":"In this paper, we present an overview of the FML framework, along with the important computational details for its successful implementation.","meta":{"url":"http://arxiv.org/abs/2307.11013v1"},"cats":{"new-dataset":0.122163544,"dev-research":0.2927359954,"prompt-eng":0.434000163,"data-quality":0.1286896224,"ml-security":0.0519119302}}
{"text":"We also present a set of well defined benchmark problems for learning unknown dynamical systems.","meta":{"url":"http://arxiv.org/abs/2307.11013v1"},"cats":{"new-dataset":0.1569540266,"dev-research":0.2076084567,"prompt-eng":0.3804820043,"data-quality":0.1792164496,"ml-security":0.3309937273}}
{"text":"All the numerical details of these problems are presented, along with their FML results, to ensure that the problems are accessible for cross-examination and the results are reproducible.","meta":{"url":"http://arxiv.org/abs/2307.11013v1"},"cats":{"new-dataset":0.1423032182,"dev-research":0.2213835019,"prompt-eng":0.381687003,"data-quality":0.1646471984,"ml-security":0.0661623454}}
{"text":"Deep Neural Networks~(DNNs) have been widely deployed in software to address various tasks~(e.g., autonomous driving, medical diagnosis).","meta":{"url":"http://arxiv.org/abs/2307.11011v1"},"cats":{"new-dataset":0.2497313075,"dev-research":0.3627175693,"prompt-eng":0.3411335419,"data-quality":0.1582370249,"ml-security":0.2874086105}}
{"text":"However, they could also produce incorrect behaviors that result in financial losses and even threaten human safety.","meta":{"url":"http://arxiv.org/abs/2307.11011v1"},"cats":{"new-dataset":0.0106193743,"dev-research":0.3160493923,"prompt-eng":0.3297218918,"data-quality":0.3751978311,"ml-security":0.4999325332}}
{"text":"To reveal the incorrect behaviors in DNN and repair them, DNN developers often collect rich unlabeled datasets from the natural world and label them to test the DNN models.","meta":{"url":"http://arxiv.org/abs/2307.11011v1"},"cats":{"new-dataset":0.3313748887,"dev-research":0.3897868777,"prompt-eng":0.4106502734,"data-quality":0.618000836,"ml-security":0.3588028305}}
{"text":"However, properly labeling a large number of unlabeled datasets is a highly expensive and time-consuming task.   ","meta":{"url":"http://arxiv.org/abs/2307.11011v1"},"cats":{"new-dataset":0.1634252953,"dev-research":0.3312361279,"prompt-eng":0.3803596577,"data-quality":0.4282582667,"ml-security":0.1311497498}}
{"text":"To address the above-mentioned problem, we propose NSS, Neuron Sensitivity guided test case Selection, which can reduce the labeling time by selecting valuable test cases from unlabeled datasets.","meta":{"url":"http://arxiv.org/abs/2307.11011v1"},"cats":{"new-dataset":0.1619229071,"dev-research":0.2971406451,"prompt-eng":0.4817008118,"data-quality":0.4087458953,"ml-security":0.2147835606}}
{"text":"NSS leverages the internal neuron's information induced by test cases to select valuable test cases, which have high confidence in causing the model to behave incorrectly.","meta":{"url":"http://arxiv.org/abs/2307.11011v1"},"cats":{"new-dataset":0.0062848839,"dev-research":0.3357880993,"prompt-eng":0.4149158122,"data-quality":0.229940195,"ml-security":0.373673217}}
{"text":"We evaluate NSS with four widely used datasets and four well-designed DNN models compared to SOTA baseline methods.","meta":{"url":"http://arxiv.org/abs/2307.11011v1"},"cats":{"new-dataset":0.3559778718,"dev-research":0.2661422324,"prompt-eng":0.3649916842,"data-quality":0.2163662177,"ml-security":0.1415947737}}
{"text":"The results show that NSS performs well in assessing the test cases' probability of fault triggering and model improvement capabilities.","meta":{"url":"http://arxiv.org/abs/2307.11011v1"},"cats":{"new-dataset":0.0463917544,"dev-research":0.408762067,"prompt-eng":0.4739185747,"data-quality":0.2456806751,"ml-security":0.1918750294}}
{"text":"Specifically, compared with baseline approaches, NSS obtains a higher fault detection rate~(e.g., when selecting 5\\% test case from the unlabeled dataset in MNIST \\& LeNet1 experiment, NSS can obtain 81.8\\% fault detection rate, 20\\% higher than baselines).","meta":{"url":"http://arxiv.org/abs/2307.11011v1"},"cats":{"new-dataset":0.0422568952,"dev-research":0.3660247367,"prompt-eng":0.4044890492,"data-quality":0.3466278218,"ml-security":0.1790517991}}
{"text":"Complex software can be hard to read, adapt, and maintain.","meta":{"url":"http://arxiv.org/abs/2307.11010v1"},"cats":{"new-dataset":0.0236484147,"dev-research":0.5242271497,"prompt-eng":0.3316754528,"data-quality":0.174267628,"ml-security":0.1655860045}}
{"text":"Refactoring it can create cleaner and self-explanatory code.","meta":{"url":"http://arxiv.org/abs/2307.11010v1"},"cats":{"new-dataset":0.0363892519,"dev-research":0.5964380461,"prompt-eng":0.4212678262,"data-quality":0.2337923051,"ml-security":0.1502079186}}
{"text":"Refactoring tools try to guide developers towards better code, with more quality.","meta":{"url":"http://arxiv.org/abs/2307.11010v1"},"cats":{"new-dataset":0.0194120759,"dev-research":0.6193358727,"prompt-eng":0.3709189994,"data-quality":0.2361439925,"ml-security":0.1213696268}}
{"text":"However, most of them take too long to provide feedback, support, and guidance on how developers should improve their software.","meta":{"url":"http://arxiv.org/abs/2307.11010v1"},"cats":{"new-dataset":0.031726968,"dev-research":0.6035051236,"prompt-eng":0.4062094173,"data-quality":0.217038163,"ml-security":0.1785289455}}
{"text":"To reduce this problem, we explored the concept of Live Refactoring, focusing on visually suggesting and applying refactorings, in real-time.","meta":{"url":"http://arxiv.org/abs/2307.11010v1"},"cats":{"new-dataset":0.0982633369,"dev-research":0.5108685803,"prompt-eng":0.4201578081,"data-quality":0.1609229885,"ml-security":0.1150811484}}
{"text":"With this in mind, we developed a Live Refactoring Environment that visually identifies, recommends, and applies Extract Method refactorings.","meta":{"url":"http://arxiv.org/abs/2307.11010v1"},"cats":{"new-dataset":0.1002446391,"dev-research":0.4743036354,"prompt-eng":0.4535930012,"data-quality":0.190714885,"ml-security":0.1189473573}}
{"text":"To validate it, we conducted an empirical experiment.","meta":{"url":"http://arxiv.org/abs/2307.11010v1"},"cats":{"new-dataset":0.0148588864,"dev-research":0.2869656901,"prompt-eng":0.4044262534,"data-quality":0.2725820008,"ml-security":0.1064395545}}
{"text":"Early results showed that our approach improved several code quality metrics.","meta":{"url":"http://arxiv.org/abs/2307.11010v1"},"cats":{"new-dataset":0.0608255413,"dev-research":0.545762569,"prompt-eng":0.4080503028,"data-quality":0.4199197809,"ml-security":0.0885149086}}
{"text":"Besides, we also concluded that our results were significantly different and better than the ones from refactoring the code manually without further help.","meta":{"url":"http://arxiv.org/abs/2307.11010v1"},"cats":{"new-dataset":0.0789287914,"dev-research":0.3571648709,"prompt-eng":0.3989706797,"data-quality":0.3345989941,"ml-security":0.052098356}}
{"text":"Despite extensive studies, the underlying reason as to why overparameterized neural networks can generalize remains elusive.","meta":{"url":"http://arxiv.org/abs/2307.11007v1"},"cats":{"new-dataset":0.0030022774,"dev-research":0.2435709808,"prompt-eng":0.3107519607,"data-quality":0.1909602413,"ml-security":0.479989825}}
{"text":"Existing theory shows that common stochastic optimizers prefer flatter minimizers of the training loss, and thus a natural potential explanation is that flatness implies generalization.","meta":{"url":"http://arxiv.org/abs/2307.11007v1"},"cats":{"new-dataset":0.0009288258,"dev-research":0.2265976275,"prompt-eng":0.3586329947,"data-quality":0.2147975522,"ml-security":0.3218274988}}
{"text":"This work critically examines this explanation.","meta":{"url":"http://arxiv.org/abs/2307.11007v1"},"cats":{"new-dataset":0.0119973108,"dev-research":0.3221723491,"prompt-eng":0.3062280243,"data-quality":0.1623407706,"ml-security":0.1265315552}}
{"text":"Through theoretical and empirical investigation, we identify the following three scenarios for two-layer ReLU networks: (1) flatness provably implies generalization; (2) there exist non-generalizing flattest models and sharpness minimization algorithms fail to generalize, and (3) perhaps most surprisingly, there exist non-generalizing flattest models, but sharpness minimization algorithms still generalize.","meta":{"url":"http://arxiv.org/abs/2307.11007v1"},"cats":{"new-dataset":0.0072583393,"dev-research":0.1933259198,"prompt-eng":0.3349150324,"data-quality":0.2395940581,"ml-security":0.3173111927}}
{"text":"Our results suggest that the relationship between sharpness and generalization subtly depends on the data distributions and the model architectures and sharpness minimization algorithms do not only minimize sharpness to achieve better generalization.","meta":{"url":"http://arxiv.org/abs/2307.11007v1"},"cats":{"new-dataset":0.0070861891,"dev-research":0.3156239534,"prompt-eng":0.3830515423,"data-quality":0.2883236165,"ml-security":0.2043935923}}
{"text":"This calls for the search for other explanations for the generalization of over-parameterized neural networks.","meta":{"url":"http://arxiv.org/abs/2307.11007v1"},"cats":{"new-dataset":0.0100231521,"dev-research":0.258708469,"prompt-eng":0.3721662209,"data-quality":0.1736884093,"ml-security":0.4047872756}}
{"text":"There has been an increased interest in the integration of pretrained speech recognition (ASR) and language models (LM) into the SLU framework.","meta":{"url":"http://arxiv.org/abs/2307.11005v1"},"cats":{"new-dataset":0.0800835517,"dev-research":0.2070954372,"prompt-eng":0.3961486871,"data-quality":0.1711490748,"ml-security":0.1017304059}}
{"text":"However, prior methods often struggle with a vocabulary mismatch between pretrained models, and LM cannot be directly utilized as they diverge from its NLU formulation.","meta":{"url":"http://arxiv.org/abs/2307.11005v1"},"cats":{"new-dataset":0.005008541,"dev-research":0.2380877799,"prompt-eng":0.3611575009,"data-quality":0.3407896253,"ml-security":0.1368343117}}
{"text":"In this study, we propose a three-pass end-to-end (E2E) SLU system that effectively integrates ASR and LM subnetworks into the SLU formulation for sequence generation tasks.","meta":{"url":"http://arxiv.org/abs/2307.11005v1"},"cats":{"new-dataset":0.1299553135,"dev-research":0.2860433953,"prompt-eng":0.4283096601,"data-quality":0.131700238,"ml-security":0.0876062261}}
{"text":"In the first pass, our architecture predicts ASR transcripts using the ASR subnetwork.","meta":{"url":"http://arxiv.org/abs/2307.11005v1"},"cats":{"new-dataset":0.1831509493,"dev-research":0.2672322031,"prompt-eng":0.4376171328,"data-quality":0.2163054496,"ml-security":0.1789710925}}
{"text":"This is followed by the LM subnetwork, which makes an initial SLU prediction.","meta":{"url":"http://arxiv.org/abs/2307.11005v1"},"cats":{"new-dataset":0.0796356515,"dev-research":0.1945811488,"prompt-eng":0.404562038,"data-quality":0.1666465242,"ml-security":0.1286078976}}
{"text":"Finally, in the third pass, the deliberation subnetwork conditions on representations from the ASR and LM subnetworks to make the final prediction.","meta":{"url":"http://arxiv.org/abs/2307.11005v1"},"cats":{"new-dataset":0.1133306121,"dev-research":0.1940143132,"prompt-eng":0.4316873415,"data-quality":0.2048449246,"ml-security":0.137655252}}
{"text":"Our proposed three-pass SLU system shows improved performance over cascaded and E2E SLU models on two benchmark SLU datasets, SLURP and SLUE, especially on acoustically challenging utterances.","meta":{"url":"http://arxiv.org/abs/2307.11005v1"},"cats":{"new-dataset":0.1981255782,"dev-research":0.2019047555,"prompt-eng":0.3901339974,"data-quality":0.2379120766,"ml-security":0.1197704523}}
{"text":"We propose new techniques for reducing communication in private federated learning without the need for setting or tuning compression rates.","meta":{"url":"http://arxiv.org/abs/2307.10999v1"},"cats":{"new-dataset":0.0609436983,"dev-research":0.2286423029,"prompt-eng":0.3331377815,"data-quality":0.1847563185,"ml-security":0.3667797639}}
{"text":"Our on-the-fly methods automatically adjust the compression rate based on the error induced during training, while maintaining provable privacy guarantees through the use of secure aggregation and differential privacy.","meta":{"url":"http://arxiv.org/abs/2307.10999v1"},"cats":{"new-dataset":0.0950723871,"dev-research":0.2820249252,"prompt-eng":0.4167163598,"data-quality":0.2985508623,"ml-security":0.4580751303}}
{"text":"Our techniques are provably instance-optimal for mean estimation, meaning that they can adapt to the ``hardness of the problem\" with minimal interactivity.","meta":{"url":"http://arxiv.org/abs/2307.10999v1"},"cats":{"new-dataset":0.0138061435,"dev-research":0.2429192798,"prompt-eng":0.4067611623,"data-quality":0.2114087171,"ml-security":0.2117770792}}
{"text":"We demonstrate the effectiveness of our approach on real-world datasets by achieving favorable compression rates without the need for tuning.","meta":{"url":"http://arxiv.org/abs/2307.10999v1"},"cats":{"new-dataset":0.5537821888,"dev-research":0.2497399667,"prompt-eng":0.3607479424,"data-quality":0.2536798266,"ml-security":0.1479181742}}
{"text":"Deep learning models are usually black boxes when deployed on machine learning platforms.","meta":{"url":"http://arxiv.org/abs/2307.10997v1"},"cats":{"new-dataset":0.0240466481,"dev-research":0.2276077824,"prompt-eng":0.2904336575,"data-quality":0.1690246112,"ml-security":0.474416104}}
{"text":"Prior works have shown that the attributes ($e.g.$, the number of convolutional layers) of a target black-box neural network can be exposed through a sequence of queries.","meta":{"url":"http://arxiv.org/abs/2307.10997v1"},"cats":{"new-dataset":0.0419458665,"dev-research":0.2086027317,"prompt-eng":0.3881988089,"data-quality":0.1396098643,"ml-security":0.4707089276}}
{"text":"There is a crucial limitation: these works assume the dataset used for training the target model to be known beforehand and leverage this dataset for model attribute attack.","meta":{"url":"http://arxiv.org/abs/2307.10997v1"},"cats":{"new-dataset":0.0587833402,"dev-research":0.210824893,"prompt-eng":0.3566886506,"data-quality":0.2376057773,"ml-security":0.5899160269}}
{"text":"However, it is difficult to access the training dataset of the target black-box model in reality.","meta":{"url":"http://arxiv.org/abs/2307.10997v1"},"cats":{"new-dataset":0.1738886068,"dev-research":0.1797734395,"prompt-eng":0.3326080397,"data-quality":0.1722330167,"ml-security":0.2952866004}}
{"text":"Therefore, whether the attributes of a target black-box model could be still revealed in this case is doubtful.","meta":{"url":"http://arxiv.org/abs/2307.10997v1"},"cats":{"new-dataset":0.0114492859,"dev-research":0.1795241152,"prompt-eng":0.4392274636,"data-quality":0.2326082674,"ml-security":0.3753189052}}
{"text":"In this paper, we investigate a new problem of Domain-agnostic Reverse Engineering the Attributes of a black-box target Model, called DREAM, without requiring the availability of the target model's training dataset, and put forward a general and principled framework by casting this problem as an out of distribution (OOD) generalization problem.","meta":{"url":"http://arxiv.org/abs/2307.10997v1"},"cats":{"new-dataset":0.1301650455,"dev-research":0.219473173,"prompt-eng":0.4323450471,"data-quality":0.2625103601,"ml-security":0.484443922}}
{"text":"In this way, we can learn a domain-agnostic model to inversely infer the attributes of a target black-box model with unknown training data.","meta":{"url":"http://arxiv.org/abs/2307.10997v1"},"cats":{"new-dataset":0.042247869,"dev-research":0.1883880768,"prompt-eng":0.422568239,"data-quality":0.2195301049,"ml-security":0.4027345146}}
{"text":"This makes our method one of the kinds that can gracefully apply to an arbitrary domain for model attribute reverse engineering with strong generalization ability.","meta":{"url":"http://arxiv.org/abs/2307.10997v1"},"cats":{"new-dataset":0.0384268014,"dev-research":0.2737033613,"prompt-eng":0.4956593312,"data-quality":0.2868688656,"ml-security":0.3166759106}}
{"text":"Extensive experimental studies are conducted and the results validate the superiority of our proposed method over the baselines.","meta":{"url":"http://arxiv.org/abs/2307.10997v1"},"cats":{"new-dataset":0.0161983413,"dev-research":0.2680685962,"prompt-eng":0.4273306664,"data-quality":0.1686688677,"ml-security":0.0610360814}}
{"text":"This paper aims to apply a new deep learning approach to the task of generating raw audio files.","meta":{"url":"http://arxiv.org/abs/2307.10994v1"},"cats":{"new-dataset":0.2544610159,"dev-research":0.2561142166,"prompt-eng":0.3163019871,"data-quality":0.297294857,"ml-security":0.1439420025}}
{"text":"It is based on diffusion models, a recent type of deep generative model.","meta":{"url":"http://arxiv.org/abs/2307.10994v1"},"cats":{"new-dataset":0.0473597915,"dev-research":0.1758173461,"prompt-eng":0.3574592308,"data-quality":0.1193152717,"ml-security":0.1114991162}}
{"text":"This new type of method has recently shown outstanding results with image generation.","meta":{"url":"http://arxiv.org/abs/2307.10994v1"},"cats":{"new-dataset":0.171011181,"dev-research":0.2482295499,"prompt-eng":0.415525476,"data-quality":0.1595869954,"ml-security":0.0462052022}}
{"text":"A lot of focus has been given to those models by the computer vision community.","meta":{"url":"http://arxiv.org/abs/2307.10994v1"},"cats":{"new-dataset":0.1073906888,"dev-research":0.263950855,"prompt-eng":0.3791768871,"data-quality":0.1121184274,"ml-security":0.080521257}}
{"text":"On the other hand, really few have been given for other types of applications such as music generation in waveform domain.   ","meta":{"url":"http://arxiv.org/abs/2307.10994v1"},"cats":{"new-dataset":0.0247695916,"dev-research":0.2522882039,"prompt-eng":0.3247592501,"data-quality":0.1322578552,"ml-security":0.0694937344}}
{"text":"In this paper the model for unconditional generating applied to music is implemented: Progressive distillation diffusion with 1D U-Net.","meta":{"url":"http://arxiv.org/abs/2307.10994v1"},"cats":{"new-dataset":0.0242453764,"dev-research":0.175242917,"prompt-eng":0.3622711176,"data-quality":0.1597619928,"ml-security":0.0785172149}}
{"text":"Then, a comparison of different parameters of diffusion and their value in a full result is presented.","meta":{"url":"http://arxiv.org/abs/2307.10994v1"},"cats":{"new-dataset":0.0148075396,"dev-research":0.1551557096,"prompt-eng":0.3726969744,"data-quality":0.0870120158,"ml-security":0.0734539479}}
{"text":"One big advantage of the methods implemented through this work is the fact that the model is able to deal with progressing audio processing and generating , using transformation from 1-channel 128 x 384 to 3-channel 128 x 128 mel-spectrograms and looped generation.","meta":{"url":"http://arxiv.org/abs/2307.10994v1"},"cats":{"new-dataset":0.082912341,"dev-research":0.2915955921,"prompt-eng":0.3935288658,"data-quality":0.1469311264,"ml-security":0.0634125864}}
{"text":"The empirical comparisons are realized across different self-collected datasets.","meta":{"url":"http://arxiv.org/abs/2307.10994v1"},"cats":{"new-dataset":0.270399884,"dev-research":0.2926936553,"prompt-eng":0.3493283742,"data-quality":0.2407714661,"ml-security":0.0903841588}}
{"text":"Deep Learning (DL) , a variant of the neural network algorithms originally proposed in the 1980s, has made surprising progress in Artificial Intelligence (AI), ranging from language translation, protein folding, autonomous cars, and more recently human-like language models (CHATbots), all that seemed intractable until very recently.","meta":{"url":"http://arxiv.org/abs/2307.10991v1"},"cats":{"new-dataset":0.1362115107,"dev-research":0.2863659528,"prompt-eng":0.2948713398,"data-quality":0.1655563915,"ml-security":0.2199586696}}
{"text":"Despite the growing use of Deep Learning (DL) networks, little is actually understood about the learning mechanisms and representations that makes these networks effective across such a diverse range of applications.","meta":{"url":"http://arxiv.org/abs/2307.10991v1"},"cats":{"new-dataset":0.0378118236,"dev-research":0.2921095751,"prompt-eng":0.3096112561,"data-quality":0.2015974765,"ml-security":0.2786485908}}
{"text":"Part of the answer must be the huge scale of the architecture and of course the large scale of the data, since not much has changed since 1987.","meta":{"url":"http://arxiv.org/abs/2307.10991v1"},"cats":{"new-dataset":0.2633293836,"dev-research":0.257820693,"prompt-eng":0.3147503637,"data-quality":0.0999405974,"ml-security":0.0768702594}}
{"text":"But the nature of deep learned representations remain largely unknown.","meta":{"url":"http://arxiv.org/abs/2307.10991v1"},"cats":{"new-dataset":0.0191476744,"dev-research":0.1911581081,"prompt-eng":0.2598720025,"data-quality":0.211374394,"ml-security":0.2277154985}}
{"text":"Unfortunately training sets with millions or billions of tokens have unknown combinatorics and Networks with millions or billions of hidden units cannot easily be visualized and their mechanisms cannot be easily revealed.","meta":{"url":"http://arxiv.org/abs/2307.10991v1"},"cats":{"new-dataset":0.2797351455,"dev-research":0.2367223343,"prompt-eng":0.4009563289,"data-quality":0.2872169066,"ml-security":0.356487876}}
{"text":"In this paper, we explore these questions with a large (1.24M weights; VGG) DL in a novel high density sample task (5 unique tokens with at minimum 500 exemplars per token) which allows us to more carefully follow the emergence of category structure and feature construction.","meta":{"url":"http://arxiv.org/abs/2307.10991v1"},"cats":{"new-dataset":0.1951697421,"dev-research":0.2476392292,"prompt-eng":0.453344555,"data-quality":0.2425017796,"ml-security":0.1272398833}}
{"text":"We use various visualization methods for following the emergence of the classification and the development of the coupling of feature detectors and structures that provide a type of graphical bootstrapping, From these results we harvest some basic observations of the learning dynamics of DL and propose a new theory of complex feature construction based on our results.","meta":{"url":"http://arxiv.org/abs/2307.10991v1"},"cats":{"new-dataset":0.1333900673,"dev-research":0.3379853972,"prompt-eng":0.3972749925,"data-quality":0.2144649501,"ml-security":0.1494482609}}
{"text":"Many machine learning regression methods leverage large datasets for training predictive models.","meta":{"url":"http://arxiv.org/abs/2307.10988v1"},"cats":{"new-dataset":0.1177647929,"dev-research":0.2642852571,"prompt-eng":0.3341879893,"data-quality":0.1615586079,"ml-security":0.4601412645}}
{"text":"However, using large datasets may not be feasible due to computational limitations or high labelling costs.","meta":{"url":"http://arxiv.org/abs/2307.10988v1"},"cats":{"new-dataset":0.1406869983,"dev-research":0.2308032143,"prompt-eng":0.3033943702,"data-quality":0.2046931292,"ml-security":0.1911040384}}
{"text":"Therefore, sampling small training sets from large pools of unlabelled data points is essential to maximize model performance while maintaining computational efficiency.","meta":{"url":"http://arxiv.org/abs/2307.10988v1"},"cats":{"new-dataset":0.0964156909,"dev-research":0.2052763448,"prompt-eng":0.3801767441,"data-quality":0.294377834,"ml-security":0.1969333158}}
{"text":"In this work, we study a sampling approach aimed to minimize the fill distance of the selected set.","meta":{"url":"http://arxiv.org/abs/2307.10988v1"},"cats":{"new-dataset":0.1277200732,"dev-research":0.1734813935,"prompt-eng":0.3837280565,"data-quality":0.1948965873,"ml-security":0.0687549074}}
{"text":"We derive an upper bound for the maximum expected prediction error that linearly depends on the training set fill distance, conditional to the knowledge of data features.","meta":{"url":"http://arxiv.org/abs/2307.10988v1"},"cats":{"new-dataset":0.0917628743,"dev-research":0.2092236748,"prompt-eng":0.374055147,"data-quality":0.3300316492,"ml-security":0.3169473621}}
{"text":"For empirical validation, we perform experiments using two regression models on two datasets.","meta":{"url":"http://arxiv.org/abs/2307.10988v1"},"cats":{"new-dataset":0.0703685859,"dev-research":0.2301428872,"prompt-eng":0.451422874,"data-quality":0.2713068928,"ml-security":0.1828979732}}
{"text":"We empirically show that selecting a training set by aiming to minimize the fill distance, thereby minimizing the bound, significantly reduces the maximum prediction error of various regression models, outperforming existing sampling approaches by a large margin.","meta":{"url":"http://arxiv.org/abs/2307.10988v1"},"cats":{"new-dataset":0.0695627913,"dev-research":0.2069336656,"prompt-eng":0.3873544204,"data-quality":0.2620139611,"ml-security":0.3201432994}}
{"text":"How should my own decisions affect my beliefs about the outcomes I expect to achieve?","meta":{"url":"http://arxiv.org/abs/2307.10987v1"},"cats":{"new-dataset":0.0100342826,"dev-research":0.3109707987,"prompt-eng":0.3460258378,"data-quality":0.0888250138,"ml-security":0.1347085537}}
{"text":"If taking a certain action makes me view myself as a certain type of person, it might affect how I think others view me, and how I view others who are similar to me.","meta":{"url":"http://arxiv.org/abs/2307.10987v1"},"cats":{"new-dataset":0.0027680144,"dev-research":0.2681309702,"prompt-eng":0.3609714884,"data-quality":0.1027220798,"ml-security":0.2035384245}}
{"text":"This can influence my expected utility calculations and change which action I perceive to be best.","meta":{"url":"http://arxiv.org/abs/2307.10987v1"},"cats":{"new-dataset":0.0037660103,"dev-research":0.3107895839,"prompt-eng":0.4590145566,"data-quality":0.1075660324,"ml-security":0.1454400756}}
{"text":"Whether and how it should is subject to debate, with contenders for how to think about it including evidential decision theory, causal decision theory, and functional decision theory.","meta":{"url":"http://arxiv.org/abs/2307.10987v1"},"cats":{"new-dataset":0.0094451238,"dev-research":0.2468497501,"prompt-eng":0.3619440116,"data-quality":0.1586829084,"ml-security":0.1197352217}}
{"text":"In this paper, we show that mechanised causal models can be used to characterise and differentiate the most important decision theories, and generate a taxonomy of different decision theories.","meta":{"url":"http://arxiv.org/abs/2307.10987v1"},"cats":{"new-dataset":0.0376061492,"dev-research":0.2886569516,"prompt-eng":0.4121499288,"data-quality":0.1392522503,"ml-security":0.1676866135}}
{"text":"Fair resource allocation is an important problem in many real-world scenarios, where resources such as goods and chores must be allocated among agents.","meta":{"url":"http://arxiv.org/abs/2307.10985v1"},"cats":{"new-dataset":0.0553080009,"dev-research":0.274546391,"prompt-eng":0.3492386395,"data-quality":0.117139666,"ml-security":0.1809546755}}
{"text":"In this survey, we delve into the intricacies of fair allocation, focusing specifically on the challenges associated with indivisible resources.","meta":{"url":"http://arxiv.org/abs/2307.10985v1"},"cats":{"new-dataset":0.1178200452,"dev-research":0.23618959,"prompt-eng":0.372541161,"data-quality":0.1499166414,"ml-security":0.2088786074}}
{"text":"We define fairness and efficiency within this context and thoroughly survey existential results, algorithms, and approximations that satisfy various fairness criteria, including envyfreeness, proportionality, MMS, and their relaxations.","meta":{"url":"http://arxiv.org/abs/2307.10985v1"},"cats":{"new-dataset":0.0343126722,"dev-research":0.2283401054,"prompt-eng":0.3882494727,"data-quality":0.1298135654,"ml-security":0.2480034499}}
{"text":"Additionally, we discuss algorithms that achieve fairness and efficiency, such as Pareto Optimality and Utilitarian Welfare.","meta":{"url":"http://arxiv.org/abs/2307.10985v1"},"cats":{"new-dataset":0.035540788,"dev-research":0.2292519886,"prompt-eng":0.380567003,"data-quality":0.1070428643,"ml-security":0.2044606267}}
{"text":"We also study the computational complexity of these algorithms, the likelihood of finding fair allocations, and the price of fairness for each fairness notion.","meta":{"url":"http://arxiv.org/abs/2307.10985v1"},"cats":{"new-dataset":0.0279050763,"dev-research":0.2130573495,"prompt-eng":0.3651629767,"data-quality":0.154722521,"ml-security":0.2740637573}}
{"text":"We also cover mixed instances of indivisible and divisible items and investigate different valuation and allocation settings.","meta":{"url":"http://arxiv.org/abs/2307.10985v1"},"cats":{"new-dataset":0.23307897,"dev-research":0.1963613864,"prompt-eng":0.4119490886,"data-quality":0.1569844195,"ml-security":0.0803426616}}
{"text":"By summarizing the state-of-the-art research, this survey provides valuable insights into fair resource allocation of indivisible goods and chores, highlighting computational complexities, fairness guarantees, and trade-offs between fairness and efficiency.","meta":{"url":"http://arxiv.org/abs/2307.10985v1"},"cats":{"new-dataset":0.0460608567,"dev-research":0.2641661833,"prompt-eng":0.3855573712,"data-quality":0.1089118392,"ml-security":0.1808521359}}
{"text":"It serves as a foundation for future advancements in this vital field.","meta":{"url":"http://arxiv.org/abs/2307.10985v1"},"cats":{"new-dataset":0.0771767208,"dev-research":0.2218920814,"prompt-eng":0.3174553314,"data-quality":0.0511376286,"ml-security":0.0499163622}}
{"text":"Reconstructing accurate 3D scenes from images is a long-standing vision task.","meta":{"url":"http://arxiv.org/abs/2307.10984v1"},"cats":{"new-dataset":0.1797474033,"dev-research":0.2329060274,"prompt-eng":0.3686508164,"data-quality":0.1276096303,"ml-security":0.0456918576}}
{"text":"Due to the ill-posedness of the single-image reconstruction problem, most well-established methods are built upon multi-view geometry.","meta":{"url":"http://arxiv.org/abs/2307.10984v1"},"cats":{"new-dataset":0.0405138074,"dev-research":0.2031732011,"prompt-eng":0.3337015838,"data-quality":0.1071038275,"ml-security":0.0684195182}}
{"text":"State-of-the-art (SOTA) monocular metric depth estimation methods can only handle a single camera model and are unable to perform mixed-data training due to the metric ambiguity.","meta":{"url":"http://arxiv.org/abs/2307.10984v1"},"cats":{"new-dataset":0.192595959,"dev-research":0.1641254429,"prompt-eng":0.324130634,"data-quality":0.1465575459,"ml-security":0.0602248031}}
{"text":"Meanwhile, SOTA monocular methods trained on large mixed datasets achieve zero-shot generalization by learning affine-invariant depths, which cannot recover real-world metrics.","meta":{"url":"http://arxiv.org/abs/2307.10984v1"},"cats":{"new-dataset":0.1108252564,"dev-research":0.1873016322,"prompt-eng":0.2744100847,"data-quality":0.2099713417,"ml-security":0.1482033582}}
{"text":"In this work, we show that the key to a zero-shot single-view metric depth model lies in the combination of large-scale data training and resolving the metric ambiguity from various camera models.","meta":{"url":"http://arxiv.org/abs/2307.10984v1"},"cats":{"new-dataset":0.4301015423,"dev-research":0.1682557336,"prompt-eng":0.3469679175,"data-quality":0.1962638454,"ml-security":0.0964766448}}
{"text":"We propose a canonical camera space transformation module, which explicitly addresses the ambiguity problems and can be effortlessly plugged into existing monocular models.","meta":{"url":"http://arxiv.org/abs/2307.10984v1"},"cats":{"new-dataset":0.1613684618,"dev-research":0.2196882077,"prompt-eng":0.4365270531,"data-quality":0.1542680507,"ml-security":0.0543614467}}
{"text":"Equipped with our module, monocular models can be stably trained with over 8 million images with thousands of camera models, resulting in zero-shot generalization to in-the-wild images with unseen camera settings.","meta":{"url":"http://arxiv.org/abs/2307.10984v1"},"cats":{"new-dataset":0.2947723634,"dev-research":0.1672222581,"prompt-eng":0.3968191218,"data-quality":0.1478304411,"ml-security":0.1530022359}}
{"text":"Experiments demonstrate SOTA performance of our method on 7 zero-shot benchmarks.","meta":{"url":"http://arxiv.org/abs/2307.10984v1"},"cats":{"new-dataset":0.0765171124,"dev-research":0.2177446855,"prompt-eng":0.3746646213,"data-quality":0.1477031688,"ml-security":0.0873759526}}
{"text":"Notably, our method won the championship in the 2nd Monocular Depth Estimation Challenge.","meta":{"url":"http://arxiv.org/abs/2307.10984v1"},"cats":{"new-dataset":0.0564726642,"dev-research":0.2078241663,"prompt-eng":0.3982308066,"data-quality":0.1861632609,"ml-security":0.0687653977}}
{"text":"Our method enables the accurate recovery of metric 3D structures on randomly collected internet images, paving the way for plausible single-image metrology.","meta":{"url":"http://arxiv.org/abs/2307.10984v1"},"cats":{"new-dataset":0.1672860557,"dev-research":0.2213650829,"prompt-eng":0.3944778918,"data-quality":0.2110580103,"ml-security":0.0609826878}}
{"text":"The potential benefits extend to downstream tasks, which can be significantly improved by simply plugging in our model.","meta":{"url":"http://arxiv.org/abs/2307.10984v1"},"cats":{"new-dataset":0.0112422414,"dev-research":0.3125087271,"prompt-eng":0.4240019562,"data-quality":0.0848296999,"ml-security":0.0841531267}}
{"text":"For example, our model relieves the scale drift issues of monocular-SLAM (Fig. 1), leading to high-quality metric scale dense mapping.","meta":{"url":"http://arxiv.org/abs/2307.10984v1"},"cats":{"new-dataset":0.076117639,"dev-research":0.2442701858,"prompt-eng":0.3756982106,"data-quality":0.1822543052,"ml-security":0.0730495073}}
{"text":"The code is available at https://github.com/YvanYin/Metric3D.","meta":{"url":"http://arxiv.org/abs/2307.10984v1"},"cats":{"new-dataset":0.3714063102,"dev-research":0.2722135894,"prompt-eng":0.4091012919,"data-quality":0.1357648233,"ml-security":0.053862311}}
{"text":"In the recent years, speech representation learning is constructed primarily as a self-supervised learning (SSL) task, using the raw audio signal alone, while ignoring the side-information that is often available for a given speech recording.","meta":{"url":"http://arxiv.org/abs/2307.10982v1"},"cats":{"new-dataset":0.0575465971,"dev-research":0.2475610752,"prompt-eng":0.3487197683,"data-quality":0.2620630742,"ml-security":0.1797136413}}
{"text":"In this paper, we propose MASR, a Metadata Aware Speech Representation learning framework, which addresses the aforementioned limitations.","meta":{"url":"http://arxiv.org/abs/2307.10982v1"},"cats":{"new-dataset":0.2128944363,"dev-research":0.2520427918,"prompt-eng":0.402976437,"data-quality":0.3849384736,"ml-security":0.1167774179}}
{"text":"MASR enables the inclusion of multiple external knowledge sources to enhance the utilization of meta-data information.","meta":{"url":"http://arxiv.org/abs/2307.10982v1"},"cats":{"new-dataset":0.1261497034,"dev-research":0.3025740026,"prompt-eng":0.3814709036,"data-quality":0.1676405159,"ml-security":0.068483626}}
{"text":"The external knowledge sources are incorporated in the form of sample-level pair-wise similarity matrices that are useful in a hard-mining loss.","meta":{"url":"http://arxiv.org/abs/2307.10982v1"},"cats":{"new-dataset":0.0855547028,"dev-research":0.2951745193,"prompt-eng":0.3585245284,"data-quality":0.248084865,"ml-security":0.1622775249}}
{"text":"A key advantage of the MASR framework is that it can be combined with any choice of SSL method.","meta":{"url":"http://arxiv.org/abs/2307.10982v1"},"cats":{"new-dataset":0.0050311713,"dev-research":0.2696120229,"prompt-eng":0.3315358265,"data-quality":0.0561131434,"ml-security":0.1215587503}}
{"text":"Using MASR representations, we perform evaluations on several downstream tasks such as language identification, speech recognition and other non-semantic tasks such as speaker and emotion recognition.","meta":{"url":"http://arxiv.org/abs/2307.10982v1"},"cats":{"new-dataset":0.063987679,"dev-research":0.2554114896,"prompt-eng":0.4670202959,"data-quality":0.2836484538,"ml-security":0.070881417}}
{"text":"In these experiments, we illustrate significant performance improvements for the MASR over other established benchmarks.","meta":{"url":"http://arxiv.org/abs/2307.10982v1"},"cats":{"new-dataset":0.0309331303,"dev-research":0.2220804596,"prompt-eng":0.4587296038,"data-quality":0.1294411951,"ml-security":0.0600609438}}
{"text":"We perform a detailed analysis on the language identification task to provide insights on how the proposed loss function enables the representations to separate closely related languages.","meta":{"url":"http://arxiv.org/abs/2307.10982v1"},"cats":{"new-dataset":0.0427050041,"dev-research":0.2370496783,"prompt-eng":0.4256783184,"data-quality":0.4516368935,"ml-security":0.1793294029}}
{"text":"Collaborative inference has been a promising solution to enable resource-constrained edge devices to perform inference using state-of-the-art deep neural networks (DNNs).","meta":{"url":"http://arxiv.org/abs/2307.10981v1"},"cats":{"new-dataset":0.0991015032,"dev-research":0.2972062877,"prompt-eng":0.3706991821,"data-quality":0.1109409425,"ml-security":0.1660810754}}
{"text":"In collaborative inference, the edge device first feeds the input to a partial DNN locally and then uploads the intermediate result to the cloud to complete the inference.","meta":{"url":"http://arxiv.org/abs/2307.10981v1"},"cats":{"new-dataset":0.03434635,"dev-research":0.3377206307,"prompt-eng":0.3781612208,"data-quality":0.119582053,"ml-security":0.1028245475}}
{"text":"However, recent research indicates model inversion attacks (MIAs) can reconstruct input data from intermediate results, posing serious privacy concerns for collaborative inference.","meta":{"url":"http://arxiv.org/abs/2307.10981v1"},"cats":{"new-dataset":0.0774364685,"dev-research":0.2448496916,"prompt-eng":0.4133187851,"data-quality":0.2178198304,"ml-security":0.7467447156}}
{"text":"Existing perturbation and cryptography techniques are inefficient and unreliable in defending against MIAs while performing accurate inference.","meta":{"url":"http://arxiv.org/abs/2307.10981v1"},"cats":{"new-dataset":0.0311493436,"dev-research":0.2789056532,"prompt-eng":0.3815819627,"data-quality":0.2356434326,"ml-security":0.4892630826}}
{"text":"This paper provides a viable solution, named PATROL, which develops privacy-oriented pruning to balance privacy, efficiency, and utility of collaborative inference.","meta":{"url":"http://arxiv.org/abs/2307.10981v1"},"cats":{"new-dataset":0.1175057164,"dev-research":0.2963674413,"prompt-eng":0.4535327358,"data-quality":0.1770141174,"ml-security":0.488883416}}
{"text":"PATROL takes advantage of the fact that later layers in a DNN can extract more task-specific features.","meta":{"url":"http://arxiv.org/abs/2307.10981v1"},"cats":{"new-dataset":0.020276082,"dev-research":0.3349697154,"prompt-eng":0.40530457,"data-quality":0.1381172694,"ml-security":0.2829569489}}
{"text":"Given limited local resources for collaborative inference, PATROL intends to deploy more layers at the edge based on pruning techniques to enforce task-specific features for inference and reduce task-irrelevant but sensitive features for privacy preservation.","meta":{"url":"http://arxiv.org/abs/2307.10981v1"},"cats":{"new-dataset":0.0912506624,"dev-research":0.2920234342,"prompt-eng":0.4632807395,"data-quality":0.1754977582,"ml-security":0.4572958891}}
{"text":"To achieve privacy-oriented pruning, PATROL introduces two key components: Lipschitz regularization and adversarial reconstruction training, which increase the reconstruction errors by reducing the stability of MIAs and enhance the target inference model by adversarial training, respectively.","meta":{"url":"http://arxiv.org/abs/2307.10981v1"},"cats":{"new-dataset":0.0714961135,"dev-research":0.1977031163,"prompt-eng":0.376437298,"data-quality":0.246875984,"ml-security":0.6740848758}}
{"text":"U-Net, known for its simple yet efficient architecture, is widely utilized for image processing tasks and is particularly suitable for deployment on neuromorphic chips.","meta":{"url":"http://arxiv.org/abs/2307.10974v1"},"cats":{"new-dataset":0.0697233172,"dev-research":0.2894297657,"prompt-eng":0.3943759953,"data-quality":0.1043630398,"ml-security":0.1421080475}}
{"text":"This paper introduces the novel concept of Spiking-UNet for image processing, which combines the power of Spiking Neural Networks (SNNs) with the U-Net architecture.","meta":{"url":"http://arxiv.org/abs/2307.10974v1"},"cats":{"new-dataset":0.1763454933,"dev-research":0.2842682213,"prompt-eng":0.357342213,"data-quality":0.13069272,"ml-security":0.1645155969}}
{"text":"To achieve an efficient Spiking-UNet, we face two primary challenges: ensuring high-fidelity information propagation through the network via spikes and formulating an effective training strategy.","meta":{"url":"http://arxiv.org/abs/2307.10974v1"},"cats":{"new-dataset":0.0901541283,"dev-research":0.3066282001,"prompt-eng":0.4258439983,"data-quality":0.2054946148,"ml-security":0.241654086}}
{"text":"To address the issue of information loss, we introduce multi-threshold spiking neurons, which improve the efficiency of information transmission within the Spiking-UNet.","meta":{"url":"http://arxiv.org/abs/2307.10974v1"},"cats":{"new-dataset":0.0737004046,"dev-research":0.2572503531,"prompt-eng":0.4060717978,"data-quality":0.1909394874,"ml-security":0.2375657146}}
{"text":"For the training strategy, we adopt a conversion and fine-tuning pipeline that leverage pre-trained U-Net models.","meta":{"url":"http://arxiv.org/abs/2307.10974v1"},"cats":{"new-dataset":0.1011373951,"dev-research":0.2444517755,"prompt-eng":0.4597728258,"data-quality":0.1806408708,"ml-security":0.2029338951}}
{"text":"During the conversion process, significant variability in data distribution across different parts is observed when utilizing skip connections.","meta":{"url":"http://arxiv.org/abs/2307.10974v1"},"cats":{"new-dataset":0.0442860953,"dev-research":0.3090285413,"prompt-eng":0.4472982231,"data-quality":0.2435620923,"ml-security":0.0562915613}}
{"text":"Therefore, we propose a connection-wise normalization method to prevent inaccurate firing rates.","meta":{"url":"http://arxiv.org/abs/2307.10974v1"},"cats":{"new-dataset":0.0171940188,"dev-research":0.3002879494,"prompt-eng":0.4830490326,"data-quality":0.2564922629,"ml-security":0.1846834997}}
{"text":"Furthermore, we adopt a flow-based training method to fine-tune the converted models, reducing time steps while preserving performance.","meta":{"url":"http://arxiv.org/abs/2307.10974v1"},"cats":{"new-dataset":0.0420080824,"dev-research":0.2471896638,"prompt-eng":0.4503623337,"data-quality":0.2059171021,"ml-security":0.1218046149}}
{"text":"Experimental results show that, on image segmentation and denoising, our Spiking-UNet achieves comparable performance to its non-spiking counterpart, surpassing existing SNN methods.","meta":{"url":"http://arxiv.org/abs/2307.10974v1"},"cats":{"new-dataset":0.1686746111,"dev-research":0.2579188726,"prompt-eng":0.3689995385,"data-quality":0.1931461907,"ml-security":0.1403188429}}
{"text":"Compared with the converted Spiking-UNet without fine-tuning, our Spiking-UNet reduces inference time by approximately 90\\%.","meta":{"url":"http://arxiv.org/abs/2307.10974v1"},"cats":{"new-dataset":0.0188107154,"dev-research":0.301982545,"prompt-eng":0.4358760534,"data-quality":0.1462523936,"ml-security":0.1315518081}}
{"text":"This research broadens the application scope of SNNs in image processing and is expected to inspire further exploration in the field of neuromorphic engineering.","meta":{"url":"http://arxiv.org/abs/2307.10974v1"},"cats":{"new-dataset":0.116819626,"dev-research":0.2202593944,"prompt-eng":0.3723844649,"data-quality":0.1397208371,"ml-security":0.1853040209}}
{"text":"The code for our Spiking-UNet implementation is available at https://github.com/SNNresearch/Spiking-UNet.","meta":{"url":"http://arxiv.org/abs/2307.10974v1"},"cats":{"new-dataset":0.3229648347,"dev-research":0.2798704113,"prompt-eng":0.4449459912,"data-quality":0.1352166796,"ml-security":0.08697076}}
{"text":"This paper presents a novel approach to assessing human lighting adjustment behavior and preference in diverse lighting conditions through the evaluation of emotional feedback and behavioral data using VR.","meta":{"url":"http://arxiv.org/abs/2307.10969v1"},"cats":{"new-dataset":0.0694580225,"dev-research":0.3544697089,"prompt-eng":0.4608896578,"data-quality":0.1283393739,"ml-security":0.0797151323}}
{"text":"Participants (n= 27) were exposed to different lighting (n=17) conditions with different levels of illuminance and correlated color temperature (CCT) with a randomized order in a virtual office environment.","meta":{"url":"http://arxiv.org/abs/2307.10969v1"},"cats":{"new-dataset":0.1289105444,"dev-research":0.2649662913,"prompt-eng":0.4261213311,"data-quality":0.1178733509,"ml-security":0.1923102046}}
{"text":"Results from this study significantly advanced our understanding of preferred lighting conditions in virtual reality environments, influenced by a variety of factors such as illuminance, color temperature, order of presentation, and participant demographics.","meta":{"url":"http://arxiv.org/abs/2307.10969v1"},"cats":{"new-dataset":0.0643724629,"dev-research":0.3334643927,"prompt-eng":0.4037435536,"data-quality":0.0872459,"ml-security":0.0830335438}}
{"text":"Through a comprehensive analysis of user adjustment profiles, we obtained insightful data that can guide the optimization of lighting design across various settings.","meta":{"url":"http://arxiv.org/abs/2307.10969v1"},"cats":{"new-dataset":0.0484988938,"dev-research":0.3457250784,"prompt-eng":0.4700217614,"data-quality":0.0834801016,"ml-security":0.0626870604}}
{"text":"The Cyber threats exposure has created worldwide pressure on organizations to comply with cyber security standards and policies for protecting their digital assets.","meta":{"url":"http://arxiv.org/abs/2307.10967v1"},"cats":{"new-dataset":0.121561431,"dev-research":0.2969152768,"prompt-eng":0.3458894337,"data-quality":0.104179021,"ml-security":0.6005768148}}
{"text":"Vulnerability assessment (VA) and Penetration Testing (PT) are widely adopted Security Compliance (SC) methods to identify security gaps and anticipate security breaches.","meta":{"url":"http://arxiv.org/abs/2307.10967v1"},"cats":{"new-dataset":0.0825089044,"dev-research":0.3427794521,"prompt-eng":0.4137728748,"data-quality":0.2168120662,"ml-security":0.4770651308}}
{"text":"In the computer networks context and despite the use of autonomous tools and systems, security compliance remains highly repetitive and resources consuming.","meta":{"url":"http://arxiv.org/abs/2307.10967v1"},"cats":{"new-dataset":0.0340426944,"dev-research":0.3600523847,"prompt-eng":0.3980150184,"data-quality":0.1869228577,"ml-security":0.4495506681}}
{"text":"In this paper, we proposed a novel method to tackle the ever-growing problem of efficiency and effectiveness in network infrastructures security auditing by formally introducing, designing, and developing an Expert-System Automated Security Compliance Framework (ESASCF) that enables industrial and open-source VA and PT tools and systems to extract, process, store and re-use the expertise in a human-expert way to allow direct application in similar scenarios or during the periodic re-testing.","meta":{"url":"http://arxiv.org/abs/2307.10967v1"},"cats":{"new-dataset":0.1204233408,"dev-research":0.4165239972,"prompt-eng":0.445892106,"data-quality":0.1958541212,"ml-security":0.3861168807}}
{"text":"The implemented model was then integrated within the ESASCF and tested on different size networks and proved efficient in terms of time-efficiency and testing effectiveness allowing ESASCF to take over autonomously the SC in Re-testing and offloading Expert by automating repeated segments SC and thus enabling Experts to prioritize important tasks in Ad-Hoc compliance tests.","meta":{"url":"http://arxiv.org/abs/2307.10967v1"},"cats":{"new-dataset":0.0668025427,"dev-research":0.3002524933,"prompt-eng":0.4824493024,"data-quality":0.103097154,"ml-security":0.1429875459}}
{"text":"The obtained results validate the performance enhancement notably by cutting the time required for an expert to 50% in the context of typical corporate networks first SC and 20% in re-testing, representing a significant cost-cutting.","meta":{"url":"http://arxiv.org/abs/2307.10967v1"},"cats":{"new-dataset":0.0193165753,"dev-research":0.3552038975,"prompt-eng":0.4112902698,"data-quality":0.1711950345,"ml-security":0.0847196682}}
{"text":"In addition, the framework allows a long-term impact illustrated in the knowledge extraction, generalization, and re-utilization, which enables better SC confidence independent of the human expert skills, coverage, and wrong decisions resulting in impactful false negatives.","meta":{"url":"http://arxiv.org/abs/2307.10967v1"},"cats":{"new-dataset":0.0557110336,"dev-research":0.3893893119,"prompt-eng":0.3709601753,"data-quality":0.1600795069,"ml-security":0.1570361906}}
{"text":"Endoscopic surgery is currently an important treatment method in the field of spinal surgery and avoiding damage to the spinal nerves through video guidance is a key challenge.","meta":{"url":"http://arxiv.org/abs/2307.10955v1"},"cats":{"new-dataset":0.0192441759,"dev-research":0.2369345433,"prompt-eng":0.3386670668,"data-quality":0.0948720933,"ml-security":0.0922560305}}
{"text":"This paper presents the first real-time segmentation method for spinal nerves in endoscopic surgery, which provides crucial navigational information for surgeons.","meta":{"url":"http://arxiv.org/abs/2307.10955v1"},"cats":{"new-dataset":0.0804917751,"dev-research":0.2230620341,"prompt-eng":0.3871207479,"data-quality":0.096360351,"ml-security":0.0615914195}}
{"text":"A finely annotated segmentation dataset of approximately 10,000 consec-utive frames recorded during surgery is constructed for the first time for this field, addressing the problem of semantic segmentation.","meta":{"url":"http://arxiv.org/abs/2307.10955v1"},"cats":{"new-dataset":0.6540149329,"dev-research":0.2204764247,"prompt-eng":0.3958397528,"data-quality":0.2899982462,"ml-security":0.0790244433}}
{"text":"Based on this dataset, we propose FUnet (Frame-Unet), which achieves state-of-the-art performance by utilizing inter-frame information and self-attention mechanisms.","meta":{"url":"http://arxiv.org/abs/2307.10955v1"},"cats":{"new-dataset":0.6089499452,"dev-research":0.2595768206,"prompt-eng":0.4465827376,"data-quality":0.2325365884,"ml-security":0.081974145}}
{"text":"We also conduct extended exper-iments on a similar polyp endoscopy video dataset and show that the model has good generalization ability with advantageous performance.","meta":{"url":"http://arxiv.org/abs/2307.10955v1"},"cats":{"new-dataset":0.3552964711,"dev-research":0.1877820429,"prompt-eng":0.4200081015,"data-quality":0.1572263624,"ml-security":0.084313452}}
{"text":"The dataset and code of this work are presented at: https://github.com/zzzzzzpc/FUnet .","meta":{"url":"http://arxiv.org/abs/2307.10955v1"},"cats":{"new-dataset":0.788884201,"dev-research":0.1980798483,"prompt-eng":0.4193654521,"data-quality":0.1574258543,"ml-security":0.0618455919}}
{"text":"In CMF surgery, the planning of bony movement to achieve a desired facial outcome is a challenging task.","meta":{"url":"http://arxiv.org/abs/2307.10954v1"},"cats":{"new-dataset":0.011743508,"dev-research":0.2344498147,"prompt-eng":0.3554615421,"data-quality":0.0591367122,"ml-security":0.0716322657}}
{"text":"Current bone driven approaches focus on normalizing the bone with the expectation that the facial appearance will be corrected accordingly.","meta":{"url":"http://arxiv.org/abs/2307.10954v1"},"cats":{"new-dataset":0.0136961147,"dev-research":0.2654053189,"prompt-eng":0.3940028837,"data-quality":0.1155752139,"ml-security":0.1133434388}}
{"text":"However, due to the complex non-linear relationship between bony structure and facial soft-tissue, such bone-driven methods are insufficient to correct facial deformities.","meta":{"url":"http://arxiv.org/abs/2307.10954v1"},"cats":{"new-dataset":0.0037004506,"dev-research":0.2450341637,"prompt-eng":0.2786393945,"data-quality":0.1490293816,"ml-security":0.1920392849}}
{"text":"Despite efforts to simulate facial changes resulting from bony movement, surgical planning still relies on iterative revisions and educated guesses.","meta":{"url":"http://arxiv.org/abs/2307.10954v1"},"cats":{"new-dataset":0.0174038375,"dev-research":0.2924551907,"prompt-eng":0.399057194,"data-quality":0.127643257,"ml-security":0.1300942653}}
{"text":"To address these issues, we propose a soft-tissue driven framework that can automatically create and verify surgical plans.","meta":{"url":"http://arxiv.org/abs/2307.10954v1"},"cats":{"new-dataset":0.0796778369,"dev-research":0.2856919076,"prompt-eng":0.4330675582,"data-quality":0.0892595908,"ml-security":0.0637836885}}
{"text":"Our framework consists of a bony planner network that estimates the bony movements required to achieve the desired facial outcome and a facial simulator network that can simulate the possible facial changes resulting from the estimated bony movement plans.","meta":{"url":"http://arxiv.org/abs/2307.10954v1"},"cats":{"new-dataset":0.2666355272,"dev-research":0.2438016461,"prompt-eng":0.3961023333,"data-quality":0.0685279342,"ml-security":0.1176672709}}
{"text":"By combining these two models, we can verify and determine the final bony movement required for planning.","meta":{"url":"http://arxiv.org/abs/2307.10954v1"},"cats":{"new-dataset":0.0424017942,"dev-research":0.1651495489,"prompt-eng":0.4510272701,"data-quality":0.047094163,"ml-security":0.0370934089}}
{"text":"The proposed framework was evaluated using a clinical dataset, and our experimental results demonstrate that the soft-tissue driven approach greatly improves the accuracy and efficacy of surgical planning when compared to the conventional bone-driven approach.","meta":{"url":"http://arxiv.org/abs/2307.10954v1"},"cats":{"new-dataset":0.0498718417,"dev-research":0.2794108133,"prompt-eng":0.3511416692,"data-quality":0.073636954,"ml-security":0.0668197518}}
{"text":"Current object detection models have achieved good results on many benchmark datasets, detecting objects in dark conditions remains a large challenge.","meta":{"url":"http://arxiv.org/abs/2307.10953v1"},"cats":{"new-dataset":0.2117654466,"dev-research":0.1919170009,"prompt-eng":0.3864664723,"data-quality":0.270865298,"ml-security":0.2351332428}}
{"text":"To address this issue, we propose a pyramid enhanced network (PENet) and joint it with YOLOv3 to build a dark object detection framework named PE-YOLO.","meta":{"url":"http://arxiv.org/abs/2307.10953v1"},"cats":{"new-dataset":0.295682795,"dev-research":0.2193378872,"prompt-eng":0.3929942864,"data-quality":0.2150339886,"ml-security":0.2406462764}}
{"text":"Firstly, PENet decomposes the image into four components of different resolutions using the Laplacian pyramid.","meta":{"url":"http://arxiv.org/abs/2307.10953v1"},"cats":{"new-dataset":0.3065857456,"dev-research":0.1781425093,"prompt-eng":0.3504856564,"data-quality":0.158936091,"ml-security":0.0480852164}}
{"text":"Specifically we propose a detail processing module (DPM) to enhance the detail of images, which consists of context branch and edge branch.","meta":{"url":"http://arxiv.org/abs/2307.10953v1"},"cats":{"new-dataset":0.219604443,"dev-research":0.326443201,"prompt-eng":0.4274282071,"data-quality":0.1662097319,"ml-security":0.0374858238}}
{"text":"In addition, we propose a low-frequency enhancement filter (LEF) to capture low-frequency semantics and prevent high-frequency noise.","meta":{"url":"http://arxiv.org/abs/2307.10953v1"},"cats":{"new-dataset":0.0744857625,"dev-research":0.3629225901,"prompt-eng":0.4494301957,"data-quality":0.4666048675,"ml-security":0.09940041}}
{"text":"PE-YOLO adopts an end-to-end joint training approach and only uses normal detection loss to simplify the training process.","meta":{"url":"http://arxiv.org/abs/2307.10953v1"},"cats":{"new-dataset":0.0186033743,"dev-research":0.2627951947,"prompt-eng":0.3745279475,"data-quality":0.2409394505,"ml-security":0.1824353101}}
{"text":"We conduct experiments on the low-light object detection dataset ExDark to demonstrate the effectiveness of ours.","meta":{"url":"http://arxiv.org/abs/2307.10953v1"},"cats":{"new-dataset":0.3787190304,"dev-research":0.1952960939,"prompt-eng":0.403010326,"data-quality":0.2465212955,"ml-security":0.1405607235}}
{"text":"The results indicate that compared with other dark detectors and low-light enhancement models, PE-YOLO achieves the advanced results, achieving 78.0% in mAP and 53.6 in FPS, respectively, which can adapt to object detection under different low-light conditions.","meta":{"url":"http://arxiv.org/abs/2307.10953v1"},"cats":{"new-dataset":0.1025054908,"dev-research":0.1986773455,"prompt-eng":0.415155326,"data-quality":0.1898250866,"ml-security":0.0928567443}}
{"text":"The code is available at https://github.com/XiangchenYin/PE-YOLO.","meta":{"url":"http://arxiv.org/abs/2307.10953v1"},"cats":{"new-dataset":0.479496462,"dev-research":0.2750212475,"prompt-eng":0.4361508929,"data-quality":0.1509067034,"ml-security":0.0785048463}}
{"text":"Autonomous driving requires accurate local scene understanding information.","meta":{"url":"http://arxiv.org/abs/2307.10947v1"},"cats":{"new-dataset":0.0505970492,"dev-research":0.3194680008,"prompt-eng":0.4023960663,"data-quality":0.277155924,"ml-security":0.1065700132}}
{"text":"To this end, autonomous agents deploy object detection and online BEV lane graph extraction methods as a part of their perception stack.","meta":{"url":"http://arxiv.org/abs/2307.10947v1"},"cats":{"new-dataset":0.165641388,"dev-research":0.2513637694,"prompt-eng":0.40339672,"data-quality":0.1798144464,"ml-security":0.1046577459}}
{"text":"In this work, we propose an architecture and loss formulation to improve the accuracy of local lane graph estimates by using 3D object detection outputs.","meta":{"url":"http://arxiv.org/abs/2307.10947v1"},"cats":{"new-dataset":0.1630245972,"dev-research":0.2432266404,"prompt-eng":0.3349614605,"data-quality":0.2505048311,"ml-security":0.094909461}}
{"text":"The proposed method learns to assign the objects to centerlines by considering the centerlines as cluster centers and the objects as data points to be assigned a probability distribution over the cluster centers.","meta":{"url":"http://arxiv.org/abs/2307.10947v1"},"cats":{"new-dataset":0.0987792514,"dev-research":0.2606762009,"prompt-eng":0.4223857207,"data-quality":0.2318824106,"ml-security":0.1055166565}}
{"text":"This training scheme ensures direct supervision on the relationship between lanes and objects, thus leading to better performance.","meta":{"url":"http://arxiv.org/abs/2307.10947v1"},"cats":{"new-dataset":0.0389965614,"dev-research":0.3542636599,"prompt-eng":0.4037781964,"data-quality":0.1753833244,"ml-security":0.1399850517}}
{"text":"The proposed method improves lane graph estimation substantially over state-of-the-art methods.","meta":{"url":"http://arxiv.org/abs/2307.10947v1"},"cats":{"new-dataset":0.0831481208,"dev-research":0.2745119321,"prompt-eng":0.33558112,"data-quality":0.1751939131,"ml-security":0.053444494}}
{"text":"The extensive ablations show that our method can achieve significant performance improvements by using the outputs of existing 3D object detection methods.","meta":{"url":"http://arxiv.org/abs/2307.10947v1"},"cats":{"new-dataset":0.1205442457,"dev-research":0.2371312714,"prompt-eng":0.4034536487,"data-quality":0.1797759418,"ml-security":0.0634003991}}
{"text":"Since our method uses the detection outputs rather than detection method intermediate representations, a single model of our method can use any detection method at test time.","meta":{"url":"http://arxiv.org/abs/2307.10947v1"},"cats":{"new-dataset":0.0282748657,"dev-research":0.2555770889,"prompt-eng":0.4928928933,"data-quality":0.2853023164,"ml-security":0.1780156572}}
{"text":"Recent advances in deep learning have significantly improved the performance of various computer vision applications.","meta":{"url":"http://arxiv.org/abs/2307.10943v1"},"cats":{"new-dataset":0.0180415528,"dev-research":0.2397678739,"prompt-eng":0.341841371,"data-quality":0.170360843,"ml-security":0.1386568179}}
{"text":"However, discovering novel categories in an incremental learning scenario remains a challenging problem due to the lack of prior knowledge about the number and nature of new categories.","meta":{"url":"http://arxiv.org/abs/2307.10943v1"},"cats":{"new-dataset":0.1261018601,"dev-research":0.2881330363,"prompt-eng":0.3621824663,"data-quality":0.243293385,"ml-security":0.1387380403}}
{"text":"Existing methods for novel category discovery are limited by their reliance on labeled datasets and prior knowledge about the number of novel categories and the proportion of novel samples in the batch.","meta":{"url":"http://arxiv.org/abs/2307.10943v1"},"cats":{"new-dataset":0.2995673055,"dev-research":0.2369214104,"prompt-eng":0.3436733172,"data-quality":0.3327109815,"ml-security":0.1420046446}}
{"text":"To address the limitations and more accurately reflect real-world scenarios, in this paper, we propose a novel unsupervised class incremental learning approach for discovering novel categories on unlabeled sets without prior knowledge.","meta":{"url":"http://arxiv.org/abs/2307.10943v1"},"cats":{"new-dataset":0.2985388212,"dev-research":0.3086321558,"prompt-eng":0.3698673324,"data-quality":0.3489698984,"ml-security":0.1568139403}}
{"text":"The proposed method fine-tunes the feature extractor and proxy anchors on labeled sets, then splits samples into old and novel categories and clusters on the unlabeled dataset.","meta":{"url":"http://arxiv.org/abs/2307.10943v1"},"cats":{"new-dataset":0.4411362059,"dev-research":0.3079157475,"prompt-eng":0.3797734413,"data-quality":0.557295677,"ml-security":0.1112565548}}
{"text":"Furthermore, the proxy anchors-based exemplar generates representative category vectors to mitigate catastrophic forgetting.","meta":{"url":"http://arxiv.org/abs/2307.10943v1"},"cats":{"new-dataset":0.0353632206,"dev-research":0.3519096045,"prompt-eng":0.4539097547,"data-quality":0.2802091126,"ml-security":0.2088305017}}
{"text":"Experimental results demonstrate that our proposed approach outperforms the state-of-the-art methods on fine-grained datasets under real-world scenarios.","meta":{"url":"http://arxiv.org/abs/2307.10943v1"},"cats":{"new-dataset":0.6787221383,"dev-research":0.2962508938,"prompt-eng":0.3789331031,"data-quality":0.2972887048,"ml-security":0.1359298101}}
{"text":"Self-supervised learning has brought about a revolutionary paradigm shift in various computing domains, including NLP, vision, and biology.","meta":{"url":"http://arxiv.org/abs/2307.10936v1"},"cats":{"new-dataset":0.0408197385,"dev-research":0.2658663071,"prompt-eng":0.3724191786,"data-quality":0.2263215337,"ml-security":0.1431028812}}
{"text":"Recent approaches involve pre-training transformer models on vast amounts of unlabeled data, serving as a starting point for efficiently solving downstream tasks.","meta":{"url":"http://arxiv.org/abs/2307.10936v1"},"cats":{"new-dataset":0.1079892774,"dev-research":0.210334069,"prompt-eng":0.4381495677,"data-quality":0.1668976085,"ml-security":0.118747195}}
{"text":"In the realm of reinforcement learning, researchers have recently adapted these approaches by developing models pre-trained on expert trajectories, enabling them to address a wide range of tasks, from robotics to recommendation systems.","meta":{"url":"http://arxiv.org/abs/2307.10936v1"},"cats":{"new-dataset":0.0578046744,"dev-research":0.2015688243,"prompt-eng":0.4643736978,"data-quality":0.0864289072,"ml-security":0.113993358}}
{"text":"However, existing methods mostly rely on intricate pre-training objectives tailored to specific downstream applications.","meta":{"url":"http://arxiv.org/abs/2307.10936v1"},"cats":{"new-dataset":0.0058440979,"dev-research":0.3436091103,"prompt-eng":0.4217947931,"data-quality":0.1789004667,"ml-security":0.2034538294}}
{"text":"This paper presents a comprehensive investigation of models we refer to as Pretrained Action-State Transformer Agents (PASTA).","meta":{"url":"http://arxiv.org/abs/2307.10936v1"},"cats":{"new-dataset":0.0333901438,"dev-research":0.1896378176,"prompt-eng":0.4316818985,"data-quality":0.0735171995,"ml-security":0.1791085561}}
{"text":"Our study uses a unified methodology and covers an extensive set of general downstream tasks including behavioral cloning, offline RL, sensor failure robustness, and dynamics change adaptation.","meta":{"url":"http://arxiv.org/abs/2307.10936v1"},"cats":{"new-dataset":0.1619236098,"dev-research":0.3107326095,"prompt-eng":0.5215669291,"data-quality":0.2165387227,"ml-security":0.1422314051}}
{"text":"Our goal is to systematically compare various design choices and provide valuable insights to practitioners for building robust models.","meta":{"url":"http://arxiv.org/abs/2307.10936v1"},"cats":{"new-dataset":0.0311246833,"dev-research":0.3030147297,"prompt-eng":0.4731884435,"data-quality":0.1568423145,"ml-security":0.1592320082}}
{"text":"Key highlights of our study include tokenization at the action and state component level, using fundamental pre-training objectives like next token prediction, training models across diverse domains simultaneously, and using parameter efficient fine-tuning (PEFT).","meta":{"url":"http://arxiv.org/abs/2307.10936v1"},"cats":{"new-dataset":0.0549197392,"dev-research":0.2310335834,"prompt-eng":0.4914866023,"data-quality":0.1882790555,"ml-security":0.1826286015}}
{"text":"The developed models in our study contain fewer than 10 million parameters and the application of PEFT enables fine-tuning of fewer than 10,000 parameters during downstream adaptation, allowing a broad community to use these models and reproduce our experiments.","meta":{"url":"http://arxiv.org/abs/2307.10936v1"},"cats":{"new-dataset":0.0329463731,"dev-research":0.1776987989,"prompt-eng":0.4555037961,"data-quality":0.111121052,"ml-security":0.0946194372}}
{"text":"We hope that this study will encourage further research into the use of transformers with first-principles design choices to represent RL trajectories and contribute to robust policy learning.","meta":{"url":"http://arxiv.org/abs/2307.10936v1"},"cats":{"new-dataset":0.0491669036,"dev-research":0.2037387833,"prompt-eng":0.387744302,"data-quality":0.090035506,"ml-security":0.1665319215}}
{"text":"Modern approaches for vision-centric environment perception for autonomous navigation make extensive use of self-supervised monocular depth estimation algorithms that output disparity maps.","meta":{"url":"http://arxiv.org/abs/2307.10934v1"},"cats":{"new-dataset":0.0989265984,"dev-research":0.22449896,"prompt-eng":0.4048354336,"data-quality":0.1592448254,"ml-security":0.1012954833}}
{"text":"However, when this disparity map is projected onto 3D space, the errors in disparity are magnified, resulting in a depth estimation error that increases quadratically as the distance from the camera increases.","meta":{"url":"http://arxiv.org/abs/2307.10934v1"},"cats":{"new-dataset":0.0273111692,"dev-research":0.2544610812,"prompt-eng":0.3057422406,"data-quality":0.2080280478,"ml-security":0.0655832292}}
{"text":"Though Light Detection and Ranging (LiDAR) can solve this issue, it is expensive and not feasible for many applications.","meta":{"url":"http://arxiv.org/abs/2307.10934v1"},"cats":{"new-dataset":0.0593535087,"dev-research":0.1974645475,"prompt-eng":0.3996251273,"data-quality":0.120781035,"ml-security":0.0716333928}}
{"text":"To address the challenge of accurate ranging with low-cost sensors, we propose, OCTraN, a transformer architecture that uses iterative-attention to convert 2D image features into 3D occupancy features and makes use of convolution and transpose convolution to efficiently operate on spatial information.","meta":{"url":"http://arxiv.org/abs/2307.10934v1"},"cats":{"new-dataset":0.3485267948,"dev-research":0.2863761411,"prompt-eng":0.3600303861,"data-quality":0.0853204507,"ml-security":0.075976471}}
{"text":"We also develop a self-supervised training pipeline to generalize the model to any scene by eliminating the need for LiDAR ground truth by substituting it with pseudo-ground truth labels obtained from boosted monocular depth estimation.","meta":{"url":"http://arxiv.org/abs/2307.10934v1"},"cats":{"new-dataset":0.1629668117,"dev-research":0.1814652781,"prompt-eng":0.4135499954,"data-quality":0.2532658098,"ml-security":0.1861324061}}
{"text":"The enhancement of unsupervised learning of sentence representations has been significantly achieved by the utility of contrastive learning.","meta":{"url":"http://arxiv.org/abs/2307.10932v1"},"cats":{"new-dataset":0.0229602404,"dev-research":0.213381423,"prompt-eng":0.3718929831,"data-quality":0.1812282949,"ml-security":0.1172377462}}
{"text":"This approach clusters the augmented positive instance with the anchor instance to create a desired embedding space.","meta":{"url":"http://arxiv.org/abs/2307.10932v1"},"cats":{"new-dataset":0.03641879,"dev-research":0.2517723705,"prompt-eng":0.4378541398,"data-quality":0.2390729051,"ml-security":0.0965831479}}
{"text":"However, relying solely on the contrastive objective can result in sub-optimal outcomes due to its inability to differentiate subtle semantic variations between positive pairs.","meta":{"url":"http://arxiv.org/abs/2307.10932v1"},"cats":{"new-dataset":0.0034559687,"dev-research":0.2562199072,"prompt-eng":0.3704479489,"data-quality":0.2536764687,"ml-security":0.086916082}}
{"text":"Specifically, common data augmentation techniques frequently introduce semantic distortion, leading to a semantic margin between the positive pair.","meta":{"url":"http://arxiv.org/abs/2307.10932v1"},"cats":{"new-dataset":0.0423941205,"dev-research":0.3780114084,"prompt-eng":0.4103871671,"data-quality":0.4808687126,"ml-security":0.1127432355}}
{"text":"While the InfoNCE loss function overlooks the semantic margin and prioritizes similarity maximization between positive pairs during training, leading to the insensitive semantic comprehension ability of the trained model.","meta":{"url":"http://arxiv.org/abs/2307.10932v1"},"cats":{"new-dataset":0.0171765729,"dev-research":0.3177860747,"prompt-eng":0.3796425536,"data-quality":0.3701995555,"ml-security":0.2246041624}}
{"text":"In this paper, we introduce a novel Identical and Fraternal Twins of Contrastive Learning (named IFTCL) framework, capable of simultaneously adapting to various positive pairs generated by different augmentation techniques.","meta":{"url":"http://arxiv.org/abs/2307.10932v1"},"cats":{"new-dataset":0.0846321912,"dev-research":0.2458830604,"prompt-eng":0.3901620494,"data-quality":0.1769507493,"ml-security":0.1326036429}}
{"text":"We propose a \\textit{Twins Loss} to preserve the innate margin during training and promote the potential of data enhancement in order to overcome the sub-optimal issue.","meta":{"url":"http://arxiv.org/abs/2307.10932v1"},"cats":{"new-dataset":0.0892573623,"dev-research":0.2377554673,"prompt-eng":0.3728705779,"data-quality":0.350986349,"ml-security":0.3052176611}}
{"text":"We also present proof-of-concept experiments combined with the contrastive objective to prove the validity of the proposed Twins Loss.","meta":{"url":"http://arxiv.org/abs/2307.10932v1"},"cats":{"new-dataset":0.0148217801,"dev-research":0.2117462124,"prompt-eng":0.3832209304,"data-quality":0.3650953404,"ml-security":0.2245995565}}
{"text":"Furthermore, we propose a hippocampus queue mechanism to restore and reuse the negative instances without additional calculation, which further enhances the efficiency and performance of the IFCL.","meta":{"url":"http://arxiv.org/abs/2307.10932v1"},"cats":{"new-dataset":0.0257613467,"dev-research":0.3071744831,"prompt-eng":0.4709275999,"data-quality":0.1743033691,"ml-security":0.1557642088}}
{"text":"We verify the IFCL framework on nine semantic textual similarity tasks with both English and Chinese datasets, and the experimental results show that IFCL outperforms state-of-the-art methods.","meta":{"url":"http://arxiv.org/abs/2307.10932v1"},"cats":{"new-dataset":0.2276836689,"dev-research":0.2886671126,"prompt-eng":0.3971806139,"data-quality":0.3805453437,"ml-security":0.0551650769}}
{"text":"The development of large language models (LLMs) has seen rapid progress in recent years.","meta":{"url":"http://arxiv.org/abs/2307.10930v1"},"cats":{"new-dataset":0.0853821662,"dev-research":0.2030670641,"prompt-eng":0.3854761482,"data-quality":0.1296686776,"ml-security":0.0906699935}}
{"text":"One of the most widely used LLMs is the Generative Pre-trained Transformer (GPT) series, which has been applied in various fields, including the media domain.","meta":{"url":"http://arxiv.org/abs/2307.10930v1"},"cats":{"new-dataset":0.0879696834,"dev-research":0.1609430614,"prompt-eng":0.4240798743,"data-quality":0.1088745205,"ml-security":0.0730404882}}
{"text":"However, in practical applications, the differences between the media's use cases and the general-purpose applications of LLMs have become increasingly apparent, especially Chinese.","meta":{"url":"http://arxiv.org/abs/2307.10930v1"},"cats":{"new-dataset":0.0186069401,"dev-research":0.250998284,"prompt-eng":0.3352182224,"data-quality":0.1427269445,"ml-security":0.1223862119}}
{"text":"As a result, there is a growing need to develop LLM that are specifically tailored to the unique requirements of the media domain.","meta":{"url":"http://arxiv.org/abs/2307.10930v1"},"cats":{"new-dataset":0.0343134224,"dev-research":0.2638224866,"prompt-eng":0.3679172982,"data-quality":0.1027383271,"ml-security":0.0877225918}}
{"text":"In this paper, we present MediaGPT, a large language model training on variety of media data and addressing the practical needs of Chinese media.","meta":{"url":"http://arxiv.org/abs/2307.10930v1"},"cats":{"new-dataset":0.4786463782,"dev-research":0.1914195213,"prompt-eng":0.3783929009,"data-quality":0.2847327562,"ml-security":0.074116834}}
{"text":"We have designed a diverse set of task instruction types to cater to the specific requirements of the domain.","meta":{"url":"http://arxiv.org/abs/2307.10930v1"},"cats":{"new-dataset":0.1123255359,"dev-research":0.3042492905,"prompt-eng":0.4586062552,"data-quality":0.0835910713,"ml-security":0.0603847019}}
{"text":"To further validate the effectiveness of our proposed LLM, we have constructed unique datasets that are tailored to the media domain and have also developed verification methods that are specifically designed for generative-type tasks.","meta":{"url":"http://arxiv.org/abs/2307.10930v1"},"cats":{"new-dataset":0.2852913165,"dev-research":0.2260700899,"prompt-eng":0.4692415699,"data-quality":0.3051392241,"ml-security":0.0811066937}}
{"text":"By doing so, we aim to bridge the gap between the general-purpose LLM and the requirements of the media domain, and to pave the way for more effective and efficient use of LLM in this field.","meta":{"url":"http://arxiv.org/abs/2307.10930v1"},"cats":{"new-dataset":0.0438905556,"dev-research":0.2390099934,"prompt-eng":0.4171245553,"data-quality":0.1228703479,"ml-security":0.0670734043}}
{"text":"This paper aims to explore the challenges and opportunities of developing LLM for media applications and to propose potential solutions for addressing these challenges.","meta":{"url":"http://arxiv.org/abs/2307.10930v1"},"cats":{"new-dataset":0.0502646096,"dev-research":0.2408372031,"prompt-eng":0.3877091892,"data-quality":0.1647716346,"ml-security":0.0812250963}}
{"text":"Evaluation of Large Language Models (LLMs) is challenging because aligning to human values requires the composition of multiple skills and the required set of skills varies depending on the instruction.","meta":{"url":"http://arxiv.org/abs/2307.10928v1"},"cats":{"new-dataset":0.0250930486,"dev-research":0.2326635778,"prompt-eng":0.4282715439,"data-quality":0.2109593196,"ml-security":0.1093913152}}
{"text":"Recent studies have evaluated the performance of LLMs in two ways, (1) automatic evaluation on several independent benchmarks and (2) human or machined-based evaluation giving an overall score to the response.","meta":{"url":"http://arxiv.org/abs/2307.10928v1"},"cats":{"new-dataset":0.0223064471,"dev-research":0.2476012562,"prompt-eng":0.5102158746,"data-quality":0.1920258626,"ml-security":0.0790541478}}
{"text":"However, both settings are coarse-grained evaluations, not considering the nature of user instructions that require instance-wise skill composition, which limits the interpretation of the true capabilities of LLMs.","meta":{"url":"http://arxiv.org/abs/2307.10928v1"},"cats":{"new-dataset":0.005743373,"dev-research":0.301531098,"prompt-eng":0.406882957,"data-quality":0.1573701507,"ml-security":0.1130901944}}
{"text":"In this paper, we introduce FLASK (Fine-grained Language Model Evaluation based on Alignment SKill Sets), a fine-grained evaluation protocol that can be used for both model-based and human-based evaluation which decomposes coarse-level scoring to an instance-wise skill set-level.","meta":{"url":"http://arxiv.org/abs/2307.10928v1"},"cats":{"new-dataset":0.2451427483,"dev-research":0.3174064158,"prompt-eng":0.4534082408,"data-quality":0.3383504683,"ml-security":0.1070550677}}
{"text":"Specifically, we define 12 fine-grained skills needed for LLMs to follow open-ended user instructions and construct an evaluation set by allocating a set of skills for each instance.","meta":{"url":"http://arxiv.org/abs/2307.10928v1"},"cats":{"new-dataset":0.1215549011,"dev-research":0.2974705037,"prompt-eng":0.4730298871,"data-quality":0.1409756208,"ml-security":0.1339449516}}
{"text":"Additionally, by annotating the target domains and difficulty level for each instance, FLASK provides a holistic view with a comprehensive analysis of a model's performance depending on skill, domain, and difficulty.","meta":{"url":"http://arxiv.org/abs/2307.10928v1"},"cats":{"new-dataset":0.0672340523,"dev-research":0.3223820991,"prompt-eng":0.4453559827,"data-quality":0.1310647322,"ml-security":0.0928674171}}
{"text":"Through using FLASK, we compare multiple open-sourced and proprietary LLMs and observe highly-correlated findings between model-based and human-based evaluations.","meta":{"url":"http://arxiv.org/abs/2307.10928v1"},"cats":{"new-dataset":0.1750328368,"dev-research":0.289622316,"prompt-eng":0.4384727104,"data-quality":0.1873534311,"ml-security":0.1493300383}}
{"text":"FLASK enables developers to more accurately measure the model performance and how it can be improved by analyzing factors that make LLMs proficient in particular skills.","meta":{"url":"http://arxiv.org/abs/2307.10928v1"},"cats":{"new-dataset":0.0242638975,"dev-research":0.3777143891,"prompt-eng":0.3959709304,"data-quality":0.1059758539,"ml-security":0.1658216093}}
{"text":"For practitioners, FLASK can be used to recommend suitable models for particular situations through comprehensive comparison among various LLMs.","meta":{"url":"http://arxiv.org/abs/2307.10928v1"},"cats":{"new-dataset":0.0388309724,"dev-research":0.2466112232,"prompt-eng":0.4182801277,"data-quality":0.0632371527,"ml-security":0.0964058935}}
{"text":"We release the evaluation data and code implementation at https://github.com/kaistAI/FLASK.","meta":{"url":"http://arxiv.org/abs/2307.10928v1"},"cats":{"new-dataset":0.4344867511,"dev-research":0.2739615657,"prompt-eng":0.4289786277,"data-quality":0.1603513588,"ml-security":0.0860573959}}
{"text":"Intrinsic decomposition is to infer the albedo and shading from the image.","meta":{"url":"http://arxiv.org/abs/2307.10924v1"},"cats":{"new-dataset":0.0189386067,"dev-research":0.229402243,"prompt-eng":0.383256124,"data-quality":0.1318057875,"ml-security":0.0939207552}}
{"text":"Since it is a heavily ill-posed problem, previous methods rely on prior assumptions from 2D images, however, the exploration of the data representation itself is limited.","meta":{"url":"http://arxiv.org/abs/2307.10924v1"},"cats":{"new-dataset":0.0953560672,"dev-research":0.1949629784,"prompt-eng":0.3391644468,"data-quality":0.1324097494,"ml-security":0.1161565366}}
{"text":"The point cloud is known as a rich format of scene representation, which naturally aligns the geometric information and the color information of an image.","meta":{"url":"http://arxiv.org/abs/2307.10924v1"},"cats":{"new-dataset":0.1055937108,"dev-research":0.2514587458,"prompt-eng":0.3490091831,"data-quality":0.1478139223,"ml-security":0.0779091904}}
{"text":"Our proposed method, Point Intrinsic Net, in short, PoInt-Net, jointly predicts the albedo, light source direction, and shading, using point cloud representation.","meta":{"url":"http://arxiv.org/abs/2307.10924v1"},"cats":{"new-dataset":0.076169335,"dev-research":0.2131497735,"prompt-eng":0.37813421,"data-quality":0.1320196262,"ml-security":0.0998427445}}
{"text":"Experiments reveal the benefits of PoInt-Net, in terms of accuracy, it outperforms 2D representation approaches on multiple metrics across datasets; in terms of efficiency, it trains on small-scale point clouds and performs stably on any-scale point clouds; in terms of robustness, it only trains on single object level dataset, and demonstrates reasonable generalization ability for unseen objects and scenes.","meta":{"url":"http://arxiv.org/abs/2307.10924v1"},"cats":{"new-dataset":0.1878737456,"dev-research":0.25352336,"prompt-eng":0.3351029856,"data-quality":0.1696900669,"ml-security":0.147890359}}
{"text":"Self-supervised learning (SSL) for clinical time series data has received significant attention in recent literature, since these data are highly rich and provide important information about a patient's physiological state.","meta":{"url":"http://arxiv.org/abs/2307.10923v1"},"cats":{"new-dataset":0.119756878,"dev-research":0.1941837212,"prompt-eng":0.3982232605,"data-quality":0.1164378289,"ml-security":0.191730742}}
{"text":"However, most existing SSL methods for clinical time series are limited in that they are designed for unimodal time series, such as a sequence of structured features (e.g., lab values and vitals signs) or an individual high-dimensional physiological signal (e.g., an electrocardiogram).","meta":{"url":"http://arxiv.org/abs/2307.10923v1"},"cats":{"new-dataset":0.0524209262,"dev-research":0.2080495038,"prompt-eng":0.3691905469,"data-quality":0.0717408707,"ml-security":0.1632743478}}
{"text":"These existing methods cannot be readily extended to model time series that exhibit multimodality, with structured features and high-dimensional data being recorded at each timestep in the sequence.","meta":{"url":"http://arxiv.org/abs/2307.10923v1"},"cats":{"new-dataset":0.1489563484,"dev-research":0.1597494449,"prompt-eng":0.3629329086,"data-quality":0.1466062557,"ml-security":0.0915054876}}
{"text":"In this work, we address this gap and propose a new SSL method -- Sequential Multi-Dimensional SSL -- where a SSL loss is applied both at the level of the entire sequence and at the level of the individual high-dimensional data points in the sequence in order to better capture information at both scales.","meta":{"url":"http://arxiv.org/abs/2307.10923v1"},"cats":{"new-dataset":0.2629310782,"dev-research":0.1756977197,"prompt-eng":0.3705291073,"data-quality":0.1269346376,"ml-security":0.195231269}}
{"text":"Our strategy is agnostic to the specific form of loss function used at each level -- it can be contrastive, as in SimCLR, or non-contrastive, as in VICReg.","meta":{"url":"http://arxiv.org/abs/2307.10923v1"},"cats":{"new-dataset":0.0130946809,"dev-research":0.1713921158,"prompt-eng":0.3863390254,"data-quality":0.1526698431,"ml-security":0.1656639783}}
{"text":"We evaluate our method on two real-world clinical datasets, where the time series contains sequences of (1) high-frequency electrocardiograms and (2) structured data from lab values and vitals signs.","meta":{"url":"http://arxiv.org/abs/2307.10923v1"},"cats":{"new-dataset":0.6296076847,"dev-research":0.2252479261,"prompt-eng":0.3813788813,"data-quality":0.1496893935,"ml-security":0.1090355611}}
{"text":"Our experimental results indicate that pre-training with our method and then fine-tuning on downstream tasks improves performance over baselines on both datasets, and in several settings, can lead to improvements across different self-supervised loss functions.","meta":{"url":"http://arxiv.org/abs/2307.10923v1"},"cats":{"new-dataset":0.0683312556,"dev-research":0.273133925,"prompt-eng":0.4488882867,"data-quality":0.3617021227,"ml-security":0.1132597592}}
{"text":"Recent contrastive language image pre-training has led to learning highly transferable and robust image representations.","meta":{"url":"http://arxiv.org/abs/2307.10922v1"},"cats":{"new-dataset":0.1857643645,"dev-research":0.193755696,"prompt-eng":0.3725775985,"data-quality":0.2405774276,"ml-security":0.1117641877}}
{"text":"However, adapting these models to video domains with minimal supervision remains an open problem.","meta":{"url":"http://arxiv.org/abs/2307.10922v1"},"cats":{"new-dataset":0.0673313902,"dev-research":0.1759020543,"prompt-eng":0.4023076082,"data-quality":0.2990412578,"ml-security":0.1249502608}}
{"text":"We explore a simple step in that direction, using language tied self-supervised learning to adapt an image CLIP model to the video domain.","meta":{"url":"http://arxiv.org/abs/2307.10922v1"},"cats":{"new-dataset":0.0930499719,"dev-research":0.1872028689,"prompt-eng":0.4258351165,"data-quality":0.3001227412,"ml-security":0.1054690333}}
{"text":"A backbone modified for temporal modeling is trained under self-distillation settings with train objectives operating in an action concept space.","meta":{"url":"http://arxiv.org/abs/2307.10922v1"},"cats":{"new-dataset":0.0610629927,"dev-research":0.2563583181,"prompt-eng":0.4328323685,"data-quality":0.1181240687,"ml-security":0.151177363}}
{"text":"Feature vectors of various action concepts extracted from a language encoder using relevant textual prompts construct this space.","meta":{"url":"http://arxiv.org/abs/2307.10922v1"},"cats":{"new-dataset":0.1932571841,"dev-research":0.3140721093,"prompt-eng":0.4851627962,"data-quality":0.2574600386,"ml-security":0.1483507199}}
{"text":"We introduce two train objectives, concept distillation and concept alignment, that retain generality of original representations while enforcing relations between actions and their attributes.","meta":{"url":"http://arxiv.org/abs/2307.10922v1"},"cats":{"new-dataset":0.062583164,"dev-research":0.3518421539,"prompt-eng":0.4185341496,"data-quality":0.3191011378,"ml-security":0.1714733106}}
{"text":"Our approach improves zero-shot and linear probing performance on three action recognition benchmarks.","meta":{"url":"http://arxiv.org/abs/2307.10922v1"},"cats":{"new-dataset":0.1833001508,"dev-research":0.2032339115,"prompt-eng":0.3924488297,"data-quality":0.2497128345,"ml-security":0.11841573}}
{"text":"Despite the rapid progress in self-supervised learning (SSL), end-to-end fine-tuning still remains the dominant fine-tuning strategy for medical imaging analysis.","meta":{"url":"http://arxiv.org/abs/2307.10915v1"},"cats":{"new-dataset":0.0443290922,"dev-research":0.2415564203,"prompt-eng":0.4023206045,"data-quality":0.2625949713,"ml-security":0.1888227717}}
{"text":"However, it remains unclear whether this approach is truly optimal for effectively utilizing the pre-trained knowledge, especially considering the diverse categories of SSL that capture different types of features.","meta":{"url":"http://arxiv.org/abs/2307.10915v1"},"cats":{"new-dataset":0.017153428,"dev-research":0.2725429908,"prompt-eng":0.4397156016,"data-quality":0.1744804683,"ml-security":0.2515077691}}
{"text":"In this paper, we first establish strong contrastive and restorative SSL baselines that outperform SOTA methods across four diverse downstream tasks.","meta":{"url":"http://arxiv.org/abs/2307.10915v1"},"cats":{"new-dataset":0.0871617409,"dev-research":0.2358415434,"prompt-eng":0.3912638275,"data-quality":0.214738508,"ml-security":0.1230202577}}
{"text":"Building upon these strong baselines, we conduct an extensive fine-tuning analysis across multiple pre-training and fine-tuning datasets, as well as various fine-tuning dataset sizes.","meta":{"url":"http://arxiv.org/abs/2307.10915v1"},"cats":{"new-dataset":0.5571357083,"dev-research":0.3163783631,"prompt-eng":0.4457724136,"data-quality":0.3320446249,"ml-security":0.1411612943}}
{"text":"Contrary to the conventional wisdom of fine-tuning only the last few layers of a pre-trained network, we show that fine-tuning intermediate layers is more effective, with fine-tuning the second quarter (25-50%) of the network being optimal for contrastive SSL whereas fine-tuning the third quarter (50-75%) of the network being optimal for restorative SSL.","meta":{"url":"http://arxiv.org/abs/2307.10915v1"},"cats":{"new-dataset":0.0173859032,"dev-research":0.2482999481,"prompt-eng":0.4142242958,"data-quality":0.2123151812,"ml-security":0.2517365702}}
{"text":"Compared to the de-facto standard of end-to-end fine-tuning, our best fine-tuning strategy, which fine-tunes a shallower network consisting of the first three quarters (0-75%) of the pre-trained network, yields improvements of as much as 5.48%.","meta":{"url":"http://arxiv.org/abs/2307.10915v1"},"cats":{"new-dataset":0.0303111508,"dev-research":0.3304949741,"prompt-eng":0.4262828838,"data-quality":0.2665697604,"ml-security":0.1708558445}}
{"text":"Additionally, using these insights, we propose a simple yet effective method to leverage the complementary strengths of multiple SSL models, resulting in enhancements of up to 3.57% compared to using the best model alone.","meta":{"url":"http://arxiv.org/abs/2307.10915v1"},"cats":{"new-dataset":0.0376502771,"dev-research":0.1835499952,"prompt-eng":0.437473365,"data-quality":0.1330264037,"ml-security":0.181985677}}
{"text":"Hence, our fine-tuning strategies not only enhance the performance of individual SSL models, but also enable effective utilization of the complementary strengths offered by multiple SSL models, leading to significant improvements in self-supervised medical imaging analysis.","meta":{"url":"http://arxiv.org/abs/2307.10915v1"},"cats":{"new-dataset":0.0337875545,"dev-research":0.1902319132,"prompt-eng":0.4586220657,"data-quality":0.1545220756,"ml-security":0.192005928}}
{"text":"Limited by expensive pixel-level labels, polyp segmentation models are plagued by data shortage and suffer from impaired generalization.","meta":{"url":"http://arxiv.org/abs/2307.10912v1"},"cats":{"new-dataset":0.0597827367,"dev-research":0.2294425742,"prompt-eng":0.3747385801,"data-quality":0.3302611952,"ml-security":0.2035353359}}
{"text":"In contrast, polyp bounding box annotations are much cheaper and more accessible.","meta":{"url":"http://arxiv.org/abs/2307.10912v1"},"cats":{"new-dataset":0.014184876,"dev-research":0.3734091011,"prompt-eng":0.3402514451,"data-quality":0.1924064083,"ml-security":0.1162652945}}
{"text":"Thus, to reduce labeling cost, we propose to learn a weakly supervised polyp segmentation model (i.e., WeakPolyp) completely based on bounding box annotations.","meta":{"url":"http://arxiv.org/abs/2307.10912v1"},"cats":{"new-dataset":0.2531787534,"dev-research":0.2175793935,"prompt-eng":0.4162868353,"data-quality":0.4057973953,"ml-security":0.1672778}}
{"text":"However, coarse bounding boxes contain too much noise.","meta":{"url":"http://arxiv.org/abs/2307.10912v1"},"cats":{"new-dataset":0.0329982543,"dev-research":0.2472403803,"prompt-eng":0.3352224456,"data-quality":0.3022898702,"ml-security":0.1919317926}}
{"text":"To avoid interference, we introduce the mask-to-box (M2B) transformation.","meta":{"url":"http://arxiv.org/abs/2307.10912v1"},"cats":{"new-dataset":0.0756562461,"dev-research":0.215682188,"prompt-eng":0.4260457872,"data-quality":0.1149637122,"ml-security":0.1163061332}}
{"text":"By supervising the outer box mask of the prediction instead of the prediction itself, M2B greatly mitigates the mismatch between the coarse label and the precise prediction.","meta":{"url":"http://arxiv.org/abs/2307.10912v1"},"cats":{"new-dataset":0.0092846308,"dev-research":0.2796852329,"prompt-eng":0.4622735064,"data-quality":0.4617435249,"ml-security":0.2171568779}}
{"text":"But, M2B only provides sparse supervision, leading to non-unique predictions.","meta":{"url":"http://arxiv.org/abs/2307.10912v1"},"cats":{"new-dataset":0.0340120964,"dev-research":0.1886909338,"prompt-eng":0.3890621784,"data-quality":0.2201461065,"ml-security":0.1618163776}}
{"text":"Therefore, we further propose a scale consistency (SC) loss for dense supervision.","meta":{"url":"http://arxiv.org/abs/2307.10912v1"},"cats":{"new-dataset":0.0420643889,"dev-research":0.2413157894,"prompt-eng":0.4595669965,"data-quality":0.4898992715,"ml-security":0.0978394772}}
{"text":"By explicitly aligning predictions across the same image at different scales, the SC loss largely reduces the variation of predictions.","meta":{"url":"http://arxiv.org/abs/2307.10912v1"},"cats":{"new-dataset":0.0073897897,"dev-research":0.2128676395,"prompt-eng":0.3984832996,"data-quality":0.225484519,"ml-security":0.1220925312}}
{"text":"Note that our WeakPolyp is a plug-and-play model, which can be easily ported to other appealing backbones.","meta":{"url":"http://arxiv.org/abs/2307.10912v1"},"cats":{"new-dataset":0.114246076,"dev-research":0.1980134675,"prompt-eng":0.4252908364,"data-quality":0.1169184563,"ml-security":0.1775380441}}
{"text":"Besides, the proposed modules are only used during training, bringing no computation cost to inference.","meta":{"url":"http://arxiv.org/abs/2307.10912v1"},"cats":{"new-dataset":0.0339595711,"dev-research":0.2478492765,"prompt-eng":0.3148896809,"data-quality":0.0936667013,"ml-security":0.1343642031}}
{"text":"Extensive experiments demonstrate the effectiveness of our proposed WeakPolyp, which surprisingly achieves a comparable performance with a fully supervised model, requiring no mask annotations at all.","meta":{"url":"http://arxiv.org/abs/2307.10912v1"},"cats":{"new-dataset":0.1615947361,"dev-research":0.202725125,"prompt-eng":0.4593765584,"data-quality":0.382228248,"ml-security":0.3059339338}}
{"text":"The mechanisms behind the success of multi-view self-supervised learning (MVSSL) are not yet fully understood.","meta":{"url":"http://arxiv.org/abs/2307.10907v1"},"cats":{"new-dataset":0.0174039542,"dev-research":0.2212838291,"prompt-eng":0.3494594407,"data-quality":0.2843164394,"ml-security":0.1541407711}}
{"text":"Contrastive MVSSL methods have been studied through the lens of InfoNCE, a lower bound of the Mutual Information (MI).","meta":{"url":"http://arxiv.org/abs/2307.10907v1"},"cats":{"new-dataset":0.0267688521,"dev-research":0.2414338506,"prompt-eng":0.3462700797,"data-quality":0.1492990316,"ml-security":0.0747398289}}
{"text":"However, the relation between other MVSSL methods and MI remains unclear.","meta":{"url":"http://arxiv.org/abs/2307.10907v1"},"cats":{"new-dataset":0.0113614083,"dev-research":0.2739537243,"prompt-eng":0.3368681962,"data-quality":0.1838212463,"ml-security":0.08313953}}
{"text":"We consider a different lower bound on the MI consisting of an entropy and a reconstruction term (ER), and analyze the main MVSSL families through its lens.","meta":{"url":"http://arxiv.org/abs/2307.10907v1"},"cats":{"new-dataset":0.1168069637,"dev-research":0.1364188345,"prompt-eng":0.3779463557,"data-quality":0.1469493528,"ml-security":0.1017246701}}
{"text":"Through this ER bound, we show that clustering-based methods such as DeepCluster and SwAV maximize the MI.","meta":{"url":"http://arxiv.org/abs/2307.10907v1"},"cats":{"new-dataset":0.128393645,"dev-research":0.2153470608,"prompt-eng":0.3781575252,"data-quality":0.186303374,"ml-security":0.1469594739}}
{"text":"We also re-interpret the mechanisms of distillation-based approaches such as BYOL and DINO, showing that they explicitly maximize the reconstruction term and implicitly encourage a stable entropy, and we confirm this empirically.","meta":{"url":"http://arxiv.org/abs/2307.10907v1"},"cats":{"new-dataset":0.0202507457,"dev-research":0.2286748637,"prompt-eng":0.3952573121,"data-quality":0.1584487249,"ml-security":0.1311851192}}
{"text":"We show that replacing the objectives of common MVSSL methods with this ER bound achieves competitive performance, while making them stable when training with smaller batch sizes or smaller exponential moving average (EMA) coefficients.   ","meta":{"url":"http://arxiv.org/abs/2307.10907v1"},"cats":{"new-dataset":0.0206516978,"dev-research":0.2248953839,"prompt-eng":0.3549493224,"data-quality":0.1567430502,"ml-security":0.2355724826}}
{"text":"Github repo: https://github.com/apple/ml-entropy-reconstruction.","meta":{"url":"http://arxiv.org/abs/2307.10907v1"},"cats":{"new-dataset":0.3675120664,"dev-research":0.2136792233,"prompt-eng":0.4042740632,"data-quality":0.1774485113,"ml-security":0.0941366137}}
{"text":"Digital democracy and new forms for direct digital participation in policy making gain unprecedented momentum.","meta":{"url":"http://arxiv.org/abs/2307.10903v1"},"cats":{"new-dataset":0.07279143,"dev-research":0.2557532919,"prompt-eng":0.3031566847,"data-quality":0.098901681,"ml-security":0.1413792212}}
{"text":"This is particularly the case for preferential voting methods and decision-support systems designed to promote fairer, more inclusive and legitimate collective decision-making processes in citizens assemblies, participatory budgeting and elections.","meta":{"url":"http://arxiv.org/abs/2307.10903v1"},"cats":{"new-dataset":0.0379836388,"dev-research":0.2808387407,"prompt-eng":0.4328646102,"data-quality":0.1402257222,"ml-security":0.1502967617}}
{"text":"However, a systematic human experimentation with different voting methods is cumbersome and costly.","meta":{"url":"http://arxiv.org/abs/2307.10903v1"},"cats":{"new-dataset":0.0141820341,"dev-research":0.3250351485,"prompt-eng":0.4244616138,"data-quality":0.1473420373,"ml-security":0.1360841127}}
{"text":"This paper introduces VoteLab, an open-source and thoroughly-documented platform for modular and adaptive design of voting experiments.","meta":{"url":"http://arxiv.org/abs/2307.10903v1"},"cats":{"new-dataset":0.1846724914,"dev-research":0.2968646835,"prompt-eng":0.518765196,"data-quality":0.2610582114,"ml-security":0.1746858997}}
{"text":"It supports to visually and interactively build reusable campaigns with a choice of different voting methods, while voters can easily respond to subscribed voting questions on a smartphone.","meta":{"url":"http://arxiv.org/abs/2307.10903v1"},"cats":{"new-dataset":0.0414996799,"dev-research":0.314507072,"prompt-eng":0.4290239685,"data-quality":0.1006720619,"ml-security":0.1186077416}}
{"text":"A proof-of-concept with four voting methods and questions on COVID-19 in an online lab experiment have been used to study the consistency of voting outcomes.","meta":{"url":"http://arxiv.org/abs/2307.10903v1"},"cats":{"new-dataset":0.0418659084,"dev-research":0.2525487526,"prompt-eng":0.4423785633,"data-quality":0.201221762,"ml-security":0.1599671751}}
{"text":"It demonstrates the capability of VoteLab to support rigorous experimentation of complex voting scenarios.","meta":{"url":"http://arxiv.org/abs/2307.10903v1"},"cats":{"new-dataset":0.1016919682,"dev-research":0.3512278707,"prompt-eng":0.4825642607,"data-quality":0.1683629586,"ml-security":0.1357142077}}
{"text":"We show that computing the strongest polynomial invariant for single-path loops with polynomial assignments is at least as hard as the Skolem problem, a famous problem whose decidability has been open for almost a century.","meta":{"url":"http://arxiv.org/abs/2307.10902v1"},"cats":{"new-dataset":0.0526513341,"dev-research":0.2634183138,"prompt-eng":0.4212096224,"data-quality":0.1863769175,"ml-security":0.2005096076}}
{"text":"While the strongest polynomial invariants are computable for affine loops, for polynomial loops the problem remained wide open.","meta":{"url":"http://arxiv.org/abs/2307.10902v1"},"cats":{"new-dataset":0.0361904947,"dev-research":0.2341562072,"prompt-eng":0.3383884046,"data-quality":0.1662476134,"ml-security":0.2245863674}}
{"text":"As an intermediate result of independent interest, we prove that reachability for discrete polynomial dynamical systems is Skolem-hard as well.","meta":{"url":"http://arxiv.org/abs/2307.10902v1"},"cats":{"new-dataset":0.0277869342,"dev-research":0.1944968035,"prompt-eng":0.3768027312,"data-quality":0.0915026889,"ml-security":0.1910848046}}
{"text":"Furthermore, we generalize the notion of invariant ideals and introduce moment invariant ideals for probabilistic programs.","meta":{"url":"http://arxiv.org/abs/2307.10902v1"},"cats":{"new-dataset":0.0347059888,"dev-research":0.3303228975,"prompt-eng":0.4430427588,"data-quality":0.1368641655,"ml-security":0.2167836836}}
{"text":"With this tool, we further show that the strongest polynomial moment invariant is (i) uncomputable, for probabilistic loops with branching statements, and (ii) Skolem-hard to compute for polynomial probabilistic loops without branching statements.","meta":{"url":"http://arxiv.org/abs/2307.10902v1"},"cats":{"new-dataset":0.0484509608,"dev-research":0.2326474871,"prompt-eng":0.4309673546,"data-quality":0.1386408823,"ml-security":0.145778931}}
{"text":"Finally, we identify a class of probabilistic loops for which the strongest polynomial moment invariant is computable and provide an algorithm for it.","meta":{"url":"http://arxiv.org/abs/2307.10902v1"},"cats":{"new-dataset":0.1218558031,"dev-research":0.2153244218,"prompt-eng":0.4214886558,"data-quality":0.1294811742,"ml-security":0.1514170647}}
{"text":"As social robots see increasing deployment within the general public, improving the interaction with those robots is essential.","meta":{"url":"http://arxiv.org/abs/2307.10897v1"},"cats":{"new-dataset":0.0285946087,"dev-research":0.2800388124,"prompt-eng":0.400677972,"data-quality":0.1018122076,"ml-security":0.1291404965}}
{"text":"Spoken language offers an intuitive interface for the human-robot interaction (HRI), with dialogue management (DM) being a key component in those interactive systems.","meta":{"url":"http://arxiv.org/abs/2307.10897v1"},"cats":{"new-dataset":0.1390521965,"dev-research":0.3371322503,"prompt-eng":0.4177678622,"data-quality":0.1181711389,"ml-security":0.0660862987}}
{"text":"Yet, to overcome current challenges and manage smooth, informative and engaging interaction a more structural approach to combining HRI and DM is needed.","meta":{"url":"http://arxiv.org/abs/2307.10897v1"},"cats":{"new-dataset":0.060361621,"dev-research":0.3350885819,"prompt-eng":0.4307271978,"data-quality":0.1003776604,"ml-security":0.0380107023}}
{"text":"In this systematic review, we analyse the current use of DM in HRI and focus on the type of dialogue manager used, its capabilities, evaluation methods and the challenges specific to DM in HRI.","meta":{"url":"http://arxiv.org/abs/2307.10897v1"},"cats":{"new-dataset":0.1209902403,"dev-research":0.3289319332,"prompt-eng":0.3945586291,"data-quality":0.1705323272,"ml-security":0.0621054346}}
{"text":"We identify the challenges and current scientific frontier related to the DM approach, interaction domain, robot appearance, physical situatedness and multimodality.","meta":{"url":"http://arxiv.org/abs/2307.10897v1"},"cats":{"new-dataset":0.1105733512,"dev-research":0.2611111372,"prompt-eng":0.4221373325,"data-quality":0.0826752678,"ml-security":0.0596711002}}
{"text":"For companies producing related products, a Software Product Line (SPL) is a software reuse method that improves time-to-market and software quality, achieving substantial cost reductions.","meta":{"url":"http://arxiv.org/abs/2307.10896v1"},"cats":{"new-dataset":0.0922102701,"dev-research":0.5227491912,"prompt-eng":0.3395751343,"data-quality":0.1278223203,"ml-security":0.1064383707}}
{"text":"These benefits do not come for free.","meta":{"url":"http://arxiv.org/abs/2307.10896v1"},"cats":{"new-dataset":0.0360198489,"dev-research":0.2396292021,"prompt-eng":0.2705861256,"data-quality":0.0844447668,"ml-security":0.1866578727}}
{"text":"It often takes years to re-architect and re-engineer a codebase to support SPL and, once adopted, it must be maintained.","meta":{"url":"http://arxiv.org/abs/2307.10896v1"},"cats":{"new-dataset":0.0540677008,"dev-research":0.4930632695,"prompt-eng":0.3522028588,"data-quality":0.1316127262,"ml-security":0.0808966817}}
{"text":"Current SPL practice relies on a collection of tools, tailored for different reengineering phases, whose output developers must coordinate and integrate.","meta":{"url":"http://arxiv.org/abs/2307.10896v1"},"cats":{"new-dataset":0.0728674348,"dev-research":0.5067989918,"prompt-eng":0.396322533,"data-quality":0.1222009898,"ml-security":0.0879371945}}
{"text":"We present Foundry, a general automated approach for leveraging software transplantation to speed conversion to and maintenance of SPL.","meta":{"url":"http://arxiv.org/abs/2307.10896v1"},"cats":{"new-dataset":0.1574569187,"dev-research":0.452902752,"prompt-eng":0.4407171419,"data-quality":0.1727205213,"ml-security":0.1065949611}}
{"text":"Foundry facilitates feature extraction and migration.","meta":{"url":"http://arxiv.org/abs/2307.10896v1"},"cats":{"new-dataset":0.087282635,"dev-research":0.4164902459,"prompt-eng":0.3987969931,"data-quality":0.187242753,"ml-security":0.0746423719}}
{"text":"It can efficiently, repeatedly, transplant a sequence of features, implemented in multiple files.","meta":{"url":"http://arxiv.org/abs/2307.10896v1"},"cats":{"new-dataset":0.0826510277,"dev-research":0.3693680255,"prompt-eng":0.4085333488,"data-quality":0.1343914885,"ml-security":0.0897885057}}
{"text":"We used Foundry to create two valid product lines that integrate features from three real-world systems in an automated way.","meta":{"url":"http://arxiv.org/abs/2307.10896v1"},"cats":{"new-dataset":0.15630203,"dev-research":0.4239385436,"prompt-eng":0.4807584182,"data-quality":0.198447301,"ml-security":0.0742192245}}
{"text":"Moreover, we conducted an experiment comparing Foundry's feature migration with manual effort.","meta":{"url":"http://arxiv.org/abs/2307.10896v1"},"cats":{"new-dataset":0.0412673722,"dev-research":0.4141814649,"prompt-eng":0.4655042044,"data-quality":0.227094591,"ml-security":0.084968999}}
{"text":"We show that Foundry automatically migrated features across codebases 4.8 times faster, on average, than the average time a group of SPL experts took to accomplish the task.","meta":{"url":"http://arxiv.org/abs/2307.10896v1"},"cats":{"new-dataset":0.0841387229,"dev-research":0.4746511607,"prompt-eng":0.4218780324,"data-quality":0.1965480083,"ml-security":0.1136418479}}
{"text":"Digital dentistry has made significant advancements in recent years, yet numerous challenges remain to be addressed.","meta":{"url":"http://arxiv.org/abs/2307.10895v1"},"cats":{"new-dataset":0.0176323648,"dev-research":0.2880855989,"prompt-eng":0.3024215501,"data-quality":0.113611336,"ml-security":0.0998826327}}
{"text":"In this study, we release a new extensive dataset of tooth meshes to encourage further research.","meta":{"url":"http://arxiv.org/abs/2307.10895v1"},"cats":{"new-dataset":0.2308158188,"dev-research":0.2100063499,"prompt-eng":0.3207671605,"data-quality":0.0995954075,"ml-security":0.0986458271}}
{"text":"Additionally, we propose Variational FoldingNet (VF-Net), which extends FoldingNet to enable probabilistic learning of point cloud representations.","meta":{"url":"http://arxiv.org/abs/2307.10895v1"},"cats":{"new-dataset":0.1177580016,"dev-research":0.2091968724,"prompt-eng":0.3532460439,"data-quality":0.1268398268,"ml-security":0.1321816856}}
{"text":"A key challenge in existing latent variable models for point clouds is the lack of a 1-to-1 mapping between input points and output points.","meta":{"url":"http://arxiv.org/abs/2307.10895v1"},"cats":{"new-dataset":0.1158635288,"dev-research":0.1468227717,"prompt-eng":0.4180594693,"data-quality":0.1675063759,"ml-security":0.0894848481}}
{"text":"Instead, they must rely on optimizing Chamfer distances, a metric that does not have a normalized distributional counterpart, preventing its usage in probabilistic models.","meta":{"url":"http://arxiv.org/abs/2307.10895v1"},"cats":{"new-dataset":0.006434783,"dev-research":0.1811111356,"prompt-eng":0.3659348969,"data-quality":0.1585317442,"ml-security":0.1230155326}}
{"text":"We demonstrate that explicit minimization of Chamfer distances can be replaced by a suitable encoder, which allows us to increase computational efficiency while simplifying the probabilistic extension.","meta":{"url":"http://arxiv.org/abs/2307.10895v1"},"cats":{"new-dataset":0.0873635422,"dev-research":0.2170913322,"prompt-eng":0.4175342442,"data-quality":0.1714403265,"ml-security":0.0896602774}}
{"text":"Our experimental findings present empirical evidence demonstrating the superior performance of VF-Net over existing models in terms of dental scan reconstruction and extrapolation.","meta":{"url":"http://arxiv.org/abs/2307.10895v1"},"cats":{"new-dataset":0.0101573602,"dev-research":0.2271023493,"prompt-eng":0.3416775487,"data-quality":0.1647980306,"ml-security":0.1246782065}}
{"text":"Additionally, our investigation highlights the robustness of VF-Net's latent representations.","meta":{"url":"http://arxiv.org/abs/2307.10895v1"},"cats":{"new-dataset":0.0319671914,"dev-research":0.2226335045,"prompt-eng":0.4013554941,"data-quality":0.3678554411,"ml-security":0.2214215612}}
{"text":"These results underscore the promising prospects of VF-Net as an effective and reliable method for point cloud reconstruction and analysis.","meta":{"url":"http://arxiv.org/abs/2307.10895v1"},"cats":{"new-dataset":0.0907942416,"dev-research":0.2345340484,"prompt-eng":0.3198678628,"data-quality":0.1520530389,"ml-security":0.0921797025}}
{"text":"Human motion generation aims to generate natural human pose sequences and shows immense potential for real-world applications.","meta":{"url":"http://arxiv.org/abs/2307.10894v1"},"cats":{"new-dataset":0.3316566755,"dev-research":0.2663891493,"prompt-eng":0.3864843709,"data-quality":0.0642359042,"ml-security":0.0631959273}}
{"text":"Substantial progress has been made recently in motion data collection technologies and generation methods, laying the foundation for increasing interest in human motion generation.","meta":{"url":"http://arxiv.org/abs/2307.10894v1"},"cats":{"new-dataset":0.6402457001,"dev-research":0.2351255121,"prompt-eng":0.391420564,"data-quality":0.0806645156,"ml-security":0.0309794045}}
{"text":"Most research within this field focuses on generating human motions based on conditional signals, such as text, audio, and scene contexts.","meta":{"url":"http://arxiv.org/abs/2307.10894v1"},"cats":{"new-dataset":0.1918763706,"dev-research":0.2498678167,"prompt-eng":0.4278594753,"data-quality":0.1001840709,"ml-security":0.044281106}}
{"text":"While significant advancements have been made in recent years, the task continues to pose challenges due to the intricate nature of human motion and its implicit relationship with conditional signals.","meta":{"url":"http://arxiv.org/abs/2307.10894v1"},"cats":{"new-dataset":0.0806242954,"dev-research":0.2861429819,"prompt-eng":0.4552196542,"data-quality":0.0965505428,"ml-security":0.0890413166}}
{"text":"In this survey, we present a comprehensive literature review of human motion generation, which, to the best of our knowledge, is the first of its kind in this field.","meta":{"url":"http://arxiv.org/abs/2307.10894v1"},"cats":{"new-dataset":0.3144593569,"dev-research":0.208117744,"prompt-eng":0.4110450484,"data-quality":0.0673588074,"ml-security":0.038624382}}
{"text":"We begin by introducing the background of human motion and generative models, followed by an examination of representative methods for three mainstream sub-tasks: text-conditioned, audio-conditioned, and scene-conditioned human motion generation.","meta":{"url":"http://arxiv.org/abs/2307.10894v1"},"cats":{"new-dataset":0.2239675439,"dev-research":0.1981587116,"prompt-eng":0.4439899195,"data-quality":0.1502962969,"ml-security":0.0434814548}}
{"text":"Additionally, we provide an overview of common datasets and evaluation metrics.","meta":{"url":"http://arxiv.org/abs/2307.10894v1"},"cats":{"new-dataset":0.7387709494,"dev-research":0.2519435156,"prompt-eng":0.4114743236,"data-quality":0.19030841,"ml-security":0.0676392289}}
{"text":"Lastly, we discuss open problems and outline potential future research directions.","meta":{"url":"http://arxiv.org/abs/2307.10894v1"},"cats":{"new-dataset":0.202523948,"dev-research":0.2820620497,"prompt-eng":0.3616166774,"data-quality":0.129065431,"ml-security":0.0949417228}}
{"text":"We hope that this survey could provide the community with a comprehensive glimpse of this rapidly evolving field and inspire novel ideas that address the outstanding challenges.","meta":{"url":"http://arxiv.org/abs/2307.10894v1"},"cats":{"new-dataset":0.3328628782,"dev-research":0.1873358304,"prompt-eng":0.4340517752,"data-quality":0.0863270502,"ml-security":0.0800691263}}
{"text":"The ability to learn polynomials and generalize out-of-distribution is essential for simulation metamodels in many disciplines of engineering, where the time step updates are described by polynomials.","meta":{"url":"http://arxiv.org/abs/2307.10892v1"},"cats":{"new-dataset":0.0421306472,"dev-research":0.242420338,"prompt-eng":0.4024814399,"data-quality":0.1095543135,"ml-security":0.1900703442}}
{"text":"While feed forward neural networks can fit any function, they cannot generalize out-of-distribution for higher-order polynomials.","meta":{"url":"http://arxiv.org/abs/2307.10892v1"},"cats":{"new-dataset":0.0096245962,"dev-research":0.1473264154,"prompt-eng":0.2960043053,"data-quality":0.1552387214,"ml-security":0.4251206329}}
{"text":"Therefore, this paper collects and proposes multiplicative neural network (MNN) architectures that are used as recursive building blocks for approximating higher-order polynomials.","meta":{"url":"http://arxiv.org/abs/2307.10892v1"},"cats":{"new-dataset":0.0414589332,"dev-research":0.1880991315,"prompt-eng":0.3304893089,"data-quality":0.0869461516,"ml-security":0.2169399792}}
{"text":"Our experiments show that MNNs are better than baseline models at generalizing, and their performance in validation is true to their performance in out-of-distribution tests.","meta":{"url":"http://arxiv.org/abs/2307.10892v1"},"cats":{"new-dataset":0.0341620811,"dev-research":0.2175643043,"prompt-eng":0.4168588559,"data-quality":0.275795827,"ml-security":0.1654331888}}
{"text":"In addition to MNN architectures, a simulation metamodeling approach is proposed for simulations with polynomial time step updates.","meta":{"url":"http://arxiv.org/abs/2307.10892v1"},"cats":{"new-dataset":0.0332427314,"dev-research":0.2536388644,"prompt-eng":0.3911460823,"data-quality":0.0814469263,"ml-security":0.0875665023}}
{"text":"For these simulations, simulating a time interval can be performed in fewer steps by increasing the step size, which entails approximating higher-order polynomials.","meta":{"url":"http://arxiv.org/abs/2307.10892v1"},"cats":{"new-dataset":0.0261720051,"dev-research":0.1964674419,"prompt-eng":0.3635406816,"data-quality":0.0626985803,"ml-security":0.0988045643}}
{"text":"While our approach is compatible with any simulation with polynomial time step updates, a demonstration is shown for an epidemiology simulation model, which also shows the inductive bias in MNNs for learning and generalizing higher-order polynomials.","meta":{"url":"http://arxiv.org/abs/2307.10892v1"},"cats":{"new-dataset":0.0609864167,"dev-research":0.1764103366,"prompt-eng":0.4040716504,"data-quality":0.1252380402,"ml-security":0.3161249682}}
{"text":"Abstraction is a key verification technique to improve scalability.","meta":{"url":"http://arxiv.org/abs/2307.10891v1"},"cats":{"new-dataset":0.0061669974,"dev-research":0.4786024742,"prompt-eng":0.3865612031,"data-quality":0.1354881076,"ml-security":0.1311550639}}
{"text":"However, its use for neural networks is so far extremely limited.","meta":{"url":"http://arxiv.org/abs/2307.10891v1"},"cats":{"new-dataset":0.0116810853,"dev-research":0.2253012196,"prompt-eng":0.2589889999,"data-quality":0.1165817595,"ml-security":0.2274825625}}
{"text":"Previous approaches for abstracting classification networks replace several neurons with one of them that is similar enough.","meta":{"url":"http://arxiv.org/abs/2307.10891v1"},"cats":{"new-dataset":0.0377862853,"dev-research":0.2559165009,"prompt-eng":0.3444249691,"data-quality":0.2862317645,"ml-security":0.2039943343}}
{"text":"We can classify the similarity as defined either syntactically (using quantities on the connections between neurons) or semantically (on the activation values of neurons for various inputs).","meta":{"url":"http://arxiv.org/abs/2307.10891v1"},"cats":{"new-dataset":0.0181460257,"dev-research":0.2771060838,"prompt-eng":0.3837988748,"data-quality":0.2438277408,"ml-security":0.113277474}}
{"text":"Unfortunately, the previous approaches only achieve moderate reductions, when implemented at all.","meta":{"url":"http://arxiv.org/abs/2307.10891v1"},"cats":{"new-dataset":0.0079761274,"dev-research":0.2145167508,"prompt-eng":0.3780974262,"data-quality":0.156961725,"ml-security":0.0878634392}}
{"text":"In this work, we provide a more flexible framework where a neuron can be replaced with a linear combination of other neurons, improving the reduction.","meta":{"url":"http://arxiv.org/abs/2307.10891v1"},"cats":{"new-dataset":0.0591124748,"dev-research":0.211675241,"prompt-eng":0.388022219,"data-quality":0.1385023738,"ml-security":0.1562601706}}
{"text":"We apply this approach both on syntactic and semantic abstractions, and implement and evaluate them experimentally.","meta":{"url":"http://arxiv.org/abs/2307.10891v1"},"cats":{"new-dataset":0.0296342532,"dev-research":0.3771907447,"prompt-eng":0.4687136078,"data-quality":0.3148707424,"ml-security":0.0741523879}}
{"text":"Further, we introduce a refinement method for our abstractions, allowing for finding a better balance between reduction and precision.","meta":{"url":"http://arxiv.org/abs/2307.10891v1"},"cats":{"new-dataset":0.0317874164,"dev-research":0.394138617,"prompt-eng":0.444979939,"data-quality":0.2329692983,"ml-security":0.0931348518}}
{"text":"The problem of matching markets has been studied for a long time in the literature due to its wide range of applications.","meta":{"url":"http://arxiv.org/abs/2307.10890v1"},"cats":{"new-dataset":0.0334871439,"dev-research":0.1569487711,"prompt-eng":0.3464097484,"data-quality":0.1200368683,"ml-security":0.1059086981}}
{"text":"Finding a stable matching is a common equilibrium objective in this problem.","meta":{"url":"http://arxiv.org/abs/2307.10890v1"},"cats":{"new-dataset":0.0253078993,"dev-research":0.1520242982,"prompt-eng":0.3963152017,"data-quality":0.1302219227,"ml-security":0.0908731761}}
{"text":"Since market participants are usually uncertain of their preferences, a rich line of recent works study the online setting where one-side participants (players) learn their unknown preferences from iterative interactions with the other side (arms).","meta":{"url":"http://arxiv.org/abs/2307.10890v1"},"cats":{"new-dataset":0.0394888408,"dev-research":0.269614226,"prompt-eng":0.3931249728,"data-quality":0.1121783419,"ml-security":0.2065123853}}
{"text":"Most previous works in this line are only able to derive theoretical guarantees for player-pessimal stable regret, which is defined compared with the players' least-preferred stable matching.","meta":{"url":"http://arxiv.org/abs/2307.10890v1"},"cats":{"new-dataset":0.0112729889,"dev-research":0.1819386758,"prompt-eng":0.3695258929,"data-quality":0.1323927598,"ml-security":0.1617727866}}
{"text":"However, under the pessimal stable matching, players only obtain the least reward among all stable matchings.","meta":{"url":"http://arxiv.org/abs/2307.10890v1"},"cats":{"new-dataset":0.0060157186,"dev-research":0.162257464,"prompt-eng":0.3517481281,"data-quality":0.1250283697,"ml-security":0.1165749994}}
{"text":"To maximize players' profits, player-optimal stable matching would be the most desirable.","meta":{"url":"http://arxiv.org/abs/2307.10890v1"},"cats":{"new-dataset":0.0082419039,"dev-research":0.2422156096,"prompt-eng":0.390061453,"data-quality":0.0750350478,"ml-security":0.1048536072}}
{"text":"Though \\citet{basu21beyond} successfully bring an upper bound for player-optimal stable regret, their result can be exponentially large if players' preference gap is small.","meta":{"url":"http://arxiv.org/abs/2307.10890v1"},"cats":{"new-dataset":0.020540967,"dev-research":0.2416735535,"prompt-eng":0.3598516036,"data-quality":0.1112626902,"ml-security":0.1020034333}}
{"text":"Whether a polynomial guarantee for this regret exists is a significant but still open problem.","meta":{"url":"http://arxiv.org/abs/2307.10890v1"},"cats":{"new-dataset":0.011376404,"dev-research":0.2321875785,"prompt-eng":0.3584111988,"data-quality":0.1322667057,"ml-security":0.2065608189}}
{"text":"In this work, we provide a new algorithm named explore-then-Gale-Shapley (ETGS) and show that the optimal stable regret of each player can be upper bounded by $O(K\\log T/\\Delta^2)$ where $K$ is the number of arms, $T$ is the horizon and $\\Delta$ is the players' minimum preference gap among the first $N+1$-ranked arms.","meta":{"url":"http://arxiv.org/abs/2307.10890v1"},"cats":{"new-dataset":0.0443864939,"dev-research":0.1748075392,"prompt-eng":0.3871093377,"data-quality":0.0817586965,"ml-security":0.1825409712}}
{"text":"This result significantly improves previous works which either have a weaker player-pessimal stable matching objective or apply only to markets with special assumptions.","meta":{"url":"http://arxiv.org/abs/2307.10890v1"},"cats":{"new-dataset":0.0266006778,"dev-research":0.1729371972,"prompt-eng":0.3655288541,"data-quality":0.1976576894,"ml-security":0.1168065039}}
{"text":"When the preferences of participants satisfy some special conditions, our regret upper bound also matches the previously derived lower bound.","meta":{"url":"http://arxiv.org/abs/2307.10890v1"},"cats":{"new-dataset":0.0096322694,"dev-research":0.2248222129,"prompt-eng":0.408068947,"data-quality":0.0964537979,"ml-security":0.1643413978}}
{"text":"In multi-agent system design, a crucial aspect is to ensure robustness, meaning that for a coalition of agents A, small violations of adversarial assumptions only lead to small violations of A's goals.","meta":{"url":"http://arxiv.org/abs/2307.10885v1"},"cats":{"new-dataset":0.007519737,"dev-research":0.2995210209,"prompt-eng":0.3753133533,"data-quality":0.1433376172,"ml-security":0.4365975346}}
{"text":"In this paper we introduce a logical framework for robust strategic reasoning about multi-agent systems.","meta":{"url":"http://arxiv.org/abs/2307.10885v1"},"cats":{"new-dataset":0.0345101443,"dev-research":0.3193201817,"prompt-eng":0.4339582133,"data-quality":0.1162766558,"ml-security":0.167283096}}
{"text":"Specifically, inspired by recent works on robust temporal logics, we introduce and study rATL and rATL*, logics that extend the well-known Alternating-time Temporal Logic ATL and ATL* by means of an opportune multi-valued semantics for the strategy quantifiers and temporal operators.","meta":{"url":"http://arxiv.org/abs/2307.10885v1"},"cats":{"new-dataset":0.0684734783,"dev-research":0.3245886439,"prompt-eng":0.4251362534,"data-quality":0.1391945605,"ml-security":0.1296199563}}
{"text":"We study the model-checking and satisfiability problems for rATL and rATL* and show that dealing with robustness comes at no additional computational cost.","meta":{"url":"http://arxiv.org/abs/2307.10885v1"},"cats":{"new-dataset":0.0399390594,"dev-research":0.2913590899,"prompt-eng":0.4793706382,"data-quality":0.2391139829,"ml-security":0.2078618557}}
{"text":"Indeed, we show that these problems are PTime-complete and ExpTime-complete for rATL, respectively, while both are 2ExpTime-complete for rATL*.","meta":{"url":"http://arxiv.org/abs/2307.10885v1"},"cats":{"new-dataset":0.2137983616,"dev-research":0.2819036057,"prompt-eng":0.4513925377,"data-quality":0.2042760102,"ml-security":0.073596587}}
{"text":"Trajectory and control secrecy is an important issue in robotics security.","meta":{"url":"http://arxiv.org/abs/2307.10883v1"},"cats":{"new-dataset":0.0229343128,"dev-research":0.2239841976,"prompt-eng":0.3674468257,"data-quality":0.0695502752,"ml-security":0.4696940216}}
{"text":"This paper proposes a novel algorithm for the control input inference of a mobile agent without knowing its control objective.","meta":{"url":"http://arxiv.org/abs/2307.10883v1"},"cats":{"new-dataset":0.0353537257,"dev-research":0.2793416672,"prompt-eng":0.4745247743,"data-quality":0.1081514559,"ml-security":0.2611007247}}
{"text":"Specifically, the algorithm first estimates the target state by applying external perturbations.","meta":{"url":"http://arxiv.org/abs/2307.10883v1"},"cats":{"new-dataset":0.0098070287,"dev-research":0.2094503241,"prompt-eng":0.3885677608,"data-quality":0.1319213004,"ml-security":0.1481428192}}
{"text":"Then we identify the objective function based on the inverse optimal control, providing the well-posedness proof and the identifiability analysis.","meta":{"url":"http://arxiv.org/abs/2307.10883v1"},"cats":{"new-dataset":0.0270028169,"dev-research":0.2179705605,"prompt-eng":0.4262492189,"data-quality":0.1222632135,"ml-security":0.1715964449}}
{"text":"Next, we obtain the optimal estimate of the control horizon using binary search.","meta":{"url":"http://arxiv.org/abs/2307.10883v1"},"cats":{"new-dataset":0.0506090196,"dev-research":0.1421225143,"prompt-eng":0.4269593882,"data-quality":0.099378249,"ml-security":0.1393916992}}
{"text":"Finally, the agent's control optimization problem is reconstructed and solved to predict its input.","meta":{"url":"http://arxiv.org/abs/2307.10883v1"},"cats":{"new-dataset":0.0809666546,"dev-research":0.2463171405,"prompt-eng":0.4568867322,"data-quality":0.1281033152,"ml-security":0.1948191886}}
{"text":"Simulation illustrates the efficiency and the performance of the algorithm.","meta":{"url":"http://arxiv.org/abs/2307.10883v1"},"cats":{"new-dataset":0.0198604534,"dev-research":0.2727222945,"prompt-eng":0.4048714462,"data-quality":0.1099167372,"ml-security":0.0490428287}}
{"text":"Encrypted mempools are a class of solutions aimed at preventing or reducing negative externalities of MEV extraction using cryptographic privacy.","meta":{"url":"http://arxiv.org/abs/2307.10878v1"},"cats":{"new-dataset":0.1117611616,"dev-research":0.2381644994,"prompt-eng":0.3891577753,"data-quality":0.1572190902,"ml-security":0.3458227258}}
{"text":"Mempool encryption aims to hide information related to pending transactions until a block including the transactions is committed, targeting the prevention of frontrunning and similar behaviour.","meta":{"url":"http://arxiv.org/abs/2307.10878v1"},"cats":{"new-dataset":0.0400774198,"dev-research":0.2489025803,"prompt-eng":0.3845086575,"data-quality":0.1196218492,"ml-security":0.3013057852}}
{"text":"Among the various methods of encryption, threshold schemes are particularly interesting for the design of MEV mitigation mechanisms, as their distributed nature and minimal hardware requirements harmonize with a broader goal of decentralization.   ","meta":{"url":"http://arxiv.org/abs/2307.10878v1"},"cats":{"new-dataset":0.0404336184,"dev-research":0.2270290416,"prompt-eng":0.4053327352,"data-quality":0.0871333036,"ml-security":0.3410632443}}
{"text":"This work looks beyond the formal and technical cryptographic aspects of threshold encryption schemes to focus on the market and incentive implications of implementing encrypted mempools as MEV mitigation techniques.","meta":{"url":"http://arxiv.org/abs/2307.10878v1"},"cats":{"new-dataset":0.0783749445,"dev-research":0.2002627262,"prompt-eng":0.4284635943,"data-quality":0.1366886138,"ml-security":0.4135649218}}
{"text":"In particular, this paper argues that the deployment of such protocols without proper consideration and understanding of market impact invites several undesired outcomes, with the ultimate goal of stimulating further analysis of this class of solutions outside of pure cryptograhic considerations.","meta":{"url":"http://arxiv.org/abs/2307.10878v1"},"cats":{"new-dataset":0.0767241039,"dev-research":0.2238365598,"prompt-eng":0.3763254111,"data-quality":0.1292310232,"ml-security":0.3211793287}}
{"text":"Included in the paper is an overview of a series of problems, various candidate solutions in the form of mempool encryption techniques with a focus on threshold encryption, potential drawbacks to these solutions, and Osmosis as a case study.","meta":{"url":"http://arxiv.org/abs/2307.10878v1"},"cats":{"new-dataset":0.0882463446,"dev-research":0.1572848007,"prompt-eng":0.4110689505,"data-quality":0.1230347871,"ml-security":0.3479189641}}
{"text":"The paper targets a broad audience and remains agnostic to blockchain design where possible while drawing from mostly financial examples.","meta":{"url":"http://arxiv.org/abs/2307.10878v1"},"cats":{"new-dataset":0.0371724199,"dev-research":0.2265711462,"prompt-eng":0.3178147965,"data-quality":0.1050659511,"ml-security":0.133531512}}
{"text":"Industry standard frameworks are now widespread for labeling the high-level stages and granular actions of attacker and defender behavior in cyberspace.","meta":{"url":"http://arxiv.org/abs/2307.10877v1"},"cats":{"new-dataset":0.1010746675,"dev-research":0.3302752281,"prompt-eng":0.4189882582,"data-quality":0.2439925972,"ml-security":0.5317589598}}
{"text":"While these labels are used for atomic actions, and to some extent for sequences of actions, there remains a need for labeled data from realistic full-scale attacks.","meta":{"url":"http://arxiv.org/abs/2307.10877v1"},"cats":{"new-dataset":0.1472946269,"dev-research":0.2931798727,"prompt-eng":0.4059315773,"data-quality":0.4157723055,"ml-security":0.3800199187}}
{"text":"This data is valuable for better understanding human actors' decisions, behaviors, and individual attributes.","meta":{"url":"http://arxiv.org/abs/2307.10877v1"},"cats":{"new-dataset":0.4415122853,"dev-research":0.2463541013,"prompt-eng":0.3549173478,"data-quality":0.1037279149,"ml-security":0.1215226642}}
{"text":"The analysis could lead to more effective attribution and disruption of attackers.   ","meta":{"url":"http://arxiv.org/abs/2307.10877v1"},"cats":{"new-dataset":0.0204043024,"dev-research":0.4025253836,"prompt-eng":0.3835083525,"data-quality":0.1932270395,"ml-security":0.7336222338}}
{"text":"We present a methodological approach and exploratory case study for systematically analyzing human behavior during a cyber offense/defense capture-the-flag (CTF) game.","meta":{"url":"http://arxiv.org/abs/2307.10877v1"},"cats":{"new-dataset":0.287024031,"dev-research":0.4290135901,"prompt-eng":0.4363101903,"data-quality":0.1485481297,"ml-security":0.3735800828}}
{"text":"We describe the data collection and analysis to derive a metric called keystroke accuracy.","meta":{"url":"http://arxiv.org/abs/2307.10877v1"},"cats":{"new-dataset":0.2091826061,"dev-research":0.352143308,"prompt-eng":0.4654083969,"data-quality":0.2859881361,"ml-security":0.1100944938}}
{"text":"After collecting players' commands, we label them using the MITRE ATT&CK framework using a new tool called Pathfinder.","meta":{"url":"http://arxiv.org/abs/2307.10877v1"},"cats":{"new-dataset":0.3179778694,"dev-research":0.2922964562,"prompt-eng":0.5406384792,"data-quality":0.2567381463,"ml-security":0.0603898022}}
{"text":"We present results from preliminary analysis of participants' keystroke accuracy and its relation to score outcome in CTF games.","meta":{"url":"http://arxiv.org/abs/2307.10877v1"},"cats":{"new-dataset":0.1116464112,"dev-research":0.3535399344,"prompt-eng":0.4719305961,"data-quality":0.2173250313,"ml-security":0.121805212}}
{"text":"We describe frequency of action classification within the MITRE ATT&CK framework and discuss some of the mathematical trends suggested by our observations.","meta":{"url":"http://arxiv.org/abs/2307.10877v1"},"cats":{"new-dataset":0.1802474585,"dev-research":0.1865648078,"prompt-eng":0.4128145818,"data-quality":0.1852010854,"ml-security":0.1182619736}}
{"text":"We conclude with a discussion of extensions for the methodology, including performance evaluation during games and the potential use of this methodology for training artificial intelligence.","meta":{"url":"http://arxiv.org/abs/2307.10877v1"},"cats":{"new-dataset":0.0401682448,"dev-research":0.3322364715,"prompt-eng":0.4156721113,"data-quality":0.1478197674,"ml-security":0.2040327867}}
{"text":"The popularity of point cloud deep models for safety-critical purposes has increased, but the reliability and security of these models can be compromised by intentional or naturally occurring point cloud noise.","meta":{"url":"http://arxiv.org/abs/2307.10875v1"},"cats":{"new-dataset":0.105653089,"dev-research":0.2251417632,"prompt-eng":0.3976374697,"data-quality":0.2868678763,"ml-security":0.4804388799}}
{"text":"To combat this issue, we present a novel point cloud outlier removal method called PointCVaR, which empowers standard-trained models to eliminate additional outliers and restore the data.","meta":{"url":"http://arxiv.org/abs/2307.10875v1"},"cats":{"new-dataset":0.2837078852,"dev-research":0.2572553717,"prompt-eng":0.3728081679,"data-quality":0.3522345585,"ml-security":0.3351812507}}
{"text":"Our approach begins by conducting attribution analysis to determine the influence of each point on the model output, which we refer to as point risk.","meta":{"url":"http://arxiv.org/abs/2307.10875v1"},"cats":{"new-dataset":0.0386883166,"dev-research":0.219954465,"prompt-eng":0.4535189268,"data-quality":0.2407796698,"ml-security":0.24800677}}
{"text":"We then optimize the process of filtering high-risk points using Conditional Value at Risk (CVaR) as the objective.","meta":{"url":"http://arxiv.org/abs/2307.10875v1"},"cats":{"new-dataset":0.034036782,"dev-research":0.252721695,"prompt-eng":0.4452818267,"data-quality":0.1776628826,"ml-security":0.2173432487}}
{"text":"The rationale for this approach is based on the observation that noise points in point clouds tend to cluster in the tail of the risk distribution, with a low frequency but a high level of risk, resulting in significant interference with classification results.","meta":{"url":"http://arxiv.org/abs/2307.10875v1"},"cats":{"new-dataset":0.0111187497,"dev-research":0.240614525,"prompt-eng":0.3572448686,"data-quality":0.3671977818,"ml-security":0.4285426114}}
{"text":"Despite requiring no additional training effort, our method produces exceptional results in various removal-and-classification experiments for noisy point clouds, which are corrupted by random noise, adversarial noise, and backdoor trigger noise.","meta":{"url":"http://arxiv.org/abs/2307.10875v1"},"cats":{"new-dataset":0.1508126323,"dev-research":0.1986166592,"prompt-eng":0.3774947782,"data-quality":0.472269378,"ml-security":0.4791901871}}
{"text":"Impressively, it achieves 87% accuracy in defense against the backdoor attack by removing triggers.","meta":{"url":"http://arxiv.org/abs/2307.10875v1"},"cats":{"new-dataset":0.0139457492,"dev-research":0.3405794275,"prompt-eng":0.45833794,"data-quality":0.2032214179,"ml-security":0.3786562415}}
{"text":"Overall, the proposed PointCVaR effectively eliminates noise points and enhances point cloud classification, making it a promising plug-in module for various models in different scenarios.","meta":{"url":"http://arxiv.org/abs/2307.10875v1"},"cats":{"new-dataset":0.192644778,"dev-research":0.2756515025,"prompt-eng":0.4373859454,"data-quality":0.2413779507,"ml-security":0.145856631}}
{"text":"Having efficient testing strategies is a core challenge that needs to be overcome for the release of automated driving.","meta":{"url":"http://arxiv.org/abs/2307.10873v1"},"cats":{"new-dataset":0.0283874577,"dev-research":0.4156566321,"prompt-eng":0.4834402707,"data-quality":0.1438182108,"ml-security":0.2160806891}}
{"text":"This necessitates clear requirements as well as suitable methods for testing.","meta":{"url":"http://arxiv.org/abs/2307.10873v1"},"cats":{"new-dataset":0.0270672343,"dev-research":0.3485604938,"prompt-eng":0.5080796217,"data-quality":0.1872207787,"ml-security":0.1157750225}}
{"text":"In this work, the requirements for perception modules are considered with respect to relevance.","meta":{"url":"http://arxiv.org/abs/2307.10873v1"},"cats":{"new-dataset":0.0539829929,"dev-research":0.2277811323,"prompt-eng":0.4456158669,"data-quality":0.2246374826,"ml-security":0.053957903}}
{"text":"The concept of relevance currently remains insufficiently defined and specified.","meta":{"url":"http://arxiv.org/abs/2307.10873v1"},"cats":{"new-dataset":0.0185710199,"dev-research":0.2822574,"prompt-eng":0.2870337271,"data-quality":0.3254038638,"ml-security":0.0943003902}}
{"text":"In this paper, we propose a novel methodology to overcome this challenge by exemplary application to collision safety in the highway domain.","meta":{"url":"http://arxiv.org/abs/2307.10873v1"},"cats":{"new-dataset":0.1792092349,"dev-research":0.29374485,"prompt-eng":0.3760817963,"data-quality":0.1810304205,"ml-security":0.2572955064}}
{"text":"Using this general system and use case specification, a corresponding concept for relevance is derived.","meta":{"url":"http://arxiv.org/abs/2307.10873v1"},"cats":{"new-dataset":0.0162734263,"dev-research":0.3410545458,"prompt-eng":0.466219878,"data-quality":0.1690391664,"ml-security":0.0974311958}}
{"text":"Irrelevant objects are thus defined as objects which do not limit the set of safe actions available to the ego vehicle under consideration of all uncertainties.","meta":{"url":"http://arxiv.org/abs/2307.10873v1"},"cats":{"new-dataset":0.0387298769,"dev-research":0.2780086346,"prompt-eng":0.3655367109,"data-quality":0.2008606813,"ml-security":0.2068854389}}
{"text":"As an initial step, the use case is decomposed into functional scenarios with respect to collision relevance.","meta":{"url":"http://arxiv.org/abs/2307.10873v1"},"cats":{"new-dataset":0.0149169951,"dev-research":0.3700435001,"prompt-eng":0.424823906,"data-quality":0.1226471641,"ml-security":0.1686703658}}
{"text":"For each functional scenario, possible actions of both the ego vehicle and any other dynamic object are formalized as equations.","meta":{"url":"http://arxiv.org/abs/2307.10873v1"},"cats":{"new-dataset":0.0199540672,"dev-research":0.2523873534,"prompt-eng":0.4189018158,"data-quality":0.0934036753,"ml-security":0.1111228251}}
{"text":"This set of possible actions is constrained by traffic rules, yielding relevance criteria.","meta":{"url":"http://arxiv.org/abs/2307.10873v1"},"cats":{"new-dataset":0.0462211693,"dev-research":0.2162714449,"prompt-eng":0.4006729201,"data-quality":0.135478631,"ml-security":0.159362542}}
{"text":"As a result, we present a conservative estimation which dynamic objects are relevant for perception and need to be considered for a complete evaluation.","meta":{"url":"http://arxiv.org/abs/2307.10873v1"},"cats":{"new-dataset":0.0953797876,"dev-research":0.1878893164,"prompt-eng":0.4597194738,"data-quality":0.1748490101,"ml-security":0.0917800159}}
{"text":"The estimation provides requirements which are applicable for offline testing and validation of perception components.","meta":{"url":"http://arxiv.org/abs/2307.10873v1"},"cats":{"new-dataset":0.0537960989,"dev-research":0.2441367968,"prompt-eng":0.4820183579,"data-quality":0.1794316445,"ml-security":0.0653854652}}
{"text":"A visualization is presented for examples from the highD dataset, showing the plausibility of the results.","meta":{"url":"http://arxiv.org/abs/2307.10873v1"},"cats":{"new-dataset":0.6698813011,"dev-research":0.2483951475,"prompt-eng":0.3929671921,"data-quality":0.2004656062,"ml-security":0.1102548261}}
{"text":"Finally, a possibility for a future validation of the presented relevance concept is outlined.","meta":{"url":"http://arxiv.org/abs/2307.10873v1"},"cats":{"new-dataset":0.024911657,"dev-research":0.2751171173,"prompt-eng":0.4394307675,"data-quality":0.2616773303,"ml-security":0.0843342617}}
{"text":"Performance issues permeate large-scale cloud service systems, which can lead to huge revenue losses.","meta":{"url":"http://arxiv.org/abs/2307.10869v1"},"cats":{"new-dataset":0.0165632884,"dev-research":0.2984714313,"prompt-eng":0.3375191309,"data-quality":0.1826882082,"ml-security":0.2812399738}}
{"text":"To ensure reliable performance, it's essential to accurately identify and localize these issues using service monitoring metrics.","meta":{"url":"http://arxiv.org/abs/2307.10869v1"},"cats":{"new-dataset":0.0303616255,"dev-research":0.4045672187,"prompt-eng":0.4581221208,"data-quality":0.4264192292,"ml-security":0.1127581098}}
{"text":"Given the complexity and scale of modern cloud systems, this task can be challenging and may require extensive expertise and resources beyond the capacity of individual humans.","meta":{"url":"http://arxiv.org/abs/2307.10869v1"},"cats":{"new-dataset":0.184074968,"dev-research":0.2474846494,"prompt-eng":0.4033040792,"data-quality":0.0756031885,"ml-security":0.1020024189}}
{"text":"Some existing methods tackle this problem by analyzing each metric independently to detect anomalies.","meta":{"url":"http://arxiv.org/abs/2307.10869v1"},"cats":{"new-dataset":0.077923818,"dev-research":0.2582233881,"prompt-eng":0.4411713801,"data-quality":0.3973586825,"ml-security":0.1624354224}}
{"text":"However, this could incur overwhelming alert storms that are difficult for engineers to diagnose manually.","meta":{"url":"http://arxiv.org/abs/2307.10869v1"},"cats":{"new-dataset":0.0229721293,"dev-research":0.4448890691,"prompt-eng":0.5045293549,"data-quality":0.2872827186,"ml-security":0.2811638683}}
{"text":"To pursue better performance, not only the temporal patterns of metrics but also the correlation between metrics (i.e., relational patterns) should be considered, which can be formulated as a multivariate metrics anomaly detection problem.","meta":{"url":"http://arxiv.org/abs/2307.10869v1"},"cats":{"new-dataset":0.0429018181,"dev-research":0.3363309679,"prompt-eng":0.3751501234,"data-quality":0.2025387385,"ml-security":0.1418651266}}
{"text":"However, most of the studies fall short of extracting these two types of features explicitly.","meta":{"url":"http://arxiv.org/abs/2307.10869v1"},"cats":{"new-dataset":0.0332928093,"dev-research":0.2442919606,"prompt-eng":0.3391684415,"data-quality":0.2463797094,"ml-security":0.1017921187}}
{"text":"Moreover, there exist some unlabeled anomalies mixed in the training data, which may hinder the detection performance.","meta":{"url":"http://arxiv.org/abs/2307.10869v1"},"cats":{"new-dataset":0.036607559,"dev-research":0.2493552774,"prompt-eng":0.3923294464,"data-quality":0.5747256998,"ml-security":0.3791248425}}
{"text":"To address these limitations, we propose the Relational- Temporal Anomaly Detection Model (RTAnomaly) that combines the relational and temporal information of metrics.","meta":{"url":"http://arxiv.org/abs/2307.10869v1"},"cats":{"new-dataset":0.1462953003,"dev-research":0.2950150527,"prompt-eng":0.3773010429,"data-quality":0.2635657763,"ml-security":0.3333803489}}
{"text":"RTAnomaly employs a graph attention layer to learn the dependencies among metrics, which will further help pinpoint the anomalous metrics that may cause the anomaly effectively.","meta":{"url":"http://arxiv.org/abs/2307.10869v1"},"cats":{"new-dataset":0.1609444528,"dev-research":0.3113149412,"prompt-eng":0.3865113183,"data-quality":0.3219572663,"ml-security":0.2122950425}}
{"text":"In addition, we exploit the concept of positive unlabeled learning to address the issue of potential anomalies in the training data.","meta":{"url":"http://arxiv.org/abs/2307.10869v1"},"cats":{"new-dataset":0.0767597285,"dev-research":0.2402636772,"prompt-eng":0.4245771717,"data-quality":0.5971252727,"ml-security":0.350008014}}
{"text":"To evaluate our method, we conduct experiments on a public dataset and two industrial datasets.","meta":{"url":"http://arxiv.org/abs/2307.10869v1"},"cats":{"new-dataset":0.5398811093,"dev-research":0.2227949123,"prompt-eng":0.39425158,"data-quality":0.232892739,"ml-security":0.1462381211}}
{"text":"RTAnomaly outperforms all the baseline models by achieving an average F1 score of 0.929 and Hit@3 of 0.920, demonstrating its superiority.","meta":{"url":"http://arxiv.org/abs/2307.10869v1"},"cats":{"new-dataset":0.0761448916,"dev-research":0.2327481514,"prompt-eng":0.4112261459,"data-quality":0.2225951036,"ml-security":0.1373376254}}
{"text":"Captions are crucial for understanding scientific visualizations and documents.","meta":{"url":"http://arxiv.org/abs/2307.10867v1"},"cats":{"new-dataset":0.0994082601,"dev-research":0.3189991837,"prompt-eng":0.3285239158,"data-quality":0.2598875512,"ml-security":0.0555703696}}
{"text":"Existing captioning methods for scientific figures rely on figure-caption pairs extracted from documents for training, many of which fall short with respect to metrics like helpfulness, explainability, and visual-descriptiveness [15] leading to generated captions being misaligned with reader preferences.","meta":{"url":"http://arxiv.org/abs/2307.10867v1"},"cats":{"new-dataset":0.1781054928,"dev-research":0.2689602992,"prompt-eng":0.397330175,"data-quality":0.4028111816,"ml-security":0.0613352941}}
{"text":"To enable the generation of high-quality figure captions, we introduce FigCaps-HF a new framework for figure-caption generation that can incorporate domain expert feedback in generating captions optimized for reader preferences.","meta":{"url":"http://arxiv.org/abs/2307.10867v1"},"cats":{"new-dataset":0.3345093062,"dev-research":0.3213155026,"prompt-eng":0.473599678,"data-quality":0.3012523051,"ml-security":0.0637294423}}
{"text":"Our framework comprises of 1) an automatic method for evaluating quality of figure-caption pairs, 2) a novel reinforcement learning with human feedback (RLHF) method to optimize a generative figure-to-caption model for reader preferences.","meta":{"url":"http://arxiv.org/abs/2307.10867v1"},"cats":{"new-dataset":0.1868656709,"dev-research":0.2396424618,"prompt-eng":0.4746977207,"data-quality":0.2448573816,"ml-security":0.0570620445}}
{"text":"We demonstrate the effectiveness of our simple learning framework by improving performance over standard fine-tuning across different types of models.","meta":{"url":"http://arxiv.org/abs/2307.10867v1"},"cats":{"new-dataset":0.0467041365,"dev-research":0.2626322376,"prompt-eng":0.4451189198,"data-quality":0.2892012278,"ml-security":0.2030289484}}
{"text":"In particular, when using BLIP as the base model, our RLHF framework achieves a mean gain of 35.7%, 16.9%, and 9% in ROUGE, BLEU, and Meteor, respectively.","meta":{"url":"http://arxiv.org/abs/2307.10867v1"},"cats":{"new-dataset":0.0674349351,"dev-research":0.1744419084,"prompt-eng":0.4027889522,"data-quality":0.1200047587,"ml-security":0.0963558682}}
{"text":"Finally, we release a large-scale benchmark dataset with human feedback on figure-caption pairs to enable further evaluation and development of RLHF techniques for this problem.","meta":{"url":"http://arxiv.org/abs/2307.10867v1"},"cats":{"new-dataset":0.7250645376,"dev-research":0.2068045462,"prompt-eng":0.4186888846,"data-quality":0.2779068343,"ml-security":0.0502654364}}
{"text":"Neural Persistence is a prominent measure for quantifying neural network complexity, proposed in the emerging field of topological data analysis in deep learning.","meta":{"url":"http://arxiv.org/abs/2307.10865v1"},"cats":{"new-dataset":0.0811748835,"dev-research":0.2698155933,"prompt-eng":0.2784133996,"data-quality":0.1571858781,"ml-security":0.208438888}}
{"text":"In this work, however, we find both theoretically and empirically that the variance of network weights and spatial concentration of large weights are the main factors that impact neural persistence.","meta":{"url":"http://arxiv.org/abs/2307.10865v1"},"cats":{"new-dataset":0.0282032542,"dev-research":0.1756538098,"prompt-eng":0.3308876659,"data-quality":0.2173258797,"ml-security":0.281797169}}
{"text":"Whilst this captures useful information for linear classifiers, we find that no relevant spatial structure is present in later layers of deep neural networks, making neural persistence roughly equivalent to the variance of weights.","meta":{"url":"http://arxiv.org/abs/2307.10865v1"},"cats":{"new-dataset":0.0880757663,"dev-research":0.2038370559,"prompt-eng":0.3225313488,"data-quality":0.2910704793,"ml-security":0.3567774833}}
{"text":"Additionally, the proposed averaging procedure across layers for deep neural networks does not consider interaction between layers.","meta":{"url":"http://arxiv.org/abs/2307.10865v1"},"cats":{"new-dataset":0.0090420805,"dev-research":0.198497436,"prompt-eng":0.3219026427,"data-quality":0.2007288181,"ml-security":0.2099555732}}
{"text":"Based on our analysis, we propose an extension of the filtration underlying neural persistence to the whole neural network instead of single layers, which is equivalent to calculating neural persistence on one particular matrix.","meta":{"url":"http://arxiv.org/abs/2307.10865v1"},"cats":{"new-dataset":0.0633774518,"dev-research":0.1865404034,"prompt-eng":0.3459643221,"data-quality":0.1667829095,"ml-security":0.2444350948}}
{"text":"This yields our deep graph persistence measure, which implicitly incorporates persistent paths through the network and alleviates variance-related issues through standardisation.","meta":{"url":"http://arxiv.org/abs/2307.10865v1"},"cats":{"new-dataset":0.1401247928,"dev-research":0.2648937292,"prompt-eng":0.3672941709,"data-quality":0.2853907908,"ml-security":0.1465731485}}
{"text":"Code is available at https://github.com/ExplainableML/Deep-Graph-Persistence .","meta":{"url":"http://arxiv.org/abs/2307.10865v1"},"cats":{"new-dataset":0.2299301538,"dev-research":0.2788035883,"prompt-eng":0.3980631496,"data-quality":0.1768296377,"ml-security":0.0813892832}}
{"text":"Emerging large-scale text-to-image generative models, e.g., Stable Diffusion (SD), have exhibited overwhelming results with high fidelity.","meta":{"url":"http://arxiv.org/abs/2307.10864v1"},"cats":{"new-dataset":0.1132293102,"dev-research":0.1564789934,"prompt-eng":0.4049347571,"data-quality":0.1877633668,"ml-security":0.0907177526}}
{"text":"Despite the magnificent progress, current state-of-the-art models still struggle to generate images fully adhering to the input prompt.","meta":{"url":"http://arxiv.org/abs/2307.10864v1"},"cats":{"new-dataset":0.2233439685,"dev-research":0.2013158611,"prompt-eng":0.5648951112,"data-quality":0.1567557246,"ml-security":0.1127731267}}
{"text":"Prior work, Attend & Excite, has introduced the concept of Generative Semantic Nursing (GSN), aiming to optimize cross-attention during inference time to better incorporate the semantics.","meta":{"url":"http://arxiv.org/abs/2307.10864v1"},"cats":{"new-dataset":0.0479941305,"dev-research":0.2398096038,"prompt-eng":0.4067709907,"data-quality":0.1341647673,"ml-security":0.0602774981}}
{"text":"It demonstrates promising results in generating simple prompts, e.g., ``a cat and a dog''.","meta":{"url":"http://arxiv.org/abs/2307.10864v1"},"cats":{"new-dataset":0.1311363239,"dev-research":0.3249011689,"prompt-eng":0.595388127,"data-quality":0.178083437,"ml-security":0.1326928963}}
{"text":"However, its efficacy declines when dealing with more complex prompts, and it does not explicitly address the problem of improper attribute binding.","meta":{"url":"http://arxiv.org/abs/2307.10864v1"},"cats":{"new-dataset":0.0039300409,"dev-research":0.2662849583,"prompt-eng":0.5417412516,"data-quality":0.2334482594,"ml-security":0.1788097914}}
{"text":"To address the challenges posed by complex prompts or scenarios involving multiple entities and to achieve improved attribute binding, we propose Divide & Bind.","meta":{"url":"http://arxiv.org/abs/2307.10864v1"},"cats":{"new-dataset":0.0997546902,"dev-research":0.3312741953,"prompt-eng":0.5779012989,"data-quality":0.1777487681,"ml-security":0.1288578508}}
{"text":"We introduce two novel loss objectives for GSN: a novel attendance loss and a binding loss.","meta":{"url":"http://arxiv.org/abs/2307.10864v1"},"cats":{"new-dataset":0.0428154715,"dev-research":0.1687038254,"prompt-eng":0.3892428537,"data-quality":0.2122370001,"ml-security":0.207321356}}
{"text":"Our approach stands out in its ability to faithfully synthesize desired objects with improved attribute alignment from complex prompts and exhibits superior performance across multiple evaluation benchmarks.","meta":{"url":"http://arxiv.org/abs/2307.10864v1"},"cats":{"new-dataset":0.2588066209,"dev-research":0.3587442108,"prompt-eng":0.5433362069,"data-quality":0.222097699,"ml-security":0.1000059465}}
{"text":"More videos and updates can be found on the project page \\url{https://sites.google.com/view/divide-and-bind}.","meta":{"url":"http://arxiv.org/abs/2307.10864v1"},"cats":{"new-dataset":0.5755713951,"dev-research":0.2793363305,"prompt-eng":0.346846443,"data-quality":0.1460301507,"ml-security":0.085160691}}
{"text":"The great advancements of generative adversarial networks and face recognition models in computer vision have made it possible to swap identities on images from single sources.","meta":{"url":"http://arxiv.org/abs/2307.10854v1"},"cats":{"new-dataset":0.042238091,"dev-research":0.2080846764,"prompt-eng":0.3713230133,"data-quality":0.2402699835,"ml-security":0.3533718679}}
{"text":"Although a lot of studies seems to have proposed almost satisfactory solutions, we notice previous methods still suffer from an identity-attribute entanglement that causes undesired attributes swapping because widely used identity encoders, eg, ArcFace, have some crucial attribute biases owing to their pretraining on face recognition tasks.","meta":{"url":"http://arxiv.org/abs/2307.10854v1"},"cats":{"new-dataset":0.0142320803,"dev-research":0.270513576,"prompt-eng":0.4097488782,"data-quality":0.2313329933,"ml-security":0.2489230786}}
{"text":"To address this issue, we design BlendFace, a novel identity encoder for face-swapping.","meta":{"url":"http://arxiv.org/abs/2307.10854v1"},"cats":{"new-dataset":0.0943871495,"dev-research":0.2913967601,"prompt-eng":0.4290429254,"data-quality":0.1774725932,"ml-security":0.1744545382}}
{"text":"The key idea behind BlendFace is training face recognition models on blended images whose attributes are replaced with those of another mitigates inter-personal biases such as hairsyles.","meta":{"url":"http://arxiv.org/abs/2307.10854v1"},"cats":{"new-dataset":0.021518568,"dev-research":0.2798124521,"prompt-eng":0.3312762546,"data-quality":0.1629898371,"ml-security":0.2171201505}}
{"text":"BlendFace feeds disentangled identity features into generators and guides generators properly as an identity loss function.","meta":{"url":"http://arxiv.org/abs/2307.10854v1"},"cats":{"new-dataset":0.0390850464,"dev-research":0.2729924663,"prompt-eng":0.3784760281,"data-quality":0.2868574397,"ml-security":0.1865346272}}
{"text":"Extensive experiments demonstrate that BlendFace improves the identity-attribute disentanglement in face-swapping models, maintaining a comparable quantitative performance to previous methods.","meta":{"url":"http://arxiv.org/abs/2307.10854v1"},"cats":{"new-dataset":0.0167395611,"dev-research":0.2775650161,"prompt-eng":0.3944050008,"data-quality":0.1733123034,"ml-security":0.1448914065}}
{"text":"Weakly-supervised change detection (WSCD) aims to detect pixel-level changes with only image-level annotations.","meta":{"url":"http://arxiv.org/abs/2307.10853v1"},"cats":{"new-dataset":0.2005236743,"dev-research":0.3208506544,"prompt-eng":0.4683112341,"data-quality":0.4815255951,"ml-security":0.1688487013}}
{"text":"Owing to its label efficiency, WSCD is drawing increasing attention recently.","meta":{"url":"http://arxiv.org/abs/2307.10853v1"},"cats":{"new-dataset":0.0779141749,"dev-research":0.3408332167,"prompt-eng":0.4165714198,"data-quality":0.3304298957,"ml-security":0.076318496}}
{"text":"However, current WSCD methods often encounter the challenge of change missing and fabricating, i.e., the inconsistency between image-level annotations and pixel-level predictions.","meta":{"url":"http://arxiv.org/abs/2307.10853v1"},"cats":{"new-dataset":0.1137521948,"dev-research":0.33512963,"prompt-eng":0.4697164326,"data-quality":0.4585335944,"ml-security":0.1021435303}}
{"text":"Specifically, change missing refer to the situation that the WSCD model fails to predict any changed pixels, even though the image-level label indicates changed, and vice versa for change fabricating.","meta":{"url":"http://arxiv.org/abs/2307.10853v1"},"cats":{"new-dataset":0.0595883117,"dev-research":0.2521902465,"prompt-eng":0.4162903482,"data-quality":0.4589362469,"ml-security":0.097170566}}
{"text":"To address this challenge, in this work, we leverage global-scale and local-scale priors in WSCD and propose two components: a Dilated Prior (DP) decoder and a Label Gated (LG) constraint.","meta":{"url":"http://arxiv.org/abs/2307.10853v1"},"cats":{"new-dataset":0.1431834597,"dev-research":0.1949329925,"prompt-eng":0.4692292644,"data-quality":0.2649645435,"ml-security":0.1053875577}}
{"text":"The DP decoder decodes samples with the changed image-level label, skips samples with the unchanged label, and replaces them with an all-unchanged pixel-level label.","meta":{"url":"http://arxiv.org/abs/2307.10853v1"},"cats":{"new-dataset":0.0675896096,"dev-research":0.2598242278,"prompt-eng":0.4285326831,"data-quality":0.3761169154,"ml-security":0.0826275868}}
{"text":"The LG constraint is derived from the correspondence between changed representations and image-level labels, penalizing the model when it mispredicts the change status.","meta":{"url":"http://arxiv.org/abs/2307.10853v1"},"cats":{"new-dataset":0.0225405226,"dev-research":0.208683326,"prompt-eng":0.4307093695,"data-quality":0.3566647212,"ml-security":0.1275711018}}
{"text":"Additionally, we develop TransWCD, a simple yet powerful transformer-based model, showcasing the potential of weakly-supervised learning in change detection.","meta":{"url":"http://arxiv.org/abs/2307.10853v1"},"cats":{"new-dataset":0.1550493671,"dev-research":0.2574423889,"prompt-eng":0.4335595762,"data-quality":0.3356300252,"ml-security":0.2430351974}}
{"text":"By integrating the DP decoder and LG constraint into TransWCD, we form TransWCD-DL.","meta":{"url":"http://arxiv.org/abs/2307.10853v1"},"cats":{"new-dataset":0.0701507734,"dev-research":0.2202318798,"prompt-eng":0.4637620045,"data-quality":0.1469310615,"ml-security":0.0658775475}}
{"text":"Our proposed TransWCD and TransWCD-DL achieve significant +6.33% and +9.55% F1 score improvements over the state-of-the-art methods on the WHU-CD dataset, respectively.","meta":{"url":"http://arxiv.org/abs/2307.10853v1"},"cats":{"new-dataset":0.4209548555,"dev-research":0.2694945834,"prompt-eng":0.4370936135,"data-quality":0.2398576842,"ml-security":0.0539854174}}
{"text":"Some performance metrics even exceed several fully-supervised change detection (FSCD) competitors.","meta":{"url":"http://arxiv.org/abs/2307.10853v1"},"cats":{"new-dataset":0.087276455,"dev-research":0.2778801652,"prompt-eng":0.4390311138,"data-quality":0.3952163865,"ml-security":0.236368697}}
{"text":"Code will be available at https://github.com/zhenghuizhao/TransWCD.","meta":{"url":"http://arxiv.org/abs/2307.10853v1"},"cats":{"new-dataset":0.3671582071,"dev-research":0.2920152041,"prompt-eng":0.4505694504,"data-quality":0.1458077493,"ml-security":0.0563018564}}
{"text":"In this paper, we present novel algorithms that efficiently compute a shortest reconfiguration sequence between two given dominating sets in trees and interval graphs under the Token Sliding model.","meta":{"url":"http://arxiv.org/abs/2307.10847v1"},"cats":{"new-dataset":0.216364652,"dev-research":0.2789197873,"prompt-eng":0.376901831,"data-quality":0.1248899999,"ml-security":0.1230360927}}
{"text":"In this problem, a graph is provided along with its two dominating sets, which can be imagined as tokens placed on vertices.","meta":{"url":"http://arxiv.org/abs/2307.10847v1"},"cats":{"new-dataset":0.0589997406,"dev-research":0.2503219965,"prompt-eng":0.3964888927,"data-quality":0.2059508792,"ml-security":0.1103919522}}
{"text":"The objective is to find a shortest sequence of dominating sets that transforms one set into the other, with each set in the sequence resulting from sliding a single token in the previous set.","meta":{"url":"http://arxiv.org/abs/2307.10847v1"},"cats":{"new-dataset":0.1519666088,"dev-research":0.2354703192,"prompt-eng":0.3907916199,"data-quality":0.1070925136,"ml-security":0.1057301005}}
{"text":"While identifying any sequence has been well studied, our work presents the first polynomial algorithms for this optimization variant in the context of dominating sets.","meta":{"url":"http://arxiv.org/abs/2307.10847v1"},"cats":{"new-dataset":0.1677654166,"dev-research":0.1994958023,"prompt-eng":0.3702971047,"data-quality":0.1207919676,"ml-security":0.1796948841}}
{"text":"Goal-Conditioned Reinforcement Learning (GCRL) can enable agents to spontaneously set diverse goals to learn a set of skills.","meta":{"url":"http://arxiv.org/abs/2307.10846v1"},"cats":{"new-dataset":0.022283695,"dev-research":0.2073667384,"prompt-eng":0.4148182741,"data-quality":0.0720034531,"ml-security":0.1753740197}}
{"text":"Despite the excellent works proposed in various fields, reaching distant goals in temporally extended tasks remains a challenge for GCRL.","meta":{"url":"http://arxiv.org/abs/2307.10846v1"},"cats":{"new-dataset":0.0579509804,"dev-research":0.1989536308,"prompt-eng":0.4167799858,"data-quality":0.0883883965,"ml-security":0.0671332824}}
{"text":"Current works tackled this problem by leveraging planning algorithms to plan intermediate subgoals to augment GCRL.","meta":{"url":"http://arxiv.org/abs/2307.10846v1"},"cats":{"new-dataset":0.1136364422,"dev-research":0.2293437668,"prompt-eng":0.4209196067,"data-quality":0.0742384742,"ml-security":0.0352114328}}
{"text":"Their methods need two crucial requirements: (i) a state representation space to search valid subgoals, and (ii) a distance function to measure the reachability of subgoals.","meta":{"url":"http://arxiv.org/abs/2307.10846v1"},"cats":{"new-dataset":0.0271556097,"dev-research":0.2403768561,"prompt-eng":0.4109307052,"data-quality":0.1332152771,"ml-security":0.058871639}}
{"text":"However, they struggle to scale to high-dimensional state space due to their non-compact representations.","meta":{"url":"http://arxiv.org/abs/2307.10846v1"},"cats":{"new-dataset":0.0312969862,"dev-research":0.153421051,"prompt-eng":0.3341095966,"data-quality":0.0924110369,"ml-security":0.1008677652}}
{"text":"Moreover, they cannot collect high-quality training data through standard GC policies, which results in an inaccurate distance function.","meta":{"url":"http://arxiv.org/abs/2307.10846v1"},"cats":{"new-dataset":0.0362101299,"dev-research":0.2003446533,"prompt-eng":0.3068902246,"data-quality":0.3044768139,"ml-security":0.2110860267}}
{"text":"Both affect the efficiency and performance of planning and policy learning.","meta":{"url":"http://arxiv.org/abs/2307.10846v1"},"cats":{"new-dataset":0.0057819917,"dev-research":0.3405362882,"prompt-eng":0.3207640262,"data-quality":0.0827827684,"ml-security":0.1142335027}}
{"text":"In the paper, we propose a goal-conditioned RL algorithm combined with Disentanglement-based Reachability Planning (REPlan) to solve temporally extended tasks.","meta":{"url":"http://arxiv.org/abs/2307.10846v1"},"cats":{"new-dataset":0.055123631,"dev-research":0.2750554278,"prompt-eng":0.4038158209,"data-quality":0.0450838597,"ml-security":0.049318961}}
{"text":"In REPlan, a Disentangled Representation Module (DRM) is proposed to learn compact representations which disentangle robot poses and object positions from high-dimensional observations in a self-supervised manner.","meta":{"url":"http://arxiv.org/abs/2307.10846v1"},"cats":{"new-dataset":0.1892200301,"dev-research":0.2386573224,"prompt-eng":0.3478278771,"data-quality":0.1100183682,"ml-security":0.1067849579}}
{"text":"A simple REachability discrimination Module (REM) is also designed to determine the temporal distance of subgoals.","meta":{"url":"http://arxiv.org/abs/2307.10846v1"},"cats":{"new-dataset":0.0443814135,"dev-research":0.249105101,"prompt-eng":0.4492069186,"data-quality":0.1218736307,"ml-security":0.0798054603}}
{"text":"Moreover, REM computes intrinsic bonuses to encourage the collection of novel states for training.","meta":{"url":"http://arxiv.org/abs/2307.10846v1"},"cats":{"new-dataset":0.0129860218,"dev-research":0.2215684683,"prompt-eng":0.3909890692,"data-quality":0.1027424452,"ml-security":0.15180943}}
{"text":"We evaluate our REPlan in three vision-based simulation tasks and one real-world task.","meta":{"url":"http://arxiv.org/abs/2307.10846v1"},"cats":{"new-dataset":0.1802552445,"dev-research":0.2548995885,"prompt-eng":0.473365396,"data-quality":0.0923289912,"ml-security":0.0553079592}}
{"text":"The experiments demonstrate that our REPlan significantly outperforms the prior state-of-the-art methods in solving temporally extended tasks.","meta":{"url":"http://arxiv.org/abs/2307.10846v1"},"cats":{"new-dataset":0.027111053,"dev-research":0.3070774225,"prompt-eng":0.4185345076,"data-quality":0.0770691967,"ml-security":0.0691640372}}
{"text":"Continual learning algorithms which keep the parameters of new tasks close to that of previous tasks, are popular in preventing catastrophic forgetting in sequential task learning settings.","meta":{"url":"http://arxiv.org/abs/2307.10845v1"},"cats":{"new-dataset":0.027409827,"dev-research":0.2369042619,"prompt-eng":0.4059205526,"data-quality":0.2114592726,"ml-security":0.2707675637}}
{"text":"However, 1) the performance for the new continual learner will be degraded without distinguishing the contributions of previously learned tasks; 2) the computational cost will be greatly increased with the number of tasks, since most existing algorithms need to regularize all previous tasks when learning new tasks.","meta":{"url":"http://arxiv.org/abs/2307.10845v1"},"cats":{"new-dataset":0.0054699127,"dev-research":0.2908655112,"prompt-eng":0.3720240682,"data-quality":0.1780150494,"ml-security":0.2352444561}}
{"text":"To address the above challenges, we propose a self-paced Weight Consolidation (spWC) framework to attain robust continual learning via evaluating the discriminative contributions of previous tasks.","meta":{"url":"http://arxiv.org/abs/2307.10845v1"},"cats":{"new-dataset":0.0481060952,"dev-research":0.2271042996,"prompt-eng":0.4198977454,"data-quality":0.1954402755,"ml-security":0.1181451633}}
{"text":"To be specific, we develop a self-paced regularization to reflect the priorities of past tasks via measuring difficulty based on key performance indicator (i.e., accuracy).","meta":{"url":"http://arxiv.org/abs/2307.10845v1"},"cats":{"new-dataset":0.0323393408,"dev-research":0.3664578664,"prompt-eng":0.5342157128,"data-quality":0.1907301377,"ml-security":0.0688927578}}
{"text":"When encountering a new task, all previous tasks are sorted from \"difficult\" to \"easy\" based on the priorities.","meta":{"url":"http://arxiv.org/abs/2307.10845v1"},"cats":{"new-dataset":0.0130384719,"dev-research":0.389660241,"prompt-eng":0.425120102,"data-quality":0.1253780936,"ml-security":0.0638697775}}
{"text":"Then the parameters of the new continual learner will be learned via selectively maintaining the knowledge amongst more difficult past tasks, which could well overcome catastrophic forgetting with less computational cost.","meta":{"url":"http://arxiv.org/abs/2307.10845v1"},"cats":{"new-dataset":0.0085923079,"dev-research":0.2980271791,"prompt-eng":0.4129205817,"data-quality":0.1668238551,"ml-security":0.2614835543}}
{"text":"We adopt an alternative convex search to iteratively update the model parameters and priority weights in the bi-convex formulation.","meta":{"url":"http://arxiv.org/abs/2307.10845v1"},"cats":{"new-dataset":0.0101422422,"dev-research":0.1591863847,"prompt-eng":0.4392436107,"data-quality":0.1240644007,"ml-security":0.1085458515}}
{"text":"The proposed spWC framework is plug-and-play, which is applicable to most continual learning algorithms (e.g., EWC, MAS and RCIL) in different directions (e.g., classification and segmentation).","meta":{"url":"http://arxiv.org/abs/2307.10845v1"},"cats":{"new-dataset":0.1287871096,"dev-research":0.2514606582,"prompt-eng":0.3497432185,"data-quality":0.1802179545,"ml-security":0.1423743111}}
{"text":"Experimental results on several public benchmark datasets demonstrate that our proposed framework can effectively improve performance when compared with other popular continual learning algorithms.","meta":{"url":"http://arxiv.org/abs/2307.10845v1"},"cats":{"new-dataset":0.2127723011,"dev-research":0.2034572187,"prompt-eng":0.358787921,"data-quality":0.2704417281,"ml-security":0.1853851549}}
{"text":"This paper presents a deep learning architecture for nowcasting of precipitation almost globally every 30 min with a 4-hour lead time.","meta":{"url":"http://arxiv.org/abs/2307.10843v1"},"cats":{"new-dataset":0.2836991894,"dev-research":0.2091960941,"prompt-eng":0.3608599659,"data-quality":0.1479263037,"ml-security":0.1699046943}}
{"text":"The architecture fuses a U-Net and a convolutional long short-term memory (LSTM) neural network and is trained using data from the Integrated MultisatellitE Retrievals for GPM (IMERG) and a few key precipitation drivers from the Global Forecast System (GFS).","meta":{"url":"http://arxiv.org/abs/2307.10843v1"},"cats":{"new-dataset":0.2833244943,"dev-research":0.1699153681,"prompt-eng":0.3510861377,"data-quality":0.094004679,"ml-security":0.1308776389}}
{"text":"The impacts of different training loss functions, including the mean-squared error (regression) and the focal-loss (classification), on the quality of precipitation nowcasts are studied.","meta":{"url":"http://arxiv.org/abs/2307.10843v1"},"cats":{"new-dataset":0.029713773,"dev-research":0.2481962877,"prompt-eng":0.3452828882,"data-quality":0.3295628558,"ml-security":0.2839033934}}
{"text":"The results indicate that the regression network performs well in capturing light precipitation (below 1.6 mm/hr), but the classification network can outperform the regression network for nowcasting of precipitation extremes (>8 mm/hr), in terms of the critical success index (CSI)..","meta":{"url":"http://arxiv.org/abs/2307.10843v1"},"cats":{"new-dataset":0.1445840075,"dev-research":0.2171042768,"prompt-eng":0.3652978999,"data-quality":0.229305419,"ml-security":0.1635758428}}
{"text":"Using the Wasserstein distance, it is shown that the predicted precipitation by the classification network has a closer class probability distribution to the IMERG than the regression network.","meta":{"url":"http://arxiv.org/abs/2307.10843v1"},"cats":{"new-dataset":0.125397722,"dev-research":0.1615313255,"prompt-eng":0.3861413638,"data-quality":0.2379707407,"ml-security":0.1873719285}}
{"text":"It is uncovered that the inclusion of the physical variables can improve precipitation nowcasting, especially at longer lead times in both networks.","meta":{"url":"http://arxiv.org/abs/2307.10843v1"},"cats":{"new-dataset":0.0416628896,"dev-research":0.2651872239,"prompt-eng":0.3761898202,"data-quality":0.1370398648,"ml-security":0.1307271139}}
{"text":"Taking IMERG as a relative reference, a multi-scale analysis in terms of fractions skill score (FSS), shows that the nowcasting machine remains skillful (FSS > 0.5) at the resolution of 10 km compared to 50 km for GFS.","meta":{"url":"http://arxiv.org/abs/2307.10843v1"},"cats":{"new-dataset":0.1173193137,"dev-research":0.2280326742,"prompt-eng":0.3920735757,"data-quality":0.1333195546,"ml-security":0.0566168292}}
{"text":"For precipitation rates greater than 4~mm/hr, only the classification network remains FSS-skillful on scales greater than 50 km within a 2-hour lead time.","meta":{"url":"http://arxiv.org/abs/2307.10843v1"},"cats":{"new-dataset":0.0837186302,"dev-research":0.1948241558,"prompt-eng":0.3706172636,"data-quality":0.2029246548,"ml-security":0.1198858871}}
{"text":"Performance of a pre-trained semantic segmentation model is likely to substantially decrease on data from a new domain.","meta":{"url":"http://arxiv.org/abs/2307.10842v1"},"cats":{"new-dataset":0.0912890204,"dev-research":0.2323177312,"prompt-eng":0.4264419647,"data-quality":0.2501180027,"ml-security":0.1312998837}}
{"text":"We show a pre-trained model can be adapted to unlabelled target domain data by calculating soft-label prototypes under the domain shift and making predictions according to the prototype closest to the vector with predicted class probabilities.","meta":{"url":"http://arxiv.org/abs/2307.10842v1"},"cats":{"new-dataset":0.2258257514,"dev-research":0.2392729747,"prompt-eng":0.4953784489,"data-quality":0.4357148133,"ml-security":0.2192558275}}
{"text":"The proposed adaptation procedure is fast, comes almost for free in terms of computational resources and leads to considerable performance improvements.","meta":{"url":"http://arxiv.org/abs/2307.10842v1"},"cats":{"new-dataset":0.0363596155,"dev-research":0.2878392612,"prompt-eng":0.4648969541,"data-quality":0.1644913451,"ml-security":0.0692849677}}
{"text":"We demonstrate the benefits of such label calibration on the highly-practical synthetic-to-real semantic segmentation problem.","meta":{"url":"http://arxiv.org/abs/2307.10842v1"},"cats":{"new-dataset":0.2986676162,"dev-research":0.2431630268,"prompt-eng":0.4106928961,"data-quality":0.5553158085,"ml-security":0.1363989407}}
{"text":"Soft robots have been leveraged in considerable areas like surgery, rehabilitation, and bionics due to their softness, flexibility, and safety.","meta":{"url":"http://arxiv.org/abs/2307.10838v1"},"cats":{"new-dataset":0.0406849466,"dev-research":0.2171420722,"prompt-eng":0.3208604378,"data-quality":0.0770714614,"ml-security":0.1247581502}}
{"text":"However, it is challenging to produce two same soft robots even with the same mold and manufacturing process owing to the complexity of soft materials.","meta":{"url":"http://arxiv.org/abs/2307.10838v1"},"cats":{"new-dataset":0.0132782749,"dev-research":0.2831897992,"prompt-eng":0.3745555934,"data-quality":0.0775244366,"ml-security":0.0663173851}}
{"text":"Meanwhile, widespread usage of a system requires the ability to fabricate replaceable components, which is interchangeability.","meta":{"url":"http://arxiv.org/abs/2307.10838v1"},"cats":{"new-dataset":0.0062243956,"dev-research":0.3675933186,"prompt-eng":0.408738544,"data-quality":0.1193500681,"ml-security":0.097937154}}
{"text":"Due to the necessity of this property, a hybrid adaptive controller is introduced to achieve interchangeability from the perspective of control approaches.","meta":{"url":"http://arxiv.org/abs/2307.10838v1"},"cats":{"new-dataset":0.0041158694,"dev-research":0.2335931662,"prompt-eng":0.3999192866,"data-quality":0.0773035854,"ml-security":0.0764840792}}
{"text":"This method utilizes an offline trained recurrent neural network controller to cope with the nonlinear and delayed response from soft robots.","meta":{"url":"http://arxiv.org/abs/2307.10838v1"},"cats":{"new-dataset":0.0858655608,"dev-research":0.1693777098,"prompt-eng":0.388332416,"data-quality":0.1110278842,"ml-security":0.1018130994}}
{"text":"Furthermore, an online optimizing kinematics controller is applied to decrease the error caused by the above neural network controller.","meta":{"url":"http://arxiv.org/abs/2307.10838v1"},"cats":{"new-dataset":0.0227501718,"dev-research":0.3173299207,"prompt-eng":0.3886460818,"data-quality":0.1580010701,"ml-security":0.109468624}}
{"text":"Soft pneumatic robots with different deformation properties but the same mold have been included for validation experiments.","meta":{"url":"http://arxiv.org/abs/2307.10838v1"},"cats":{"new-dataset":0.0159701644,"dev-research":0.2185854514,"prompt-eng":0.3650601101,"data-quality":0.1129370785,"ml-security":0.0991272794}}
{"text":"In the experiments, the systems with different actuation configurations and the different robots follow the desired trajectory with errors of 0.040 and 0.030 compared with the working space length, respectively.","meta":{"url":"http://arxiv.org/abs/2307.10838v1"},"cats":{"new-dataset":0.0259938549,"dev-research":0.2168582917,"prompt-eng":0.3979773247,"data-quality":0.1334868663,"ml-security":0.0736991723}}
{"text":"Such an adaptive controller also shows good performance on different control frequencies and desired velocities.","meta":{"url":"http://arxiv.org/abs/2307.10838v1"},"cats":{"new-dataset":0.0134004394,"dev-research":0.2493271019,"prompt-eng":0.4437127808,"data-quality":0.0746795927,"ml-security":0.0528001383}}
{"text":"This controller endows soft robots with the potential for wide application, and future work may include different offline and online controllers.","meta":{"url":"http://arxiv.org/abs/2307.10838v1"},"cats":{"new-dataset":0.0790396707,"dev-research":0.2178367315,"prompt-eng":0.4173762872,"data-quality":0.0711128276,"ml-security":0.0819511667}}
{"text":"A weight parameter adjusting strategy may also be proposed in the future.","meta":{"url":"http://arxiv.org/abs/2307.10838v1"},"cats":{"new-dataset":0.0043416643,"dev-research":0.2309073743,"prompt-eng":0.4900502645,"data-quality":0.0945554109,"ml-security":0.124927809}}
{"text":"This paper proposes a grant-free massive access scheme based on the millimeter wave (mmWave) extra-large-scale multiple-input multiple-output (XL-MIMO) to support massive Internet-of-Things (IoT) devices with low latency, high data rate, and high localization accuracy in the upcoming sixth-generation (6G) networks.","meta":{"url":"http://arxiv.org/abs/2307.10837v1"},"cats":{"new-dataset":0.0666569362,"dev-research":0.1992774228,"prompt-eng":0.2948103176,"data-quality":0.0656849495,"ml-security":0.1053133472}}
{"text":"The XL-MIMO consists of multiple antenna subarrays that are widely spaced over the service area to ensure line-of-sight (LoS) transmissions.","meta":{"url":"http://arxiv.org/abs/2307.10837v1"},"cats":{"new-dataset":0.0555087103,"dev-research":0.2095933125,"prompt-eng":0.3336690828,"data-quality":0.0911868526,"ml-security":0.0850709542}}
{"text":"First, we establish the XL-MIMO-based massive access model considering the near-field spatial non-stationary (SNS) property.","meta":{"url":"http://arxiv.org/abs/2307.10837v1"},"cats":{"new-dataset":0.0362957864,"dev-research":0.123662718,"prompt-eng":0.3253055892,"data-quality":0.0639995961,"ml-security":0.1255880652}}
{"text":"Then, by exploiting the block sparsity of subarrays and the SNS property, we propose a structured block orthogonal matching pursuit algorithm for efficient active user detection (AUD) and channel estimation (CE).","meta":{"url":"http://arxiv.org/abs/2307.10837v1"},"cats":{"new-dataset":0.039522296,"dev-research":0.2448849947,"prompt-eng":0.4141044938,"data-quality":0.1862477917,"ml-security":0.2312699196}}
{"text":"Furthermore, different sensing matrices are applied in different pilot subcarriers for exploiting the diversity gains.","meta":{"url":"http://arxiv.org/abs/2307.10837v1"},"cats":{"new-dataset":0.0197087797,"dev-research":0.1898213062,"prompt-eng":0.3923363799,"data-quality":0.1449157047,"ml-security":0.1469055268}}
{"text":"Additionally, a multi-subarray collaborative localization algorithm is designed for localization.","meta":{"url":"http://arxiv.org/abs/2307.10837v1"},"cats":{"new-dataset":0.079604295,"dev-research":0.3279675923,"prompt-eng":0.3783587948,"data-quality":0.1478092354,"ml-security":0.0646998185}}
{"text":"In particular, the angle of arrival (AoA) and time difference of arrival (TDoA) of the LoS links between active users and related subarrays are extracted from the estimated XL-MIMO channels, and then the coordinates of active users are acquired by jointly utilizing the AoAs and TDoAs.","meta":{"url":"http://arxiv.org/abs/2307.10837v1"},"cats":{"new-dataset":0.0649304551,"dev-research":0.2433342384,"prompt-eng":0.412673787,"data-quality":0.0829463947,"ml-security":0.0596224049}}
{"text":"Simulation results show that the proposed algorithms outperform existing algorithms in terms of AUD and CE performance and can achieve centimeter-level localization accuracy.","meta":{"url":"http://arxiv.org/abs/2307.10837v1"},"cats":{"new-dataset":0.0945629303,"dev-research":0.2481398805,"prompt-eng":0.4046581417,"data-quality":0.2162856454,"ml-security":0.0539800906}}
{"text":"Miller recently proposed a definition of contrastive (counterfactual) explanations based on the well-known Halpern-Pearl (HP) definitions of causes and (non-contrastive) explanations.","meta":{"url":"http://arxiv.org/abs/2307.10832v1"},"cats":{"new-dataset":0.0384701794,"dev-research":0.3777835409,"prompt-eng":0.3959595212,"data-quality":0.2872925564,"ml-security":0.2183068831}}
{"text":"Crucially, the Miller definition was based on the original HP definition of explanations, but this has since been modified by Halpern; presumably because the original yields counterintuitive results in many standard examples.","meta":{"url":"http://arxiv.org/abs/2307.10832v1"},"cats":{"new-dataset":0.0094425278,"dev-research":0.291155954,"prompt-eng":0.3603082835,"data-quality":0.2200275632,"ml-security":0.1420721673}}
{"text":"More recently Borner has proposed a third definition, observing that this modified HP definition may also yield counterintuitive results.","meta":{"url":"http://arxiv.org/abs/2307.10832v1"},"cats":{"new-dataset":0.021554574,"dev-research":0.3139008544,"prompt-eng":0.4501865324,"data-quality":0.2435491792,"ml-security":0.0931902827}}
{"text":"In this paper we show that the Miller definition inherits issues found in the original HP definition.","meta":{"url":"http://arxiv.org/abs/2307.10832v1"},"cats":{"new-dataset":0.0529141478,"dev-research":0.2332732316,"prompt-eng":0.4011878277,"data-quality":0.2467391481,"ml-security":0.1176407501}}
{"text":"We address these issues by proposing two improved variants based on the more robust modified HP and Borner definitions.","meta":{"url":"http://arxiv.org/abs/2307.10832v1"},"cats":{"new-dataset":0.1313554804,"dev-research":0.2977359138,"prompt-eng":0.4895374128,"data-quality":0.3003188327,"ml-security":0.1041837982}}
{"text":"We analyse our new definitions and show that they retain the spirit of the Miller definition where all three variants satisfy an alternative unified definition that is modular with respect to an underlying definition of non-contrastive explanations.","meta":{"url":"http://arxiv.org/abs/2307.10832v1"},"cats":{"new-dataset":0.0305988957,"dev-research":0.2863700374,"prompt-eng":0.3876587306,"data-quality":0.2431730452,"ml-security":0.1148695547}}
{"text":"To the best of our knowledge this paper also provides the first explicit comparison between the original and modified HP definitions.","meta":{"url":"http://arxiv.org/abs/2307.10832v1"},"cats":{"new-dataset":0.0432727038,"dev-research":0.2303314109,"prompt-eng":0.4362421858,"data-quality":0.1719867473,"ml-security":0.0643161799}}
{"text":"This study examines the relationship between Yelp reviews and food types, investigating how ratings, sentiments, and topics vary across different types of food.","meta":{"url":"http://arxiv.org/abs/2307.10826v1"},"cats":{"new-dataset":0.0152649156,"dev-research":0.2744286131,"prompt-eng":0.3252539419,"data-quality":0.1899063646,"ml-security":0.0641357493}}
{"text":"Specifically, we analyze how ratings and sentiments of reviews vary across food types, cluster food types based on ratings and sentiments, infer review topics using machine learning models, and compare topic distributions among different food types.","meta":{"url":"http://arxiv.org/abs/2307.10826v1"},"cats":{"new-dataset":0.0189617987,"dev-research":0.2428835036,"prompt-eng":0.3758485161,"data-quality":0.252359459,"ml-security":0.1165855966}}
{"text":"Our analyses reveal that some food types have similar ratings, sentiments, and topics distributions, while others have distinct patterns.","meta":{"url":"http://arxiv.org/abs/2307.10826v1"},"cats":{"new-dataset":0.0397914992,"dev-research":0.199829969,"prompt-eng":0.3872195846,"data-quality":0.2040857295,"ml-security":0.0785370227}}
{"text":"We identify four clusters of food types based on ratings and sentiments and find that reviewers tend to focus on different topics when reviewing certain food types.","meta":{"url":"http://arxiv.org/abs/2307.10826v1"},"cats":{"new-dataset":0.022569733,"dev-research":0.2595873032,"prompt-eng":0.417819187,"data-quality":0.2362597081,"ml-security":0.0637357434}}
{"text":"These findings have important implications for understanding user behavior and cultural influence on digital media platforms and promoting cross-cultural understanding and appreciation.","meta":{"url":"http://arxiv.org/abs/2307.10826v1"},"cats":{"new-dataset":0.0461840103,"dev-research":0.3242843342,"prompt-eng":0.3348860894,"data-quality":0.2539488823,"ml-security":0.1066869238}}
{"text":"Incremental semantic segmentation aims to continually learn the segmentation of new coming classes without accessing the training data of previously learned classes.","meta":{"url":"http://arxiv.org/abs/2307.10822v1"},"cats":{"new-dataset":0.275748825,"dev-research":0.2670357993,"prompt-eng":0.3577802726,"data-quality":0.2741614956,"ml-security":0.174378714}}
{"text":"However, most current methods fail to address catastrophic forgetting and background shift since they 1) treat all previous classes equally without considering different forgetting paces caused by imbalanced gradient back-propagation; 2) lack strong semantic guidance between classes.","meta":{"url":"http://arxiv.org/abs/2307.10822v1"},"cats":{"new-dataset":0.0113209483,"dev-research":0.2930315596,"prompt-eng":0.3925378168,"data-quality":0.3780799077,"ml-security":0.2927063683}}
{"text":"To tackle the above challenges, in this paper, we propose a Gradient-Semantic Compensation (GSC) model, which surmounts incremental semantic segmentation from both gradient and semantic perspectives.","meta":{"url":"http://arxiv.org/abs/2307.10822v1"},"cats":{"new-dataset":0.2145640533,"dev-research":0.214312141,"prompt-eng":0.3822315797,"data-quality":0.3410125891,"ml-security":0.1230502157}}
{"text":"Specifically, to address catastrophic forgetting from the gradient aspect, we develop a step-aware gradient compensation that can balance forgetting paces of previously seen classes via re-weighting gradient backpropagation.","meta":{"url":"http://arxiv.org/abs/2307.10822v1"},"cats":{"new-dataset":0.0320188323,"dev-research":0.3020029826,"prompt-eng":0.4388347584,"data-quality":0.3269862723,"ml-security":0.3493468033}}
{"text":"Meanwhile, we propose a soft-sharp semantic relation distillation to distill consistent inter-class semantic relations via soft labels for alleviating catastrophic forgetting from the semantic aspect.","meta":{"url":"http://arxiv.org/abs/2307.10822v1"},"cats":{"new-dataset":0.0729695989,"dev-research":0.3585401676,"prompt-eng":0.4102549629,"data-quality":0.5332229127,"ml-security":0.1353413719}}
{"text":"In addition, we develop a prototypical pseudo re-labeling that provides strong semantic guidance to mitigate background shift.","meta":{"url":"http://arxiv.org/abs/2307.10822v1"},"cats":{"new-dataset":0.0783724543,"dev-research":0.3424608583,"prompt-eng":0.4878982423,"data-quality":0.4142735848,"ml-security":0.1726194802}}
{"text":"It produces high-quality pseudo labels for old classes in the background by measuring distances between pixels and class-wise prototypes.","meta":{"url":"http://arxiv.org/abs/2307.10822v1"},"cats":{"new-dataset":0.192519888,"dev-research":0.337289803,"prompt-eng":0.4522255072,"data-quality":0.3337362296,"ml-security":0.0928144592}}
{"text":"Extensive experiments on three public datasets, i.e., Pascal VOC 2012, ADE20K, and Cityscapes, demonstrate the effectiveness of our proposed GSC model.","meta":{"url":"http://arxiv.org/abs/2307.10822v1"},"cats":{"new-dataset":0.4341420323,"dev-research":0.1757081673,"prompt-eng":0.3907882274,"data-quality":0.2012922054,"ml-security":0.1989951799}}
{"text":"A physical simulation engine (PSE) is a software system that simulates physical environments and objects.","meta":{"url":"http://arxiv.org/abs/2307.10818v1"},"cats":{"new-dataset":0.0448801387,"dev-research":0.3307416556,"prompt-eng":0.463669682,"data-quality":0.0813713162,"ml-security":0.1081631927}}
{"text":"Modern PSEs feature both forward and backward simulations, where the forward phase predicts the behavior of a simulated system, and the backward phase provides gradients (guidance) for learning-based control tasks, such as a robot arm learning to fetch items.","meta":{"url":"http://arxiv.org/abs/2307.10818v1"},"cats":{"new-dataset":0.0338669986,"dev-research":0.2278783113,"prompt-eng":0.4618110838,"data-quality":0.0901024733,"ml-security":0.2336856244}}
{"text":"This way, modern PSEs show promising support for learning-based control methods.","meta":{"url":"http://arxiv.org/abs/2307.10818v1"},"cats":{"new-dataset":0.036389775,"dev-research":0.3090316928,"prompt-eng":0.4712034321,"data-quality":0.1262655887,"ml-security":0.2368933359}}
{"text":"To date, PSEs have been largely used in various high-profitable, commercial applications, such as games, movies, virtual reality (VR), and robotics.","meta":{"url":"http://arxiv.org/abs/2307.10818v1"},"cats":{"new-dataset":0.0249078823,"dev-research":0.3062335125,"prompt-eng":0.3750425223,"data-quality":0.0532414613,"ml-security":0.1122549235}}
{"text":"Despite the prosperous development and usage of PSEs by academia and industrial manufacturers such as Google and NVIDIA, PSEs may produce incorrect simulations, which may lead to negative results, from poor user experience in entertainment to accidents in robotics-involved manufacturing and surgical operations.   ","meta":{"url":"http://arxiv.org/abs/2307.10818v1"},"cats":{"new-dataset":0.0208441142,"dev-research":0.3832923875,"prompt-eng":0.4385036859,"data-quality":0.2400448245,"ml-security":0.2122922022}}
{"text":"This paper introduces PHYFU, a fuzzing framework designed specifically for PSEs to uncover errors in both forward and backward simulation phases.","meta":{"url":"http://arxiv.org/abs/2307.10818v1"},"cats":{"new-dataset":0.1124502184,"dev-research":0.3851485093,"prompt-eng":0.5206335918,"data-quality":0.2874137114,"ml-security":0.2773017}}
{"text":"PHYFU mutates initial states and asserts if the PSE under test behaves consistently with respect to basic Physics Laws (PLs).","meta":{"url":"http://arxiv.org/abs/2307.10818v1"},"cats":{"new-dataset":0.0211519476,"dev-research":0.2094658949,"prompt-eng":0.4549937357,"data-quality":0.1617190869,"ml-security":0.1635343247}}
{"text":"We further use feedback-driven test input scheduling to guide and accelerate the search for errors.","meta":{"url":"http://arxiv.org/abs/2307.10818v1"},"cats":{"new-dataset":0.0705684055,"dev-research":0.4224353934,"prompt-eng":0.5887608018,"data-quality":0.3558573828,"ml-security":0.1630024087}}
{"text":"Our study of four PSEs covers mainstream industrial vendors (Google and NVIDIA) as well as academic products.","meta":{"url":"http://arxiv.org/abs/2307.10818v1"},"cats":{"new-dataset":0.3800390807,"dev-research":0.228414005,"prompt-eng":0.3869163979,"data-quality":0.116795953,"ml-security":0.1055905217}}
{"text":"We successfully uncover over 5K error-triggering inputs that generate incorrect simulation results spanning across the whole software stack of PSEs.","meta":{"url":"http://arxiv.org/abs/2307.10818v1"},"cats":{"new-dataset":0.2373095303,"dev-research":0.3862560496,"prompt-eng":0.5076053244,"data-quality":0.4562047065,"ml-security":0.3290481862}}
{"text":"Recent text-to-image diffusion models have demonstrated an astonishing capacity to generate high-quality images.","meta":{"url":"http://arxiv.org/abs/2307.10816v1"},"cats":{"new-dataset":0.0946205648,"dev-research":0.1586071508,"prompt-eng":0.3819642794,"data-quality":0.1745398829,"ml-security":0.0971913627}}
{"text":"However, researchers mainly studied the way of synthesizing images with only text prompts.","meta":{"url":"http://arxiv.org/abs/2307.10816v1"},"cats":{"new-dataset":0.0518551663,"dev-research":0.2929142927,"prompt-eng":0.4440242234,"data-quality":0.1671310857,"ml-security":0.1271749564}}
{"text":"While some works have explored using other modalities as conditions, considerable paired data, e.g., box/mask-image pairs, and fine-tuning time are required for nurturing models.","meta":{"url":"http://arxiv.org/abs/2307.10816v1"},"cats":{"new-dataset":0.0659662893,"dev-research":0.1782437234,"prompt-eng":0.4392080325,"data-quality":0.0984524075,"ml-security":0.0971837687}}
{"text":"As such paired data is time-consuming and labor-intensive to acquire and restricted to a closed set, this potentially becomes the bottleneck for applications in an open world.","meta":{"url":"http://arxiv.org/abs/2307.10816v1"},"cats":{"new-dataset":0.2064357382,"dev-research":0.3219791856,"prompt-eng":0.3085327003,"data-quality":0.1199398837,"ml-security":0.1355105374}}
{"text":"This paper focuses on the simplest form of user-provided conditions, e.g., box or scribble.","meta":{"url":"http://arxiv.org/abs/2307.10816v1"},"cats":{"new-dataset":0.0239341901,"dev-research":0.348503612,"prompt-eng":0.4754193064,"data-quality":0.1269847852,"ml-security":0.0969843578}}
{"text":"To mitigate the aforementioned problem, we propose a training-free method to control objects and contexts in the synthesized images adhering to the given spatial conditions.","meta":{"url":"http://arxiv.org/abs/2307.10816v1"},"cats":{"new-dataset":0.2145199989,"dev-research":0.2830787821,"prompt-eng":0.4181756949,"data-quality":0.1612237957,"ml-security":0.167884809}}
{"text":"Specifically, three spatial constraints, i.e., Inner-Box, Outer-Box, and Corner Constraints, are designed and seamlessly integrated into the denoising step of diffusion models, requiring no additional training and massive annotated layout data.","meta":{"url":"http://arxiv.org/abs/2307.10816v1"},"cats":{"new-dataset":0.0984755393,"dev-research":0.2106663708,"prompt-eng":0.3741135245,"data-quality":0.098696769,"ml-security":0.1147632091}}
{"text":"Extensive results show that the proposed constraints can control what and where to present in the images while retaining the ability of the Stable Diffusion model to synthesize with high fidelity and diverse concept coverage.","meta":{"url":"http://arxiv.org/abs/2307.10816v1"},"cats":{"new-dataset":0.0864298119,"dev-research":0.1724339598,"prompt-eng":0.4162401648,"data-quality":0.1643282208,"ml-security":0.1250147716}}
{"text":"The code is publicly available at https://github.com/Sierkinhane/BoxDiff.","meta":{"url":"http://arxiv.org/abs/2307.10816v1"},"cats":{"new-dataset":0.4593844601,"dev-research":0.2523891094,"prompt-eng":0.396152456,"data-quality":0.1361370557,"ml-security":0.1108273168}}
{"text":"In a conventional Speech emotion recognition (SER) task, a classifier for a given language is trained on a pre-existing dataset for that same language.","meta":{"url":"http://arxiv.org/abs/2307.10814v1"},"cats":{"new-dataset":0.1382696902,"dev-research":0.2561801691,"prompt-eng":0.3631625083,"data-quality":0.2781557206,"ml-security":0.164641637}}
{"text":"However, where training data for a language does not exist, data from other languages can be used instead.","meta":{"url":"http://arxiv.org/abs/2307.10814v1"},"cats":{"new-dataset":0.2034279662,"dev-research":0.2366128334,"prompt-eng":0.2906981521,"data-quality":0.3576232846,"ml-security":0.2011924258}}
{"text":"We experiment with cross-lingual and multilingual SER, working with Amharic, English, German and URDU.","meta":{"url":"http://arxiv.org/abs/2307.10814v1"},"cats":{"new-dataset":0.1644198094,"dev-research":0.2104481557,"prompt-eng":0.4241772238,"data-quality":0.1786584166,"ml-security":0.0556055303}}
{"text":"For Amharic, we use our own publicly-available Amharic Speech Emotion Dataset (ASED).","meta":{"url":"http://arxiv.org/abs/2307.10814v1"},"cats":{"new-dataset":0.7104206812,"dev-research":0.2159688437,"prompt-eng":0.3544068656,"data-quality":0.2141359367,"ml-security":0.079241659}}
{"text":"For English, German and Urdu we use the existing RAVDESS, EMO-DB and URDU datasets.","meta":{"url":"http://arxiv.org/abs/2307.10814v1"},"cats":{"new-dataset":0.8000148046,"dev-research":0.2214956601,"prompt-eng":0.3714988265,"data-quality":0.2378943712,"ml-security":0.0705418903}}
{"text":"We followed previous research in mapping labels for all datasets to just two classes, positive and negative.","meta":{"url":"http://arxiv.org/abs/2307.10814v1"},"cats":{"new-dataset":0.3878280246,"dev-research":0.2123557002,"prompt-eng":0.4046098319,"data-quality":0.4569738564,"ml-security":0.0961424127}}
{"text":"Thus we can compare performance on different languages directly, and combine languages for training and testing.","meta":{"url":"http://arxiv.org/abs/2307.10814v1"},"cats":{"new-dataset":0.0299129548,"dev-research":0.376236894,"prompt-eng":0.4190651765,"data-quality":0.2186184209,"ml-security":0.0801968661}}
{"text":"In Experiment 1, monolingual SER trials were carried out using three classifiers, AlexNet, VGGE (a proposed variant of VGG), and ResNet50.","meta":{"url":"http://arxiv.org/abs/2307.10814v1"},"cats":{"new-dataset":0.0839476261,"dev-research":0.2512303775,"prompt-eng":0.4296529418,"data-quality":0.3472259731,"ml-security":0.1481686018}}
{"text":"Results averaged for the three models were very similar for ASED and RAVDESS, suggesting that Amharic and English SER are equally difficult.","meta":{"url":"http://arxiv.org/abs/2307.10814v1"},"cats":{"new-dataset":0.0228509131,"dev-research":0.1756948603,"prompt-eng":0.420182143,"data-quality":0.1406555731,"ml-security":0.0366920556}}
{"text":"Similarly, German SER is more difficult, and Urdu SER is easier.","meta":{"url":"http://arxiv.org/abs/2307.10814v1"},"cats":{"new-dataset":0.0035043144,"dev-research":0.2691037236,"prompt-eng":0.3459944575,"data-quality":0.1183171769,"ml-security":0.0663730079}}
{"text":"In Experiment 2, we trained on one language and tested on another, in both directions for each pair:","meta":{"url":"http://arxiv.org/abs/2307.10814v1"},"cats":{"new-dataset":0.0393037701,"dev-research":0.2372817853,"prompt-eng":0.4025245362,"data-quality":0.2572563237,"ml-security":0.0962311316}}
{"text":"Amharic<->German, Amharic<->English, and Amharic<->Urdu.","meta":{"url":"http://arxiv.org/abs/2307.10814v1"},"cats":{"new-dataset":0.3845750965,"dev-research":0.2075846812,"prompt-eng":0.34243435,"data-quality":0.2090474712,"ml-security":0.0456309615}}
{"text":"Results with Amharic as target suggested that using English or German as source will give the best result.","meta":{"url":"http://arxiv.org/abs/2307.10814v1"},"cats":{"new-dataset":0.4961918739,"dev-research":0.1741201641,"prompt-eng":0.4171693401,"data-quality":0.2426678125,"ml-security":0.0301737062}}
{"text":"In Experiment 3, we trained on several non-Amharic languages and then tested on Amharic.","meta":{"url":"http://arxiv.org/abs/2307.10814v1"},"cats":{"new-dataset":0.0852367224,"dev-research":0.1870191747,"prompt-eng":0.3782503967,"data-quality":0.2509395563,"ml-security":0.0772024241}}
{"text":"The best accuracy obtained was several percent greater than the best accuracy in Experiment 2, suggesting that a better result can be obtained when using two or three non-Amharic languages for training than when using just one non-Amharic language.","meta":{"url":"http://arxiv.org/abs/2307.10814v1"},"cats":{"new-dataset":0.02152201,"dev-research":0.2451051015,"prompt-eng":0.377148341,"data-quality":0.389528881,"ml-security":0.0783134752}}
{"text":"Overall, the results suggest that cross-lingual and multilingual training can be an effective strategy for training a SER classifier when resources for a language are scarce.","meta":{"url":"http://arxiv.org/abs/2307.10814v1"},"cats":{"new-dataset":0.036889964,"dev-research":0.2627872927,"prompt-eng":0.3654176072,"data-quality":0.3070841559,"ml-security":0.1706639339}}
{"text":"Omnidirectional videos (ODVs) play an increasingly important role in the application fields of medical, education, advertising, tourism, etc.","meta":{"url":"http://arxiv.org/abs/2307.10813v1"},"cats":{"new-dataset":0.1498899282,"dev-research":0.2160870544,"prompt-eng":0.3375610957,"data-quality":0.1384791595,"ml-security":0.0830826599}}
{"text":"Assessing the quality of ODVs is significant for service-providers to improve the user's Quality of Experience (QoE).","meta":{"url":"http://arxiv.org/abs/2307.10813v1"},"cats":{"new-dataset":0.0185203616,"dev-research":0.3400694005,"prompt-eng":0.3832027165,"data-quality":0.1822173275,"ml-security":0.0775283239}}
{"text":"However, most existing quality assessment studies for ODVs only focus on the visual distortions of videos, while ignoring that the overall QoE also depends on the accompanying audio signals.","meta":{"url":"http://arxiv.org/abs/2307.10813v1"},"cats":{"new-dataset":0.0205472425,"dev-research":0.2611939764,"prompt-eng":0.3554012924,"data-quality":0.2789264949,"ml-security":0.0573147457}}
{"text":"In this paper, we first establish a large-scale audio-visual quality assessment dataset for omnidirectional videos, which includes 375 distorted omnidirectional audio-visual (A/V) sequences generated from 15 high-quality pristine omnidirectional A/V contents, and the corresponding perceptual audio-visual quality scores.","meta":{"url":"http://arxiv.org/abs/2307.10813v1"},"cats":{"new-dataset":0.3917504785,"dev-research":0.2715369416,"prompt-eng":0.3376043009,"data-quality":0.3519016492,"ml-security":0.0703655792}}
{"text":"Then, we design three baseline methods for full-reference omnidirectional audio-visual quality assessment (OAVQA), which combine existing state-of-the-art single-mode audio and video QA models via multimodal fusion strategies.","meta":{"url":"http://arxiv.org/abs/2307.10813v1"},"cats":{"new-dataset":0.1098633619,"dev-research":0.274703467,"prompt-eng":0.3705251898,"data-quality":0.2713768619,"ml-security":0.0394968936}}
{"text":"We validate the effectiveness of the A/V multimodal fusion method for OAVQA on our dataset, which provides a new benchmark for omnidirectional QoE evaluation.","meta":{"url":"http://arxiv.org/abs/2307.10813v1"},"cats":{"new-dataset":0.2649668955,"dev-research":0.2262564729,"prompt-eng":0.4107449176,"data-quality":0.1869130555,"ml-security":0.062478878}}
{"text":"Our dataset is available at https://github.com/iamazxl/OAVQA.","meta":{"url":"http://arxiv.org/abs/2307.10813v1"},"cats":{"new-dataset":0.8803161788,"dev-research":0.2021325388,"prompt-eng":0.371342341,"data-quality":0.1463452099,"ml-security":0.0868842225}}
{"text":"Imitation learning (IL) seeks to teach agents specific tasks through expert demonstrations.","meta":{"url":"http://arxiv.org/abs/2307.10810v1"},"cats":{"new-dataset":0.0321082478,"dev-research":0.2363303133,"prompt-eng":0.3913621698,"data-quality":0.093819665,"ml-security":0.1454836459}}
{"text":"One of the key approaches to IL is to define a distance between agent and expert and to find an agent policy that minimizes that distance.","meta":{"url":"http://arxiv.org/abs/2307.10810v1"},"cats":{"new-dataset":0.0159289353,"dev-research":0.3093357625,"prompt-eng":0.3524593299,"data-quality":0.0912259434,"ml-security":0.1350607713}}
{"text":"Optimal transport methods have been widely used in imitation learning as they provide ways to measure meaningful distances between agent and expert trajectories.","meta":{"url":"http://arxiv.org/abs/2307.10810v1"},"cats":{"new-dataset":0.0178498694,"dev-research":0.2153335457,"prompt-eng":0.3921509758,"data-quality":0.0801969623,"ml-security":0.1046881903}}
{"text":"However, the problem of how to optimally combine multiple expert demonstrations has not been widely studied.","meta":{"url":"http://arxiv.org/abs/2307.10810v1"},"cats":{"new-dataset":0.0196171929,"dev-research":0.2696952969,"prompt-eng":0.4395638067,"data-quality":0.1204403507,"ml-security":0.1326474519}}
{"text":"The standard method is to simply concatenate state (-action) trajectories, which is problematic when trajectories are multi-modal.","meta":{"url":"http://arxiv.org/abs/2307.10810v1"},"cats":{"new-dataset":0.0288895231,"dev-research":0.1580135463,"prompt-eng":0.4279960253,"data-quality":0.107981712,"ml-security":0.0458842665}}
{"text":"We propose an alternative method that uses a multi-marginal optimal transport distance and enables the combination of multiple and diverse state-trajectories in the OT sense, providing a more sensible geometric average of the demonstrations.","meta":{"url":"http://arxiv.org/abs/2307.10810v1"},"cats":{"new-dataset":0.039109416,"dev-research":0.1579425035,"prompt-eng":0.4205793595,"data-quality":0.0518014088,"ml-security":0.0488661228}}
{"text":"Our approach enables an agent to learn from several experts, and its efficiency is analyzed on OpenAI Gym control environments and demonstrates that the standard method is not always optimal.","meta":{"url":"http://arxiv.org/abs/2307.10810v1"},"cats":{"new-dataset":0.0767930505,"dev-research":0.2848345813,"prompt-eng":0.4191502472,"data-quality":0.111333082,"ml-security":0.258111839}}
{"text":"This paper proposes a novel communication-efficient split learning (SL) framework, named SplitFC, which reduces the communication overhead required for transmitting intermediate feature and gradient vectors during the SL training process.","meta":{"url":"http://arxiv.org/abs/2307.10805v1"},"cats":{"new-dataset":0.0521354444,"dev-research":0.2676033613,"prompt-eng":0.3613583568,"data-quality":0.2182582698,"ml-security":0.2254561901}}
{"text":"The key idea of SplitFC is to leverage different dispersion degrees exhibited in the columns of the matrices.","meta":{"url":"http://arxiv.org/abs/2307.10805v1"},"cats":{"new-dataset":0.0293684089,"dev-research":0.1754347669,"prompt-eng":0.3430447922,"data-quality":0.093072544,"ml-security":0.0563884276}}
{"text":"SplitFC incorporates two compression strategies: (i) adaptive feature-wise dropout and (ii) adaptive feature-wise quantization.","meta":{"url":"http://arxiv.org/abs/2307.10805v1"},"cats":{"new-dataset":0.0522044157,"dev-research":0.2660866619,"prompt-eng":0.3661244099,"data-quality":0.276033904,"ml-security":0.0900194402}}
{"text":"In the first strategy, the intermediate feature vectors are dropped with adaptive dropout probabilities determined based on the standard deviation of these vectors.","meta":{"url":"http://arxiv.org/abs/2307.10805v1"},"cats":{"new-dataset":0.0210661078,"dev-research":0.2110094877,"prompt-eng":0.4044232144,"data-quality":0.2328027009,"ml-security":0.1451317756}}
{"text":"Then, by the chain rule, the intermediate gradient vectors associated with the dropped feature vectors are also dropped.","meta":{"url":"http://arxiv.org/abs/2307.10805v1"},"cats":{"new-dataset":0.0045438211,"dev-research":0.2363102914,"prompt-eng":0.3444455861,"data-quality":0.267228498,"ml-security":0.1743837527}}
{"text":"In the second strategy, the non-dropped intermediate feature and gradient vectors are quantized using adaptive quantization levels determined based on the ranges of the vectors.","meta":{"url":"http://arxiv.org/abs/2307.10805v1"},"cats":{"new-dataset":0.0153006993,"dev-research":0.2084273408,"prompt-eng":0.3686113566,"data-quality":0.1640561091,"ml-security":0.1381359058}}
{"text":"To minimize the quantization error, the optimal quantization levels of this strategy are derived in a closed-form expression.","meta":{"url":"http://arxiv.org/abs/2307.10805v1"},"cats":{"new-dataset":0.0063880942,"dev-research":0.2570321409,"prompt-eng":0.3752378712,"data-quality":0.2044247694,"ml-security":0.1620091575}}
{"text":"Simulation results on the MNIST, CIFAR-10, and CelebA datasets demonstrate that SplitFC provides more than a 5.6% increase in classification accuracy compared to state-of-the-art SL frameworks, while they require 320 times less communication overhead compared to the vanilla SL framework without compression.","meta":{"url":"http://arxiv.org/abs/2307.10805v1"},"cats":{"new-dataset":0.1590816086,"dev-research":0.271945419,"prompt-eng":0.3824190217,"data-quality":0.280354805,"ml-security":0.1460083239}}
{"text":"With the increasing amount of spatial-temporal~(ST) ocean data, numerous spatial-temporal data mining (STDM) studies have been conducted to address various oceanic issues, e.g., climate forecasting and disaster warning.","meta":{"url":"http://arxiv.org/abs/2307.10803v1"},"cats":{"new-dataset":0.2652024143,"dev-research":0.2802347381,"prompt-eng":0.3350941879,"data-quality":0.1649056487,"ml-security":0.1222141756}}
{"text":"Compared with typical ST data (e.g., traffic data), ST ocean data is more complicated with some unique characteristics, e.g., diverse regionality and high sparsity.","meta":{"url":"http://arxiv.org/abs/2307.10803v1"},"cats":{"new-dataset":0.0627226006,"dev-research":0.2673416348,"prompt-eng":0.2889903434,"data-quality":0.122475562,"ml-security":0.0929637835}}
{"text":"These characteristics make it difficult to design and train STDM models.","meta":{"url":"http://arxiv.org/abs/2307.10803v1"},"cats":{"new-dataset":0.0041890296,"dev-research":0.2950503194,"prompt-eng":0.42120808,"data-quality":0.1101025358,"ml-security":0.1845287288}}
{"text":"Unfortunately, an overview of these studies is still missing, hindering computer scientists to identify the research issues in ocean while discouraging researchers in ocean science from applying advanced STDM techniques.","meta":{"url":"http://arxiv.org/abs/2307.10803v1"},"cats":{"new-dataset":0.1058821312,"dev-research":0.248991157,"prompt-eng":0.3328900952,"data-quality":0.1609219503,"ml-security":0.0782414202}}
{"text":"To remedy this situation, we provide a comprehensive survey to summarize existing STDM studies in ocean.","meta":{"url":"http://arxiv.org/abs/2307.10803v1"},"cats":{"new-dataset":0.1344290134,"dev-research":0.2212042663,"prompt-eng":0.3940913135,"data-quality":0.1205275494,"ml-security":0.0777493284}}
{"text":"Concretely, we first summarize the widely-used ST ocean datasets and identify their unique characteristics.","meta":{"url":"http://arxiv.org/abs/2307.10803v1"},"cats":{"new-dataset":0.6753399978,"dev-research":0.2155930211,"prompt-eng":0.3370876449,"data-quality":0.163224143,"ml-security":0.0759770435}}
{"text":"Then, typical ST ocean data quality enhancement techniques are discussed.","meta":{"url":"http://arxiv.org/abs/2307.10803v1"},"cats":{"new-dataset":0.0654082356,"dev-research":0.2918735511,"prompt-eng":0.3519853562,"data-quality":0.2501042972,"ml-security":0.099573285}}
{"text":"Next, we classify existing STDM studies for ocean into four types of tasks, i.e., prediction, event detection, pattern mining, and anomaly detection, and elaborate the techniques for these tasks.","meta":{"url":"http://arxiv.org/abs/2307.10803v1"},"cats":{"new-dataset":0.1649353166,"dev-research":0.251212828,"prompt-eng":0.4198839359,"data-quality":0.1747775092,"ml-security":0.1468469119}}
{"text":"Finally, promising research opportunities are highlighted.","meta":{"url":"http://arxiv.org/abs/2307.10803v1"},"cats":{"new-dataset":0.0511908497,"dev-research":0.2403322257,"prompt-eng":0.3297301677,"data-quality":0.1056173649,"ml-security":0.0772733592}}
{"text":"This survey will help scientists from the fields of both computer science and ocean science have a better understanding of the fundamental concepts, key techniques, and open challenges of STDM in ocean.","meta":{"url":"http://arxiv.org/abs/2307.10803v1"},"cats":{"new-dataset":0.2035494367,"dev-research":0.2454814694,"prompt-eng":0.3950895467,"data-quality":0.1248170842,"ml-security":0.0797402203}}
{"text":"Multimodal learning aims to build models that can process and relate information from multiple modalities.","meta":{"url":"http://arxiv.org/abs/2307.10802v1"},"cats":{"new-dataset":0.0765739262,"dev-research":0.2411205766,"prompt-eng":0.3895808518,"data-quality":0.1363639043,"ml-security":0.105791951}}
{"text":"Despite years of development in this field, it still remains challenging to design a unified network for processing various modalities ($\\textit{e.g.}$ natural language, 2D images, 3D point clouds, audio, video, time series, tabular data) due to the inherent gaps among them.","meta":{"url":"http://arxiv.org/abs/2307.10802v1"},"cats":{"new-dataset":0.2630807137,"dev-research":0.2391511741,"prompt-eng":0.3537954617,"data-quality":0.1679302496,"ml-security":0.0637970926}}
{"text":"In this work, we propose a framework, named Meta-Transformer, that leverages a $\\textbf{frozen}$ encoder to perform multimodal perception without any paired multimodal training data.","meta":{"url":"http://arxiv.org/abs/2307.10802v1"},"cats":{"new-dataset":0.2700147417,"dev-research":0.1957647687,"prompt-eng":0.4097582803,"data-quality":0.1577418889,"ml-security":0.1024406226}}
{"text":"In Meta-Transformer, the raw input data from various modalities are mapped into a shared token space, allowing a subsequent encoder with frozen parameters to extract high-level semantic features of the input data.","meta":{"url":"http://arxiv.org/abs/2307.10802v1"},"cats":{"new-dataset":0.1024408431,"dev-research":0.2774608316,"prompt-eng":0.4613335924,"data-quality":0.2292029353,"ml-security":0.1195036168}}
{"text":"Composed of three main components: a unified data tokenizer, a modality-shared encoder, and task-specific heads for downstream tasks, Meta-Transformer is the first framework to perform unified learning across 12 modalities with unpaired data.","meta":{"url":"http://arxiv.org/abs/2307.10802v1"},"cats":{"new-dataset":0.1232008278,"dev-research":0.2340251218,"prompt-eng":0.4000977139,"data-quality":0.1607370254,"ml-security":0.0929047768}}
{"text":"Experiments on different benchmarks reveal that Meta-Transformer can handle a wide range of tasks including fundamental perception (text, image, point cloud, audio, video), practical application (X-Ray, infrared, hyperspectral, and IMU), and data mining (graph, tabular, and time-series).","meta":{"url":"http://arxiv.org/abs/2307.10802v1"},"cats":{"new-dataset":0.0700414979,"dev-research":0.2116452006,"prompt-eng":0.4056406522,"data-quality":0.1186853116,"ml-security":0.0498879906}}
{"text":"Meta-Transformer indicates a promising future for developing unified multimodal intelligence with transformers.","meta":{"url":"http://arxiv.org/abs/2307.10802v1"},"cats":{"new-dataset":0.1809252396,"dev-research":0.2615695056,"prompt-eng":0.4200407096,"data-quality":0.1050153461,"ml-security":0.0680510707}}
{"text":"Code will be available at https://github.com/invictus717/MetaTransformer","meta":{"url":"http://arxiv.org/abs/2307.10802v1"},"cats":{"new-dataset":0.3419009427,"dev-research":0.2688999733,"prompt-eng":0.4576888721,"data-quality":0.1952968367,"ml-security":0.0547634874}}
{"text":"Despite successes across a broad range of applications, sequence-to-sequence models' construct of solutions are argued to be less compositional than human-like generalization.","meta":{"url":"http://arxiv.org/abs/2307.10799v1"},"cats":{"new-dataset":0.0241363559,"dev-research":0.2328273258,"prompt-eng":0.3644041752,"data-quality":0.1083501871,"ml-security":0.1342954396}}
{"text":"There is mounting evidence that one of the reasons hindering compositional generalization is representations of the encoder and decoder uppermost layer are entangled.","meta":{"url":"http://arxiv.org/abs/2307.10799v1"},"cats":{"new-dataset":0.0028029828,"dev-research":0.2510750119,"prompt-eng":0.3313502593,"data-quality":0.1368591584,"ml-security":0.1571451193}}
{"text":"In other words, the syntactic and semantic representations of sequences are twisted inappropriately.","meta":{"url":"http://arxiv.org/abs/2307.10799v1"},"cats":{"new-dataset":0.0241194697,"dev-research":0.2775738823,"prompt-eng":0.3214636725,"data-quality":0.2961159962,"ml-security":0.157880208}}
{"text":"However, most previous studies mainly concentrate on enhancing token-level semantic information to alleviate the representations entanglement problem, rather than composing and using the syntactic and semantic representations of sequences appropriately as humans do.","meta":{"url":"http://arxiv.org/abs/2307.10799v1"},"cats":{"new-dataset":0.0104054993,"dev-research":0.3185338607,"prompt-eng":0.3884548638,"data-quality":0.2157269106,"ml-security":0.1037744681}}
{"text":"In addition, we explain why the entanglement problem exists from the perspective of recent studies about training deeper Transformer, mainly owing to the ``shallow'' residual connections and its simple, one-step operations, which fails to fuse previous layers' information effectively.","meta":{"url":"http://arxiv.org/abs/2307.10799v1"},"cats":{"new-dataset":0.0222212902,"dev-research":0.1957848326,"prompt-eng":0.3356306746,"data-quality":0.1628536452,"ml-security":0.2436119988}}
{"text":"Starting from this finding and inspired by humans' strategies, we propose \\textsc{FuSion} (\\textbf{Fu}sing \\textbf{S}yntactic and Semant\\textbf{i}c Representati\\textbf{on}s), an extension to sequence-to-sequence models to learn to fuse previous layers' information back into the encoding and decoding process appropriately through introducing a \\emph{fuse-attention module} at each encoder and decoder layer.","meta":{"url":"http://arxiv.org/abs/2307.10799v1"},"cats":{"new-dataset":0.3806115699,"dev-research":0.261567123,"prompt-eng":0.4536058036,"data-quality":0.1931032293,"ml-security":0.156860555}}
{"text":"\\textsc{FuSion} achieves competitive and even \\textbf{state-of-the-art} results on two realistic benchmarks, which empirically demonstrates the effectiveness of our proposal.","meta":{"url":"http://arxiv.org/abs/2307.10799v1"},"cats":{"new-dataset":0.0375144609,"dev-research":0.3019897537,"prompt-eng":0.4192027816,"data-quality":0.2124581577,"ml-security":0.1028004947}}
{"text":"In this paper, we present our method for neural face reenactment, called HyperReenact, that aims to generate realistic talking head images of a source identity, driven by a target facial pose.","meta":{"url":"http://arxiv.org/abs/2307.10797v1"},"cats":{"new-dataset":0.1731425427,"dev-research":0.2340366517,"prompt-eng":0.3554494601,"data-quality":0.1450744796,"ml-security":0.1582506886}}
{"text":"Existing state-of-the-art face reenactment methods train controllable generative models that learn to synthesize realistic facial images, yet producing reenacted faces that are prone to significant visual artifacts, especially under the challenging condition of extreme head pose changes, or requiring expensive few-shot fine-tuning to better preserve the source identity characteristics.","meta":{"url":"http://arxiv.org/abs/2307.10797v1"},"cats":{"new-dataset":0.080090414,"dev-research":0.2338231051,"prompt-eng":0.4248170792,"data-quality":0.1516850396,"ml-security":0.1797257806}}
{"text":"We propose to address these limitations by leveraging the photorealistic generation ability and the disentangled properties of a pretrained StyleGAN2 generator, by first inverting the real images into its latent space and then using a hypernetwork to perform: (i) refinement of the source identity characteristics and (ii) facial pose re-targeting, eliminating this way the dependence on external editing methods that typically produce artifacts.","meta":{"url":"http://arxiv.org/abs/2307.10797v1"},"cats":{"new-dataset":0.1501265398,"dev-research":0.2980934434,"prompt-eng":0.3597443228,"data-quality":0.2248946055,"ml-security":0.1826140168}}
{"text":"Our method operates under the one-shot setting (i.e., using a single source frame) and allows for cross-subject reenactment, without requiring any subject-specific fine-tuning.","meta":{"url":"http://arxiv.org/abs/2307.10797v1"},"cats":{"new-dataset":0.021885615,"dev-research":0.2071679033,"prompt-eng":0.454952994,"data-quality":0.1362039602,"ml-security":0.0592060246}}
{"text":"We compare our method both quantitatively and qualitatively against several state-of-the-art techniques on the standard benchmarks of VoxCeleb1 and VoxCeleb2, demonstrating the superiority of our approach in producing artifact-free images, exhibiting remarkable robustness even under extreme head pose changes.","meta":{"url":"http://arxiv.org/abs/2307.10797v1"},"cats":{"new-dataset":0.2203920757,"dev-research":0.2438789909,"prompt-eng":0.3904983066,"data-quality":0.2070764197,"ml-security":0.1049547599}}
{"text":"We make the code and the pretrained models publicly available at: https://github.com/StelaBou/HyperReenact .","meta":{"url":"http://arxiv.org/abs/2307.10797v1"},"cats":{"new-dataset":0.3992238707,"dev-research":0.2400659236,"prompt-eng":0.4443490167,"data-quality":0.1446563787,"ml-security":0.124194565}}
{"text":"Compiler error messages serve as an initial resource for programmers dealing with compilation errors.","meta":{"url":"http://arxiv.org/abs/2307.10793v1"},"cats":{"new-dataset":0.1107497973,"dev-research":0.6201183298,"prompt-eng":0.4216528885,"data-quality":0.5096619991,"ml-security":0.2091653391}}
{"text":"However, previous studies indicate that they often lack sufficient targeted information to resolve code issues.","meta":{"url":"http://arxiv.org/abs/2307.10793v1"},"cats":{"new-dataset":0.0173830303,"dev-research":0.4365834977,"prompt-eng":0.3571167129,"data-quality":0.3375778463,"ml-security":0.2265112931}}
{"text":"Consequently, programmers typically rely on their own research to fix errors.","meta":{"url":"http://arxiv.org/abs/2307.10793v1"},"cats":{"new-dataset":0.0121434207,"dev-research":0.6200610041,"prompt-eng":0.3796185056,"data-quality":0.4478513181,"ml-security":0.1753784437}}
{"text":"Historically, Stack Overflow has been the primary resource for such information, but recent advances in large language models offer alternatives.","meta":{"url":"http://arxiv.org/abs/2307.10793v1"},"cats":{"new-dataset":0.2008756444,"dev-research":0.2430830965,"prompt-eng":0.3969659792,"data-quality":0.1676916749,"ml-security":0.0921422921}}
{"text":"This study systematically examines 100 compiler error messages from three sources to determine the most effective approach for programmers encountering compiler errors.","meta":{"url":"http://arxiv.org/abs/2307.10793v1"},"cats":{"new-dataset":0.0789241139,"dev-research":0.68207287,"prompt-eng":0.4470917152,"data-quality":0.592255034,"ml-security":0.2369715608}}
{"text":"Factors considered include Stack Overflow search methods and the impact of model version and prompt phrasing when using large language models.","meta":{"url":"http://arxiv.org/abs/2307.10793v1"},"cats":{"new-dataset":0.0104381372,"dev-research":0.2721729884,"prompt-eng":0.4105321073,"data-quality":0.1977408169,"ml-security":0.1689122631}}
{"text":"The results reveal that GPT-4 outperforms Stack Overflow in explaining compiler error messages, the effectiveness of adding code snippets to Stack Overflow searches depends on the search method, and results for Stack Overflow differ significantly between Google and StackExchange API searches.","meta":{"url":"http://arxiv.org/abs/2307.10793v1"},"cats":{"new-dataset":0.0455147175,"dev-research":0.3753445745,"prompt-eng":0.361043054,"data-quality":0.2425054394,"ml-security":0.1375657515}}
{"text":"Furthermore, GPT-4 surpasses GPT-3.5, with \"How to fix\" prompts yielding superior outcomes to \"What does this error mean\" prompts.","meta":{"url":"http://arxiv.org/abs/2307.10793v1"},"cats":{"new-dataset":0.0753906872,"dev-research":0.3373158071,"prompt-eng":0.4781441107,"data-quality":0.3948599131,"ml-security":0.1483435081}}
{"text":"These results offer valuable guidance for programmers seeking assistance with compiler error messages, underscoring the transformative potential of advanced large language models like GPT-4 in debugging and opening new avenues of exploration for researchers in AI-assisted programming.","meta":{"url":"http://arxiv.org/abs/2307.10793v1"},"cats":{"new-dataset":0.1543656535,"dev-research":0.4918942957,"prompt-eng":0.43341416,"data-quality":0.3211164333,"ml-security":0.190682656}}
{"text":"Few-shot anomaly detection (AD) is an emerging sub-field of general AD, and tries to distinguish between normal and anomalous data using only few selected samples.","meta":{"url":"http://arxiv.org/abs/2307.10792v1"},"cats":{"new-dataset":0.1270372709,"dev-research":0.2603861851,"prompt-eng":0.3502926847,"data-quality":0.3036454721,"ml-security":0.3423573219}}
{"text":"While newly proposed few-shot AD methods do compare against pre-existing algorithms developed for the full-shot domain as baselines, they do not dedicatedly optimize them for the few-shot setting.","meta":{"url":"http://arxiv.org/abs/2307.10792v1"},"cats":{"new-dataset":0.0508724035,"dev-research":0.2809762899,"prompt-eng":0.3633005771,"data-quality":0.1999326886,"ml-security":0.0998189316}}
{"text":"It thus remains unclear if the performance of such pre-existing algorithms can be further improved.","meta":{"url":"http://arxiv.org/abs/2307.10792v1"},"cats":{"new-dataset":0.0261702625,"dev-research":0.2421709722,"prompt-eng":0.3934992436,"data-quality":0.1834552836,"ml-security":0.0736486735}}
{"text":"We address said question in this work.","meta":{"url":"http://arxiv.org/abs/2307.10792v1"},"cats":{"new-dataset":0.1551859695,"dev-research":0.3008834571,"prompt-eng":0.3905453909,"data-quality":0.1671195996,"ml-security":0.0877797137}}
{"text":"Specifically, we present a study on the AD/anomaly segmentation (AS) performance of PatchCore, the current state-of-the-art full-shot AD/AS algorithm, in both the few-shot and the many-shot settings.","meta":{"url":"http://arxiv.org/abs/2307.10792v1"},"cats":{"new-dataset":0.2377343107,"dev-research":0.3771734146,"prompt-eng":0.4027015604,"data-quality":0.3104934131,"ml-security":0.2884823424}}
{"text":"We hypothesize that further performance improvements can be realized by (I) optimizing its various hyperparameters, and by (II) transferring techniques known to improve few-shot supervised learning to the AD domain.","meta":{"url":"http://arxiv.org/abs/2307.10792v1"},"cats":{"new-dataset":0.0585855613,"dev-research":0.2368276556,"prompt-eng":0.4236088946,"data-quality":0.3243883316,"ml-security":0.2154527177}}
{"text":"Exhaustive experiments on the public VisA and MVTec AD datasets reveal that (I) significant performance improvements can be realized by optimizing hyperparameters such as the underlying feature extractor, and that (II) image-level augmentations can, but are not guaranteed, to improve performance.","meta":{"url":"http://arxiv.org/abs/2307.10792v1"},"cats":{"new-dataset":0.0683227391,"dev-research":0.2774520575,"prompt-eng":0.4201357349,"data-quality":0.2702605031,"ml-security":0.1767678195}}
{"text":"Based on these findings, we achieve a new state of the art in few-shot AD on VisA, further demonstrating the merit of adapting pre-existing AD/AS methods to the few-shot setting.","meta":{"url":"http://arxiv.org/abs/2307.10792v1"},"cats":{"new-dataset":0.2134544799,"dev-research":0.2568001455,"prompt-eng":0.3850380657,"data-quality":0.2118417284,"ml-security":0.0886354587}}
{"text":"Last, we identify the investigation of feature extractors with a strong inductive bias as a potential future research direction for (few-shot) AD/AS.","meta":{"url":"http://arxiv.org/abs/2307.10792v1"},"cats":{"new-dataset":0.0767957017,"dev-research":0.267165062,"prompt-eng":0.4232314074,"data-quality":0.3589999065,"ml-security":0.1868383624}}
{"text":"To be successful, Vision-and-Language Navigation (VLN) agents must be able to ground instructions to actions based on their surroundings.","meta":{"url":"http://arxiv.org/abs/2307.10790v1"},"cats":{"new-dataset":0.0794768437,"dev-research":0.3429546995,"prompt-eng":0.4164631057,"data-quality":0.1364777408,"ml-security":0.1233332878}}
{"text":"In this work, we develop a methodology to study agent behavior on a skill-specific basis -- examining how well existing agents ground instructions about stopping, turning, and moving towards specified objects or rooms.","meta":{"url":"http://arxiv.org/abs/2307.10790v1"},"cats":{"new-dataset":0.1759715406,"dev-research":0.3190895322,"prompt-eng":0.4585405058,"data-quality":0.0858113761,"ml-security":0.1911451187}}
{"text":"Our approach is based on generating skill-specific interventions and measuring changes in agent predictions.","meta":{"url":"http://arxiv.org/abs/2307.10790v1"},"cats":{"new-dataset":0.0384097647,"dev-research":0.312513422,"prompt-eng":0.4579860842,"data-quality":0.1094795126,"ml-security":0.1897116849}}
{"text":"We present a detailed case study analyzing the behavior of a recent agent and then compare multiple agents in terms of skill-specific competency scores.","meta":{"url":"http://arxiv.org/abs/2307.10790v1"},"cats":{"new-dataset":0.0552181297,"dev-research":0.2561461177,"prompt-eng":0.4182868319,"data-quality":0.1015730696,"ml-security":0.1381943229}}
{"text":"This analysis suggests that biases from training have lasting effects on agent behavior and that existing models are able to ground simple referring expressions.","meta":{"url":"http://arxiv.org/abs/2307.10790v1"},"cats":{"new-dataset":0.0314104512,"dev-research":0.2780252599,"prompt-eng":0.3754223482,"data-quality":0.2808017567,"ml-security":0.2822821838}}
{"text":"Our comparisons between models show that skill-specific scores correlate with improvements in overall VLN task performance.","meta":{"url":"http://arxiv.org/abs/2307.10790v1"},"cats":{"new-dataset":0.0155379316,"dev-research":0.3407798895,"prompt-eng":0.4382115735,"data-quality":0.1619155366,"ml-security":0.1713692587}}
{"text":"Mixtures of classifiers (a.k.a. randomized ensembles) have been proposed as a way to improve robustness against adversarial attacks.","meta":{"url":"http://arxiv.org/abs/2307.10788v1"},"cats":{"new-dataset":0.0216360832,"dev-research":0.2657405768,"prompt-eng":0.39949212,"data-quality":0.4046658371,"ml-security":0.8328446632}}
{"text":"However, it has been shown that existing attacks are not well suited for this kind of classifiers.","meta":{"url":"http://arxiv.org/abs/2307.10788v1"},"cats":{"new-dataset":0.0143418144,"dev-research":0.2501843595,"prompt-eng":0.4122356254,"data-quality":0.3637325631,"ml-security":0.7660258502}}
{"text":"In this paper, we discuss the problem of attacking a mixture in a principled way and introduce two desirable properties of attacks based on a geometrical analysis of the problem (effectiveness and maximality).","meta":{"url":"http://arxiv.org/abs/2307.10788v1"},"cats":{"new-dataset":0.0206625433,"dev-research":0.2866800455,"prompt-eng":0.369006498,"data-quality":0.1773374507,"ml-security":0.6304325283}}
{"text":"We then show that existing attacks do not meet both of these properties.","meta":{"url":"http://arxiv.org/abs/2307.10788v1"},"cats":{"new-dataset":0.0248439883,"dev-research":0.2532708514,"prompt-eng":0.3815816518,"data-quality":0.1719589149,"ml-security":0.5501314011}}
{"text":"Finally, we introduce a new attack called lattice climber attack with theoretical guarantees on the binary linear setting, and we demonstrate its performance by conducting experiments on synthetic and real datasets.","meta":{"url":"http://arxiv.org/abs/2307.10788v1"},"cats":{"new-dataset":0.1886890724,"dev-research":0.2418009899,"prompt-eng":0.3929390812,"data-quality":0.1743510661,"ml-security":0.5916802774}}
{"text":"Source-free domain adaptation has become popular because of its practical usefulness and no need to access source data.","meta":{"url":"http://arxiv.org/abs/2307.10787v1"},"cats":{"new-dataset":0.1182700045,"dev-research":0.2916230741,"prompt-eng":0.3569078793,"data-quality":0.2146826079,"ml-security":0.2028323242}}
{"text":"However, the adaptation process still takes a considerable amount of time and is predominantly based on optimization that relies on back-propagation.","meta":{"url":"http://arxiv.org/abs/2307.10787v1"},"cats":{"new-dataset":0.0041787944,"dev-research":0.2523054564,"prompt-eng":0.3889629597,"data-quality":0.0890068588,"ml-security":0.0970083104}}
{"text":"In this work we present a simple feed-forward approach that challenges the need for back-propagation based adaptation.","meta":{"url":"http://arxiv.org/abs/2307.10787v1"},"cats":{"new-dataset":0.054505187,"dev-research":0.262735802,"prompt-eng":0.4923090863,"data-quality":0.2794178837,"ml-security":0.13196454}}
{"text":"Our approach is based on computing prototypes of classes under the domain shift using a pre-trained model.","meta":{"url":"http://arxiv.org/abs/2307.10787v1"},"cats":{"new-dataset":0.1613523553,"dev-research":0.2684763239,"prompt-eng":0.4874483353,"data-quality":0.2443153154,"ml-security":0.1406926019}}
{"text":"It achieves strong improvements in accuracy compared to the pre-trained model and requires only a small fraction of time of existing domain adaptation methods.","meta":{"url":"http://arxiv.org/abs/2307.10787v1"},"cats":{"new-dataset":0.0226184776,"dev-research":0.2519302951,"prompt-eng":0.405044679,"data-quality":0.2118088575,"ml-security":0.1231988412}}
{"text":"The 4D Millimeter wave (mmWave) radar is a promising technology for vehicle sensing due to its cost-effectiveness and operability in adverse weather conditions.","meta":{"url":"http://arxiv.org/abs/2307.10784v1"},"cats":{"new-dataset":0.0285752118,"dev-research":0.2622119362,"prompt-eng":0.3181375407,"data-quality":0.0786191149,"ml-security":0.0903851553}}
{"text":"However, the adoption of this technology has been hindered by sparsity and noise issues in radar point cloud data.","meta":{"url":"http://arxiv.org/abs/2307.10784v1"},"cats":{"new-dataset":0.0283915871,"dev-research":0.2740853053,"prompt-eng":0.3427873449,"data-quality":0.2561833516,"ml-security":0.1277999285}}
{"text":"This paper introduces spatial multi-representation fusion (SMURF), a novel approach to 3D object detection using a single 4D imaging radar.","meta":{"url":"http://arxiv.org/abs/2307.10784v1"},"cats":{"new-dataset":0.0836230007,"dev-research":0.2298666731,"prompt-eng":0.3579569075,"data-quality":0.1192011368,"ml-security":0.0789799454}}
{"text":"SMURF leverages multiple representations of radar detection points, including pillarization and density features of a multi-dimensional Gaussian mixture distribution through kernel density estimation (KDE).","meta":{"url":"http://arxiv.org/abs/2307.10784v1"},"cats":{"new-dataset":0.0519881383,"dev-research":0.2288744678,"prompt-eng":0.3888842718,"data-quality":0.183096828,"ml-security":0.1154830557}}
{"text":"KDE effectively mitigates measurement inaccuracy caused by limited angular resolution and multi-path propagation of radar signals.","meta":{"url":"http://arxiv.org/abs/2307.10784v1"},"cats":{"new-dataset":0.0132471111,"dev-research":0.3268312218,"prompt-eng":0.3732514562,"data-quality":0.194514865,"ml-security":0.1464112875}}
{"text":"Additionally, KDE helps alleviate point cloud sparsity by capturing density features.","meta":{"url":"http://arxiv.org/abs/2307.10784v1"},"cats":{"new-dataset":0.0393491377,"dev-research":0.3063947634,"prompt-eng":0.3868671268,"data-quality":0.1294683287,"ml-security":0.0877441187}}
{"text":"Experimental evaluations on View-of-Delft (VoD) and TJ4DRadSet datasets demonstrate the effectiveness and generalization ability of SMURF, outperforming recently proposed 4D imaging radar-based single-representation models.","meta":{"url":"http://arxiv.org/abs/2307.10784v1"},"cats":{"new-dataset":0.1422965031,"dev-research":0.200554408,"prompt-eng":0.3533727061,"data-quality":0.1306099051,"ml-security":0.0794415066}}
{"text":"Moreover, while using 4D imaging radar only, SMURF still achieves comparable performance to the state-of-the-art 4D imaging radar and camera fusion-based method, with an increase of 1.22% in the mean average precision on bird's-eye view of TJ4DRadSet dataset and 1.32% in the 3D mean average precision on the entire annotated area of VoD dataset.","meta":{"url":"http://arxiv.org/abs/2307.10784v1"},"cats":{"new-dataset":0.1307542403,"dev-research":0.2329406662,"prompt-eng":0.3530401934,"data-quality":0.1419727735,"ml-security":0.050671266}}
{"text":"Our proposed method demonstrates impressive inference time and addresses the challenges of real-time detection, with the inference time no more than 0.05 seconds for most scans on both datasets.","meta":{"url":"http://arxiv.org/abs/2307.10784v1"},"cats":{"new-dataset":0.3880692835,"dev-research":0.2606672027,"prompt-eng":0.4378276282,"data-quality":0.2509416001,"ml-security":0.1724969602}}
{"text":"This research highlights the benefits of 4D mmWave radar and is a strong benchmark for subsequent works regarding 3D object detection with 4D imaging radar.","meta":{"url":"http://arxiv.org/abs/2307.10784v1"},"cats":{"new-dataset":0.0595254038,"dev-research":0.2486134913,"prompt-eng":0.3481797403,"data-quality":0.104465141,"ml-security":0.0740987489}}
{"text":"Zero-shot point cloud segmentation aims to make deep models capable of recognizing novel objects in point cloud that are unseen in the training phase.","meta":{"url":"http://arxiv.org/abs/2307.10782v1"},"cats":{"new-dataset":0.2027244076,"dev-research":0.1522432113,"prompt-eng":0.336316561,"data-quality":0.2226086909,"ml-security":0.2150946906}}
{"text":"Recent trends favor the pipeline which transfers knowledge from seen classes with labels to unseen classes without labels.","meta":{"url":"http://arxiv.org/abs/2307.10782v1"},"cats":{"new-dataset":0.1400261027,"dev-research":0.2231337561,"prompt-eng":0.3871595561,"data-quality":0.3603165137,"ml-security":0.1821644311}}
{"text":"They typically align visual features with semantic features obtained from word embedding by the supervision of seen classes' annotations.","meta":{"url":"http://arxiv.org/abs/2307.10782v1"},"cats":{"new-dataset":0.0832217718,"dev-research":0.3563637121,"prompt-eng":0.3870820183,"data-quality":0.3649752089,"ml-security":0.0581090428}}
{"text":"However, point cloud contains limited information to fully match with semantic features.","meta":{"url":"http://arxiv.org/abs/2307.10782v1"},"cats":{"new-dataset":0.0657353324,"dev-research":0.2243080362,"prompt-eng":0.3211465705,"data-quality":0.1915783485,"ml-security":0.1001565011}}
{"text":"In fact, the rich appearance information of images is a natural complement to the textureless point cloud, which is not well explored in previous literature.","meta":{"url":"http://arxiv.org/abs/2307.10782v1"},"cats":{"new-dataset":0.1197592274,"dev-research":0.1590478874,"prompt-eng":0.3522297497,"data-quality":0.1882625369,"ml-security":0.1157411961}}
{"text":"Motivated by this, we propose a novel multi-modal zero-shot learning method to better utilize the complementary information of point clouds and images for more accurate visual-semantic alignment.","meta":{"url":"http://arxiv.org/abs/2307.10782v1"},"cats":{"new-dataset":0.2823085173,"dev-research":0.1941631493,"prompt-eng":0.3299897654,"data-quality":0.2440826943,"ml-security":0.0799751044}}
{"text":"Extensive experiments are performed in two popular benchmarks, i.e., SemanticKITTI and nuScenes, and our method outperforms current SOTA methods with 52% and 49% improvement on average for unseen class mIoU, respectively.","meta":{"url":"http://arxiv.org/abs/2307.10782v1"},"cats":{"new-dataset":0.0848411629,"dev-research":0.2476153572,"prompt-eng":0.3880318543,"data-quality":0.321135383,"ml-security":0.0765519735}}
{"text":"5G non-public networks (NPNs) play a key role in enabling critical Industrial Internet of Things (IoT) applications in various vertical industries.","meta":{"url":"http://arxiv.org/abs/2307.10781v1"},"cats":{"new-dataset":0.1156026419,"dev-research":0.2009920488,"prompt-eng":0.2599475668,"data-quality":0.0526636827,"ml-security":0.1472257559}}
{"text":"Among other features, 5G NPNs enable novel operation models, where the roles and responsibilities for setting up and operating the network can be distributed among several stakeholders, i.e., among the public mobile network operators (MNOs), the industrial party who uses the 5G NPN services and 3rd parties.","meta":{"url":"http://arxiv.org/abs/2307.10781v1"},"cats":{"new-dataset":0.0445465867,"dev-research":0.2431418342,"prompt-eng":0.2751621907,"data-quality":0.0612409286,"ml-security":0.1233318158}}
{"text":"This results in many theoretically feasible operation models for 5G NPN, each with its own advantages and disadvantages.","meta":{"url":"http://arxiv.org/abs/2307.10781v1"},"cats":{"new-dataset":0.0208066917,"dev-research":0.1733085883,"prompt-eng":0.3589061624,"data-quality":0.0580323829,"ml-security":0.1449888473}}
{"text":"We investigate the resulting operation models and identify a set of nine prime models taking into account today's practical considerations.","meta":{"url":"http://arxiv.org/abs/2307.10781v1"},"cats":{"new-dataset":0.0818400071,"dev-research":0.1750370566,"prompt-eng":0.4393251298,"data-quality":0.082742837,"ml-security":0.1527794989}}
{"text":"Additionally, we define a framework to qualitatively analyze the operation models and use it to evaluate and compare the identified operation models.","meta":{"url":"http://arxiv.org/abs/2307.10781v1"},"cats":{"new-dataset":0.0592198982,"dev-research":0.3377717426,"prompt-eng":0.4335664919,"data-quality":0.121550851,"ml-security":0.1290740056}}
{"text":"Vision transformers have demonstrated remarkable success in a wide range of computer vision tasks over the last years.","meta":{"url":"http://arxiv.org/abs/2307.10780v1"},"cats":{"new-dataset":0.0735608233,"dev-research":0.2565877455,"prompt-eng":0.3980140389,"data-quality":0.0939454628,"ml-security":0.0713052004}}
{"text":"However, their high computational costs remain a significant barrier to their practical deployment.","meta":{"url":"http://arxiv.org/abs/2307.10780v1"},"cats":{"new-dataset":0.0681178015,"dev-research":0.3207086134,"prompt-eng":0.3472331617,"data-quality":0.0762730414,"ml-security":0.1539762262}}
{"text":"In particular, the complexity of transformer models is quadratic with respect to the number of input tokens.","meta":{"url":"http://arxiv.org/abs/2307.10780v1"},"cats":{"new-dataset":0.005894864,"dev-research":0.2042135009,"prompt-eng":0.3444299025,"data-quality":0.0991345667,"ml-security":0.1191452019}}
{"text":"Therefore techniques that reduce the number of input tokens that need to be processed have been proposed.","meta":{"url":"http://arxiv.org/abs/2307.10780v1"},"cats":{"new-dataset":0.0160522192,"dev-research":0.3263759814,"prompt-eng":0.4695969485,"data-quality":0.2019928684,"ml-security":0.1545438793}}
{"text":"This paper introduces Learned Thresholds token Merging and Pruning (LTMP), a novel approach that leverages the strengths of both token merging and token pruning.","meta":{"url":"http://arxiv.org/abs/2307.10780v1"},"cats":{"new-dataset":0.1093736869,"dev-research":0.3116043797,"prompt-eng":0.4499861589,"data-quality":0.3568378338,"ml-security":0.2177307346}}
{"text":"LTMP uses learned threshold masking modules that dynamically determine which tokens to merge and which to prune.","meta":{"url":"http://arxiv.org/abs/2307.10780v1"},"cats":{"new-dataset":0.0394056324,"dev-research":0.2805239009,"prompt-eng":0.4230062336,"data-quality":0.2643406566,"ml-security":0.1518790257}}
{"text":"We demonstrate our approach with extensive experiments on vision transformers on the ImageNet classification task.","meta":{"url":"http://arxiv.org/abs/2307.10780v1"},"cats":{"new-dataset":0.071268519,"dev-research":0.2254122782,"prompt-eng":0.4105488858,"data-quality":0.3082663015,"ml-security":0.2164132609}}
{"text":"Our results demonstrate that LTMP achieves state-of-the-art accuracy across reduction rates while requiring only a single fine-tuning epoch, which is an order of magnitude faster than previous methods.","meta":{"url":"http://arxiv.org/abs/2307.10780v1"},"cats":{"new-dataset":0.0806467074,"dev-research":0.2896199422,"prompt-eng":0.4168491034,"data-quality":0.2407429654,"ml-security":0.1041245531}}
{"text":"Code is available at https://github.com/Mxbonn/ltmp .","meta":{"url":"http://arxiv.org/abs/2307.10780v1"},"cats":{"new-dataset":0.3465228606,"dev-research":0.2912242185,"prompt-eng":0.4428098406,"data-quality":0.1523655852,"ml-security":0.071854057}}
{"text":"Beam Tree Recursive Neural Network (BT-RvNN) was recently proposed as a simple extension of Gumbel Tree RvNN and it was shown to achieve state-of-the-art length generalization performance in ListOps while maintaining comparable performance on other tasks.","meta":{"url":"http://arxiv.org/abs/2307.10779v1"},"cats":{"new-dataset":0.1532014227,"dev-research":0.2570919609,"prompt-eng":0.3613534664,"data-quality":0.143697903,"ml-security":0.1118584443}}
{"text":"However, although not the worst in its kind, BT-RvNN can be still exorbitantly expensive in memory usage.","meta":{"url":"http://arxiv.org/abs/2307.10779v1"},"cats":{"new-dataset":0.0287356081,"dev-research":0.3265063825,"prompt-eng":0.3469445995,"data-quality":0.1751638018,"ml-security":0.1616810068}}
{"text":"In this paper, we identify the main bottleneck in BT-RvNN's memory usage to be the entanglement of the scorer function and the recursive cell function.","meta":{"url":"http://arxiv.org/abs/2307.10779v1"},"cats":{"new-dataset":0.069557158,"dev-research":0.2403909475,"prompt-eng":0.3710956588,"data-quality":0.1076249296,"ml-security":0.1257427696}}
{"text":"We propose strategies to remove this bottleneck and further simplify its memory usage.","meta":{"url":"http://arxiv.org/abs/2307.10779v1"},"cats":{"new-dataset":0.0694282303,"dev-research":0.3805723555,"prompt-eng":0.4055773992,"data-quality":0.1729057899,"ml-security":0.1695530396}}
{"text":"Overall, our strategies not only reduce the memory usage of BT-RvNN by $10$-$16$ times but also create a new state-of-the-art in ListOps while maintaining similar performance in other tasks.","meta":{"url":"http://arxiv.org/abs/2307.10779v1"},"cats":{"new-dataset":0.1410128065,"dev-research":0.3307486619,"prompt-eng":0.3859548154,"data-quality":0.1084422985,"ml-security":0.1111325718}}
{"text":"In addition, we also propose a strategy to utilize the induced latent-tree node representations produced by BT-RvNN to turn BT-RvNN from a sentence encoder of the form $f:\\mathbb{R}^{n \\times d} \\rightarrow \\mathbb{R}^{d}$ into a sequence contextualizer of the form $f:\\mathbb{R}^{n \\times d} \\rightarrow \\mathbb{R}^{n \\times d}$. Thus, our proposals not only open up a path for further scalability of RvNNs","meta":{"url":"http://arxiv.org/abs/2307.10779v1"},"cats":{"new-dataset":0.1551687924,"dev-research":0.2065683231,"prompt-eng":0.3522492637,"data-quality":0.2304178109,"ml-security":0.113829857}}
{"text":"but also standardize a way to use BT-RvNNs as another building block in the deep learning toolkit that can be easily stacked or interfaced with other popular models such as Transformers and Structured State Space models.","meta":{"url":"http://arxiv.org/abs/2307.10779v1"},"cats":{"new-dataset":0.0976715137,"dev-research":0.2166666395,"prompt-eng":0.361249455,"data-quality":0.1079572731,"ml-security":0.1694471622}}
{"text":"Online job ads serve as a valuable source of information for skill requirements, playing a crucial role in labor market analysis and e-recruitment processes.","meta":{"url":"http://arxiv.org/abs/2307.10778v1"},"cats":{"new-dataset":0.0218310009,"dev-research":0.2630884184,"prompt-eng":0.3291509112,"data-quality":0.1331562737,"ml-security":0.1777746418}}
{"text":"Since such ads are typically formatted in free text, natural language processing (NLP) technologies are required to automatically process them.","meta":{"url":"http://arxiv.org/abs/2307.10778v1"},"cats":{"new-dataset":0.0390071167,"dev-research":0.3485688693,"prompt-eng":0.3841458156,"data-quality":0.3841736139,"ml-security":0.1142423728}}
{"text":"We specifically focus on the task of detecting skills (mentioned literally, or implicitly described) and linking them to a large skill ontology, making it a challenging case of extreme multi-label classification (XMLC).","meta":{"url":"http://arxiv.org/abs/2307.10778v1"},"cats":{"new-dataset":0.1010471233,"dev-research":0.2961662324,"prompt-eng":0.4204820589,"data-quality":0.3571222321,"ml-security":0.1497301205}}
{"text":"Given that there is no sizable labeled (training) dataset are available for this specific XMLC task, we propose techniques to leverage general Large Language Models (LLMs).","meta":{"url":"http://arxiv.org/abs/2307.10778v1"},"cats":{"new-dataset":0.4968470477,"dev-research":0.2113726087,"prompt-eng":0.4263136324,"data-quality":0.3192923187,"ml-security":0.1009595673}}
{"text":"We describe a cost-effective approach to generate an accurate, fully synthetic labeled dataset for skill extraction, and present a contrastive learning strategy that proves effective in the task.","meta":{"url":"http://arxiv.org/abs/2307.10778v1"},"cats":{"new-dataset":0.5795981242,"dev-research":0.3238197906,"prompt-eng":0.4005611479,"data-quality":0.3301504618,"ml-security":0.1314680889}}
{"text":"Our results across three skill extraction benchmarks show a consistent increase of between 15 to 25 percentage points in \\textit{R-Precision@5} compared to previously published results that relied solely on distant supervision through literal matches.","meta":{"url":"http://arxiv.org/abs/2307.10778v1"},"cats":{"new-dataset":0.1639770803,"dev-research":0.3015303903,"prompt-eng":0.4369663032,"data-quality":0.3730870933,"ml-security":0.0922188289}}
{"text":"Neural Radiance Fields (NeRFs) have achieved great success in the past few years.","meta":{"url":"http://arxiv.org/abs/2307.10776v1"},"cats":{"new-dataset":0.0497277378,"dev-research":0.2286065425,"prompt-eng":0.3726390317,"data-quality":0.1730349089,"ml-security":0.1569781719}}
{"text":"However, most current methods still require intensive resources due to ray marching-based rendering.","meta":{"url":"http://arxiv.org/abs/2307.10776v1"},"cats":{"new-dataset":0.0340460799,"dev-research":0.2752127558,"prompt-eng":0.3506877932,"data-quality":0.0819150124,"ml-security":0.0633953468}}
{"text":"To construct urban-level radiance fields efficiently, we design Deformable Neural Mesh Primitive~(DNMP), and propose to parameterize the entire scene with such primitives.","meta":{"url":"http://arxiv.org/abs/2307.10776v1"},"cats":{"new-dataset":0.3535765337,"dev-research":0.2220821372,"prompt-eng":0.4162656399,"data-quality":0.0866516014,"ml-security":0.1214565782}}
{"text":"The DNMP is a flexible and compact neural variant of classic mesh representation, which enjoys both the efficiency of rasterization-based rendering and the powerful neural representation capability for photo-realistic image synthesis.","meta":{"url":"http://arxiv.org/abs/2307.10776v1"},"cats":{"new-dataset":0.1512136168,"dev-research":0.2467041021,"prompt-eng":0.3575525316,"data-quality":0.094470311,"ml-security":0.100760812}}
{"text":"Specifically, a DNMP consists of a set of connected deformable mesh vertices with paired vertex features to parameterize the geometry and radiance information of a local area.","meta":{"url":"http://arxiv.org/abs/2307.10776v1"},"cats":{"new-dataset":0.1046427608,"dev-research":0.308494977,"prompt-eng":0.3906476302,"data-quality":0.0984739878,"ml-security":0.1044176161}}
{"text":"To constrain the degree of freedom for optimization and lower the storage budgets, we enforce the shape of each primitive to be decoded from a relatively low-dimensional latent space.","meta":{"url":"http://arxiv.org/abs/2307.10776v1"},"cats":{"new-dataset":0.0425543698,"dev-research":0.1902552553,"prompt-eng":0.4136895608,"data-quality":0.1470421921,"ml-security":0.1949259031}}
{"text":"The rendering colors are decoded from the vertex features (interpolated with rasterization) by a view-dependent MLP.","meta":{"url":"http://arxiv.org/abs/2307.10776v1"},"cats":{"new-dataset":0.1266581585,"dev-research":0.2575504961,"prompt-eng":0.3722305356,"data-quality":0.1696492938,"ml-security":0.1105595115}}
{"text":"The DNMP provides a new paradigm for urban-level scene representation with appealing properties: $(1)$ High-quality rendering.","meta":{"url":"http://arxiv.org/abs/2307.10776v1"},"cats":{"new-dataset":0.332805517,"dev-research":0.2724897676,"prompt-eng":0.4198673288,"data-quality":0.1353884402,"ml-security":0.0773070509}}
{"text":"Our method achieves leading performance for novel view synthesis in urban scenarios.","meta":{"url":"http://arxiv.org/abs/2307.10776v1"},"cats":{"new-dataset":0.2676371091,"dev-research":0.3429309391,"prompt-eng":0.4012943666,"data-quality":0.1131906931,"ml-security":0.0735620366}}
{"text":"$(2)$ Low computational costs.","meta":{"url":"http://arxiv.org/abs/2307.10776v1"},"cats":{"new-dataset":0.0189212474,"dev-research":0.3058584693,"prompt-eng":0.345275112,"data-quality":0.1081748076,"ml-security":0.0942151302}}
{"text":"Our representation enables fast rendering (2.07ms/1k pixels) and low peak memory usage (110MB/1k pixels).","meta":{"url":"http://arxiv.org/abs/2307.10776v1"},"cats":{"new-dataset":0.2109255592,"dev-research":0.2737563953,"prompt-eng":0.4062907167,"data-quality":0.111189416,"ml-security":0.0616658764}}
{"text":"We also present a lightweight version that can run 33$\\times$ faster than vanilla NeRFs, and comparable to the highly-optimized Instant-NGP (0.61 vs 0.71ms/1k pixels).","meta":{"url":"http://arxiv.org/abs/2307.10776v1"},"cats":{"new-dataset":0.1435975833,"dev-research":0.2890471006,"prompt-eng":0.4197489443,"data-quality":0.1555600808,"ml-security":0.1400075749}}
{"text":"Project page: \\href{https://dnmp.github.io/}{https://dnmp.github.io/}.","meta":{"url":"http://arxiv.org/abs/2307.10776v1"},"cats":{"new-dataset":0.3744751555,"dev-research":0.34814948,"prompt-eng":0.3784482479,"data-quality":0.1481347355,"ml-security":0.0871268402}}
{"text":"Background.","meta":{"url":"http://arxiv.org/abs/2307.10774v1"},"cats":{"new-dataset":0.1610608623,"dev-research":0.2283952409,"prompt-eng":0.3687166144,"data-quality":0.1362213775,"ml-security":0.1338991711}}
{"text":"Due to the widespread adoption of Artificial Intelligence (AI) and Machine Learning (ML) for building software applications, companies are struggling to recruit employees with a deep understanding of such technologies.","meta":{"url":"http://arxiv.org/abs/2307.10774v1"},"cats":{"new-dataset":0.0301545184,"dev-research":0.4184105562,"prompt-eng":0.3478296497,"data-quality":0.1579875273,"ml-security":0.3159606004}}
{"text":"In this scenario, AutoML is soaring as a promising solution to fill the AI/ML skills gap since it promises to automate the building of end-to-end AI/ML pipelines that would normally be engineered by specialized team members.","meta":{"url":"http://arxiv.org/abs/2307.10774v1"},"cats":{"new-dataset":0.0352089195,"dev-research":0.3457300681,"prompt-eng":0.4504726394,"data-quality":0.1640021244,"ml-security":0.1525906382}}
{"text":"Aims.","meta":{"url":"http://arxiv.org/abs/2307.10774v1"},"cats":{"new-dataset":0.0911242921,"dev-research":0.229026563,"prompt-eng":0.4260756753,"data-quality":0.0867767481,"ml-security":0.0822051447}}
{"text":"Despite the growing interest and high expectations, there is a dearth of information about the extent to which AutoML is currently adopted by teams developing AI/ML-enabled systems and how it is perceived by practitioners and researchers.","meta":{"url":"http://arxiv.org/abs/2307.10774v1"},"cats":{"new-dataset":0.067591237,"dev-research":0.3937804825,"prompt-eng":0.4367739202,"data-quality":0.1812023759,"ml-security":0.1582665658}}
{"text":"Method.","meta":{"url":"http://arxiv.org/abs/2307.10774v1"},"cats":{"new-dataset":0.0138913176,"dev-research":0.3305971086,"prompt-eng":0.345395859,"data-quality":0.1456224532,"ml-security":0.1330542782}}
{"text":"To fill these gaps, in this paper, we present a mixed-method study comprising a benchmark of 12 end-to-end AutoML tools on two SE datasets and a user survey with follow-up interviews to further our understanding of AutoML adoption and perception.","meta":{"url":"http://arxiv.org/abs/2307.10774v1"},"cats":{"new-dataset":0.1815594158,"dev-research":0.3861641222,"prompt-eng":0.466942562,"data-quality":0.3107016583,"ml-security":0.0718535824}}
{"text":"Results.","meta":{"url":"http://arxiv.org/abs/2307.10774v1"},"cats":{"new-dataset":0.0535622775,"dev-research":0.2269063715,"prompt-eng":0.3419133783,"data-quality":0.1951561846,"ml-security":0.0862000603}}
{"text":"We found that AutoML solutions can generate models that outperform those trained and optimized by researchers to perform classification tasks in the SE domain.","meta":{"url":"http://arxiv.org/abs/2307.10774v1"},"cats":{"new-dataset":0.054883874,"dev-research":0.3043653031,"prompt-eng":0.4899951179,"data-quality":0.2940367747,"ml-security":0.1555121361}}
{"text":"Also, our findings show that the currently available AutoML solutions do not live up to their names as they do not equally support automation across the stages of the ML development workflow and for all the team members.","meta":{"url":"http://arxiv.org/abs/2307.10774v1"},"cats":{"new-dataset":0.1653441777,"dev-research":0.3519592225,"prompt-eng":0.4720455049,"data-quality":0.2168228901,"ml-security":0.1086468133}}
{"text":"Conclusions.","meta":{"url":"http://arxiv.org/abs/2307.10774v1"},"cats":{"new-dataset":0.0264692782,"dev-research":0.3319609105,"prompt-eng":0.3386059229,"data-quality":0.1573787015,"ml-security":0.098661385}}
{"text":"We derive insights to inform the SE research community on how AutoML can facilitate their activities and tool builders on how to design the next generation of AutoML technologies.","meta":{"url":"http://arxiv.org/abs/2307.10774v1"},"cats":{"new-dataset":0.06691122,"dev-research":0.4441326478,"prompt-eng":0.479623431,"data-quality":0.1806482304,"ml-security":0.0777424091}}
{"text":"Music recommendation systems have emerged as a vital component to enhance user experience and satisfaction for the music streaming services, which dominates music consumption.","meta":{"url":"http://arxiv.org/abs/2307.10773v1"},"cats":{"new-dataset":0.0108215131,"dev-research":0.2852700457,"prompt-eng":0.358863657,"data-quality":0.175204769,"ml-security":0.0983538602}}
{"text":"The key challenge in improving these recommender systems lies in comprehending the complexity of music data, specifically for the underpinning music genre classification.","meta":{"url":"http://arxiv.org/abs/2307.10773v1"},"cats":{"new-dataset":0.0412656431,"dev-research":0.2929315683,"prompt-eng":0.3357985609,"data-quality":0.3364096619,"ml-security":0.1460136576}}
{"text":"The limitations of manual genre classification have highlighted the need for a more advanced system, namely the Automatic Music Genre Classification (AMGC) system.","meta":{"url":"http://arxiv.org/abs/2307.10773v1"},"cats":{"new-dataset":0.0616997398,"dev-research":0.2595023,"prompt-eng":0.3683404615,"data-quality":0.2973090701,"ml-security":0.0774317412}}
{"text":"While traditional machine learning techniques have shown potential in genre classification, they heavily rely on manually engineered features and feature selection, failing to capture the full complexity of music data.","meta":{"url":"http://arxiv.org/abs/2307.10773v1"},"cats":{"new-dataset":0.024224191,"dev-research":0.3251172588,"prompt-eng":0.3537189281,"data-quality":0.3857661942,"ml-security":0.1808311178}}
{"text":"On the other hand, deep learning classification architectures like the traditional Convolutional Neural Networks (CNN) are effective in capturing the spatial hierarchies but struggle to capture the temporal dynamics inherent in music data.","meta":{"url":"http://arxiv.org/abs/2307.10773v1"},"cats":{"new-dataset":0.1128385147,"dev-research":0.1988596378,"prompt-eng":0.2713108786,"data-quality":0.2123086212,"ml-security":0.1747463919}}
{"text":"To address these challenges, this study proposes a novel approach using visual spectrograms as input, and propose a hybrid model that combines the strength of the Residual neural Network (ResNet) and the Gated Recurrent Unit (GRU).","meta":{"url":"http://arxiv.org/abs/2307.10773v1"},"cats":{"new-dataset":0.2379872274,"dev-research":0.286564689,"prompt-eng":0.3987049408,"data-quality":0.2308277264,"ml-security":0.1381803754}}
{"text":"This model is designed to provide a more comprehensive analysis of music data, offering the potential to improve the music recommender systems through achieving a more comprehensive analysis of music data and hence potentially more accurate genre classification.","meta":{"url":"http://arxiv.org/abs/2307.10773v1"},"cats":{"new-dataset":0.1248516266,"dev-research":0.2180934985,"prompt-eng":0.3683225163,"data-quality":0.2734197348,"ml-security":0.1061476871}}
{"text":"Existing action recognition methods are typically actor-specific due to the intrinsic topological and apparent differences among the actors.","meta":{"url":"http://arxiv.org/abs/2307.10763v1"},"cats":{"new-dataset":0.0346075935,"dev-research":0.2152713691,"prompt-eng":0.3393490328,"data-quality":0.165120312,"ml-security":0.0856419171}}
{"text":"This requires actor-specific pose estimation (e.g., humans vs. animals), leading to cumbersome model design complexity and high maintenance costs.","meta":{"url":"http://arxiv.org/abs/2307.10763v1"},"cats":{"new-dataset":0.0560147094,"dev-research":0.226859892,"prompt-eng":0.3845367926,"data-quality":0.0600793295,"ml-security":0.0743486311}}
{"text":"Moreover, they often focus on learning the visual modality alone and single-label classification whilst neglecting other available information sources (e.g., class name text) and the concurrent occurrence of multiple actions.","meta":{"url":"http://arxiv.org/abs/2307.10763v1"},"cats":{"new-dataset":0.0400923639,"dev-research":0.2987878623,"prompt-eng":0.3554313473,"data-quality":0.3186763305,"ml-security":0.1369794813}}
{"text":"To overcome these limitations, we propose a new approach called 'actor-agnostic multi-modal multi-label action recognition,' which offers a unified solution for various types of actors, including humans and animals.","meta":{"url":"http://arxiv.org/abs/2307.10763v1"},"cats":{"new-dataset":0.2193964686,"dev-research":0.1779365939,"prompt-eng":0.4024751428,"data-quality":0.2591659634,"ml-security":0.0756935073}}
{"text":"We further formulate a novel Multi-modal Semantic Query Network (MSQNet) model in a transformer-based object detection framework (e.g., DETR), characterized by leveraging visual and textual modalities to represent the action classes better.","meta":{"url":"http://arxiv.org/abs/2307.10763v1"},"cats":{"new-dataset":0.0732143674,"dev-research":0.2304936385,"prompt-eng":0.4142722018,"data-quality":0.2263523081,"ml-security":0.1776492883}}
{"text":"The elimination of actor-specific model designs is a key advantage, as it removes the need for actor pose estimation altogether.","meta":{"url":"http://arxiv.org/abs/2307.10763v1"},"cats":{"new-dataset":0.0103174861,"dev-research":0.2676957341,"prompt-eng":0.3049970156,"data-quality":0.0601157961,"ml-security":0.1360640975}}
{"text":"Extensive experiments on five publicly available benchmarks show that our MSQNet consistently outperforms the prior arts of actor-specific alternatives on human and animal single- and multi-label action recognition tasks by up to 50%.","meta":{"url":"http://arxiv.org/abs/2307.10763v1"},"cats":{"new-dataset":0.1200122877,"dev-research":0.2259404096,"prompt-eng":0.3966703461,"data-quality":0.2768329504,"ml-security":0.1047634598}}
{"text":"Code will be released at https://github.com/mondalanindya/MSQNet.","meta":{"url":"http://arxiv.org/abs/2307.10763v1"},"cats":{"new-dataset":0.4444498049,"dev-research":0.325718155,"prompt-eng":0.4194792983,"data-quality":0.1936562529,"ml-security":0.1210262236}}
{"text":"This paper presents a paradigm that adapts general large-scale pretrained models (PTMs) to speech emotion recognition task.","meta":{"url":"http://arxiv.org/abs/2307.10757v1"},"cats":{"new-dataset":0.137186078,"dev-research":0.1954680024,"prompt-eng":0.4000899098,"data-quality":0.1571238476,"ml-security":0.1502381973}}
{"text":"Although PTMs shed new light on artificial general intelligence, they are constructed with general tasks in mind, and thus, their efficacy for specific tasks can be further improved.","meta":{"url":"http://arxiv.org/abs/2307.10757v1"},"cats":{"new-dataset":0.0293605009,"dev-research":0.2806734133,"prompt-eng":0.4467281921,"data-quality":0.1228593631,"ml-security":0.1564656595}}
{"text":"Additionally, employing PTMs in practical applications can be challenging due to their considerable size.","meta":{"url":"http://arxiv.org/abs/2307.10757v1"},"cats":{"new-dataset":0.0203654947,"dev-research":0.3252846169,"prompt-eng":0.4327941234,"data-quality":0.1124021538,"ml-security":0.1438718302}}
{"text":"Above limitations spawn another research direction, namely, optimizing large-scale PTMs for specific tasks to generate task-specific PTMs that are both compact and effective.","meta":{"url":"http://arxiv.org/abs/2307.10757v1"},"cats":{"new-dataset":0.060805904,"dev-research":0.2608079477,"prompt-eng":0.4413806336,"data-quality":0.0790191208,"ml-security":0.0872626335}}
{"text":"In this paper, we focus on the speech emotion recognition task and propose an improved emotion-specific pretrained encoder called Vesper.","meta":{"url":"http://arxiv.org/abs/2307.10757v1"},"cats":{"new-dataset":0.1853649792,"dev-research":0.2292603884,"prompt-eng":0.3907067105,"data-quality":0.1524198845,"ml-security":0.0974843859}}
{"text":"Vesper is pretrained on a speech dataset based on WavLM and takes into account emotional characteristics.","meta":{"url":"http://arxiv.org/abs/2307.10757v1"},"cats":{"new-dataset":0.2208364621,"dev-research":0.2678752663,"prompt-eng":0.3714988207,"data-quality":0.1541110536,"ml-security":0.1046885252}}
{"text":"To enhance sensitivity to emotional information, Vesper employs an emotion-guided masking strategy to identify the regions that need masking.","meta":{"url":"http://arxiv.org/abs/2307.10757v1"},"cats":{"new-dataset":0.020460145,"dev-research":0.3055274486,"prompt-eng":0.412273761,"data-quality":0.1304487358,"ml-security":0.1743859111}}
{"text":"Subsequently, Vesper employs hierarchical and cross-layer self-supervision to improve its ability to capture acoustic and semantic representations, both of which are crucial for emotion recognition.","meta":{"url":"http://arxiv.org/abs/2307.10757v1"},"cats":{"new-dataset":0.0983108015,"dev-research":0.2628841347,"prompt-eng":0.3906535889,"data-quality":0.1728018412,"ml-security":0.0990890405}}
{"text":"Experimental results on the IEMOCAP, MELD, and CREMA-D datasets demonstrate that Vesper with 4 layers outperforms WavLM Base with 12 layers, and the performance of Vesper with 12 layers surpasses that of WavLM Large with 24 layers.","meta":{"url":"http://arxiv.org/abs/2307.10757v1"},"cats":{"new-dataset":0.2974469963,"dev-research":0.2449879273,"prompt-eng":0.3678859122,"data-quality":0.1352551621,"ml-security":0.1139315829}}
{"text":"One-class classification (OCC) aims to train a classifier only with the target class data and attracts great attention for its strong applicability in real-world application.","meta":{"url":"http://arxiv.org/abs/2307.10753v1"},"cats":{"new-dataset":0.0817381359,"dev-research":0.2846454179,"prompt-eng":0.3923805904,"data-quality":0.2739604093,"ml-security":0.2684409567}}
{"text":"Despite a lot of advances have been made in OCC, it still lacks the effective OCC loss functions for deep learning.","meta":{"url":"http://arxiv.org/abs/2307.10753v1"},"cats":{"new-dataset":0.0487290777,"dev-research":0.2375350123,"prompt-eng":0.3013457007,"data-quality":0.2133932875,"ml-security":0.1763299242}}
{"text":"In this paper, a novel logarithmic barrier function based OCC loss (LBL) that assigns large gradients to the margin samples and thus derives more compact hypersphere, is first proposed by approximating the OCC objective smoothly.","meta":{"url":"http://arxiv.org/abs/2307.10753v1"},"cats":{"new-dataset":0.0294348427,"dev-research":0.1838049975,"prompt-eng":0.34260669,"data-quality":0.1997911103,"ml-security":0.1785786557}}
{"text":"But the optimization of LBL may be instability especially when samples lie on the boundary leading to the infinity loss.","meta":{"url":"http://arxiv.org/abs/2307.10753v1"},"cats":{"new-dataset":0.0037658706,"dev-research":0.1754071203,"prompt-eng":0.3418784488,"data-quality":0.2447173169,"ml-security":0.2258991542}}
{"text":"To address this issue, then, a unilateral relaxation Sigmoid function is introduced into LBL and a novel OCC loss named LBLSig is proposed.","meta":{"url":"http://arxiv.org/abs/2307.10753v1"},"cats":{"new-dataset":0.0206474161,"dev-research":0.1997772645,"prompt-eng":0.4151615887,"data-quality":0.2194075564,"ml-security":0.122627713}}
{"text":"The LBLSig can be seen as the fusion of the mean square error (MSE) and the cross entropy (CE) and the optimization of LBLSig is smoother owing to the unilateral relaxation Sigmoid function.","meta":{"url":"http://arxiv.org/abs/2307.10753v1"},"cats":{"new-dataset":0.0064188543,"dev-research":0.2183185014,"prompt-eng":0.3556123264,"data-quality":0.1518637821,"ml-security":0.0842529953}}
{"text":"The effectiveness of the proposed LBL and LBLSig is experimentally demonstrated in comparisons with several state-of-the-art OCC algorithms on different network structures.","meta":{"url":"http://arxiv.org/abs/2307.10753v1"},"cats":{"new-dataset":0.0264412569,"dev-research":0.2589102592,"prompt-eng":0.3729435667,"data-quality":0.1690502252,"ml-security":0.1076038024}}
{"text":"The source code can be found at https://github.com/ML-HDU/LBL_LBLSig.","meta":{"url":"http://arxiv.org/abs/2307.10753v1"},"cats":{"new-dataset":0.3007289286,"dev-research":0.2679137638,"prompt-eng":0.416185779,"data-quality":0.2007952707,"ml-security":0.0828750868}}
{"text":"Artificial Intelligence (AI), and in particular generative models, are transformative tools for knowledge work.","meta":{"url":"http://arxiv.org/abs/2307.10751v1"},"cats":{"new-dataset":0.0445515039,"dev-research":0.2632653529,"prompt-eng":0.3630679043,"data-quality":0.108022751,"ml-security":0.0962932464}}
{"text":"They problematise notions of creativity, originality, plagiarism, the attribution of credit, and copyright ownership.","meta":{"url":"http://arxiv.org/abs/2307.10751v1"},"cats":{"new-dataset":0.1431366116,"dev-research":0.3398841189,"prompt-eng":0.3069037791,"data-quality":0.319927684,"ml-security":0.1712852543}}
{"text":"Critics of generative models emphasise the reliance on large amounts of training data, and view the output of these models as no more than randomised plagiarism, remix, or collage of the source data.","meta":{"url":"http://arxiv.org/abs/2307.10751v1"},"cats":{"new-dataset":0.095587896,"dev-research":0.2385095864,"prompt-eng":0.3484178695,"data-quality":0.2461198516,"ml-security":0.1913080545}}
{"text":"On these grounds, many have argued for stronger regulations on the deployment, use, and attribution of the output of these models.","meta":{"url":"http://arxiv.org/abs/2307.10751v1"},"cats":{"new-dataset":0.0473132011,"dev-research":0.2920739566,"prompt-eng":0.3470785241,"data-quality":0.1735559694,"ml-security":0.2982761219}}
{"text":"However, these issues are not new or unique to artificial intelligence.","meta":{"url":"http://arxiv.org/abs/2307.10751v1"},"cats":{"new-dataset":0.0481300678,"dev-research":0.3142287703,"prompt-eng":0.286773259,"data-quality":0.2482352333,"ml-security":0.1770507829}}
{"text":"In this position paper, using examples from literary criticism, the history of art, and copyright law, I show how creativity and originality resist definition as a notatable or information-theoretic property of an object, and instead can be seen as the property of a process, an author, or a viewer.","meta":{"url":"http://arxiv.org/abs/2307.10751v1"},"cats":{"new-dataset":0.0727481788,"dev-research":0.3247012613,"prompt-eng":0.3227325399,"data-quality":0.2415500089,"ml-security":0.1537791473}}
{"text":"Further alternative views hold that all creative work is essentially reuse (mostly without attribution), or that randomness itself can be creative.","meta":{"url":"http://arxiv.org/abs/2307.10751v1"},"cats":{"new-dataset":0.0390624888,"dev-research":0.3272843088,"prompt-eng":0.3219008454,"data-quality":0.1838384707,"ml-security":0.1562624302}}
{"text":"I suggest that creativity is ultimately defined by communities of creators and receivers, and the deemed sources of creativity in a workflow often depend on which parts of the workflow can be automated.","meta":{"url":"http://arxiv.org/abs/2307.10751v1"},"cats":{"new-dataset":0.0442613598,"dev-research":0.4948197614,"prompt-eng":0.385262499,"data-quality":0.204319011,"ml-security":0.074161809}}
{"text":"Using examples from recent studies of AI in creative knowledge work, I suggest that AI shifts knowledge work from material production to critical integration.","meta":{"url":"http://arxiv.org/abs/2307.10751v1"},"cats":{"new-dataset":0.0488865622,"dev-research":0.3646356258,"prompt-eng":0.3081004497,"data-quality":0.1880299013,"ml-security":0.0956376965}}
{"text":"This position paper aims to begin a conversation around a more nuanced approach to the problems of creativity and credit assignment for generative models, one which more fully recognises the importance of the creative and curatorial voice of the users of these models and moves away from simpler notational or information-theoretic views.","meta":{"url":"http://arxiv.org/abs/2307.10751v1"},"cats":{"new-dataset":0.0768380693,"dev-research":0.2642353978,"prompt-eng":0.4170005174,"data-quality":0.2539312985,"ml-security":0.1108205839}}
{"text":"The aggregation of multiple opinions plays a crucial role in decision-making, such as in hiring and loan review, and in labeling data for supervised learning.","meta":{"url":"http://arxiv.org/abs/2307.10749v1"},"cats":{"new-dataset":0.0151624007,"dev-research":0.293062156,"prompt-eng":0.3533047099,"data-quality":0.3560245479,"ml-security":0.0640782438}}
{"text":"Although majority voting and existing opinion aggregation models are effective for simple tasks, they are inappropriate for tasks without objectively true labels in which disagreements may occur.","meta":{"url":"http://arxiv.org/abs/2307.10749v1"},"cats":{"new-dataset":0.0165122083,"dev-research":0.2857239355,"prompt-eng":0.4311658003,"data-quality":0.3589915932,"ml-security":0.1596554564}}
{"text":"In particular, when voter attributes such as gender or race introduce bias into opinions, the aggregation results may vary depending on the composition of voter attributes.","meta":{"url":"http://arxiv.org/abs/2307.10749v1"},"cats":{"new-dataset":0.0062025553,"dev-research":0.2482425497,"prompt-eng":0.367882032,"data-quality":0.2170981206,"ml-security":0.1159666159}}
{"text":"A balanced group of voters is desirable for fair aggregation results but may be difficult to prepare.","meta":{"url":"http://arxiv.org/abs/2307.10749v1"},"cats":{"new-dataset":0.0105176866,"dev-research":0.2060312733,"prompt-eng":0.395042712,"data-quality":0.1195394658,"ml-security":0.1015889009}}
{"text":"In this study, we consider methods to achieve fair opinion aggregation based on voter attributes and evaluate the fairness of the aggregated results.","meta":{"url":"http://arxiv.org/abs/2307.10749v1"},"cats":{"new-dataset":0.043500456,"dev-research":0.2627995727,"prompt-eng":0.3970207088,"data-quality":0.2858292586,"ml-security":0.1317277471}}
{"text":"To this end, we consider an approach that combines opinion aggregation models such as majority voting and the Dawid and Skene model (D&S model) with fairness options such as sample weighting.","meta":{"url":"http://arxiv.org/abs/2307.10749v1"},"cats":{"new-dataset":0.0584100407,"dev-research":0.1999911562,"prompt-eng":0.4247967947,"data-quality":0.2904527861,"ml-security":0.1566194755}}
{"text":"To evaluate the fairness of opinion aggregation, probabilistic soft labels are preferred over discrete class labels.","meta":{"url":"http://arxiv.org/abs/2307.10749v1"},"cats":{"new-dataset":0.0236058388,"dev-research":0.2563579047,"prompt-eng":0.3987226172,"data-quality":0.5663125402,"ml-security":0.1798893256}}
{"text":"First, we address the problem of soft label estimation without considering voter attributes and identify some issues with the D&S model.","meta":{"url":"http://arxiv.org/abs/2307.10749v1"},"cats":{"new-dataset":0.1988883674,"dev-research":0.1886502844,"prompt-eng":0.4487850122,"data-quality":0.5922103494,"ml-security":0.1347757021}}
{"text":"To address these limitations, we propose a new Soft D&S model with improved accuracy in estimating soft labels.","meta":{"url":"http://arxiv.org/abs/2307.10749v1"},"cats":{"new-dataset":0.1303391049,"dev-research":0.198880081,"prompt-eng":0.4315311637,"data-quality":0.4985074101,"ml-security":0.0869372965}}
{"text":"Moreover, we evaluated the fairness of an opinion aggregation model, including Soft D&S, in combination with different fairness options using synthetic and semi-synthetic data.","meta":{"url":"http://arxiv.org/abs/2307.10749v1"},"cats":{"new-dataset":0.049429652,"dev-research":0.2274184542,"prompt-eng":0.365559969,"data-quality":0.2840336543,"ml-security":0.1816315306}}
{"text":"The experimental results suggest that the combination of Soft D&S and data splitting as a fairness option is effective for dense data, whereas weighted majority voting is effective for sparse data.","meta":{"url":"http://arxiv.org/abs/2307.10749v1"},"cats":{"new-dataset":0.0244101468,"dev-research":0.226841648,"prompt-eng":0.3718674459,"data-quality":0.2352103365,"ml-security":0.20180692}}
{"text":"These findings should prove particularly valuable in supporting decision-making by human and machine-learning models with balanced opinion aggregation.","meta":{"url":"http://arxiv.org/abs/2307.10749v1"},"cats":{"new-dataset":0.0308070926,"dev-research":0.2697922967,"prompt-eng":0.3894812608,"data-quality":0.2856666667,"ml-security":0.1587043433}}
{"text":"Recommending suitable jobs to users is a critical task in online recruitment platforms, as it can enhance users' satisfaction and the platforms' profitability.","meta":{"url":"http://arxiv.org/abs/2307.10747v1"},"cats":{"new-dataset":0.0088093002,"dev-research":0.2920284238,"prompt-eng":0.4110386966,"data-quality":0.0959731052,"ml-security":0.1431731335}}
{"text":"While existing job recommendation methods encounter challenges such as the low quality of users' resumes, which hampers their accuracy and practical effectiveness.","meta":{"url":"http://arxiv.org/abs/2307.10747v1"},"cats":{"new-dataset":0.0079187963,"dev-research":0.3378715789,"prompt-eng":0.3913663098,"data-quality":0.2005325837,"ml-security":0.1315775496}}
{"text":"With the rapid development of large language models (LLMs), utilizing the rich external knowledge encapsulated within them, as well as their powerful capabilities of text processing and reasoning, is a promising way to complete users' resumes for more accurate recommendations.","meta":{"url":"http://arxiv.org/abs/2307.10747v1"},"cats":{"new-dataset":0.0859299247,"dev-research":0.2406562612,"prompt-eng":0.4438710284,"data-quality":0.156865858,"ml-security":0.0847385824}}
{"text":"However, directly leveraging LLMs to enhance recommendation results is not a one-size-fits-all solution, as LLMs may suffer from fabricated generation and few-shot problems, which degrade the quality of resume completion.","meta":{"url":"http://arxiv.org/abs/2307.10747v1"},"cats":{"new-dataset":0.0299812242,"dev-research":0.2350892435,"prompt-eng":0.4421926144,"data-quality":0.151736235,"ml-security":0.0756851553}}
{"text":"In this paper, we propose a novel LLM-based approach for job recommendation.","meta":{"url":"http://arxiv.org/abs/2307.10747v1"},"cats":{"new-dataset":0.0348875013,"dev-research":0.2042891871,"prompt-eng":0.422131948,"data-quality":0.1160559428,"ml-security":0.1047492929}}
{"text":"To alleviate the limitation of fabricated generation for LLMs, we extract accurate and valuable information beyond users' self-description, which helps the LLMs better profile users for resume completion.","meta":{"url":"http://arxiv.org/abs/2307.10747v1"},"cats":{"new-dataset":0.184453615,"dev-research":0.3059765337,"prompt-eng":0.529048985,"data-quality":0.2165070866,"ml-security":0.104700784}}
{"text":"Specifically, we not only extract users' explicit properties (e.g., skills, interests) from their self-description but also infer users' implicit characteristics from their behaviors for more accurate and meaningful resume completion.","meta":{"url":"http://arxiv.org/abs/2307.10747v1"},"cats":{"new-dataset":0.0144510665,"dev-research":0.2771530446,"prompt-eng":0.486027927,"data-quality":0.1839569164,"ml-security":0.1508117773}}
{"text":"Nevertheless, some users still suffer from few-shot problems, which arise due to scarce interaction records, leading to limited guidance for the models in generating high-quality resumes.","meta":{"url":"http://arxiv.org/abs/2307.10747v1"},"cats":{"new-dataset":0.0715604709,"dev-research":0.2823825121,"prompt-eng":0.4284023166,"data-quality":0.1598776853,"ml-security":0.1237843677}}
{"text":"To address this issue, we propose aligning unpaired low-quality with high-quality generated resumes by Generative Adversarial Networks (GANs), which can refine the resume representations for better recommendation results.","meta":{"url":"http://arxiv.org/abs/2307.10747v1"},"cats":{"new-dataset":0.1092568234,"dev-research":0.2164624805,"prompt-eng":0.3830033892,"data-quality":0.2410367516,"ml-security":0.1243706931}}
{"text":"Extensive experiments on three large real-world recruitment datasets demonstrate the effectiveness of our proposed method.","meta":{"url":"http://arxiv.org/abs/2307.10747v1"},"cats":{"new-dataset":0.1651980548,"dev-research":0.1999901534,"prompt-eng":0.4275840188,"data-quality":0.1329538906,"ml-security":0.2142487285}}
{"text":"Active learning algorithms have become increasingly popular for training models with limited data.","meta":{"url":"http://arxiv.org/abs/2307.10745v1"},"cats":{"new-dataset":0.0470662837,"dev-research":0.1732028984,"prompt-eng":0.3133156966,"data-quality":0.1255129151,"ml-security":0.3354621487}}
{"text":"However, selecting data for annotation remains a challenging problem due to the limited information available on unseen data.","meta":{"url":"http://arxiv.org/abs/2307.10745v1"},"cats":{"new-dataset":0.3143726096,"dev-research":0.2523131126,"prompt-eng":0.4112177901,"data-quality":0.4052342556,"ml-security":0.130835118}}
{"text":"To address this issue, we propose EdgeAL, which utilizes the edge information of unseen images as {\\it a priori} information for measuring uncertainty.","meta":{"url":"http://arxiv.org/abs/2307.10745v1"},"cats":{"new-dataset":0.1038257237,"dev-research":0.2677270599,"prompt-eng":0.4165877949,"data-quality":0.2891702605,"ml-security":0.1024812154}}
{"text":"The uncertainty is quantified by analyzing the divergence and entropy in model predictions across edges.","meta":{"url":"http://arxiv.org/abs/2307.10745v1"},"cats":{"new-dataset":0.0178170404,"dev-research":0.2463437543,"prompt-eng":0.3737163525,"data-quality":0.237244062,"ml-security":0.1269905205}}
{"text":"This measure is then used to select superpixels for annotation.","meta":{"url":"http://arxiv.org/abs/2307.10745v1"},"cats":{"new-dataset":0.0850178623,"dev-research":0.2555751381,"prompt-eng":0.3973249209,"data-quality":0.1852705129,"ml-security":0.0421095209}}
{"text":"We demonstrate the effectiveness of EdgeAL on multi-class Optical Coherence Tomography (OCT) segmentation tasks, where we achieved a 99% dice score while reducing the annotation label cost to 12%, 2.3%, and 3%, respectively, on three publicly available datasets (Duke, AROI, and UMN).","meta":{"url":"http://arxiv.org/abs/2307.10745v1"},"cats":{"new-dataset":0.2156163669,"dev-research":0.2246386047,"prompt-eng":0.3795811047,"data-quality":0.2367212058,"ml-security":0.0599064092}}
{"text":"The source code is available at \\url{https://github.com/Mak-Ta-Reque/EdgeAL}","meta":{"url":"http://arxiv.org/abs/2307.10745v1"},"cats":{"new-dataset":0.3823661223,"dev-research":0.2920407919,"prompt-eng":0.3744922702,"data-quality":0.1634207784,"ml-security":0.0933650334}}
{"text":"This work addresses human intention identification during physical Human-Robot Interaction (pHRI) tasks to include this information in an assistive controller.","meta":{"url":"http://arxiv.org/abs/2307.10743v1"},"cats":{"new-dataset":0.0379675964,"dev-research":0.2854882069,"prompt-eng":0.4996452993,"data-quality":0.1555509608,"ml-security":0.0931073064}}
{"text":"To this purpose, human intention is defined as the desired trajectory that the human wants to follow over a finite rolling prediction horizon so that the robot can assist in pursuing it.","meta":{"url":"http://arxiv.org/abs/2307.10743v1"},"cats":{"new-dataset":0.0124452613,"dev-research":0.2818522613,"prompt-eng":0.4048209028,"data-quality":0.0670263639,"ml-security":0.1077730304}}
{"text":"This work investigates a Recurrent Neural Network (RNN), specifically, Long-Short Term Memory (LSTM) cascaded with a Fully Connected layer.","meta":{"url":"http://arxiv.org/abs/2307.10743v1"},"cats":{"new-dataset":0.1707184163,"dev-research":0.179845535,"prompt-eng":0.34706894,"data-quality":0.1254897071,"ml-security":0.1875932529}}
{"text":"In particular, we propose an iterative training procedure to adapt the model.","meta":{"url":"http://arxiv.org/abs/2307.10743v1"},"cats":{"new-dataset":0.0446568176,"dev-research":0.1847675817,"prompt-eng":0.4483580596,"data-quality":0.171583551,"ml-security":0.0970103076}}
{"text":"Such an iterative procedure is powerful in reducing the prediction error.","meta":{"url":"http://arxiv.org/abs/2307.10743v1"},"cats":{"new-dataset":0.0100140877,"dev-research":0.2874944239,"prompt-eng":0.4359468076,"data-quality":0.1785711824,"ml-security":0.1443964899}}
{"text":"Still, it has the drawback that it is time-consuming and does not generalize to different users or different co-manipulated objects.","meta":{"url":"http://arxiv.org/abs/2307.10743v1"},"cats":{"new-dataset":0.0037500519,"dev-research":0.4272871776,"prompt-eng":0.3741881038,"data-quality":0.101859526,"ml-security":0.1494580561}}
{"text":"To overcome this issue, Transfer Learning (TL) adapts the pre-trained model to new trajectories, users, and co-manipulated objects by freezing the LSTM layer and fine-tuning the last FC layer, which makes the procedure faster.","meta":{"url":"http://arxiv.org/abs/2307.10743v1"},"cats":{"new-dataset":0.0882155892,"dev-research":0.2652289828,"prompt-eng":0.4134124778,"data-quality":0.0987254422,"ml-security":0.1328924733}}
{"text":"Experiments show that the iterative procedure adapts the model and reduces prediction error.","meta":{"url":"http://arxiv.org/abs/2307.10743v1"},"cats":{"new-dataset":0.0057280006,"dev-research":0.2985599746,"prompt-eng":0.4494614725,"data-quality":0.1686034677,"ml-security":0.1308304846}}
{"text":"Experiments also show that TL adapts to different users and to the co-manipulation of a large object.","meta":{"url":"http://arxiv.org/abs/2307.10743v1"},"cats":{"new-dataset":0.0146483179,"dev-research":0.3353756999,"prompt-eng":0.4412673119,"data-quality":0.0824160475,"ml-security":0.1049130476}}
{"text":"Finally, to check the utility of adopting the proposed method, we compare the proposed controller enhanced by the intention prediction with the other two standard controllers of pHRI.","meta":{"url":"http://arxiv.org/abs/2307.10743v1"},"cats":{"new-dataset":0.010398309,"dev-research":0.2770856706,"prompt-eng":0.5018899569,"data-quality":0.1022764058,"ml-security":0.0838874827}}
{"text":"Applications involving humans and robots working together are spreading nowadays.","meta":{"url":"http://arxiv.org/abs/2307.10739v1"},"cats":{"new-dataset":0.0618552837,"dev-research":0.3078020289,"prompt-eng":0.3544285014,"data-quality":0.0706510305,"ml-security":0.13042129}}
{"text":"Alongside, modeling and control techniques that allow physical Human-Robot Interaction (pHRI) are widely investigated.","meta":{"url":"http://arxiv.org/abs/2307.10739v1"},"cats":{"new-dataset":0.0563493246,"dev-research":0.2806978368,"prompt-eng":0.4296909663,"data-quality":0.0449634363,"ml-security":0.0724103379}}
{"text":"To better understand its potential application in pHRI, this work investigates the Cooperative Differential Game Theory modeling of pHRI in a cooperative reaching task, specifically for reference tracking.","meta":{"url":"http://arxiv.org/abs/2307.10739v1"},"cats":{"new-dataset":0.0683593159,"dev-research":0.2657888183,"prompt-eng":0.4107762553,"data-quality":0.0745866727,"ml-security":0.1014079393}}
{"text":"The proposed controller based on Collaborative Game Theory is deeply analyzed and compared in simulations with two other techniques, Linear Quadratic Regulator (LQR) and Non-Cooperative Game-Theoretic Controller.","meta":{"url":"http://arxiv.org/abs/2307.10739v1"},"cats":{"new-dataset":0.025164122,"dev-research":0.2795040681,"prompt-eng":0.3821787864,"data-quality":0.0494396451,"ml-security":0.0927200629}}
{"text":"The set of simulations shows how different tuning of control parameters affects the system response and control efforts of both the players for the three controllers, suggesting the use of Cooperative GT in the case the robot should assist the human, while Non-Cooperative GT represents a better choice in the case the robot should lead the action.","meta":{"url":"http://arxiv.org/abs/2307.10739v1"},"cats":{"new-dataset":0.0185748071,"dev-research":0.2572528988,"prompt-eng":0.426572298,"data-quality":0.0641750052,"ml-security":0.0778948777}}
{"text":"Finally, preliminary tests with a trained human are performed to extract useful information on the real applicability and limitations of the proposed method.","meta":{"url":"http://arxiv.org/abs/2307.10739v1"},"cats":{"new-dataset":0.0153839525,"dev-research":0.3007588544,"prompt-eng":0.4291140825,"data-quality":0.2104676065,"ml-security":0.1280640571}}
{"text":"Federated learning (FL) has enabled multiple data owners (a.k.a. FL clients) to train machine learning models collaboratively without revealing private data.","meta":{"url":"http://arxiv.org/abs/2307.10738v1"},"cats":{"new-dataset":0.1508815638,"dev-research":0.2323663816,"prompt-eng":0.3258236686,"data-quality":0.1257813606,"ml-security":0.3487540824}}
{"text":"Since the FL server can only engage a limited number of clients in each training round, FL client selection has become an important research problem.","meta":{"url":"http://arxiv.org/abs/2307.10738v1"},"cats":{"new-dataset":0.0132932225,"dev-research":0.2297121812,"prompt-eng":0.3555763307,"data-quality":0.1100216543,"ml-security":0.2277183904}}
{"text":"Existing approaches generally focus on either enhancing FL model performance or enhancing the fair treatment of FL clients.","meta":{"url":"http://arxiv.org/abs/2307.10738v1"},"cats":{"new-dataset":0.0061604874,"dev-research":0.2767833241,"prompt-eng":0.3740720629,"data-quality":0.1186596345,"ml-security":0.1361024389}}
{"text":"The problem of balancing performance and fairness considerations when selecting FL clients remains open.","meta":{"url":"http://arxiv.org/abs/2307.10738v1"},"cats":{"new-dataset":0.0087502249,"dev-research":0.2483564353,"prompt-eng":0.3824594564,"data-quality":0.1142882482,"ml-security":0.1862308424}}
{"text":"To address this problem, we propose the Fairness-aware Federated Client Selection (FairFedCS) approach.","meta":{"url":"http://arxiv.org/abs/2307.10738v1"},"cats":{"new-dataset":0.0267944641,"dev-research":0.2209161743,"prompt-eng":0.4327256359,"data-quality":0.1488290529,"ml-security":0.2377730494}}
{"text":"Based on Lyapunov optimization, it dynamically adjusts FL clients' selection probabilities by jointly considering their reputations, times of participation in FL tasks and contributions to the resulting model performance.","meta":{"url":"http://arxiv.org/abs/2307.10738v1"},"cats":{"new-dataset":0.013341084,"dev-research":0.2295626691,"prompt-eng":0.475974646,"data-quality":0.089066143,"ml-security":0.2057582507}}
{"text":"By not using threshold-based reputation filtering, it provides FL clients with opportunities to redeem their reputations after a perceived poor performance, thereby further enhancing fair client treatment.","meta":{"url":"http://arxiv.org/abs/2307.10738v1"},"cats":{"new-dataset":0.0115654197,"dev-research":0.281705155,"prompt-eng":0.383527389,"data-quality":0.2038925531,"ml-security":0.3625577987}}
{"text":"Extensive experiments based on real-world multimedia datasets show that FairFedCS achieves 19.6% higher fairness and 0.73% higher test accuracy on average than the best-performing state-of-the-art approach.","meta":{"url":"http://arxiv.org/abs/2307.10738v1"},"cats":{"new-dataset":0.2358077471,"dev-research":0.264626655,"prompt-eng":0.3948182146,"data-quality":0.3035841623,"ml-security":0.1785715086}}
{"text":"We suggest a simple Gaussian mixture model for data generation that complies with Feldman's long tail theory (2020).","meta":{"url":"http://arxiv.org/abs/2307.10736v1"},"cats":{"new-dataset":0.2389870383,"dev-research":0.148175155,"prompt-eng":0.4452999542,"data-quality":0.1152952946,"ml-security":0.0739225801}}
{"text":"We demonstrate that a linear classifier cannot decrease the generalization error below a certain level in the proposed model, whereas a nonlinear classifier with a memorization capacity can.","meta":{"url":"http://arxiv.org/abs/2307.10736v1"},"cats":{"new-dataset":0.0151858111,"dev-research":0.2172579639,"prompt-eng":0.368898649,"data-quality":0.4381314324,"ml-security":0.3718960062}}
{"text":"This confirms that for long-tailed distributions, rare training examples must be considered for optimal generalization to new data.","meta":{"url":"http://arxiv.org/abs/2307.10736v1"},"cats":{"new-dataset":0.0449209804,"dev-research":0.1480925704,"prompt-eng":0.3837242573,"data-quality":0.2413316851,"ml-security":0.3099679447}}
{"text":"Finally, we show that the performance gap between linear and nonlinear models can be lessened as the tail becomes shorter in the subpopulation frequency distribution, as confirmed by experiments on synthetic and real data.","meta":{"url":"http://arxiv.org/abs/2307.10736v1"},"cats":{"new-dataset":0.0391391008,"dev-research":0.1645740568,"prompt-eng":0.3809750013,"data-quality":0.158394522,"ml-security":0.1507042527}}
{"text":"Managing shared mutable states in high concurrency state access operations is a persistent challenge in Network Functions Virtualization (NFV).","meta":{"url":"http://arxiv.org/abs/2307.10732v1"},"cats":{"new-dataset":0.0536823078,"dev-research":0.3087562585,"prompt-eng":0.3893188504,"data-quality":0.1233093404,"ml-security":0.197196807}}
{"text":"This is particularly true when striving to meet chain output equivalence (COE) requirements.","meta":{"url":"http://arxiv.org/abs/2307.10732v1"},"cats":{"new-dataset":0.0033685105,"dev-research":0.249614198,"prompt-eng":0.4127800899,"data-quality":0.2062495805,"ml-security":0.1010934384}}
{"text":"This paper presents TransNFV, an innovative NFV framework that incorporates transactional semantics to optimize NFV state management.","meta":{"url":"http://arxiv.org/abs/2307.10732v1"},"cats":{"new-dataset":0.1496428184,"dev-research":0.3208879339,"prompt-eng":0.4088937408,"data-quality":0.1697446607,"ml-security":0.0771156651}}
{"text":"The TransNFV integrates VNF state access operations as transactions, resolves transaction dependencies, schedules transactions dynamically, and executes transactions efficiently.","meta":{"url":"http://arxiv.org/abs/2307.10732v1"},"cats":{"new-dataset":0.0681253793,"dev-research":0.2848170453,"prompt-eng":0.3647962189,"data-quality":0.0919847812,"ml-security":0.0949646791}}
{"text":"Initial findings suggest that TransNFV maintains shared VNF state consistency, meets COE requirements, and skillfully handles complex cross-flow states in dynamic network conditions.","meta":{"url":"http://arxiv.org/abs/2307.10732v1"},"cats":{"new-dataset":0.0514788231,"dev-research":0.2617289358,"prompt-eng":0.4030764245,"data-quality":0.1456116668,"ml-security":0.0954976362}}
{"text":"TransNFV thus provides a promising solution to enhance state management and overall performance in future NFV platforms.","meta":{"url":"http://arxiv.org/abs/2307.10732v1"},"cats":{"new-dataset":0.1059550501,"dev-research":0.2766065665,"prompt-eng":0.4079452564,"data-quality":0.0936227872,"ml-security":0.0823794804}}
{"text":"In frequency division duplexing (FDD) cell-free massive MIMO, the acquisition of the channel state information (CSI) is very challenging because of the large overhead required for the training and feedback of the downlink channels of multiple cooperating base stations (BSs).","meta":{"url":"http://arxiv.org/abs/2307.10730v1"},"cats":{"new-dataset":0.0884603338,"dev-research":0.2378784517,"prompt-eng":0.392545351,"data-quality":0.0945356441,"ml-security":0.0938187772}}
{"text":"In this paper, for systems with partial uplink-downlink channel reciprocity, and a general spatial domain channel model with variations in the average port power and correlation among port coefficients, we propose a joint-port-selection-based CSI acquisition and feedback scheme for the downlink transmission with zero-forcing precoding.","meta":{"url":"http://arxiv.org/abs/2307.10730v1"},"cats":{"new-dataset":0.0362118907,"dev-research":0.2343145628,"prompt-eng":0.4350594747,"data-quality":0.1171246171,"ml-security":0.1640454896}}
{"text":"The scheme uses an eigenvalue-decomposition-based transformation to reduce the feedback overhead by exploring the port correlation.","meta":{"url":"http://arxiv.org/abs/2307.10730v1"},"cats":{"new-dataset":0.027152069,"dev-research":0.2998695364,"prompt-eng":0.4368057044,"data-quality":0.1046843219,"ml-security":0.1128489427}}
{"text":"We derive the sum-rate of the system for any port selection.","meta":{"url":"http://arxiv.org/abs/2307.10730v1"},"cats":{"new-dataset":0.0171233884,"dev-research":0.1751249724,"prompt-eng":0.3600730789,"data-quality":0.0559710124,"ml-security":0.0997261983}}
{"text":"Based on the sum-rate result, we propose a low-complexity greedy-search-based joint port selection (GS-JPS) algorithm.","meta":{"url":"http://arxiv.org/abs/2307.10730v1"},"cats":{"new-dataset":0.0197911887,"dev-research":0.2278234176,"prompt-eng":0.3380473561,"data-quality":0.075105301,"ml-security":0.0902981871}}
{"text":"Moreover, to adapt to fast time-varying scenarios, a supervised deep learning-enhanced joint port selection (DL-JPS) algorithm is proposed.","meta":{"url":"http://arxiv.org/abs/2307.10730v1"},"cats":{"new-dataset":0.0493989621,"dev-research":0.2605791858,"prompt-eng":0.3277234494,"data-quality":0.1170405215,"ml-security":0.1854992022}}
{"text":"Simulations verify the effectiveness of our proposed schemes and their advantage over existing port-selection channel acquisition schemes.","meta":{"url":"http://arxiv.org/abs/2307.10730v1"},"cats":{"new-dataset":0.0408305194,"dev-research":0.2314118581,"prompt-eng":0.4078280533,"data-quality":0.1007422422,"ml-security":0.0997414412}}
{"text":"The development of an electronic voting system that would replace traditional election procedures is a research topic of great interest for many years.","meta":{"url":"http://arxiv.org/abs/2307.10726v1"},"cats":{"new-dataset":0.0402122641,"dev-research":0.2727481158,"prompt-eng":0.4391728758,"data-quality":0.1054351915,"ml-security":0.1139658534}}
{"text":"Blockchain technology could provide some guarantees and fulfill strong requirements for electronic voting platforms, such as transparency, immutability, and confidentiality.","meta":{"url":"http://arxiv.org/abs/2307.10726v1"},"cats":{"new-dataset":0.0204493032,"dev-research":0.2183471539,"prompt-eng":0.3524017884,"data-quality":0.1011257413,"ml-security":0.2400735998}}
{"text":"From time to time research is conducted to address problems in voting systems.","meta":{"url":"http://arxiv.org/abs/2307.10726v1"},"cats":{"new-dataset":0.0354791919,"dev-research":0.290595656,"prompt-eng":0.4057541729,"data-quality":0.1981107131,"ml-security":0.1970449097}}
{"text":"Many research works attempt to implement secure and reliable voting systems, which address known security, anonymity, and fraud issues that might threaten such systems.   ","meta":{"url":"http://arxiv.org/abs/2307.10726v1"},"cats":{"new-dataset":0.0514302981,"dev-research":0.256384428,"prompt-eng":0.4100189368,"data-quality":0.2001423703,"ml-security":0.4496879018}}
{"text":"This paper presents a proposal of a secure electronic voting system, the EtherVote, using the Ethereum Blockchain network that focuses deeply on the field of identification of eligible citizens.","meta":{"url":"http://arxiv.org/abs/2307.10726v1"},"cats":{"new-dataset":0.1351960806,"dev-research":0.2070598461,"prompt-eng":0.4211363917,"data-quality":0.1289701644,"ml-security":0.2332479559}}
{"text":"The proposed system will be entirely based on Blockchain without any central authority servers or databases, thus improving security, privacy, and election cost.","meta":{"url":"http://arxiv.org/abs/2307.10726v1"},"cats":{"new-dataset":0.0824494673,"dev-research":0.1880741967,"prompt-eng":0.329630299,"data-quality":0.0749674298,"ml-security":0.2850215933}}
{"text":"Limitations, problems, and solutions are discussed, in order to make the proposed electronic voting system ideal and ready to use for national elections.","meta":{"url":"http://arxiv.org/abs/2307.10726v1"},"cats":{"new-dataset":0.0679383405,"dev-research":0.2424702333,"prompt-eng":0.4073014582,"data-quality":0.1018266126,"ml-security":0.106834205}}
{"text":"Large language models (LLMs) have exhibited impressive capabilities in comprehending complex instructions.","meta":{"url":"http://arxiv.org/abs/2307.10719v1"},"cats":{"new-dataset":0.068586615,"dev-research":0.2538965064,"prompt-eng":0.4258881228,"data-quality":0.1183693066,"ml-security":0.1063268259}}
{"text":"However, their blind adherence to provided instructions has led to concerns regarding risks of malicious use.","meta":{"url":"http://arxiv.org/abs/2307.10719v1"},"cats":{"new-dataset":0.0168870714,"dev-research":0.368490116,"prompt-eng":0.3807868366,"data-quality":0.2684144223,"ml-security":0.536300938}}
{"text":"Existing defence mechanisms, such as model fine-tuning or output censorship using LLMs, have proven to be fallible, as LLMs can still generate problematic responses.","meta":{"url":"http://arxiv.org/abs/2307.10719v1"},"cats":{"new-dataset":0.021167736,"dev-research":0.2624000072,"prompt-eng":0.4400891744,"data-quality":0.2878566474,"ml-security":0.7332742234}}
{"text":"Commonly employed censorship approaches treat the issue as a machine learning problem and rely on another LM to detect undesirable content in LLM outputs.","meta":{"url":"http://arxiv.org/abs/2307.10719v1"},"cats":{"new-dataset":0.0339070361,"dev-research":0.2306356837,"prompt-eng":0.3569227121,"data-quality":0.4580853631,"ml-security":0.5211806818}}
{"text":"In this paper, we present the theoretical limitations of such semantic censorship approaches.","meta":{"url":"http://arxiv.org/abs/2307.10719v1"},"cats":{"new-dataset":0.02358245,"dev-research":0.2406072914,"prompt-eng":0.3072446768,"data-quality":0.387492635,"ml-security":0.2722606436}}
{"text":"Specifically, we demonstrate that semantic censorship can be perceived as an undecidable problem, highlighting the inherent challenges in censorship that arise due to LLMs' programmatic and instruction-following capabilities.","meta":{"url":"http://arxiv.org/abs/2307.10719v1"},"cats":{"new-dataset":0.0466808045,"dev-research":0.3305961085,"prompt-eng":0.3573879209,"data-quality":0.3490891316,"ml-security":0.4156841871}}
{"text":"Furthermore, we argue that the challenges extend beyond semantic censorship, as knowledgeable attackers can reconstruct impermissible outputs from a collection of permissible ones.","meta":{"url":"http://arxiv.org/abs/2307.10719v1"},"cats":{"new-dataset":0.1576959288,"dev-research":0.2951968409,"prompt-eng":0.3597205767,"data-quality":0.3276724742,"ml-security":0.7354763256}}
{"text":"As a result, we propose that the problem of censorship needs to be reevaluated; it should be treated as a security problem which warrants the adaptation of security-based approaches to mitigate potential risks.","meta":{"url":"http://arxiv.org/abs/2307.10719v1"},"cats":{"new-dataset":0.0778288757,"dev-research":0.252945361,"prompt-eng":0.3588142517,"data-quality":0.2959578462,"ml-security":0.6654385149}}
{"text":"Extracting noisy or incorrectly labeled samples from a labeled dataset with hard/difficult samples is an important yet under-explored topic.","meta":{"url":"http://arxiv.org/abs/2307.10718v1"},"cats":{"new-dataset":0.2961859918,"dev-research":0.2776785843,"prompt-eng":0.4197147752,"data-quality":0.7777218919,"ml-security":0.1717840618}}
{"text":"Two general and often independent lines of work exist, one focuses on addressing noisy labels, and another deals with hard samples.","meta":{"url":"http://arxiv.org/abs/2307.10718v1"},"cats":{"new-dataset":0.0552818639,"dev-research":0.3051469429,"prompt-eng":0.376177931,"data-quality":0.5138855865,"ml-security":0.0867684008}}
{"text":"However, when both types of data are present, most existing methods treat them equally, which results in a decline in the overall performance of the model.","meta":{"url":"http://arxiv.org/abs/2307.10718v1"},"cats":{"new-dataset":0.0043811121,"dev-research":0.2511496592,"prompt-eng":0.3339408437,"data-quality":0.1614026668,"ml-security":0.0897441394}}
{"text":"In this paper, we first design various synthetic datasets with custom hardness and noisiness levels for different samples.","meta":{"url":"http://arxiv.org/abs/2307.10718v1"},"cats":{"new-dataset":0.6846790701,"dev-research":0.223606669,"prompt-eng":0.3839579368,"data-quality":0.26558537,"ml-security":0.1665714039}}
{"text":"Our proposed systematic empirical study enables us to better understand the similarities and more importantly the differences between hard-to-learn samples and incorrectly-labeled samples.","meta":{"url":"http://arxiv.org/abs/2307.10718v1"},"cats":{"new-dataset":0.0986715132,"dev-research":0.3498273627,"prompt-eng":0.3993833074,"data-quality":0.5355305618,"ml-security":0.1096916153}}
{"text":"These controlled experiments pave the way for the development of methods that distinguish between hard and noisy samples.","meta":{"url":"http://arxiv.org/abs/2307.10718v1"},"cats":{"new-dataset":0.0381090783,"dev-research":0.3377866121,"prompt-eng":0.4393605733,"data-quality":0.3091578963,"ml-security":0.1511586371}}
{"text":"Through our study, we introduce a simple yet effective metric that filters out noisy-labeled samples while keeping the hard samples.","meta":{"url":"http://arxiv.org/abs/2307.10718v1"},"cats":{"new-dataset":0.1497897323,"dev-research":0.263302414,"prompt-eng":0.4602544588,"data-quality":0.6008491916,"ml-security":0.1405611506}}
{"text":"We study various data partitioning methods in the presence of label noise and observe that filtering out noisy samples from hard samples with this proposed metric results in the best datasets as evidenced by the high test accuracy achieved after models are trained on the filtered datasets.","meta":{"url":"http://arxiv.org/abs/2307.10718v1"},"cats":{"new-dataset":0.2846282666,"dev-research":0.2562952183,"prompt-eng":0.3836356728,"data-quality":0.6146946044,"ml-security":0.195573101}}
{"text":"We demonstrate this for both our created synthetic datasets and for datasets with real-world label noise.","meta":{"url":"http://arxiv.org/abs/2307.10718v1"},"cats":{"new-dataset":0.8293648614,"dev-research":0.2298139484,"prompt-eng":0.3884125775,"data-quality":0.5676369244,"ml-security":0.1903606881}}
{"text":"Furthermore, our proposed data partitioning method significantly outperforms other methods when employed within a semi-supervised learning framework.","meta":{"url":"http://arxiv.org/abs/2307.10718v1"},"cats":{"new-dataset":0.0974380877,"dev-research":0.246806755,"prompt-eng":0.3925680077,"data-quality":0.3344966114,"ml-security":0.1364217286}}
{"text":"Self-supervised monocular depth estimation (SS-MDE) has the potential to scale to vast quantities of data.","meta":{"url":"http://arxiv.org/abs/2307.10713v1"},"cats":{"new-dataset":0.1304952717,"dev-research":0.1696096377,"prompt-eng":0.3872621767,"data-quality":0.1382127481,"ml-security":0.0766967333}}
{"text":"Unfortunately, existing approaches limit themselves to the automotive domain, resulting in models incapable of generalizing to complex environments such as natural or indoor settings.   ","meta":{"url":"http://arxiv.org/abs/2307.10713v1"},"cats":{"new-dataset":0.0340098273,"dev-research":0.2593729779,"prompt-eng":0.3814303702,"data-quality":0.1154830253,"ml-security":0.1633295142}}
{"text":"To address this, we propose a large-scale SlowTV dataset curated from YouTube, containing an order of magnitude more data than existing automotive datasets.","meta":{"url":"http://arxiv.org/abs/2307.10713v1"},"cats":{"new-dataset":0.8410203835,"dev-research":0.2415236841,"prompt-eng":0.3358674332,"data-quality":0.1616104963,"ml-security":0.1003185355}}
{"text":"SlowTV contains 1.7M images from a rich diversity of environments, such as worldwide seasonal hiking, scenic driving and scuba diving.","meta":{"url":"http://arxiv.org/abs/2307.10713v1"},"cats":{"new-dataset":0.3586467217,"dev-research":0.1934459776,"prompt-eng":0.301317384,"data-quality":0.0951360209,"ml-security":0.0615065564}}
{"text":"Using this dataset, we train an SS-MDE model that provides zero-shot generalization to a large collection of indoor/outdoor datasets.","meta":{"url":"http://arxiv.org/abs/2307.10713v1"},"cats":{"new-dataset":0.6297027413,"dev-research":0.183063733,"prompt-eng":0.3974214082,"data-quality":0.1462411869,"ml-security":0.1638785358}}
{"text":"The resulting model outperforms all existing SSL approaches and closes the gap on supervised SoTA, despite using a more efficient architecture.   ","meta":{"url":"http://arxiv.org/abs/2307.10713v1"},"cats":{"new-dataset":0.1000360258,"dev-research":0.1950450047,"prompt-eng":0.4029015406,"data-quality":0.2026314498,"ml-security":0.2153125391}}
{"text":"We additionally introduce a collection of best-practices to further maximize performance and zero-shot generalization.","meta":{"url":"http://arxiv.org/abs/2307.10713v1"},"cats":{"new-dataset":0.0821252656,"dev-research":0.3222818501,"prompt-eng":0.4213132332,"data-quality":0.2163517796,"ml-security":0.1527244331}}
{"text":"This includes 1) aspect ratio augmentation, 2) camera intrinsic estimation, 3) support frame randomization and 4) flexible motion estimation.","meta":{"url":"http://arxiv.org/abs/2307.10713v1"},"cats":{"new-dataset":0.2615590024,"dev-research":0.1962393115,"prompt-eng":0.3710854007,"data-quality":0.1311055403,"ml-security":0.0464869322}}
{"text":"Code is available at https://github.com/jspenmar/slowtv_monodepth.","meta":{"url":"http://arxiv.org/abs/2307.10713v1"},"cats":{"new-dataset":0.3127249819,"dev-research":0.3053681559,"prompt-eng":0.4296792814,"data-quality":0.1490841241,"ml-security":0.0681165878}}
{"text":"Existing customization methods require access to multiple reference examples to align pre-trained diffusion probabilistic models (DPMs) with user-provided concepts.","meta":{"url":"http://arxiv.org/abs/2307.10711v1"},"cats":{"new-dataset":0.0885999264,"dev-research":0.1981771081,"prompt-eng":0.4396791063,"data-quality":0.1475842958,"ml-security":0.1530665588}}
{"text":"This paper aims to address the challenge of DPM customization when the only available supervision is a differentiable metric defined on the generated contents.","meta":{"url":"http://arxiv.org/abs/2307.10711v1"},"cats":{"new-dataset":0.1996578164,"dev-research":0.3219978502,"prompt-eng":0.480935063,"data-quality":0.2784542048,"ml-security":0.1629193859}}
{"text":"Since the sampling procedure of DPMs involves recursive calls to the denoising UNet, na\\\"ive gradient backpropagation requires storing the intermediate states of all iterations, resulting in extremely high memory consumption.","meta":{"url":"http://arxiv.org/abs/2307.10711v1"},"cats":{"new-dataset":0.0341972678,"dev-research":0.2566356817,"prompt-eng":0.309207106,"data-quality":0.1243816969,"ml-security":0.2290455024}}
{"text":"To overcome this issue, we propose a novel method AdjointDPM, which first generates new samples from diffusion models by solving the corresponding probability-flow ODEs.","meta":{"url":"http://arxiv.org/abs/2307.10711v1"},"cats":{"new-dataset":0.0614333104,"dev-research":0.1659897042,"prompt-eng":0.3822952788,"data-quality":0.1383019591,"ml-security":0.1437215154}}
{"text":"It then uses the adjoint sensitivity method to backpropagate the gradients of the loss to the models' parameters (including conditioning signals, network weights, and initial noises) by solving another augmented ODE.","meta":{"url":"http://arxiv.org/abs/2307.10711v1"},"cats":{"new-dataset":0.0047948244,"dev-research":0.24995408,"prompt-eng":0.404415509,"data-quality":0.2104883921,"ml-security":0.2460342933}}
{"text":"To reduce numerical errors in both the forward generation and gradient backpropagation processes, we further reparameterize the probability-flow ODE and augmented ODE as simple non-stiff ODEs using exponential integration.","meta":{"url":"http://arxiv.org/abs/2307.10711v1"},"cats":{"new-dataset":0.0259735851,"dev-research":0.22417451,"prompt-eng":0.3847437308,"data-quality":0.1413112446,"ml-security":0.2059187313}}
{"text":"Finally, we demonstrate the effectiveness of AdjointDPM on three interesting tasks: converting visual effects into identification text embeddings, finetuning DPMs for specific types of stylization, and optimizing initial noise to generate adversarial samples for security auditing.","meta":{"url":"http://arxiv.org/abs/2307.10711v1"},"cats":{"new-dataset":0.164503606,"dev-research":0.2726392456,"prompt-eng":0.4308868668,"data-quality":0.3949234606,"ml-security":0.6137801243}}
{"text":"We investigate the challenge of parametrizing policies for reinforcement learning (RL) in high-dimensional continuous action spaces.","meta":{"url":"http://arxiv.org/abs/2307.10710v1"},"cats":{"new-dataset":0.0744864709,"dev-research":0.1786442525,"prompt-eng":0.369369343,"data-quality":0.0795475611,"ml-security":0.2454164431}}
{"text":"Our objective is to develop a multimodal policy that overcomes limitations inherent in the commonly-used Gaussian parameterization.","meta":{"url":"http://arxiv.org/abs/2307.10710v1"},"cats":{"new-dataset":0.0490301906,"dev-research":0.2110063559,"prompt-eng":0.4458073502,"data-quality":0.1317647575,"ml-security":0.1330807314}}
{"text":"To achieve this, we propose a principled framework that models the continuous RL policy as a generative model of optimal trajectories.","meta":{"url":"http://arxiv.org/abs/2307.10710v1"},"cats":{"new-dataset":0.0437294774,"dev-research":0.1598896783,"prompt-eng":0.4046183254,"data-quality":0.0728622989,"ml-security":0.0800154869}}
{"text":"By conditioning the policy on a latent variable, we derive a novel variational bound as the optimization objective, which promotes exploration of the environment.","meta":{"url":"http://arxiv.org/abs/2307.10710v1"},"cats":{"new-dataset":0.0202031741,"dev-research":0.2020989521,"prompt-eng":0.405942211,"data-quality":0.1071499867,"ml-security":0.1935314431}}
{"text":"We then present a practical model-based RL method, called Reparameterized Policy Gradient (RPG), which leverages the multimodal policy parameterization and learned world model to achieve strong exploration capabilities and high data efficiency.","meta":{"url":"http://arxiv.org/abs/2307.10710v1"},"cats":{"new-dataset":0.1938163935,"dev-research":0.2015755095,"prompt-eng":0.4246120829,"data-quality":0.0902932454,"ml-security":0.1248375261}}
{"text":"Empirical results demonstrate that our method can help agents evade local optima in tasks with dense rewards and solve challenging sparse-reward environments by incorporating an object-centric intrinsic reward.","meta":{"url":"http://arxiv.org/abs/2307.10710v1"},"cats":{"new-dataset":0.0211758357,"dev-research":0.2161204262,"prompt-eng":0.4225600864,"data-quality":0.1238919575,"ml-security":0.1783702807}}
{"text":"Our method consistently outperforms previous approaches across a range of tasks.","meta":{"url":"http://arxiv.org/abs/2307.10710v1"},"cats":{"new-dataset":0.0124432617,"dev-research":0.3594473614,"prompt-eng":0.4032052926,"data-quality":0.2371432986,"ml-security":0.072849071}}
{"text":"Code and supplementary materials are available on the project page https://haosulab.github.io/RPG/","meta":{"url":"http://arxiv.org/abs/2307.10710v1"},"cats":{"new-dataset":0.5699507443,"dev-research":0.2632592652,"prompt-eng":0.4149387487,"data-quality":0.0998722507,"ml-security":0.058249786}}
{"text":"Reconfigurable intelligent surface (RIS) architectures not limited to diagonal phase shift matrices have recently been considered to increase their flexibility in shaping the wireless channel.","meta":{"url":"http://arxiv.org/abs/2307.10707v1"},"cats":{"new-dataset":0.0171586395,"dev-research":0.2166101735,"prompt-eng":0.3746390614,"data-quality":0.0554679908,"ml-security":0.0822564684}}
{"text":"One of these beyond-diagonal RIS or BD-RIS architectures leads to a unitary and symmetric RIS matrix.","meta":{"url":"http://arxiv.org/abs/2307.10707v1"},"cats":{"new-dataset":0.0600595984,"dev-research":0.1597970015,"prompt-eng":0.3462581327,"data-quality":0.0771691291,"ml-security":0.0988875025}}
{"text":"In this letter, we consider the problem of maximizing the signal-to-noise ratio (SNR) in single and multiple antenna links assisted by a BD-RIS.","meta":{"url":"http://arxiv.org/abs/2307.10707v1"},"cats":{"new-dataset":0.0407876027,"dev-research":0.1587311397,"prompt-eng":0.3817801042,"data-quality":0.1949988999,"ml-security":0.117206093}}
{"text":"The Max-SNR problem admits a closed-form solution based on the Takagi factorization of a certain complex and symmetric matrix.","meta":{"url":"http://arxiv.org/abs/2307.10707v1"},"cats":{"new-dataset":0.1126971758,"dev-research":0.1686831663,"prompt-eng":0.3446164376,"data-quality":0.1262327485,"ml-security":0.1009434162}}
{"text":"This allows us to solve the max-SNR problem for SISO, SIMO, and MISO channels.","meta":{"url":"http://arxiv.org/abs/2307.10707v1"},"cats":{"new-dataset":0.0710491216,"dev-research":0.2445046854,"prompt-eng":0.3847687525,"data-quality":0.1586649598,"ml-security":0.0971425878}}
{"text":"Semantic segmentation is a common task in autonomous driving to understand the surrounding environment.","meta":{"url":"http://arxiv.org/abs/2307.10705v1"},"cats":{"new-dataset":0.0844882528,"dev-research":0.3194183249,"prompt-eng":0.4074897334,"data-quality":0.2326762301,"ml-security":0.1176414854}}
{"text":"Driveable Area Segmentation and Lane Detection are particularly important for safe and efficient navigation on the road.","meta":{"url":"http://arxiv.org/abs/2307.10705v1"},"cats":{"new-dataset":0.0481789642,"dev-research":0.2709446643,"prompt-eng":0.3591024468,"data-quality":0.1181635234,"ml-security":0.1052304626}}
{"text":"However, original semantic segmentation models are computationally expensive and require high-end hardware, which is not feasible for embedded systems in autonomous vehicles.","meta":{"url":"http://arxiv.org/abs/2307.10705v1"},"cats":{"new-dataset":0.0787323322,"dev-research":0.2415849486,"prompt-eng":0.3618351263,"data-quality":0.1540952307,"ml-security":0.1260583079}}
{"text":"This paper proposes a lightweight model for the driveable area and lane line segmentation.","meta":{"url":"http://arxiv.org/abs/2307.10705v1"},"cats":{"new-dataset":0.1343383188,"dev-research":0.2376000438,"prompt-eng":0.4056142534,"data-quality":0.1155588277,"ml-security":0.0723116273}}
{"text":"TwinLiteNet is designed cheaply but achieves accurate and efficient segmentation results.","meta":{"url":"http://arxiv.org/abs/2307.10705v1"},"cats":{"new-dataset":0.1248134789,"dev-research":0.2628809982,"prompt-eng":0.3923421478,"data-quality":0.189244533,"ml-security":0.0863196539}}
{"text":"We evaluate TwinLiteNet on the BDD100K dataset and compare it with modern models.","meta":{"url":"http://arxiv.org/abs/2307.10705v1"},"cats":{"new-dataset":0.456488118,"dev-research":0.2035079352,"prompt-eng":0.3879445564,"data-quality":0.2120218247,"ml-security":0.1548383288}}
{"text":"Experimental results show that our TwinLiteNet performs similarly to existing approaches, requiring significantly fewer computational resources.","meta":{"url":"http://arxiv.org/abs/2307.10705v1"},"cats":{"new-dataset":0.050820802,"dev-research":0.2925713468,"prompt-eng":0.3780238578,"data-quality":0.1631539366,"ml-security":0.1044045082}}
{"text":"Specifically, TwinLiteNet achieves a mIoU score of 91.3% for the Drivable Area task and 31.08% IoU for the Lane Detection task with only 0.4 million parameters and achieves 415 FPS on GPU RTX A5000.","meta":{"url":"http://arxiv.org/abs/2307.10705v1"},"cats":{"new-dataset":0.1046349156,"dev-research":0.2314866038,"prompt-eng":0.3951546636,"data-quality":0.1407671722,"ml-security":0.0887313561}}
{"text":"Furthermore, TwinLiteNet can run in real-time on embedded devices with limited computing power, especially since it achieves 60FPS on Jetson Xavier NX, making it an ideal solution for self-driving vehicles.","meta":{"url":"http://arxiv.org/abs/2307.10705v1"},"cats":{"new-dataset":0.0577305601,"dev-research":0.3487890867,"prompt-eng":0.3618991852,"data-quality":0.1016530523,"ml-security":0.1586828438}}
{"text":"Code is available: url{https://github.com/chequanghuy/TwinLiteNet}.","meta":{"url":"http://arxiv.org/abs/2307.10705v1"},"cats":{"new-dataset":0.2992656843,"dev-research":0.2750236395,"prompt-eng":0.4017007356,"data-quality":0.1712840909,"ml-security":0.0878408737}}
{"text":"The drastic growth of electric vehicles and photovoltaics can introduce new challenges, such as electrical current congestion and voltage limit violations due to peak load demands.","meta":{"url":"http://arxiv.org/abs/2307.10704v1"},"cats":{"new-dataset":0.0345344402,"dev-research":0.2981427253,"prompt-eng":0.3284061198,"data-quality":0.1638139359,"ml-security":0.2063179704}}
{"text":"These issues can be mitigated by controlling the operation of electric vehicles i.e., smart charging.","meta":{"url":"http://arxiv.org/abs/2307.10704v1"},"cats":{"new-dataset":0.0146370011,"dev-research":0.3428657645,"prompt-eng":0.4022781607,"data-quality":0.261547818,"ml-security":0.274094699}}
{"text":"Centralized smart charging solutions have already been proposed in the literature.","meta":{"url":"http://arxiv.org/abs/2307.10704v1"},"cats":{"new-dataset":0.0400915509,"dev-research":0.2329019723,"prompt-eng":0.3463925936,"data-quality":0.100229953,"ml-security":0.1401048015}}
{"text":"But such solutions may lack scalability and suffer from inherent drawbacks of centralization, such as a single point of failure, and data privacy concerns.","meta":{"url":"http://arxiv.org/abs/2307.10704v1"},"cats":{"new-dataset":0.0102439277,"dev-research":0.3088099619,"prompt-eng":0.3315945064,"data-quality":0.2011455625,"ml-security":0.3494690024}}
{"text":"Decentralization can help tackle these challenges.","meta":{"url":"http://arxiv.org/abs/2307.10704v1"},"cats":{"new-dataset":0.0345357053,"dev-research":0.3009044568,"prompt-eng":0.342767178,"data-quality":0.0919992617,"ml-security":0.1610171354}}
{"text":"In this paper, a fully decentralized smart charging system is proposed using the philosophy of adaptive multi-agent systems.","meta":{"url":"http://arxiv.org/abs/2307.10704v1"},"cats":{"new-dataset":0.0265163365,"dev-research":0.2018415562,"prompt-eng":0.368822654,"data-quality":0.0897228256,"ml-security":0.1658924675}}
{"text":"The proposed system utilizes multi-armed bandit learning to handle uncertainties in the system.","meta":{"url":"http://arxiv.org/abs/2307.10704v1"},"cats":{"new-dataset":0.0447958637,"dev-research":0.2496689091,"prompt-eng":0.4444222935,"data-quality":0.1998010207,"ml-security":0.2247379776}}
{"text":"The presented system is decentralized, scalable, real-time, model-free, and takes fairness among different players into account.","meta":{"url":"http://arxiv.org/abs/2307.10704v1"},"cats":{"new-dataset":0.0728498818,"dev-research":0.283014816,"prompt-eng":0.3781781845,"data-quality":0.0884006522,"ml-security":0.1784864744}}
{"text":"A detailed case study is also presented for performance evaluation.","meta":{"url":"http://arxiv.org/abs/2307.10704v1"},"cats":{"new-dataset":0.0208691048,"dev-research":0.3742726959,"prompt-eng":0.4115100215,"data-quality":0.1150647974,"ml-security":0.0701461527}}
{"text":"Granger causality (GC) is often considered not an actual form of causality.","meta":{"url":"http://arxiv.org/abs/2307.10703v1"},"cats":{"new-dataset":0.0035375857,"dev-research":0.2114396871,"prompt-eng":0.3109893602,"data-quality":0.1463779889,"ml-security":0.1213409701}}
{"text":"Still, it is arguably the most widely used method to assess the predictability of a time series from another one.","meta":{"url":"http://arxiv.org/abs/2307.10703v1"},"cats":{"new-dataset":0.0056289132,"dev-research":0.2981028676,"prompt-eng":0.3529467453,"data-quality":0.1416934147,"ml-security":0.1086309119}}
{"text":"Granger causality has been widely used in many applied disciplines, from neuroscience and econometrics to Earth sciences.","meta":{"url":"http://arxiv.org/abs/2307.10703v1"},"cats":{"new-dataset":0.0177088944,"dev-research":0.2298064549,"prompt-eng":0.3849216861,"data-quality":0.0961945359,"ml-security":0.1093226168}}
{"text":"We revisit GC under a graphical perspective of state-space models.","meta":{"url":"http://arxiv.org/abs/2307.10703v1"},"cats":{"new-dataset":0.029066737,"dev-research":0.1525043505,"prompt-eng":0.4071526376,"data-quality":0.129376982,"ml-security":0.1269406875}}
{"text":"For that, we use GraphEM, a recently presented expectation-maximisation algorithm for estimating the linear matrix operator in the state equation of a linear-Gaussian state-space model.","meta":{"url":"http://arxiv.org/abs/2307.10703v1"},"cats":{"new-dataset":0.0410389278,"dev-research":0.1528859821,"prompt-eng":0.3849756687,"data-quality":0.1102087884,"ml-security":0.1470751208}}
