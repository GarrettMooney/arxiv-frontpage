{"text":"Automated logging statement generation techniques facilitate developers in writing appropriate logging statements that document software behaviors.","cats":{"new-dataset":0,"dev-research":1}}
{"text":"Although existing large language models (LLMs) might be a good fit for the task due to their great success in natural language generation and programming language comprehension, their effectiveness and generalization capabilities have not been explored.","cats":{"new-dataset":0,"dev-research":0}}
{"text":"Specifically, we evaluate LLM's logging effectiveness by studying 1) their ability to decide logging ingredients, 2) the impact of the internal characteristics of LLMs, and 3) the influence of external factors.","cats":{"new-dataset":0,"dev-research":0}}
{"text":"We further evaluate LLM's logging generalization capabilities using unseen data derived from code transformation techniques.","cats":{"new-dataset":0,"dev-research":0}}
{"text":"Our study demonstrates that existing LLMs fall short of practical requirements for generating proper logging statement texts.","cats":{"new-dataset":0,"dev-research":0}}
{"text":"We also disclose the impact of internal characteristics and external factors for LLMs in automated logging.","cats":{"new-dataset":0,"dev-research":0}}
{"text":"In addition, we observe that existing LLMs cannot generalize to logging unseen code, revealing their unsatisfactory generalization capabilities.","cats":{"new-dataset":0,"dev-research":0}}
{"text":"Based on our findings, we further discuss three implications that can enhance logging statement generation in the future, such as developing a unified metric for logging quality, incorporating shareable code knowledge into LLMs, and devising suitable prompts.","cats":{"new-dataset":0,"dev-research":0}}
{"text":"SLPerf can facilitate SL algorithm development and fair performance comparisons.","cats":{"dev-research":0}}
{"text":"Our evaluation shows that RAPGen can generate performance improvement suggestions equivalent or better than a developer in ~60% of the cases, getting ~39% of them verbatim, in an expert-verified dataset of past performance changes made by C# developers.","cats":{"dev-research":1}}
{"text":"In addition, we develop a feature-strengthened modularized unit to further boost the reconstruction performance.","cats":{"dev-research":0}}
{"text":"So far, several studies have been performed to develop robust hate speech detection systems.","cats":{"dev-research":0}}
{"text":"The key challenges in developing GDBs are achieving high performance, scalability, programmability, and portability.","cats":{"dev-research":0}}
{"text":"Extensive simulation studies are conducted to assess the finite sample performance of our developed methods.","cats":{"dev-research":0}}
{"text":"Motivated by the performance of each RNNs, a meta-model is developed to improve the overall recognition performance by combining the predictions of the individual RNNs.","cats":{"dev-research":0,"ml-security":0}}
{"text":"However, so far, the focus of developing drift detectors is on detection quality, e.g.~accuracy, but not on computational performance, such as running time.","cats":{"dev-research":0}}
{"text":"We developed a deep learning pipeline to segment the cortical mantle by benchmarking the performance of nine deep neural architectures.","cats":{"dev-research":0}}
{"text":"Therefore, improving the development process indirectly improves the software product, too.","cats":{"dev-research":1}}
{"text":"Overall, this work opens new doors for creating systems and methods that can help programmers write efficient code.","cats":{"dev-research":1}}
{"text":"By aligning the reasoning of automated debugging more closely with that of human developers, we aim to produce intelligible explanations of how a specific patch has been generated, with the hope that the explanation will lead to more efficient and accurate developer decisions.","cats":{"dev-research":1}}
{"text":"This often makes it difficult to achieve good results in practice.","cats":{"dev-research":0}}
{"text":"We start with the intuition that developers tend to consciously and unconsciously have a collection of semantics facts in mind when working on coding tasks.","cats":{"dev-research":1}}
{"text":"Thus, developers need to debug their systems to ensure that the expected behavior is delivered.","cats":{"dev-research":1}}
{"text":"Developers often face challenges in code understanding, which is crucial for building and maintaining high-quality software systems.","cats":{"dev-research":1}}
{"text":"This paper adopts a cognitive psychology perspective to investigate the recurring mistakes in code resulting from the mental set (Einstellung) effect.","cats":{"dev-research":1}}
{"text":"During the experiment, participants were given two sets of four programming tasks.","cats":{"dev-research":0}}
{"text":"The study contributes to the existing literature by providing insights into creativity support during problem-solving in software development and offering a framework for experimental research in this field.","cats":{"dev-research":0}}
{"text":" The Einstellung effect is the tendency to approach problem-solving with a preconceived mindset, often overlooking better solutions that may be available.","cats":{"dev-research":0}}
{"text":"This effect can significantly impact creative thinking, as the development of patterns of thought can hinder the emergence of novel and creative ideas.","cats":{"dev-research":0}}
{"text":"Our study aims to test the Einstellung effect and the two mechanisms of its overcoming in the field of programming.","cats":{"dev-research":0}}
{"text":"The first intervention was the change of the color scheme of the code editor to the less habitual one.","cats":{"dev-research":0}}
{"text":"The second intervention was a combination of instruction to \"forget the previous solutions and tasks\" and the change in the color scheme.","cats":{"dev-research":0}}
{"text":"Each task had two possible solutions: one using suboptimal code dicd recommended methodology.","cats":{"dev-research":0}}
{"text":"Between the sets, participants either received no treatment or one of two interventions aimed at helping them overcome the mental set.","cats":{"dev-research":0}}
{"text":"The results of our experiment suggest that the tested techniques were insufficient to support overcoming the mental set, which we attribute to the specificity of the programming domain.","cats":{"dev-research":0}}
{"text":"To this end, this paper performs the first extensive study on applying LLMs for logging statement generation.","cats":{"dev-research":0}}
{"text":" Current retrieval-based and learning-based logging methods fail to provide accurate logging statements in complex software.","cats":{"dev-research":0}}
{"text":"We build LogBench, the first logging ight state-of-the-art LLMs, which include general-purpose and code-specific models ranging from 60M to 175B in size.","cats":{"dev-research":0}}
{"text":"Performance bugs are non-functional bugs that can even manifest in well-tested commercial products.","cats":{"dev-research":0}}
{"text":"Fixing these performance bugs is an important yet challenging problem.","cats":{"dev-research":0}}
{"text":"In this work, we address this challenge and present a new approach called Retrieval-Augmented Prompt Generation (RAPGen).","cats":{"dev-research":0}}
{"text":"Given a code snippet with a performance issue, RAPGen first retrieves a prompt instruction from a pre-constructed knowledge-base of previous performance bug fixes and then generates a prompt using the retrieved instruction.","cats":{"dev-research":0}}
{"text":"It then uses this prompt on a Large Language Model (such as Codex) in zero-shot to generate a fix.","cats":{"dev-research":0}}
{"text":"We compare our approach with the various prompt variations and state of the art methods in the task of performance bug fixing.","cats":{"dev-research":0}}
{"text":"Our experimental results show that CodeGen and Codex are sensitive to the superficial modifications of problem descriptions and significantly impact code generation performance.","cats":{"dev-research":1}}
{"text":"Using large language models (LLMs) for source code has recently gained attention.","cats":{"dev-research":0}}
{"text":"LLMs, such as Transformer-based models like Codex and ChatGPT, have been shown to be highly capable of solving a wide range of programming problems.","cats":{"dev-research":0}}
{"text":"However, the extent to which LLMs understand problem descriptions and generate programs accordingly or just retrieve source code from the most relevant problem in training data based on superficial cues has not been discovered yet.","cats":{"dev-research":0}}
{"text":"To explore this research question, we conduct experiments to understand the robustness of several popular LLMs, CodeGen and GPT-3.5 series models, capable of tackling code generation tasks in introductory programming problems.  ","cats":{"dev-research":0}}
{"text":"Furthermore, we observe that Codex relies on variable names, as randomized variables decrease the solved rate significantly.","cats":{"dev-research":0}}
{"text":"However, the state-of-the-art (SOTA) models, such as InstructGPT and ChatGPT, show higher robustness to superficial modifications and have an outstanding capability for solving programming problems.","cats":{"dev-research":0}}
{"text":"This highlights the fact that slight modifications to the prompts given to the LLMs can greatly affect code generation performance, and careful formatting of prompts is essential for high-quality code generation, while the SOTA models are becoming more robust to perturbations.","cats":{"dev-research":0}}
{"text":"However, it is hard and expensive to debug DNNs","cats":{"dev-research":1}}
{"text":"To address the challenges of debugging DNN models, we propose a novel data-driven approach that leverages model features to learn problem patterns","cats":{"dev-research":1}}
{"text":"Also, our methodology automatically links bug symptoms to their root causes, without the need for manually crafted mappings, so that developers can take the necessary steps to fix faults","cats":{"dev-research":1}}
{"text":"Deep Learning (DL) applications are being used to solve problems in critical domains (e.g., autonomous driving or medical diagnosis systems).  ","cats":{"dev-research":0}}
{"text":"However, it is hard and expensive to debug DNNs.","cats":{"dev-research":0}}
{"text":"When the failure symptoms or unsatisfied accurtraceability as to which part of the DNN program is responsible for the failure.","cats":{"dev-research":0}}
{"text":"Even worse, sometimes, a deep learning program has different types of bugs.","cats":{"dev-research":0}}
{"text":"To address the challenges of debugging DNN models, we propose a novel data-driven approach that leverages model features to learn problem pattas a training dataset to learn and infer DNN fault patterns.","cats":{"dev-research":0}}
{"text":"Also, our methodology automatically links bug symptoms to their root causes, without the need for manually crafted mappings, so that developers can take the necessary steps to fix faults.","cats":{"dev-research":0}}
{"text":"We evaluate our approach using real-world and mutated models.","cats":{"dev-research":0}}
{"text":"Our results demonstrate that our technh achieved comparable results for real-world models in terms of accuracy and performance to the state-of-the-art.","cats":{"dev-research":0}}
{"text":"We conduct an exploratory user study with 32 participants to understand the usefulness and effectiveness, as well as individual preferences in the usage of, this LLM-powered information support tool","cats":{"dev-research":1}}
{"text":" Code comments and documentation can provide some context for the code, but are often scarce or missing.","cats":{"dev-research":0}}
{"text":"This challenge has become even more pressing with the rise of large language model (LLM) based code generation tools.","cats":{"dev-research":0}}
{"text":"To understand unfamiliar code, most software developers rely on general-purpose search engines to search through various programming information resources, which often requires multiple iterations of query rewriting and information foraging.","cats":{"dev-research":0}}
{"text":"More recently, developers have turned to online chatbots powered by LLMs, such as ChatGPT, which can provide more customized responses but also incur more overhead as developers need to communicate a significant amount of context to the LLM via a textual interface.","cats":{"dev-research":0}}
{"text":"In this study, we provide the investigation of an LLM-based conversational UI in the IDE.","cats":{"dev-research":0}}
{"text":"We aim to understand the promises and obstacles for tools powered by LLMs that are contextually aware, in that they automatically leverage the developer's programming context to answer queries.","cats":{"dev-research":0}}
{"text":"To this end, we develop an IDE Plugin that allows users to query back-ends such as OpenAI's GPT-3.5 and GPT-4 with high-level requests, like: explaining a highlighted section of code, explaining key domain-specific terms, or providing usage examples for an API.","cats":{"dev-research":0}}
{"text":"We conduct an exploratory user study with 32 participants to understand the usefulness and effectiveness, as well as individual prefiffered by participants' experience levels.","cats":{"dev-research":0}}
{"text":"Compiler error messages serve as an initial resource for programmers dealing with compilation errors","cats":{"dev-research":1}}
{"text":"Consequently, programmers typically rely on their own research to fix errors.","cats":{"dev-research":0}}
{"text":"This study systematically examines 100 compiler error messages from three sources to determine the most effective approach for programmers encountering compiler errors","cats":{"dev-research":1}}
{"text":"However, previous studies indicate that they often lack sufficient targeted information to resolve code issues.","cats":{"dev-research":0}}
{"text":"Historically, Stack Ovs in large language models offer alternatives.","cats":{"dev-research":0}}
{"text":"This study systematically examines 100 compiler error messages from three sources to determine the most effective approach for programmers encountering compiler errors.","cats":{"dev-research":0}}
{"text":"Factors tperforms Stack Overflow in explaining compiler error messages, the effectiveness of adding code snippets to Stack Overflow searches depends on the search method, and results for Stack Overflow differ significantly between Google and StackExchange API searches.","cats":{"dev-research":0}}
{"text":"Furthermore, GPT-4 surpasses GPT-3.5, with \"How to fix\" prompts yielding superior outcomes to \"What does this error mean\" prompts.","cats":{"dev-research":0}}
{"text":"These results offer valuable guidance for programmers seeking assistance with compiler error messages, underscoring the transformative potential of advanced large language models like GPT-4 in debugging and opening new avenues of exploration for researchers in AI-assisted programming.","cats":{"dev-research":0}}
{"text":"This research paper delves into the efficacy of ChatGPT in solving programming problems, examining both the correctness and the efficiency of its solution in terms of time and memory complexity","cats":{"dev-research":1}}
{"text":"However, it struggles to improve solutions based on feedback, pointing to potential shortcomings in debugging tasks.","cats":{"dev-research":0}}
{"text":"Large-scale language models (LLMs) have emerged as a groundbreaking innovation in the realm of question-answering and conversational agents.","cats":{"dev-research":0}}
{"text":"These models, leveraging different deep learning architectures such as Transformers, are trained on vast corpora to predict sentences based on given queries.","cats":{"dev-research":0}}
{"text":"Among these LLMs, ChatGPT, developed by OpenAI, has ushered in a new era by utilizing artificial intelligence (AI) to tackle diverse problem domains, ranging from composing essays and biographies to solving intricate mathematical integrals.","cats":{"dev-research":0}}
{"text":"The versatile applications enabled by ChatGPT offer immense value to users.","cats":{"dev-research":0}}
{"text":"However, assessing the performance of ChatGPT's output poses a challenge, particularly in scenarios where queries lack clear objective criteria for correctness.","cats":{"dev-research":0}}
{"text":"For instance, evaluating the quality of generated essays becomes arduous and relies heavily on manual labor, in stark contrast to evaluating solutions to well-defined, closed-ended questions such as mathematical problems. .","cats":{"dev-research":0}}
{"text":"The research reveals a commendable overall success rate of 71.875\\%, denoting the proportion of problems for which ChatGPT was able to provide correct solutions that successfully satisfied all the test cases present in Leetcode.","cats":{"dev-research":0}}
{"text":"It exhibits strengths in structured problems and shows a linear correlation between its success rate and problem acceptance rates.","cats":{"dev-research":0}}
{"text":"These findings provide a compact yet insightful glimpse into ChatGPT's capab","cats":{"dev-research":0}}
{"text":"Although traditional methods such as automated software testing, fault localization, and repair have been intensively studied, static analysis tools are most commonly used and have an inherent false positives rate, posing a solid challenge to developer productivity.","cats":{"dev-research":1}}
{"text":"SecureFalcon achieved an impressive 94% accuracy rate in detecting software vulnerabilities, emphasizing its significant potential to redefine software vulnerability detection methods in cybersecurity.","cats":{"dev-research":0}}
{"text":"Software vulnerabilities leading to various detriments such as crashes, data loss, and security breaches, significantly hinder the quality, affecting the market adoption of software applications and systems.  ","cats":{"dev-research":0}}
{"text":"Large Language Models (LLMs) offer a promising solution to these persistent issues.","cats":{"dev-research":0}}
{"text":"Among these, FalconLLM has shown substantial potential in identifying intricate patterns and complex vulnerabilities, hence crucial in software vulnerability detection.","cats":{"dev-research":0}}
{"text":"In this paper, for the first time, FalconLLM is being fine-tuned for cybersecurity applications, thus introducing SecureFalcon, an innovative model architecture built upon FalconLLM.","cats":{"dev-research":0}}
{"text":"SecureFalcon is trained to differentiate between vulnerable and non-vulnerable C code samples.","cats":{"dev-research":0}}
{"text":"We build a new training dataset, FormAI, constructed thanks to Generative Artificial Intelligence (AI) and formal verification to evaluate its performance.","cats":{"dev-research":0}}
{"text":"This study has enabled us to assert that DepEx can provide researchers and practitioners with a better understanding of the implicit software dependencies in order to improve the stability, performance, and functionality of their software as well as to reduce the risk of issues arising during maintenance, updating, or migration.","cats":{"dev-research":1}}
{"text":"An Operating System (OS) combines multiple interdependent software packages, which usually have their own independently developed architectures.","cats":{"dev-research":0}}
{"text":"When a multitude of independent packages are placed together in an OS, an implicit inter-package architecture is formed.","cats":{"dev-research":0}}
{"text":"For an evolutionary effort, designers/developers of OS can greatly benefit from fully understanding the system-wide dependency focused on individual files, specifically executable files, and dynamically loadable libraries.","cats":{"dev-research":0}}
{"text":"We propose a framework, DepEx, aimed at discovering the detailed package relations at the level of individual binary files and their associated evolutionary changes.","cats":{"dev-research":0}}
{"text":"We demonstrate the utility of DepEx by systematically investigating the evolution of a large-scale Open Source OS, Ubuntu.","cats":{"dev-research":0}}
{"text":"DepEx enabled us to systematically acquire and analyze the dependencies in different versions of Ubuntu released between 2005 (5.04) to 2023 (23.04).","cats":{"dev-research":0}}
{"text":"Our analysis revealed various evolutionary trends in package management and their implications based on the analysis of the 84 consecutive versions available for download (these include beta versions).","cats":{"dev-research":0}}
